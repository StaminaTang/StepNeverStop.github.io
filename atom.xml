<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Keavnn&#39;Blog</title>
  
  <subtitle>If it is to be, it is up to me.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://StepNeverStop.github.io/"/>
  <updated>2020-04-12T10:54:30.671Z</updated>
  <id>http://StepNeverStop.github.io/</id>
  
  <author>
    <name>Keavnn</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>蓝猫淘气三千问</title>
    <link href="http://StepNeverStop.github.io/rl3000questions.html"/>
    <id>http://StepNeverStop.github.io/rl3000questions.html</id>
    <published>2020-04-12T10:31:27.000Z</published>
    <updated>2020-04-12T10:54:30.671Z</updated>
    
    <content type="html"><![CDATA[<p>此篇博文用于记录学习RL或者实现RL方法过程中遇到疑难杂症及相应解决思路。</p><a id="more"></a><h1 id="待解决"><a href="#待解决" class="headerlink" title="待解决"></a>待解决</h1><ol><li>如果场景中存在多个图像输入，而且维度不一致，该如何处理呢？假设有汽车有前后两个摄像头，分辨率分别是$Vis_1=10\times10\times3$和$Vis_2=15\times5\times3$，那么该如何设计图像处理过程呢？是表示成$(CNN_1(Vis_1), CNN_2(Vis_2))$呢？还是表示成$CNN(Concat(Vis_1, Resize(Vis_2)))$呢？</li><li>如果智能体的动作既包括离散的也包括连续的，该如何处理？</li></ol><h1 id="已解决"><a href="#已解决" class="headerlink" title="已解决"></a>已解决</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此篇博文用于记录学习RL或者实现RL方法过程中遇到疑难杂症及相应解决思路。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>PaStaNet: Toward Human Activity Knowledge Engine</title>
    <link href="http://StepNeverStop.github.io/PaStaNet.html"/>
    <id>http://StepNeverStop.github.io/PaStaNet.html</id>
    <published>2020-04-11T17:23:03.000Z</published>
    <updated>2020-04-12T10:40:44.339Z</updated>
    
    <content type="html"><![CDATA[<p>这篇论文是上海交大<a href="http://mvig.sjtu.edu.cn/" rel="external nofollow" target="_blank">卢策吾</a>老师团队下<a href="https://dirtyharrylyl.github.io/" rel="external nofollow" target="_blank">李永露</a>博士在2020CVPR会议三连中中的其中一篇。方向为HOIs方向，即人物交互。</p><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/2004.00945" rel="external nofollow" target="_blank">http://arxiv.org/abs/2004.00945</a></p><p>作者开源的代码和数据集：<a href="http://hake-mvig.cn/" rel="external nofollow" target="_blank">http://hake-mvig.cn/</a></p><p>PaSta是Part State的缩写，它是<strong>细粒度动作语义标记</strong>(ﬁne-grained action semantic tokens)，是人类活动/行为的更精细的表达，比如一个人类的行为是开汽车，那么这个行为的part state就包括手握方向盘、脚踩油门等等，这种part state用三元组形式表示，比如：&#60;hand, hold, something&#62;。</p><p>这篇论文主要有两个比较大的贡献：</p><ol><li>建立了一个大型知识库PaStaNet（其实是HAKE数据集），目前标注了700w+局部状态。</li><li>设计了一个分层的动作识别模型（为什么分层呢？因为作者提到现有的基于图像的动作识别理解方法主要采取直接映射/端到端的方式，可能会遇到性能瓶颈。）<ol><li>第一层是Activity2Vec模型，用来从原始图片中提取PaSta特征，PaSta是组成多种人类行为的通用表示，比如一个PaSta是hold，那么开汽车时有hold方向盘，吃苹果时有hold苹果，两种不同的行为共享同样的PaSta；</li><li>第二层使用了PaSta-based Reasoning（PaSta-R，基于局部状态的推理）方法，用这种方法从第一层中识别的PaSta来推测图片中的人类行为活动。</li></ol></li></ol><p>下文中以下概念术语等同：</p><ul><li>PaStaNet——数据集</li><li>人类行为理解——动作识别</li><li>PaSta——局部状态</li></ul><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><p>在大规模的基准中，基于<strong>实例层次的语义（instance-level semantics）</strong>使用one-stage从像素理解人类行为存在性能瓶颈，主要有以下几个原因：</p><ol><li>long-tail data distribution，长尾数据分布</li><li>complex visual patterns，复杂的视觉模式</li></ol><p>作者认为(argue that)在人类局部的语义层次上进行感知是一个非常有前景的方向，这种方式之前被忽略了。</p><p><strong>作者的核心思想是：人类动作由细粒度的原子主体部分状态（PaSta）组成。</strong></p><blockquote><p>Our core idea is that human instance actions are composed of ﬁne-grained atomic body part states.</p></blockquote><p><img src="./PaStaNet/fig1.png" alt=""></p><p>先识别PaSta再推理行为有什么好处呢？</p><ol><li><p>与简化理论(reductionism)有强烈的直接关系</p><blockquote><p>This lies in strong relationships with reductionism.</p></blockquote></li><li><p>可以帮助我们选择有区别的部分，忽略不相关的部分</p><blockquote><p>the part-level path can help us to pick up discriminative parts and disregard irrelevant ones.</p></blockquote></li><li><p>从人体局部编码知识是实现人类活动知识引擎的关键步骤</p><blockquote><p>encoding knowledge from human parts is a crucial step toward human activity knowledge engine.</p></blockquote></li><li><p>Reusability and Transferability——可重用性和可转移性，多个行为的局部状态存在共享，比如一个PaSta是hold，那么开汽车时有hold方向盘，吃苹果时有hold苹果，两种不同的行为共享同样的PaSta。因此，我们可以用更少的PaSta来描述和区分大量的行为。对于few-shot学习，可重用性可以极大地缓解其学习困难。</p><blockquote><p>PaSta are basic components of actions, their relationship can be in analogy with the amino acid and protein, letter and word, etc. Hence, PaSta are reusable, e.g., 〈 hand, hold, something 〉 is shared by various actions like “hold horse” and “eat apple”. Therefore, we get the capacity to describe and differentiate plenty of activities with a much smaller set of PaSta, i.e. one-time labeling and transferability. For fewshot learning, reusability can greatly alleviate its learning difﬁculty. Thus our approach shows signiﬁcant improvements, e.g. we boost 13.9 mAP on one-shot sets of HICO</p></blockquote></li><li><p>Interpretability——可解释性，当模型预测一个人在做什么时，我们很容易知道原因:它的身体的各个部分在做什么。</p><blockquote><p>we obtain not only more powerful activity representations, but also better interpretation. When the model predicts what a person is doing, we can easily know the reasons: what the body parts are doing.</p></blockquote></li></ol><h2 id="PaStaNet数据集"><a href="#PaStaNet数据集" class="headerlink" title="PaStaNet数据集"></a>PaStaNet数据集</h2><p>该数据集目前已经标注了11.8w张图片，包括28.5w个人物，25w个交互的实体对象（比如球之类的），72.4万个行为，以及700w个人类局部状态。</p><p>该数据集目前有156个行为分类，76个PaSta分类。</p><p>广泛的分析证明，一般来说，PaStaNet可以覆盖大部分的局部级知识，可以很好的概括大部分情况。</p><p><img src="./PaStaNet/fig6.png" alt=""></p><p>下图为数据集中的行为和交互物体类别。</p><p><img src="./PaStaNet/table6.png" alt=""></p><p>下图为数据集中的局部状态PaSta类别。</p><p><img src="./PaStaNet/table7.png" alt=""></p><h3 id="PaSta的定义"><a href="#PaSta的定义" class="headerlink" title="PaSta的定义"></a>PaSta的定义</h3><p>将人体解耦成十个部分：head, two upper arms, two hands, hip, two thighs, two feet，即</p><ol><li>头</li><li>左臂</li><li>右臂</li><li>左手</li><li>右手</li><li>臀部</li><li>左腿</li><li>右腿</li><li>左脚</li><li>右脚</li></ol><p>每一个PaSta表示目标局部部分的表示，比如hand可以是hold something, push something；head可以是watch something, eat something。注意，当一个人同时有多个行为动作，他的某个局部身体部位可以有多个PaSta。</p><h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><p>两种方式：</p><ol><li>通过众包收集以人为中心的行为图像(3万张图片，具有粗糙的活动标签)；</li><li>现有的设计良好的数据集(18.5万张)。</li></ol><p>其中的数据围绕丰富的语义本体论(semantic ontology)、多样性和行为的可变性构建。最终，收集了超过20万张的不同行为类别的图片。</p><h3 id="行为标签"><a href="#行为标签" class="headerlink" title="行为标签"></a>行为标签</h3><p>根据人类最常见的日常活动，与人和物的互动。从11.8万张图片中选择了156种行为，包括人与物体的互动和身体动作（包含bounding boxes）。</p><h3 id="身体局部的盒子"><a href="#身体局部的盒子" class="headerlink" title="身体局部的盒子"></a>身体局部的盒子</h3><blockquote><p>Estimation errors are addressed manually to ensure high-quality annotation. Each part box is centered with a joint, and the box size is pre-deﬁned by scaling the distance between the joints of the neck and pelvis. A joint with conﬁdence higher than 0.7 will be seen as visible. When not all joints can be detected, we use body knowledge-based rules. That is, if the neck or pelvis is invisible, we conﬁgure the part boxes according to other visible joint groups (head, main body, arms, legs), e.g., if only the upper body is visible, we set the size of the hand box to twice the pupil distance.</p></blockquote><h3 id="局部状态PaSta标注"><a href="#局部状态PaSta标注" class="headerlink" title="局部状态PaSta标注"></a>局部状态PaSta标注</h3><p>通过众包方式进行标注，共收到224159条标注上传。</p><p>过程如下：</p><ol><li>基于156种行为的动词，从WordNet选取200个PaSta动词。如果某个局部部位没有可以的状态，则描述为”no_action”；</li><li>为了找到最通用的PaSta（可以作为可转移的行为知识），邀请了来自不同背景的150名注释者来标注156个行为的1w张图片；</li><li>基于它们的注释，使用规范化的<strong>点对点互信息(Normalized Point-wise Mutual Information，NPMI，<em>Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexicography. In Computational linguistics, 1990.</em>)</strong>来计算行为和PaSta之间的共生/共现关系，最后选择76个具有最高NPMI值的候选局部状态为PaSta集合；</li><li>以之前的1w张打了标签的图片为种子，自动生成其余图片的初始PaSta标签，然后另外210名注释者仅需要去检查这些标注即可；</li><li>考虑到一个人可能有多个动作，对于每个动作，分别标注其对应的10个PaSta。然后把所有动作的PaSta组合在一起；</li><li>为了确保质量，每幅图像都将被标注两次，并由自动程序和主管进行检查。</li></ol><p><img src="./PaStaNet/fig2.png" alt=""></p><p><strong>疑问：为什么ride bicycle与head look at的共现如此之低呢？</strong></p><h3 id="行为解析树"><a href="#行为解析树" class="headerlink" title="行为解析树"></a>行为解析树</h3><p>为了说明PaSta和行为之间的关系，作者使用它们的统计相关性来构建一个图：行为是根节点，PaSta是子节点，边是共现。</p><blockquote><p>To illustrate the relationships between PaSta and activities, we use their statistical correlations to construct a graph (Fig. 2): activities are root nodes, PaSta are son nodes and edges are co-occurrence.</p></blockquote><p>PaStaNet可以为实例级和局部级提供丰富的行为知识，并帮助构建大型行为解析树。</p><blockquote><p>PaStaNet can provide abundant activity knowledge for both instance and part levels and help construct a large-scale activity parsing tree</p></blockquote><p><img src="./PaStaNet/fig8.png" alt=""></p><p>作者将解析树表示为行为和PaSta的共现矩阵(看起来极其稀疏)。</p><p><img src="./PaStaNet/fig9.png" alt=""></p><h2 id="分层行为理解模型"><a href="#分层行为理解模型" class="headerlink" title="分层行为理解模型"></a>分层行为理解模型</h2><p>这一部分数学符号很多，而且似乎故意把符号设计的复杂，导致阅读理解起来有些不顺畅。</p><p>对于行为的识别，有两种模型：</p><ol><li><p>传统模式，采用直接映射。</p><script type="math/tex; mode=display">\mathcal{S}_{i n s t}=\mathcal{F}_{i n s t}\left(I, b_{h}, \mathcal{B}_{o}\right)</script><p>其中，$I$表示图像输入，$b_h$是人的box，$\mathcal{B}_{o}=\left\{b_{o}^{i}\right\}_{i=1}^{m}$是与人交互的物体的box，假设有$m$个物体。$\mathcal{S}_{i n s t}$代表实体级别的动作评分（评估结果）。</p></li><li><p>作者提出的PaStaNet模式，利用通用的局部知识，分成两步：</p><ol><li><p>PaSta局部状态识别和特征提取（其实是识别层之前的隐特征）</p><script type="math/tex; mode=display">f_{P a S t a}=\mathcal{R}_{A 2 V}\left(I, \mathcal{B}_{p}, b_{o}\right) = \left\{f_{P a S t a}^{(i)}\right\}_{i=1}^{10}</script><p>其中，$\mathcal{B}_{p}=\left\{b_{p}^{(i)}\right\}_{i}^{10}$是人的局部部位的box，使用<em>Pairwise body-part attention for recognizing human-object interactions. In ECCV, 2018</em>自动生成。$\mathcal{R}_{A 2 V}(\cdot)$表示Activity2Vec模型，用于提取PaSta的特征表示，</p></li><li><p>PaSta-Based推理（PaSta-R），从局部状态推理行为语义</p><script type="math/tex; mode=display">\mathcal{S}_{p a r t}=\mathcal{F}_{P a S t a-R}\left(f_{P a S t a}, f_{o}\right)</script><p>其中，$\mathcal{F}_{P a S t a-R}(\cdot)$代表PaSta-R方法，$f_{o}$是物体的特征表示，$\mathcal{S}_{p a r t}$是局部状态层面的动作评分。<em>注意，如果场景中人没有与物进行交互，比如”跳舞“这个动作，那么使用图像的ROI池化特征来表示$f_o$。如果场景中存在多个交互物体，则依次处理human-object pair$\left(f_{P a S t a}, f_{o}^{(i)}\right)$，并且声称各自独立的Activity2Vec Embedding</em>。</p></li></ol></li></ol><p><img src="./PaStaNet/fig3.png" alt=""></p><p>上图为PaSta的识别与特征表示部分的框架图。</p><p>识别部分主要为红色线条部分，特征表示部分主要为蓝色线条部分。</p><h3 id="局部状态PaSta识别"><a href="#局部状态PaSta识别" class="headerlink" title="局部状态PaSta识别"></a>局部状态PaSta识别</h3><p>这部分的输入为$I, \mathcal{B}_{p}, b_{o}$，输出为局部状态的视觉特征$f_{PaSta}^{V}$和识别结果$P_{PaSta}$。</p><p>对于输入，$\mathcal{B}_{p}, b_{o}$都是使用在COCO数据集上预训练的Faster R-CNN做特征提取：</p><ul><li>对于物体$b_o$，$b_o\rightarrow Faster R-CNN \rightarrow f_o$，如果图片内不存在与物体进行交互，则使用图像的特征，即$I\rightarrow Faster R-CNN \rightarrow f_c \rightarrow f_o$</li><li>对于人的身体的每一个部位（共10个）$b_{p}^{(i)}$，$b_{p}^{(i)} \rightarrow Faster R-CNN \rightarrow f_{p}^{(i)}$</li></ul><p>得到特征表示后，首先输入到一个被称为Part Relevance Predictor的结构中，去计算每一个部位的attention，这个PRP结构由全连接组成，最后激活为softmax函数，给每一个局部部位特征输出一个注意力权重：</p><script type="math/tex; mode=display">a_{i}=\mathcal{P}_{p a}\left(f_{p}^{(i)}, f_{o}\right)</script><p>其中$\mathcal{P}_{p a}(\cdot)$即是局部注意力预测器。<strong>在这里，我感觉这个注意力权重应该指的是某个身体部位与物体的相关性，比如，手跟茶杯很相关，而脚和苹果则不太相关。</strong>然后，将注意力权重与原始局部特征表示进行加权：</p><script type="math/tex; mode=display">f_{p}^{(i) \star}=f_{p}^{(i)} \times a_{i}</script><p>接下来进行局部状态PaSta的分类/识别，此时将$f_{p}^{(i) \star}$与$f_o$进行concat操作之后，传入max池化层，以及两层512的全连接，最终获得PaSta的分类结果$\mathcal{S}_{P a S t a}^{(i)}$。<strong>这里的$\mathcal{S}$应该是logits，而$P_{PaSta}$表示概率。</strong></p><p><em>注意，这里存在一个身体部位有多种状态的可能，比如头部可以同时进行”吃”和”看”的动作，因此是一个多标签分类任务。</em></p><p>识别部分的交叉熵损失函数如下：</p><script type="math/tex; mode=display">\mathcal{L}_{P a S t a}=\sum_{i}^{10}\left(\mathcal{L}_{P a S t a}^{(i)}+\mathcal{L}_{a t t}^{(i)}\right)</script><h3 id="Activity2Vec"><a href="#Activity2Vec" class="headerlink" title="Activity2Vec"></a>Activity2Vec</h3><p>这一部分的输入为局部状态的视觉特征$f_{PaSta}^{V}$、识别结果$P_{PaSta}$和PaSta的语言特征$f_{B e r t}^{(i, k)}$，输出为PaSta的最终特征表示$f_{PaSta}$。</p><blockquote><p>With PaStaNet, we convert a human instance into a vector consisting of PaSta representations. Activity2Vec extracts part-level semantic representation via PaSta recognition and combines its language representation. Since PaSta encodes common knowledge of activities, Activity2Vec works as a general feature extractor for both seen and unseen activities.</p></blockquote><p>Activity2Vec将一个人类实例转换为一个由PaSta表示组成的向量。通过局部状态识别提取局部层次的语义表示，并且与该局部状态的语言表示相结合。</p><p>在这一环节的主要任务是将局部状态PaSta的语义知识嵌入到它的特征向量表示中去，那么，如何结合呢？</p><p>对于图像特征，在上一部分已经获得，提取PaSta的分类结果前一层的隐状态即可，$\color{red}{f_{\text {PaSta}}^{V(i)} \in \mathbb{R}^{512}}$。</p><p>对于语言特征，作者使用<strong>BERT-Base预训练模型</strong>先将数据集中的token预转换为$\color{red}{f_{B e r t}^{(i, k)} \in \mathbb{R}^{2304}}$，并且在整个过程中保持不变。token指的是三元组&#60;part, verb, object&#62;，object来自目标检测。所有的token即$\left\{t_{p}^{(i, k)}, t_{v}^{(i, k)}, t_{o}^{(i, k)}\right\}_{k=1}^{n}$，$i$代表身体部位的数量，这里为10，$n$代表每一个部位具有的PaSta数量，其中的每个$t$都是768的向量长度。</p><script type="math/tex; mode=display">f_{B e r t}^{(i, k)}=\mathcal{R}_{B e r t}\left(t_{p}^{(i, k)}, t_{v}^{(i, k)}, t_{o}^{(i, k)}\right)</script><script type="math/tex; mode=display">f_{B e r t}^{(i)} \in \mathbb{R}^{2304 * n}</script><p>将部分的BERT表示与该部分的分类结果相乘，即PaSta的语言特征表示：</p><script type="math/tex; mode=display">f_{P a S t a}^{L(i)}=f_{B e r t}^{(i)} \times P_{P a S t a}^{(i)}, \text { where } P_{P a S t a}^{(i)}=\operatorname{Sigmoid}\left(\mathcal{S}_{P a S t a}^{(i)}\right) \in \mathbb{R}^{n}</script><script type="math/tex; mode=display">P_{P a S t a}=\left\{P_{P a S t a}^{(i)}\right\}_{i=1}^{10}</script><script type="math/tex; mode=display">f_{P a S t a}^{L(i)} \in \mathbb{R}^{2304 * n}</script><p><strong>最后</strong>，池化、resize语言特征$f_{P a S t a}^{L(i)}$后再与图像特征$f_{PaSta}^{V}$concat即获得最终的PaSta特征表示$f_{P a S t a}^{(i)} \in \mathbb{R}^{m}$。输出的$f_{\text {PaSta}}=\left\{f_{\text {PaSta}}^{(i)}\right\}_{i=1}^{10}$是局部级别的行为特征表示，可用于下游任务，像行为检测，标题生成等等。</p><h3 id="PaSta-R"><a href="#PaSta-R" class="headerlink" title="PaSta-R"></a>PaSta-R</h3><p>这一部分主要是从局部状态的特征表示推断出图片中人的行为。其输入为特征表示$f_{PaSta}$，输出为动作评分$\mathcal{S}$。</p><blockquote><p>A Part State Based Reasoning method (PaSta-R) is further presented. We construct a Hierarchical Activity Graph consisting of human instance and part semantic representations, and infer the activities by combining both instance and part level sub-graph states.</p></blockquote><p>作者构造了一个由人类实例和局部语义表示组成的层次行为图（Hierarchical Activity Graph），并结合实例和局部层次子图状态来推断行为。</p><p><img src="./PaStaNet/fig4.png" alt=""></p><p>HAG如上图中间所示，节点为局部的状态特征或者物体的特征，边分两种，第一种是body part与object的边，表示为$e_{p o}=\left(v_{p}^{i}, v_{o}\right) \in \mathcal{V}_{p} \times \mathcal{V}_{o}$，第二种是body part与body part的边，表示为$e_{p p}^{i j}=\left(v_{p}^{i}, v_{p}^{j}\right) \in \mathcal{V}_{p} \times \mathcal{V}_{p}$。<strong><em>说实话，边究竟是如何表示的，完全没有看懂-.-</em></strong>。</p><p>作者的目标是解析HAG，然后推理出图像中的行为。即</p><script type="math/tex; mode=display">\mathcal{S}_{p a r t}=\mathcal{F}_{P a S t a-R}\left(f_{P a S t a}, f_{o}\right)</script><p>作者提出了三种结构和三种方式，如下图所示：</p><p><img src="./PaStaNet/fig10.png" alt=""></p><p>三种结构：</p><ol><li>Linear Combination，说白了就是一层全连接，激活为softmax；</li><li>MLP，说白了就是两层1024单元全连接（使用非线性激活函数），最后一层激活为softmax；</li><li>Graph Convolution Network，GCN提取全局图特征，然后接MLP输出分类结果。</li></ol><p>三种方式：</p><ol><li>上图(a)所示，将$f_{P a S t a}, f_{o}$直接concat然后输入后续网络；</li><li>上图(b)所示，按身体部位逐步输入到LSTM网络，改造成序列模型。有两种输入方式，1乱序，2固定顺序（比如从头到脚），作者说固定顺序更好；</li><li>上图(c)所示，将部位特征分层组合，例如：<ol><li>在第一层将左手左上臂特征合并为左臂，左脚左大腿特征合并为左腿，……，然后传入全连接进一步提取特征；</li><li>将头、胳膊等合并为上肢，臀、腿等合并为下肢，……，然后传入全连接进行进一步特征提取；</li><li>上下肢合并为整体，然后传入全连接，再接后续网络。</li></ol></li></ol><p>如何得到最后的分类结果呢？作者提出两种方式：</p><ol><li>early fusion——前融合，将实例层次的语义特征表示$f_{inst}$与PaSta特征表示、物体特征表示结合后再做PaSta-R推理；</li><li>late fusion——后融合，融合实例层次的分类结果和局部层次的分类结果，即$\mathcal{S}=\mathcal{S}_{i n s t}+\mathcal{S}_{p a r t}$。作者说，实验下来，这种方式效果更好。</li></ol><p>最终，整个框架的交叉熵损失函数为：</p><script type="math/tex; mode=display">\mathcal{L}_{\text {total}}=\mathcal{L}_{\text {PaSta}}+\mathcal{L}_{\text {cls}}^{\text {PaSta}}+\mathcal{L}_{\text {cls}}^{\text {inst}}</script><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>监督学习中，PaStaNet在HICO完整数据集上提升了6.4(16%)mAP，在HICO one-shot数据集上提升了13.9mAP。</p><p>迁移学习中，PaStanet在V-COCO数据集上提升了3.2mAP，在基于图像的AVA数据集上提升了4.2mAP，在HICO-DET数据集上提升了3.2mAP。</p><h2 id="类比实验"><a href="#类比实验" class="headerlink" title="类比实验"></a>类比实验</h2><p>从MNIST数据集中采集0-9数字（28X28X1），组成（128X128X1）的图片，每个图片包含3-5个数字。将数字类比为身体的局部部位，将行为设置为图片中最大的两个数字之和。图片中所有数字的union box代表一个人。为了模仿任务的移动特征，数字随机分布在图像中，而且给图像加入了高斯噪声。</p><p>最终实验结果（准确率）：</p><ul><li>端到端，10.0%</li><li>前融合：43.7%</li><li>后融合：<strong>44.2%</strong></li><li>不融合：41.4%</li></ul><p><img src="./PaStaNet/fig5.png" alt=""></p><h2 id="Image-based-Activity-Recognition"><a href="#Image-based-Activity-Recognition" class="headerlink" title="Image-based Activity Recognition"></a>Image-based Activity Recognition</h2><p><img src="./PaStaNet/table1.png" alt=""></p><h2 id="Instance-based-Activity-Detection"><a href="#Instance-based-Activity-Detection" class="headerlink" title="Instance-based Activity Detection"></a>Instance-based Activity Detection</h2><p><img src="./PaStaNet/table2.png" alt=""></p><h2 id="Transfer-Learning-with-Activity2Vec"><a href="#Transfer-Learning-with-Activity2Vec" class="headerlink" title="Transfer Learning with Activity2Vec"></a>Transfer Learning with Activity2Vec</h2><p><img src="./PaStaNet/table3.png" alt=""></p><p><img src="./PaStaNet/table4.png" alt=""></p><h2 id="可视化结果"><a href="#可视化结果" class="headerlink" title="可视化结果"></a>可视化结果</h2><p><img src="./PaStaNet/fig14.png" alt=""></p><p>图中蓝、绿、红分别指示身体部位、局部行为、交互物体。作者发现他们的模型能够检测各种行为，包括与各种对象的交互。</p><h1 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h1><p>mAP：mean Average Precision，平均的平均精度（两个平均）。先是类内求平均精确度，再是对所有类别再求平均精确度。</p><p>目标检测任务中将目标的分类结果分成四类（正即是真，负即是假）：</p><ol><li>TP——True Positive，正识别为正；</li><li>FP——False Positive，负识别为正；</li><li>TN——True Negative，负识别为负；</li><li>FN——False Negative，负识别为正</li></ol><p>准确率Precision——<strong>识别为正</strong>的数据中，真实为正的：</p><script type="math/tex; mode=display">P=\frac{T P}{T P+F P}=\frac{T P}{N_{\text {detection}}}</script><p>召回率Recall——<strong>原始为正</strong>的数据中，识别为正的：</p><script type="math/tex; mode=display">R=\frac{T P}{T P+F N}=\frac{T P}{N_{g t}}</script><p>平均精度AP即是在一组召回率阈值[0, 1]中，根据召回率计算相应准确率，然后准确率取平均。比如设置11个等间隔召回率阈值[0, 0.2, …, 1]，那么AP的计算公式如下：</p><script type="math/tex; mode=display">\begin{aligned}A P=& \frac{1}{11} \sum_{r \in\{0,0.1, \ldots, 1.0\}} \rho_{\text {interp}}(r) \\& \rho_{\text {interp}}(r)=\max _{\hat{r}: \hat{r} \geqslant r}(\hat{r})\end{aligned}</script><p>实际上就是对于每个Recall值下的Precision，取所有比当前值大的Recall对应的Precision的最大值作为当前Recall值下的Precision。</p><p>mAP是多个AP值的均值，AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><blockquote><p>In this paper, to make a step toward human activity knowledge engine, we construct PaStaNet to provide novel body part-level activity representation (PaSta). Meanwhile, a knowledge transformer Activity2Vec and a part-based reasoning method PaSta-R are proposed. PaStaNet brings in interpretability and new possibility for activity understanding. It can effectively bridge the semantic gap between pixels and activities. With PaStaNet, we signiﬁcantly boost the performance in supervised and transfer learning tasks, especially under few-shot circumstances. In the future, we plan to enrich our PaStaNet with spatio-temporal PaSta.</p></blockquote><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><ol><li><p><a href="https://blog.csdn.net/xiezongsheng1990/article/details/89608923?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1" rel="external nofollow" target="_blank">目标检测测评指标——mAP</a></p></li><li><p><a href="http://blog.sina.com.cn/s/blog_9db078090102whzw.html" rel="external nofollow" target="_blank">多标签图像分类任务的评价方法-mAP</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇论文是上海交大&lt;a href=&quot;http://mvig.sjtu.edu.cn/&quot; rel=&quot;external nofollow&quot; target=&quot;_blank&quot;&gt;卢策吾&lt;/a&gt;老师团队下&lt;a href=&quot;https://dirtyharrylyl.github.io/&quot; rel=&quot;external nofollow&quot; target=&quot;_blank&quot;&gt;李永露&lt;/a&gt;博士在2020CVPR会议三连中中的其中一篇。方向为HOIs方向，即人物交互。&lt;/p&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://StepNeverStop.github.io/categories/DeepLearning/"/>
    
    
      <category term="dl" scheme="http://StepNeverStop.github.io/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation</title>
    <link href="http://StepNeverStop.github.io/h-dqn.html"/>
    <id>http://StepNeverStop.github.io/h-dqn.html</id>
    <published>2020-04-11T04:35:31.000Z</published>
    <updated>2020-04-11T04:40:15.274Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/1604.06057" rel="external nofollow" target="_blank">http://arxiv.org/abs/1604.06057</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;http://arxiv.org/abs/1604.06057&quot; rel=&quot;
      
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Deep Exploration via Bootstrapped DQN</title>
    <link href="http://StepNeverStop.github.io/bootstrapped-dqn.html"/>
    <id>http://StepNeverStop.github.io/bootstrapped-dqn.html</id>
    <published>2020-04-11T04:35:24.000Z</published>
    <updated>2020-04-11T04:39:56.646Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/1602.04621" rel="external nofollow" target="_blank">http://arxiv.org/abs/1602.04621</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;http://arxiv.org/abs/1602.04621&quot; rel=&quot;
      
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Emergence of Locomotion Behaviours in Rich Environments</title>
    <link href="http://StepNeverStop.github.io/dppo.html"/>
    <id>http://StepNeverStop.github.io/dppo.html</id>
    <published>2020-04-11T04:35:16.000Z</published>
    <updated>2020-04-11T06:15:45.331Z</updated>
    
    <content type="html"><![CDATA[<p>这篇论文主要提出了DPPO——Distributed PPO。</p><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/1707.02286" rel="external nofollow" target="_blank">http://arxiv.org/abs/1707.02286</a></p><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><p>作者提到PG算法通常具有高方差，而且策略对于超参数的选择十分敏感。很多种方法</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇论文主要提出了DPPO——Distributed PPO。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow2.0中的高斯分布及其概率</title>
    <link href="http://StepNeverStop.github.io/tf2-gaussian-distribution.html"/>
    <id>http://StepNeverStop.github.io/tf2-gaussian-distribution.html</id>
    <published>2020-04-08T02:50:12.000Z</published>
    <updated>2020-04-08T12:03:09.830Z</updated>
    
    <content type="html"><![CDATA[<p>此篇博文用于记录和描述一些高斯分布的基本特性以及在tensorflow2.0中的不同之处。</p><a id="more"></a><h1 id="对角协方差高斯分布"><a href="#对角协方差高斯分布" class="headerlink" title="对角协方差高斯分布"></a>对角协方差高斯分布</h1><p>对角协方差矩阵： Diagonal Covariance Matrix</p><p>多元高斯分布：multivariate Gaussian distribution</p><p>拥有对角协方差的多元高斯分布，其变量的概率密度等于各个变量的一元高斯概率密度之积。</p><p>假设对角协方差矩阵是如下形式：</p><script type="math/tex; mode=display">\Sigma =\left(  \begin{array}{cccc}    \sigma_{1} & 0 & \cdots & 0 \\    0 & \sigma_{2} & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & \sigma_{k}  \end{array}\right)</script><p>那么，多元高斯函数的概率密度函数被定义为：</p><script type="math/tex; mode=display">f_{x}\left(x_{1}, \ldots, x_{k}\right)=\frac{\exp \left(-\frac{1}{2}(\vec{x}-\vec{\mu})^{T} \Sigma^{-1}(\vec{x}-\vec{\mu})\right)}{\sqrt{|2 \pi \Sigma|}}</script><p>变量替换：</p><script type="math/tex; mode=display">\vec{y}=\vec{x}-\vec{\mu}</script><p>对角协方差的逆可以表示为：</p><script type="math/tex; mode=display">\begin{aligned}\Sigma^{-1} &=\left(\begin{array}{cccc}\sigma_{1}^{2} & 0 & \cdots & 0 \\0 & \sigma_{2}^{2} & \cdots & 0 \\\vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & \sigma_{k}^{2}\end{array}\right)^{-1} \\&=\left(\begin{array}{cccc}\frac{1}{\sigma_{1}^{2}} & 0 & \cdots & 0 \\0 & \frac{1}{\sigma_{2}^{2}} & \cdots & 0 \\\vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & \frac{1}{\sigma_{k}^{2}}\end{array}\right)\end{aligned}</script><p>代入上面式子，可以得到：</p><script type="math/tex; mode=display">\begin{align}    f_x(x_1, \ldots, x_k) = \frac{\exp \Big(-\frac{1}{2} ( \frac{y_1^2}{\sigma_1^2} + \frac{y_2^2}{\sigma_2^2} + \ldots + \frac{y_k^2}{\sigma_k^2} ) \Big)}{\sqrt{(2\pi)^{k}|\Sigma|}}\end{align}</script><p>其中，分母的变化是由于矩阵秩的性质。对角协方差矩阵的秩可以展开写成：</p><script type="math/tex; mode=display">\begin{align}    |\Sigma| &=        \begin{vmatrix}            \sigma_1^2 & 0 & \cdots & 0 \\            0 & \sigma_2^2 & \cdots & 0 \\            \vdots & \vdots & \ddots & \vdots \\            0 & 0 & \cdots & \sigma_k^2 \\        \end{vmatrix} \\        &= \sigma_1^2 \cdot \sigma_2^2 \cdots \sigma_k^2        \end{align} %]]></script><p>最后，代入原概率密度方程并化简：</p><script type="math/tex; mode=display">\begin{align}    f_x(x_1, \ldots, x_k) &= \frac{\exp \Big(-\frac{1}{2} ( \frac{y_1^2}{\sigma_1^2} + \frac{y_2^2}{\sigma_2^2} + \ldots + \frac{y_k^2}{\sigma_k^2} ) \Big)}{\sqrt{(2\pi)^{k}\sigma_1^2 \cdot \sigma_2^2 \cdots \sigma_k^2}} \\    &= \frac{\exp \Big(-\frac{1}{2} ( \frac{y_1^2}{\sigma_1^2} + \frac{y_2^2}{\sigma_2^2} + \ldots + \frac{y_k^2}{\sigma_k^2} ) \Big)}{\sqrt{2\pi\sigma_1^2 \cdot 2\pi\sigma_2^2 \cdots 2\pi\sigma_k^2}} \\    &= \frac{\exp\Big(-\frac{y_1^2}{2\sigma_1^2} \Big)}{\sqrt{2\pi\sigma_1^2}}  \cdot \frac{\exp\Big(-\frac{y_2^2}{2\sigma_2^2} \Big)}{\sqrt{2\pi\sigma_2^2}}  \cdots \frac{\exp\Big(-\frac{y_k^2}{2\sigma_k^2} \Big)}{\sqrt{2\pi\sigma_k^2}} \\    &= \frac{\exp\Big(-\frac{(x_1-\mu_1)^2}{2\sigma_1^2} \Big)}{\sqrt{2\pi\sigma_1^2}}  \cdot \frac{\exp\Big(-\frac{(x_2-\mu_2)^2}{2\sigma_2^2} \Big)}{\sqrt{2\pi\sigma_2^2}}  \cdots \frac{\exp\Big(-\frac{(x_k-\mu_k)^2}{2\sigma_k^2} \Big)}{\sqrt{2\pi\sigma_k^2}} \\    &= f_1(x_1) \cdot f_2(x_2) \cdots f_k(x_k)\end{align} %]]></script><p>这样就得到了，对角协方差矩阵的多元高斯分布，其变量的联合概率密度等于各个变量独立高斯分布概率密度的连积。</p><h1 id="TensorFlow-2-0-Gaussian-Functions"><a href="#TensorFlow-2-0-Gaussian-Functions" class="headerlink" title="TensorFlow 2.0 Gaussian Functions"></a>TensorFlow 2.0 Gaussian Functions</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_probability <span class="keyword">as</span> tfp</span><br><span class="line"></span><br><span class="line">dt = tf.float32</span><br><span class="line"></span><br><span class="line">mu = tf.constant([<span class="number">1.</span>, <span class="number">2.</span>], dtype=dt)<span class="comment"># 均值</span></span><br><span class="line">sigma = tf.constant([<span class="number">1.</span>, <span class="number">2.</span>], dtype=dt)<span class="comment"># 标准差</span></span><br><span class="line">covariance = tf.constant([<span class="comment"># 协方差矩阵</span></span><br><span class="line">  [<span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">  [<span class="number">0.</span>, <span class="number">4.</span>]</span><br><span class="line">])</span><br><span class="line">x = tf.constant([<span class="number">0.92</span>, <span class="number">2.03</span>], dtype=dt)<span class="comment"># 模拟一个样本</span></span><br></pre></td></tr></table></figure><p>测试一下TF中几个不同高斯分布的1. 采样形式；2. 样本概率的表示。定义一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gaussian</span><span class="params">(dist, y)</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  param dist: 传入的高斯分布</span></span><br><span class="line"><span class="string">  param y: 传入的样本</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  x = dist.sample()</span><br><span class="line">  print(x)<span class="comment"># 输出分布的采样</span></span><br><span class="line">  p = dist.prob(x)</span><br><span class="line">  print(p)<span class="comment"># 输出采样样本的概率</span></span><br><span class="line">  p_y = dist.prob(y)</span><br><span class="line">  print(<span class="string">'prob: '</span>, p_y)<span class="comment"># 输出样本的概率</span></span><br><span class="line">  print(<span class="string">'sum_prob: '</span>, tf.reduce_sum(p_y))<span class="comment"># 输出样本的概率之和</span></span><br><span class="line">  print(<span class="string">'prod_prob: '</span>, tf.reduce_prod(p_y))<span class="comment"># 输出样本的概率之积</span></span><br><span class="line">  log_p_y = dist.log_prob(y)</span><br><span class="line">  print(<span class="string">'log_prob: '</span>, log_p_y)<span class="comment"># 输出样本的概率对数</span></span><br><span class="line">  print(<span class="string">'sum_log_prob: '</span>, tf.reduce_sum(log_p_y))<span class="comment"># 输出样本的概率对数之和</span></span><br><span class="line">  print(<span class="string">'prod_log_prob: '</span>, tf.reduce_prod(log_p_y))<span class="comment"># 输出样本的概率对数之积</span></span><br></pre></td></tr></table></figure><h2 id="Normal"><a href="#Normal" class="headerlink" title="Normal"></a>Normal</h2><p><code>test_gaussian(tfp.distributions.Normal(loc=mu, scale=sigma), x)</code>，其参数中的scale为标准差。</p><p>输出为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([-0.24812889  2.8150756 ], shape=(2,), dtype=float32)</span><br><span class="line">tf.Tensor([0.18307646 0.1835755 ], shape=(2,), dtype=float32)</span><br><span class="line">prob:  tf.Tensor([0.3976677  0.19944869], shape=(2,), dtype=float32)</span><br><span class="line">sum_prob:  tf.Tensor(0.5971164, shape=(), dtype=float32)</span><br><span class="line">prod_prob:  tf.Tensor(0.07931431, shape=(), dtype=float32)</span><br><span class="line">log_prob:  tf.Tensor([-0.9221385 -1.6121982], shape=(2,), dtype=float32)</span><br><span class="line">sum_log_prob:  tf.Tensor(-2.5343368, shape=(), dtype=float32)</span><br><span class="line">prod_log_prob:  tf.Tensor(1.4866701, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure><p>这个分布为各个变量的独立高斯分布，有$n$个变量，则采样出$n$个值，其概率为每个变量各自的概率，如$n=2$，上面结果给出概率为：prob:  tf.Tensor([0.3976677  0.19944869], shape=(2,), dtype=float32)。</p><h2 id="MultivariateNormalDiag"><a href="#MultivariateNormalDiag" class="headerlink" title="MultivariateNormalDiag"></a>MultivariateNormalDiag</h2><p><code>test_gaussian(tfp.distributions.MultivariateNormalDiag(loc=mu, scale_diag=sigma), x)</code>，其参数中的scale_diag为协方差矩阵对角线的平方根，也就是每个变量对应的标准差。</p><p>输出为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([2.550062  1.5906484], shape=(2,), dtype=float32)</span><br><span class="line">tf.Tensor(0.02343988, shape=(), dtype=float32)</span><br><span class="line">prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">sum_prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">prod_prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br><span class="line">sum_log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br><span class="line">prod_log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure><p>这个分布于各变量独立的高斯分布相同，其协方差矩阵为对角矩阵，与Normal不同的是，其概率为每个变量各自概率之积，即联合概率。其概率对数为每个变量各自概率对数之和。</p><p>prob = Normal: prod_prob</p><p>log_prob = Normal: sum_log_prob</p><h2 id="MultivariateNormalFullCovariance"><a href="#MultivariateNormalFullCovariance" class="headerlink" title="MultivariateNormalFullCovariance"></a>MultivariateNormalFullCovariance</h2><p><code>test_gaussian(tfp.distributions.MultivariateNormalFullCovariance(loc=mu, covariance_matrix=covariance), x)</code>，其参数中的covariance_matrix为协方差矩阵，因此如果使用<code>MultivariateNormalDiag</code>并指定对角线参数为<code>scale_diag=[1,2]</code>，那么实现相同的分布使用<code>MultivariateNormalFullCovariance</code>应指定协方差矩阵参数为<code>covariance_matrix=[[1,0],[0,2]]</code>。</p><p>输出为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([3.1393874 1.2191284], shape=(2,), dtype=float32)</span><br><span class="line">tf.Tensor(0.007478423, shape=(), dtype=float32)</span><br><span class="line">prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">sum_prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">prod_prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br><span class="line">sum_log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br><span class="line">prod_log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure><p>这个分布为多个变量的联合高斯分布，如果设置协方差矩阵为对角矩阵，且为<code>MultivariateNormalDiag</code>对角参数的平方，则两个分布一致。</p><h2 id="TruncatedNormal"><a href="#TruncatedNormal" class="headerlink" title="TruncatedNormal"></a>TruncatedNormal</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tfp.distributions.TruncatedNormal(</span><br><span class="line">    loc, scale, low, high, validate_args=<span class="keyword">False</span>, allow_nan_stats=<span class="keyword">True</span>,</span><br><span class="line">    name=<span class="string">'TruncatedNormal'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这个分布相比<code>Normal</code>多了两个参数：low和high，如果样本＞high或者＜low，则其概率为0，对数概率为-inf。使用这个分布采样的样本也在low和high之间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dist = tfp.distributions.TruncatedNormal(loc=[<span class="number">0.</span>, <span class="number">1.</span>], scale=<span class="number">1.0</span>, low=[<span class="number">-1.</span>, <span class="number">0.</span>], high=[<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">print(dist.prob([<span class="number">1.1</span>, <span class="number">-0.1</span>]))</span><br><span class="line">print(dist.log_prob([<span class="number">1.1</span>, <span class="number">-0.1</span>]))</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">tf.Tensor([<span class="number">0.</span> <span class="number">0.</span>], shape=(<span class="number">2</span>,), dtype=float32)</span><br><span class="line">tf.Tensor([-inf -inf], shape=(<span class="number">2</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此篇博文用于记录和描述一些高斯分布的基本特性以及在tensorflow2.0中的不同之处。&lt;/p&gt;
    
    </summary>
    
      <category term="TensorFlow" scheme="http://StepNeverStop.github.io/categories/TensorFlow/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
      <category term="tf2" scheme="http://StepNeverStop.github.io/tags/tf2/"/>
    
  </entry>
  
  <entry>
    <title>学习gRPC过程点滴记录</title>
    <link href="http://StepNeverStop.github.io/learn-grpc.html"/>
    <id>http://StepNeverStop.github.io/learn-grpc.html</id>
    <published>2020-04-05T03:14:26.000Z</published>
    <updated>2020-04-12T08:26:24.552Z</updated>
    
    <content type="html"><![CDATA[<p>此博客用于记录学习gRPC(python)的过程。</p><a id="more"></a><h1 id="安装-gRPC"><a href="#安装-gRPC" class="headerlink" title="安装 gRPC"></a>安装 gRPC</h1><p>参考：<a href="https://www.grpc.io/docs/quickstart/python/" rel="external nofollow" target="_blank">官网指导</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ python -m pip install --upgrade pip</span><br><span class="line">$ python -m pip install grpcio</span><br><span class="line">$ python -m pip install grpcio-tools</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此博客用于记录学习gRPC(python)的过程。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://StepNeverStop.github.io/categories/Python/"/>
    
    
      <category term="python" scheme="http://StepNeverStop.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>《Reinforcement Learning: An Introduction》阅读笔记</title>
    <link href="http://StepNeverStop.github.io/rl-an-introduction.html"/>
    <id>http://StepNeverStop.github.io/rl-an-introduction.html</id>
    <published>2020-04-02T13:29:07.000Z</published>
    <updated>2020-04-08T10:53:25.627Z</updated>
    
    <content type="html"><![CDATA[<p>学习RL至今(2020年04月02日21:30:19)，一直没有系统地看过这本被誉为RL界“圣经”的教科书，也没有对从该书中学到的知识点进行整理与记录，本文将记录重读《Reinforcement Learning: An Introduction》这本书时所学到的关键知识点和受到的启发。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Chapter-1-Introduction"><a href="#Chapter-1-Introduction" class="headerlink" title="Chapter 1 Introduction"></a>Chapter 1 Introduction</h2><h3 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h3><h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><h3 id="Elements-of-Reinforcement-Learning"><a href="#Elements-of-Reinforcement-Learning" class="headerlink" title="Elements of Reinforcement Learning"></a>Elements of Reinforcement Learning</h3><h3 id="Limitations-and-Scope"><a href="#Limitations-and-Scope" class="headerlink" title="Limitations and Scope"></a>Limitations and Scope</h3><h3 id="An-Extended-Example-Tic-Tac-Toe"><a href="#An-Extended-Example-Tic-Tac-Toe" class="headerlink" title="An Extended Example: Tic-Tac-Toe"></a>An Extended Example: Tic-Tac-Toe</h3><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><h3 id="Early-History-of-Reinforcement-Learning"><a href="#Early-History-of-Reinforcement-Learning" class="headerlink" title="Early History of Reinforcement Learning"></a>Early History of Reinforcement Learning</h3><h1 id="Tabular-Solution-Methods"><a href="#Tabular-Solution-Methods" class="headerlink" title="Tabular Solution Methods"></a>Tabular Solution Methods</h1><h2 id="Chapter-2-Multi-armed-Bandits"><a href="#Chapter-2-Multi-armed-Bandits" class="headerlink" title="Chapter 2 Multi-armed Bandits"></a>Chapter 2 Multi-armed Bandits</h2><h3 id="A-k-armed-Bandit-Problem"><a href="#A-k-armed-Bandit-Problem" class="headerlink" title="A k-armed Bandit Problem"></a>A k-armed Bandit Problem</h3><h3 id="Action-value-Methods"><a href="#Action-value-Methods" class="headerlink" title="Action-value Methods"></a>Action-value Methods</h3><h3 id="The-10-armed-Testbed"><a href="#The-10-armed-Testbed" class="headerlink" title="The 10-armed Testbed"></a>The 10-armed Testbed</h3><h3 id="Incremental-Implementation"><a href="#Incremental-Implementation" class="headerlink" title="Incremental Implementation"></a>Incremental Implementation</h3><h3 id="Tracking-a-Nonstationary-Problem"><a href="#Tracking-a-Nonstationary-Problem" class="headerlink" title="Tracking a Nonstationary Problem"></a>Tracking a Nonstationary Problem</h3><h3 id="Optimistic-Initial-Values"><a href="#Optimistic-Initial-Values" class="headerlink" title="Optimistic Initial Values"></a>Optimistic Initial Values</h3><h3 id="Upper-Confidence-Bound-ActionSelection"><a href="#Upper-Confidence-Bound-ActionSelection" class="headerlink" title="Upper-Confidence-Bound ActionSelection"></a>Upper-Confidence-Bound ActionSelection</h3><h3 id="Gradient-Bandit-Algorithms"><a href="#Gradient-Bandit-Algorithms" class="headerlink" title="Gradient Bandit Algorithms"></a>Gradient Bandit Algorithms</h3><h3 id="Associative-Search-Contextual-Bandits"><a href="#Associative-Search-Contextual-Bandits" class="headerlink" title="Associative Search(Contextual Bandits)"></a>Associative Search(Contextual Bandits)</h3><h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-3-Finite-Markov-Decision-Processes"><a href="#Chapter-3-Finite-Markov-Decision-Processes" class="headerlink" title="Chapter 3 Finite Markov Decision Processes"></a>Chapter 3 Finite Markov Decision Processes</h2><h3 id="The-Agent-Environment-Interface"><a href="#The-Agent-Environment-Interface" class="headerlink" title="The Agent-Environment Interface"></a>The Agent-Environment Interface</h3><h3 id="Goals-and-Rewards"><a href="#Goals-and-Rewards" class="headerlink" title="Goals and Rewards"></a>Goals and Rewards</h3><h3 id="Returns-and-Episodes"><a href="#Returns-and-Episodes" class="headerlink" title="Returns and Episodes"></a>Returns and Episodes</h3><h3 id="Unified-Nation-for-Episodic-and-Continuing-Tasks"><a href="#Unified-Nation-for-Episodic-and-Continuing-Tasks" class="headerlink" title="Unified Nation for Episodic and Continuing Tasks"></a>Unified Nation for Episodic and Continuing Tasks</h3><h3 id="Policies-and-Value-Functions"><a href="#Policies-and-Value-Functions" class="headerlink" title="Policies and Value Functions"></a>Policies and Value Functions</h3><h3 id="Optimial-Policies-and-Optimal-Value-Functions"><a href="#Optimial-Policies-and-Optimal-Value-Functions" class="headerlink" title="Optimial Policies and Optimal Value Functions"></a>Optimial Policies and Optimal Value Functions</h3><h3 id="Optimality-and-Approximation"><a href="#Optimality-and-Approximation" class="headerlink" title="Optimality and Approximation"></a>Optimality and Approximation</h3><h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-4-Dynamic-Programming"><a href="#Chapter-4-Dynamic-Programming" class="headerlink" title="Chapter 4 Dynamic Programming"></a>Chapter 4 Dynamic Programming</h2><h3 id="Policy-Evaluation-Prediction"><a href="#Policy-Evaluation-Prediction" class="headerlink" title="Policy Evaluation(Prediction)"></a>Policy Evaluation(Prediction)</h3><h3 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h3><h3 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h3><h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><h3 id="Asynchronous-Dynamic-Programming"><a href="#Asynchronous-Dynamic-Programming" class="headerlink" title="Asynchronous Dynamic Programming"></a>Asynchronous Dynamic Programming</h3><h3 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h3><h3 id="Efficiency-of-Dynamic-Programming"><a href="#Efficiency-of-Dynamic-Programming" class="headerlink" title="Efficiency of Dynamic Programming"></a>Efficiency of Dynamic Programming</h3><h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-5-Monte-Carlo-Methods"><a href="#Chapter-5-Monte-Carlo-Methods" class="headerlink" title="Chapter 5 Monte Carlo Methods"></a>Chapter 5 Monte Carlo Methods</h2><h3 id="Monte-Carlo-Prediction"><a href="#Monte-Carlo-Prediction" class="headerlink" title="Monte Carlo Prediction"></a>Monte Carlo Prediction</h3><h3 id="Monte-Carlo-Estimation-of-Action-Values"><a href="#Monte-Carlo-Estimation-of-Action-Values" class="headerlink" title="Monte Carlo Estimation of Action Values"></a>Monte Carlo Estimation of Action Values</h3><h3 id="Monte-Carlo-Control"><a href="#Monte-Carlo-Control" class="headerlink" title="Monte Carlo Control"></a>Monte Carlo Control</h3><h3 id="Monte-Carlo-Control-without-Exploring-Starts"><a href="#Monte-Carlo-Control-without-Exploring-Starts" class="headerlink" title="Monte Carlo Control without Exploring Starts"></a>Monte Carlo Control without Exploring Starts</h3><h3 id="Off-policy-Prediction-via-Improtance-Sampling"><a href="#Off-policy-Prediction-via-Improtance-Sampling" class="headerlink" title="Off-policy Prediction via Improtance Sampling"></a>Off-policy Prediction via Improtance Sampling</h3><h3 id="Incremental-Implementation-1"><a href="#Incremental-Implementation-1" class="headerlink" title="Incremental Implementation"></a>Incremental Implementation</h3><h3 id="Off-policy-Monte-Carlo-Control"><a href="#Off-policy-Monte-Carlo-Control" class="headerlink" title="Off-policy Monte Carlo Control"></a>Off-policy Monte Carlo Control</h3><h3 id="Discounting-aware-Importance-Sampling"><a href="#Discounting-aware-Importance-Sampling" class="headerlink" title="Discounting-aware Importance Sampling"></a>Discounting-aware Importance Sampling</h3><h3 id="Per-decision-Importance-Sampling"><a href="#Per-decision-Importance-Sampling" class="headerlink" title="Per-decision Importance Sampling"></a>Per-decision Importance Sampling</h3><h3 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-6-Temporal-Difference-Learning"><a href="#Chapter-6-Temporal-Difference-Learning" class="headerlink" title="Chapter 6 Temporal-Difference Learning"></a>Chapter 6 Temporal-Difference Learning</h2><h3 id="TD-Prediction"><a href="#TD-Prediction" class="headerlink" title="TD Prediction"></a>TD Prediction</h3><h3 id="Advantages-of-TD-Prediction-Methods"><a href="#Advantages-of-TD-Prediction-Methods" class="headerlink" title="Advantages of TD Prediction Methods"></a>Advantages of TD Prediction Methods</h3><h3 id="Optimality-of-TD-0"><a href="#Optimality-of-TD-0" class="headerlink" title="Optimality of TD(0)"></a>Optimality of TD(0)</h3><h3 id="Sarsa-On-policy-TD-Control"><a href="#Sarsa-On-policy-TD-Control" class="headerlink" title="Sarsa: On-policy TD Control"></a>Sarsa: On-policy TD Control</h3><h3 id="Q-learning-Off-policy-TD-Control"><a href="#Q-learning-Off-policy-TD-Control" class="headerlink" title="Q-learning: Off-policy TD Control"></a>Q-learning: Off-policy TD Control</h3><h3 id="Expected-Sarsa"><a href="#Expected-Sarsa" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h3><h3 id="Maximization-Bias-and-Double-Learning"><a href="#Maximization-Bias-and-Double-Learning" class="headerlink" title="Maximization Bias and Double Learning"></a>Maximization Bias and Double Learning</h3><h3 id="Games-Afterstates-and-Other-Special-Cases"><a href="#Games-Afterstates-and-Other-Special-Cases" class="headerlink" title="Games, Afterstates, and Other Special Cases"></a>Games, Afterstates, and Other Special Cases</h3><h3 id="Summary-5"><a href="#Summary-5" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-7-n-step-Bootstrapping"><a href="#Chapter-7-n-step-Bootstrapping" class="headerlink" title="Chapter 7 n-step Bootstrapping"></a>Chapter 7 n-step Bootstrapping</h2><h3 id="n-step-TD-Prediction"><a href="#n-step-TD-Prediction" class="headerlink" title="n-step TD Prediction"></a>n-step TD Prediction</h3><h3 id="n-step-Sarsa"><a href="#n-step-Sarsa" class="headerlink" title="n-step Sarsa"></a>n-step Sarsa</h3><h3 id="n-step-Off-policy-Learning"><a href="#n-step-Off-policy-Learning" class="headerlink" title="n-step Off-policy Learning"></a>n-step Off-policy Learning</h3><h3 id="Per-decision-Methods-with-Control-Variates"><a href="#Per-decision-Methods-with-Control-Variates" class="headerlink" title="Per-decision Methods with Control Variates"></a>Per-decision Methods with Control Variates</h3><h3 id="Off-policy-Learning-Without-Importance-Sampling-The-n-step-Tree-Backup-Algorithm"><a href="#Off-policy-Learning-Without-Importance-Sampling-The-n-step-Tree-Backup-Algorithm" class="headerlink" title="Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm"></a>Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm</h3><h3 id="A-Unifying-Algorithm-n-step-Q-sigma"><a href="#A-Unifying-Algorithm-n-step-Q-sigma" class="headerlink" title="A Unifying Algorithm: n-step $Q(\sigma)$"></a>A Unifying Algorithm: n-step $Q(\sigma)$</h3><h3 id="Summary-6"><a href="#Summary-6" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-8-Planning-and-Learning-with-Tabular-Methods"><a href="#Chapter-8-Planning-and-Learning-with-Tabular-Methods" class="headerlink" title="Chapter 8 Planning and Learning with Tabular Methods"></a>Chapter 8 Planning and Learning with Tabular Methods</h2><h3 id="Models-and-Planning"><a href="#Models-and-Planning" class="headerlink" title="Models and Planning"></a>Models and Planning</h3><h3 id="Dyna-Integrated-Planning-Acting-and-Learning"><a href="#Dyna-Integrated-Planning-Acting-and-Learning" class="headerlink" title="Dyna: Integrated Planning, Acting, and Learning"></a>Dyna: Integrated Planning, Acting, and Learning</h3><h3 id="When-the-Model-Is-Wrong"><a href="#When-the-Model-Is-Wrong" class="headerlink" title="When the Model Is Wrong"></a>When the Model Is Wrong</h3><h3 id="Prioritized-Sweeping"><a href="#Prioritized-Sweeping" class="headerlink" title="Prioritized Sweeping"></a>Prioritized Sweeping</h3><h3 id="Expected-vs-Sample-Updates"><a href="#Expected-vs-Sample-Updates" class="headerlink" title="Expected vs. Sample Updates"></a>Expected vs. Sample Updates</h3><h3 id="Trajectory-Sampling"><a href="#Trajectory-Sampling" class="headerlink" title="Trajectory Sampling"></a>Trajectory Sampling</h3><h3 id="Real-time-Dynamic-Programming"><a href="#Real-time-Dynamic-Programming" class="headerlink" title="Real-time Dynamic Programming"></a>Real-time Dynamic Programming</h3><h3 id="Planning-at-Decision-Time"><a href="#Planning-at-Decision-Time" class="headerlink" title="Planning at Decision Time"></a>Planning at Decision Time</h3><h3 id="Heuristic-Search"><a href="#Heuristic-Search" class="headerlink" title="Heuristic Search"></a>Heuristic Search</h3><h3 id="Rollout-Algorithms"><a href="#Rollout-Algorithms" class="headerlink" title="Rollout Algorithms"></a>Rollout Algorithms</h3><h3 id="Monte-Carlo-Tree-Search"><a href="#Monte-Carlo-Tree-Search" class="headerlink" title="Monte Carlo Tree Search"></a>Monte Carlo Tree Search</h3><h3 id="Summary-7"><a href="#Summary-7" class="headerlink" title="Summary"></a>Summary</h3><h1 id="Approximate-Solution-Methods"><a href="#Approximate-Solution-Methods" class="headerlink" title="Approximate Solution Methods"></a>Approximate Solution Methods</h1><h2 id="Chapter-9-On-policy-Prediction-with-Approximation"><a href="#Chapter-9-On-policy-Prediction-with-Approximation" class="headerlink" title="Chapter 9 On-policy Prediction with Approximation"></a>Chapter 9 On-policy Prediction with Approximation</h2><h3 id="Value-function-Approximation"><a href="#Value-function-Approximation" class="headerlink" title="Value-function Approximation"></a>Value-function Approximation</h3><h3 id="The-Prediction-Objective-VE"><a href="#The-Prediction-Objective-VE" class="headerlink" title="The Prediction Objective(VE)"></a>The Prediction Objective(VE)</h3><h3 id="Stochastic-gradient-and-Semi-gradient-Methods"><a href="#Stochastic-gradient-and-Semi-gradient-Methods" class="headerlink" title="Stochastic-gradient and Semi-gradient Methods"></a>Stochastic-gradient and Semi-gradient Methods</h3><h3 id="Linear-Methods"><a href="#Linear-Methods" class="headerlink" title="Linear Methods"></a>Linear Methods</h3><h3 id="Feature-Construction-for-Linear-Methods"><a href="#Feature-Construction-for-Linear-Methods" class="headerlink" title="Feature Construction for Linear Methods"></a>Feature Construction for Linear Methods</h3><h4 id="Polynomials"><a href="#Polynomials" class="headerlink" title="Polynomials"></a>Polynomials</h4><h4 id="Fourier-Basis"><a href="#Fourier-Basis" class="headerlink" title="Fourier Basis"></a>Fourier Basis</h4><h4 id="Coarse-Coding"><a href="#Coarse-Coding" class="headerlink" title="Coarse Coding"></a>Coarse Coding</h4><h4 id="Tile-Coding"><a href="#Tile-Coding" class="headerlink" title="Tile Coding"></a>Tile Coding</h4><h4 id="Radial-Basis-Functions"><a href="#Radial-Basis-Functions" class="headerlink" title="Radial Basis Functions"></a>Radial Basis Functions</h4><h3 id="Selecting-Step-size-Parameters-Manually"><a href="#Selecting-Step-size-Parameters-Manually" class="headerlink" title="Selecting Step-size Parameters Manually"></a>Selecting Step-size Parameters Manually</h3><h3 id="Nonlinear-Function-Approximation-Artiﬁcial-Neural-Networks"><a href="#Nonlinear-Function-Approximation-Artiﬁcial-Neural-Networks" class="headerlink" title="Nonlinear Function Approximation: Artiﬁcial Neural Networks"></a>Nonlinear Function Approximation: Artiﬁcial Neural Networks</h3><h3 id="Least-Squares-TD"><a href="#Least-Squares-TD" class="headerlink" title="Least-Squares TD"></a>Least-Squares TD</h3><h3 id="Memory-based-Function-Approximation"><a href="#Memory-based-Function-Approximation" class="headerlink" title="Memory-based Function Approximation"></a>Memory-based Function Approximation</h3><h3 id="Kernel-based-Function-Approximation"><a href="#Kernel-based-Function-Approximation" class="headerlink" title="Kernel-based Function Approximation"></a>Kernel-based Function Approximation</h3><h3 id="Looking-Deeper-at-On-policy-Learning-Interest-and-Emphasis"><a href="#Looking-Deeper-at-On-policy-Learning-Interest-and-Emphasis" class="headerlink" title="Looking Deeper at On-policy Learning: Interest and Emphasis"></a>Looking Deeper at On-policy Learning: Interest and Emphasis</h3><h3 id="Summary-8"><a href="#Summary-8" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-10-On-policy-Control-with-Approximation"><a href="#Chapter-10-On-policy-Control-with-Approximation" class="headerlink" title="Chapter 10 On-policy Control with Approximation"></a>Chapter 10 On-policy Control with Approximation</h2><h3 id="Episodic-Semi-gradient-Control"><a href="#Episodic-Semi-gradient-Control" class="headerlink" title="Episodic Semi-gradient Control"></a>Episodic Semi-gradient Control</h3><h3 id="Semi-gradient-n-step-Sarsa"><a href="#Semi-gradient-n-step-Sarsa" class="headerlink" title="Semi-gradient n-step Sarsa"></a>Semi-gradient n-step Sarsa</h3><h3 id="Average-Reward-A-New-Problem-Setting-for-Continuing-Tasks"><a href="#Average-Reward-A-New-Problem-Setting-for-Continuing-Tasks" class="headerlink" title="Average Reward: A New Problem Setting for Continuing Tasks"></a>Average Reward: A New Problem Setting for Continuing Tasks</h3><h3 id="Deprecating-the-Discounted-Setting"><a href="#Deprecating-the-Discounted-Setting" class="headerlink" title="Deprecating the Discounted Setting"></a>Deprecating the Discounted Setting</h3><h3 id="Differential-Semi-gradient-n-step-Sarsa"><a href="#Differential-Semi-gradient-n-step-Sarsa" class="headerlink" title="Differential Semi-gradient n-step Sarsa"></a>Differential Semi-gradient n-step Sarsa</h3><h3 id="Summary-9"><a href="#Summary-9" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-11-Off-policy-Methods-with-Approximation"><a href="#Chapter-11-Off-policy-Methods-with-Approximation" class="headerlink" title="Chapter 11 Off-policy Methods with Approximation"></a>Chapter 11 Off-policy Methods with Approximation</h2><h3 id="Semi-gradient-Methods"><a href="#Semi-gradient-Methods" class="headerlink" title="Semi-gradient Methods"></a>Semi-gradient Methods</h3><h3 id="Examples-of-Off-policy-Divergence"><a href="#Examples-of-Off-policy-Divergence" class="headerlink" title="Examples of Off-policy Divergence"></a>Examples of Off-policy Divergence</h3><h3 id="The-Deadly-Triad"><a href="#The-Deadly-Triad" class="headerlink" title="The Deadly Triad"></a>The Deadly Triad</h3><h3 id="Linear-Value-function-Geometry"><a href="#Linear-Value-function-Geometry" class="headerlink" title="Linear Value-function Geometry"></a>Linear Value-function Geometry</h3><h3 id="Gradient-Descent-in-the-Bellman-Error"><a href="#Gradient-Descent-in-the-Bellman-Error" class="headerlink" title="Gradient Descent in the Bellman Error"></a>Gradient Descent in the Bellman Error</h3><h3 id="The-Bellman-Error-is-Not-Learnable"><a href="#The-Bellman-Error-is-Not-Learnable" class="headerlink" title="The Bellman Error is Not Learnable"></a>The Bellman Error is Not Learnable</h3><h3 id="Gradient-TD-Methods"><a href="#Gradient-TD-Methods" class="headerlink" title="Gradient-TD Methods"></a>Gradient-TD Methods</h3><h3 id="Emphatic-TD-Methods"><a href="#Emphatic-TD-Methods" class="headerlink" title="Emphatic-TD Methods"></a>Emphatic-TD Methods</h3><h3 id="Reducing-Variance"><a href="#Reducing-Variance" class="headerlink" title="Reducing Variance"></a>Reducing Variance</h3><h3 id="Summary-10"><a href="#Summary-10" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-12-Eligibility-Traces"><a href="#Chapter-12-Eligibility-Traces" class="headerlink" title="Chapter 12 Eligibility Traces"></a>Chapter 12 Eligibility Traces</h2><h3 id="The-lambda-return"><a href="#The-lambda-return" class="headerlink" title="The $\lambda$-return"></a>The $\lambda$-return</h3><h3 id="TD-lambda"><a href="#TD-lambda" class="headerlink" title="TD($\lambda$)"></a>TD($\lambda$)</h3><h3 id="n-step-Truncated-lambda-return-Methods"><a href="#n-step-Truncated-lambda-return-Methods" class="headerlink" title="n-step Truncated $\lambda$-return Methods"></a>n-step Truncated $\lambda$-return Methods</h3><h3 id="Redoing-Updates-Online-lambda-return-Algorithm"><a href="#Redoing-Updates-Online-lambda-return-Algorithm" class="headerlink" title="Redoing Updates: Online $\lambda$-return Algorithm"></a>Redoing Updates: Online $\lambda$-return Algorithm</h3><h3 id="True-Online-TD-lambda"><a href="#True-Online-TD-lambda" class="headerlink" title="True Online TD($\lambda$)"></a>True Online TD($\lambda$)</h3><h3 id="Dutch-Traces-in-Monte-Carlo-Learning"><a href="#Dutch-Traces-in-Monte-Carlo-Learning" class="headerlink" title="Dutch Traces in Monte Carlo Learning"></a>Dutch Traces in Monte Carlo Learning</h3><h3 id="Sarsa-lambda"><a href="#Sarsa-lambda" class="headerlink" title="Sarsa($\lambda$)"></a>Sarsa($\lambda$)</h3><h3 id="Variable-lambda-and-gamma"><a href="#Variable-lambda-and-gamma" class="headerlink" title="Variable $\lambda$ and $\gamma$"></a>Variable $\lambda$ and $\gamma$</h3><h3 id="Off-policy-Traces-with-Control-Variates"><a href="#Off-policy-Traces-with-Control-Variates" class="headerlink" title="Off-policy Traces with Control Variates"></a>Off-policy Traces with Control Variates</h3><h3 id="Watkins’s-Q-lambda-to-Tree-Backup-lambda"><a href="#Watkins’s-Q-lambda-to-Tree-Backup-lambda" class="headerlink" title="Watkins’s Q($\lambda$) to Tree-Backup( $\lambda$)"></a>Watkins’s Q($\lambda$) to Tree-Backup( $\lambda$)</h3><h3 id="Stable-Off-policy-Methods-with-Traces"><a href="#Stable-Off-policy-Methods-with-Traces" class="headerlink" title="Stable Off-policy Methods with Traces"></a>Stable Off-policy Methods with Traces</h3><h3 id="Implementation-Issues"><a href="#Implementation-Issues" class="headerlink" title="Implementation Issues"></a>Implementation Issues</h3><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><h2 id="Chapter-13-Policy-Gradient-Methods"><a href="#Chapter-13-Policy-Gradient-Methods" class="headerlink" title="Chapter 13 Policy Gradient Methods"></a>Chapter 13 Policy Gradient Methods</h2><h3 id="Policy-Approximation-and-its-Advantages"><a href="#Policy-Approximation-and-its-Advantages" class="headerlink" title="Policy Approximation and its Advantages"></a>Policy Approximation and its Advantages</h3><h3 id="The-Policy-Gradient-Theorem"><a href="#The-Policy-Gradient-Theorem" class="headerlink" title="The Policy Gradient Theorem"></a>The Policy Gradient Theorem</h3><h3 id="REINFORCE-Monte-Carlo-Policy-Gradient"><a href="#REINFORCE-Monte-Carlo-Policy-Gradient" class="headerlink" title="REINFORCE: Monte Carlo Policy Gradient"></a>REINFORCE: Monte Carlo Policy Gradient</h3><h3 id="REINFORCE-with-Baseline"><a href="#REINFORCE-with-Baseline" class="headerlink" title="REINFORCE with Baseline"></a>REINFORCE with Baseline</h3><h3 id="Actor–Critic-Methods"><a href="#Actor–Critic-Methods" class="headerlink" title="Actor–Critic Methods"></a>Actor–Critic Methods</h3><h3 id="Policy-Gradient-for-Continuing-Problems"><a href="#Policy-Gradient-for-Continuing-Problems" class="headerlink" title="Policy Gradient for Continuing Problems"></a>Policy Gradient for Continuing Problems</h3><h3 id="Policy-Parameterization-for-Continuous-Actions"><a href="#Policy-Parameterization-for-Continuous-Actions" class="headerlink" title="Policy Parameterization for Continuous Actions"></a>Policy Parameterization for Continuous Actions</h3><h3 id="Summary-11"><a href="#Summary-11" class="headerlink" title="Summary"></a>Summary</h3><h1 id="Looking-Deeper"><a href="#Looking-Deeper" class="headerlink" title="Looking Deeper"></a>Looking Deeper</h1><h2 id="Chapter-14-Psychology"><a href="#Chapter-14-Psychology" class="headerlink" title="Chapter 14 Psychology"></a>Chapter 14 Psychology</h2><h3 id="Prediction-and-Control"><a href="#Prediction-and-Control" class="headerlink" title="Prediction and Control"></a>Prediction and Control</h3><h3 id="Classical-Conditioning"><a href="#Classical-Conditioning" class="headerlink" title="Classical Conditioning"></a>Classical Conditioning</h3><h4 id="Blocking-and-Higher-order-Conditioning"><a href="#Blocking-and-Higher-order-Conditioning" class="headerlink" title="Blocking and Higher-order Conditioning"></a>Blocking and Higher-order Conditioning</h4><h4 id="The-Rescorla–Wagner-Model"><a href="#The-Rescorla–Wagner-Model" class="headerlink" title="The Rescorla–Wagner Model"></a>The Rescorla–Wagner Model</h4><h4 id="The-TD-Model"><a href="#The-TD-Model" class="headerlink" title="The TD Model"></a>The TD Model</h4><h4 id="TD-Model-Simulations"><a href="#TD-Model-Simulations" class="headerlink" title="TD Model Simulations"></a>TD Model Simulations</h4><h3 id="Instrumental-Conditioning"><a href="#Instrumental-Conditioning" class="headerlink" title="Instrumental Conditioning"></a>Instrumental Conditioning</h3><h3 id="Delayed-Reinforcement"><a href="#Delayed-Reinforcement" class="headerlink" title="Delayed Reinforcement"></a>Delayed Reinforcement</h3><h3 id="Cognitive-Maps"><a href="#Cognitive-Maps" class="headerlink" title="Cognitive Maps"></a>Cognitive Maps</h3><h3 id="Habitual-and-Goal-directed-Behavior"><a href="#Habitual-and-Goal-directed-Behavior" class="headerlink" title="Habitual and Goal-directed Behavior"></a>Habitual and Goal-directed Behavior</h3><h3 id="Summary-12"><a href="#Summary-12" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-15-Neuroscience"><a href="#Chapter-15-Neuroscience" class="headerlink" title="Chapter 15 Neuroscience"></a>Chapter 15 Neuroscience</h2><h3 id="Neuroscience-Basics"><a href="#Neuroscience-Basics" class="headerlink" title="Neuroscience Basics"></a>Neuroscience Basics</h3><h3 id="Reward-Signals-Reinforcement-Signals-Values-and-Prediction-Errors"><a href="#Reward-Signals-Reinforcement-Signals-Values-and-Prediction-Errors" class="headerlink" title="Reward Signals, Reinforcement Signals, Values, and Prediction Errors"></a>Reward Signals, Reinforcement Signals, Values, and Prediction Errors</h3><h3 id="The-Reward-Prediction-Error-Hypothesis"><a href="#The-Reward-Prediction-Error-Hypothesis" class="headerlink" title="The Reward Prediction Error Hypothesis"></a>The Reward Prediction Error Hypothesis</h3><h3 id="Dopamine"><a href="#Dopamine" class="headerlink" title="Dopamine"></a>Dopamine</h3><h3 id="Experimental-Support-for-the-Reward-Prediction-Error-Hypothesis"><a href="#Experimental-Support-for-the-Reward-Prediction-Error-Hypothesis" class="headerlink" title="Experimental Support for the Reward Prediction Error Hypothesis"></a>Experimental Support for the Reward Prediction Error Hypothesis</h3><h3 id="TD-Error-Dopamine-Correspondence"><a href="#TD-Error-Dopamine-Correspondence" class="headerlink" title="TD Error/Dopamine Correspondence"></a>TD Error/Dopamine Correspondence</h3><h3 id="Neural-Actor–Critic"><a href="#Neural-Actor–Critic" class="headerlink" title="Neural Actor–Critic"></a>Neural Actor–Critic</h3><h3 id="Actor-and-Critic-Learning-Rules"><a href="#Actor-and-Critic-Learning-Rules" class="headerlink" title="Actor and Critic Learning Rules"></a>Actor and Critic Learning Rules</h3><h3 id="Hedonistic-Neurons"><a href="#Hedonistic-Neurons" class="headerlink" title="Hedonistic Neurons"></a>Hedonistic Neurons</h3><h3 id="Collective-Reinforcement-Learning"><a href="#Collective-Reinforcement-Learning" class="headerlink" title="Collective Reinforcement Learning"></a>Collective Reinforcement Learning</h3><h3 id="Model-based-Methods-in-the-Brain"><a href="#Model-based-Methods-in-the-Brain" class="headerlink" title="Model-based Methods in the Brain"></a>Model-based Methods in the Brain</h3><h3 id="Addiction"><a href="#Addiction" class="headerlink" title="Addiction"></a>Addiction</h3><h3 id="Summary-13"><a href="#Summary-13" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-16-Applications-and-Case-Studies"><a href="#Chapter-16-Applications-and-Case-Studies" class="headerlink" title="Chapter 16 Applications and Case Studies"></a>Chapter 16 Applications and Case Studies</h2><h3 id="TD-Gammon"><a href="#TD-Gammon" class="headerlink" title="TD-Gammon"></a>TD-Gammon</h3><h3 id="Samuel’s-Checkers-Player"><a href="#Samuel’s-Checkers-Player" class="headerlink" title="Samuel’s Checkers Player"></a>Samuel’s Checkers Player</h3><h3 id="Watson’s-Daily-Double-Wagering"><a href="#Watson’s-Daily-Double-Wagering" class="headerlink" title="Watson’s Daily-Double Wagering"></a>Watson’s Daily-Double Wagering</h3><h3 id="Optimizing-Memory-Control"><a href="#Optimizing-Memory-Control" class="headerlink" title="Optimizing Memory Control"></a>Optimizing Memory Control</h3><h3 id="Human-level-Video-Game-Play"><a href="#Human-level-Video-Game-Play" class="headerlink" title="Human-level Video Game Play"></a>Human-level Video Game Play</h3><h3 id="Mastering-the-Game-of-Go"><a href="#Mastering-the-Game-of-Go" class="headerlink" title="Mastering the Game of Go"></a>Mastering the Game of Go</h3><h4 id="AlphaGo"><a href="#AlphaGo" class="headerlink" title="AlphaGo"></a>AlphaGo</h4><h4 id="AlphaGo-Zero"><a href="#AlphaGo-Zero" class="headerlink" title="AlphaGo Zero"></a>AlphaGo Zero</h4><h3 id="Personalized-Web-Services"><a href="#Personalized-Web-Services" class="headerlink" title="Personalized Web Services"></a>Personalized Web Services</h3><h3 id="Thermal-Soaring"><a href="#Thermal-Soaring" class="headerlink" title="Thermal Soaring"></a>Thermal Soaring</h3><h2 id="Chapter-17-Frontiers"><a href="#Chapter-17-Frontiers" class="headerlink" title="Chapter 17 Frontiers"></a>Chapter 17 Frontiers</h2><h3 id="General-Value-Functions-and-Auxiliary-Tasks"><a href="#General-Value-Functions-and-Auxiliary-Tasks" class="headerlink" title="General Value Functions and Auxiliary Tasks"></a>General Value Functions and Auxiliary Tasks</h3><h3 id="Temporal-Abstraction-via-Options"><a href="#Temporal-Abstraction-via-Options" class="headerlink" title="Temporal Abstraction via Options"></a>Temporal Abstraction via Options</h3><h3 id="Observations-and-State"><a href="#Observations-and-State" class="headerlink" title="Observations and State"></a>Observations and State</h3><h3 id="Designing-Reward-Signals"><a href="#Designing-Reward-Signals" class="headerlink" title="Designing Reward Signals"></a>Designing Reward Signals</h3><h3 id="Remaining-Issues"><a href="#Remaining-Issues" class="headerlink" title="Remaining Issues"></a>Remaining Issues</h3><h3 id="The-Future-of-Artiﬁcial-Intelligence"><a href="#The-Future-of-Artiﬁcial-Intelligence" class="headerlink" title="The Future of Artiﬁcial Intelligence"></a>The Future of Artiﬁcial Intelligence</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习RL至今(2020年04月02日21:30:19)，一直没有系统地看过这本被誉为RL界“圣经”的教科书，也没有对从该书中学到的知识点进行整理与记录，本文将记录重读《Reinforcement Learning: An Introduction》这本书时所学到的关键知识点和受到的启发。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards</title>
    <link href="http://StepNeverStop.github.io/keeping-your-distance-solving-sparse-reward-tasks.html"/>
    <id>http://StepNeverStop.github.io/keeping-your-distance-solving-sparse-reward-tasks.html</id>
    <published>2020-03-30T10:09:45.000Z</published>
    <updated>2020-04-09T03:46:55.796Z</updated>
    
    <content type="html"><![CDATA[<p>这篇论文介绍了一个简单有效的model-free方法——<strong>Sibling Rivalry</strong>(同胞对抗？)，用于解决稀疏奖励问题。该方法特定于“以达到某个目标状态(goal-oriented)”为任务的问题，并且从塑性的距离目标相关奖励(distance-to-goal rewards)中学习。</p><p>推荐：</p><ul><li>self-balancing 奖励机制</li><li>基于奖励函数的创新，比较有趣</li></ul><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/1911.01417" rel="external nofollow" target="_blank">http://arxiv.org/abs/1911.01417.pdf</a></p><p>这篇文章解决的问题点是：</p><ul><li>由朴素的距离目标相关的奖励(naive distance-to-goal reward shaping)引起的动态学习在局部最优点稳定的问题</li></ul><p>这篇文章方法的优点是：</p><ul><li>增强目标但不需额外奖励工程设计，也不需领域专家知识</li><li>可以收敛至原始的稀疏奖励目标</li></ul><p>文章发表在NeurIPS 2019上。</p><p>这篇论文提出的Sibling Rivalry(SR)方法结合了塑性奖励的可学习性与稀疏奖励的通用性。它主要解决面向目标(goal-oriented)的任务，是对距离奖励函数的改进，实现了奖励函数的自平衡(self-balancing，自动退化至纯稀疏奖励形式)，它的特点是：</p><ul><li>model-free</li><li>应用在on-policy算法上</li><li>应用在<strong><em>目标状态已知</em></strong>，且<strong><em>以距离为奖励函数导向</em></strong>的任务中</li><li><strong>动态</strong>奖励塑性</li><li>保持稀疏奖励任务的原有最优策略</li><li>跳出局部最优解，寻找全局最优</li><li>可以与分层RL相结合（有实验佐证）</li></ul><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><p>文中shaped rewards一时不知道该如何做翻译，姑且就称之为“塑性奖励”好了，意为为解决某一特定问题而精心设计的奖励函数机制。</p><p>Reward shaping是一种修改奖励信号的技术，比如，它可以用于重新标注失败的经验序列，并从其中筛选出可促进任务完成的经验序列进行学习。然而，这种技术是否可以提升任务性能严重取决于塑性奖励的精心设计。这种塑性奖励有时可以解决稀疏奖励问题，但是具有两个显著特点：</p><ol><li><p>需要精心的工程设计，也可以理解为需要对环境或任务的先验知识</p><blockquote><p>requires careful engineering</p></blockquote></li><li><p>往往只适用于特定任务，特定问题，比如同样的避障问题，载具类型不同，可能就不适用</p><blockquote><p>is problem specific</p></blockquote></li></ol><p>对于现实世界的RL问题，需要手动设计一个与任务契合/对齐的奖励机制，一个好的奖励函数往往比算法的选择更加重要。但是现实世界问题非常复杂，细粒度的奖励函数也十分难以设计，往往会具有“捡了芝麻丢了西瓜”的特点。比如，在避障问题中，如果针对墙壁这种障碍物设置奖励函数，策略在学习过程中会对墙壁这种实体过拟合，导致当遇到一种新的障碍物时，策略无法适用，还不如简单的稀疏奖励（完成即获得奖励，反之则无），让智能体自己从环境中学习哪些具有障碍物共有或特有特性。</p><p>还有一个很严重的问题是，复杂的奖励机制容易使策略陷入局部最优，比如说达到状态A可以获得奖励+10，而达到状态B可以获得奖励+1，那么如果状态A十分难以到达，策略往往会收敛在状态B，或者其他相似的状态附近中去，造成策略在这些局部最优解附近稳定下来。</p><p>稀疏奖励往往不存在这种局部最优，或者考虑不周的问题。设计一个合适的稀疏奖励函数很简单，也很直接，但是从这种奖励函数中学习需要大量的时间，而且甚至学习不出来任何东西，<strong>通常需要额外的启发式探索机制去帮助智能体发现这个稀疏的正奖励</strong>。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>纯稀疏奖励的形式是这样的：</p><script type="math/tex; mode=display">r(s, g)=\left\{\begin{array}{ll}1, & d(s, g) \leq \delta \\0, & \text { otherwise }\end{array}\right.</script><p>其中，$s$代表当前状态，$g$代表目标状态。$d$是一个函数用来判断当前状态与目标状态的距离度量，例如$L_1$或$L_2$距离，这个距离可以用来表示任务的完成度，即距离越短，任务完成度越高。$\delta$代表目标点的半径，也就是说，状态不必完全等于目标状态，只需要距离度量小于一定范围$\delta$即可，这个是很常用的做法，比如Unity ML-Agents的示例环境——RollerBall，也是使用这样的设置。</p><p>这种纯稀疏奖励的设置很难解决，往往需要很大的探索才能获取到寥寥无几的正奖励样本轨迹，给智能体的学习带来了很大的困难。</p><p>其实可以对上述奖励进行修改，使得其可以按照距目标状态的距离给出不同的奖励，引导智能体朝向目标状态移动。它通常为下边这种形式：</p><script type="math/tex; mode=display">\begin{array}{ll}\tilde{r}(s, g)= & \left\{\begin{array}{ll}1, & d(s, g) \leq \delta \\-d(s, g), & \text { otherwise }\end{array}\right.\end{array}</script><p>这种奖励形式很直观，如果没有达到目标状态，则一直是负奖励，且距离越远则负奖励越大，这样将会引导智能体朝向目标状态移动。针对一些比较简单的稀疏奖励环境，这种形式的奖励设计可以带来算法性能的提升，而且往往可以解决稀疏奖励问题。但是，这种基于距离的塑性奖励（Distance-based shaped rewards）很容易陷入局部最优点，并且在局部最优点附近稳定下来，得不到进一步地策略提升。为什么这么说呢？就比如gym的MountainCar场景，智能体需要先倒退再前进才能达到目标点，也就是说要先经历负奖励的增大过程，再经历负奖励的减小过程，才能最终完成目标，从初始状态直接朝向目标点移动是不能把车子开到山顶上去的。使用这种形式的奖励将会使得智能体抵达半山腰，却永远都触及不到目标点。还有一些U-shape的路径问题，在U型的两端设置起始点与目标点，使用这种奖励机制将会使得智能体在直线朝向目标点移动时偏离轨道，永远学不会以U型的方式完成目标。</p><p>使用上述奖励形式的问题经验存在各种各样的局部最优点，而且这些局部最优点与状态空间结构（state space structure）、转移动态（transition dynamics）和环境的其他特性都有关系。</p><p>那么有没有一种对奖励进行塑性的方式可以避免局部最优点的影响呢？确实是有的，我们再对上述奖励函数做如下改进：</p><script type="math/tex; mode=display">r^{\prime}(s, g, \bar{g})=\left\{\begin{array}{ll}1, & d(s, g) \leq \delta \\\min [0,-d(s, g)+d(s, \bar{g})], & \text { otherwise }\end{array}\right.</script><p>与上式不同的是，这里多了一个符号——$\bar g$，它用来表示局部最优点的状态。论文中给出了这样一个例子：</p><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/motivating-example.png" alt=""></p><p>最左边的图是一个简单的环境，智能体的目标是到达绿色的Goal点，如果使用前文提到的$\tilde{r}$，则会存在局部最优点，也就是图中的Local Optimum。为什么会存在这个局部最优点呢？可以设想一下，假设智能体从左下角出发，那么它在上方路径和下方路径各走一步时，明显下方路径的负奖励衰减的更快，所以智能体通常会选择走下方路径，最终收敛到与Goal隔岸相望的Local Optimum。$\tilde{r}$的奖励函数图像轮廓如上面中间图像红色线条所示，图中横坐标为坐标点，纵坐标为奖励值的高低。绿色光点为智能体在轨迹终态的分布，可以看出，使用$\tilde{r}$作为奖励函数时，在这个环境中奖励图像存在一个小的山峰，相比于全局最优点，智能体更易达到局部最优点，而且会在局部最优附近稳定下来，导致无法产生全局最优策略。</p><p>最右侧的图是使用了增强后的距离奖励函数$r^{\prime}$所描绘的奖励图像轮廓，可以看出，通过将局部最优点的山峰构造成低谷，即可抵消局部最优点的影响。绿色点的分布也彰显了增强奖励函数后，智能体可以收敛到全局最优策略。为什么像$r^{\prime}$一样构造奖励函数即可解决局部最优点的影响呢？可以想象一下，将三角形的原理应用上去，当智能体选择上方路径时，两距离相减值逐渐趋于正值，也就是奖励逐渐增大，而选择下方路径时， 两距离相减横为负奖励，这种特性引导着智能体向逐渐增大的正奖励的方向靠拢。</p><p>虽然像$r^{\prime}$这种增强距离奖励的形式可以解决稀疏奖励问题，并跳出局部最优点，但是其通常具有如下几个缺点：</p><ol><li>需要领域专家知识分析局部最优点；</li><li>环境或任务复杂时，存在多种局部最优点，难以确定合适的奖励函数；</li><li>一不小心便会弄巧成拙，还会引入新的局部最优点。</li></ol><h2 id="同胞对抗——Sibling-Rivalry"><a href="#同胞对抗——Sibling-Rivalry" class="headerlink" title="同胞对抗——Sibling Rivalry"></a>同胞对抗——Sibling Rivalry</h2><p>作者十分中意这种跳出局部最优点的方式，所以，作者设想需要一个新的奖励机制，它应当满足：</p><ol><li>可以解决稀疏奖励问题</li><li>不需要专家及领域知识对环境进行全面的分析与判断</li><li>动态评估局部最优点，自适应调整奖励函数</li></ol><p>这有点像静态图与动态图的区别，$r^{\prime}$就好像是静态图，需要专家先把图构好，再解决问题，而同胞对抗（Sibling Rivalry， SR）的思想就好像是动态图，一边执行一边应对潜在的局部最优点。</p><p>先来说一下SR的思想：</p><ol><li><p>每次采样两条轨迹，同样的起始点与目标点；</p></li><li><p>距离目标近的为$\tau^c$，远的为$\tau^f$;</p></li><li><p>互相认为对方的终态$s_T$为局部最优点（假想敌），构造奖励函数，计算轨迹每一时间步的奖励。</p></li></ol><p>如下图所示，每次都roll out两条轨迹，这两条轨迹即称为同胞，然后互相指认对方的终态$S_T^c$和$S_T^f$为局部最优点，促使智能体不选择对方的路径，即为对抗。注意，这样的方式确实有可能使得策略在远离局部最优点的同时，也偏离潜在的正确路径，因为智能体并不真正了解什么样的路径是比较好或者比较坏的。</p><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/sr-example.png" alt=""></p><p>这种互相对抗的方式增加了智能体的探索能力，因为当智能体的策略不是最优时，它会趋向于选择各种不同的路径，以使得策略可以避开尽可能多的局部最优点。</p><p>基于两条同胞轨迹$\tau^{c}$和$\tau^{f}$，如何构造它们的奖励函数呢？其实也很简单，像$r^{\prime}$一样，将其中的局部最优点$\bar g$更换一下即可：</p><script type="math/tex; mode=display">r_{\tau^{f}}^{\prime}=r^{\prime}\left(s_{T}^{f}, g, s_{T}^{c}\right) \quad \& \quad r_{\tau^{c}}^{\prime}=r^{\prime}\left(s_{T}^{c}, g, s_{T}^{f}\right)</script><p>作者将这种奖励机制称为<strong>Self-balancing reward</strong>，可以翻译为自平衡/自适应奖励。什么意思呢？可以想想一下，如果策略已经接近全局最优了，那么$g \approx s_{T}^{f} \approx s_{T}^{c}$，此时的奖励从$r^{\prime}_{\tau^{f}}$和$r^{\prime}_{\tau^{c}}$逐渐退化至纯稀疏奖励$r$。为什么呢？因为此时的$-d(s, g)+d(s, \bar{g})$几乎为0呀！</p><p>这样自动退化的奖励机制有什么好处呢？会<strong>使得智能体最终的最优策略与使用纯稀疏奖励时一致</strong>，大道至简，复杂的奖励函数未必可以引导智能体习得期望的行为，然而，纯稀疏奖励虽然难以学习，但是其产生的智能体行为往往是最直接最符合设计者的期望的。</p><p>有了轨迹经验，也根据各自的奖励函数生成了奖励，那么该如何进行优化呢？既然取得了两条同胞轨迹，两条轨迹都要拿来训练么？我们结合伪代码进行分析。</p><h1 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h1><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/pseudo.png" alt=""></p><p>解析：</p><ul><li>$\rho(s_0, g)$为一个分布，用于采样任务的初始状态和目标状态，论文中的实验都是将智能体的起止点和目标点固定在一个小范围内，至于SR是否可以用在大范围随机的起始、目标点场景中，还未可知；</li><li>$m$是一个函数映射，用于将智能体状态空间$S$映射至目标状态空间$G$，$m(s): S \rightarrow G$；</li><li>$d$是一个距离度量函数，通常为$L_1,L_2$度量；</li><li>$\delta$是判定目标完成的容错半径；</li><li>注意，Critic网络V的输入不仅包括状态$s$，目标$g$，也包括局部最优点（同胞轨迹的终态）$\bar g$；</li><li>作者引入了一个“包容性阈值”$\epsilon$，这个值用来控制对使用距离目标点较近的轨迹$\tau^c$来训练的容忍度，也用来平衡探索与利用，当$\epsilon\uparrow$，利用增加，当$\epsilon\downarrow$，探索增加（学习远距离经验）；</li><li>SR是先采样轨迹$s,a,s,a,…$再计算每一步的奖励的，采样时不包括$r$；</li><li>每次rollout两条轨迹——$\tau^a$和$\tau^b$，距离目标点$g$比较近的终态轨迹标记为$\tau^c$，远的标记为$\tau^{f}$；</li><li>优化模型时，何时用$\tau^f$？<ul><li>任何时候</li><li>猜想：距离远的不太可能是局部最优点，因为距离越远，负奖励越大，算法不会稳定收敛在距离目标点很远的局部最优处，会想办法跳出来</li></ul></li><li>优化模型时，何时用$\tau^c$?<ul><li>1，当两条同胞轨迹的终态距离小于$\epsilon$时；2，当$\tau^c$的终态已足以完成目标时</li><li>猜想：当两条轨迹的终态距离小于一定阈值时，认为轨迹质量差不多，都可以用来更新</li><li>距离目标点$g$近的轨迹已经足够近，是可以获得全局最优解的轨迹，要学习</li></ul></li></ul><h1 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h1><p>作者使用PPO算法在四种类型的任务场景中进行了实验：</p><ol><li>连续动作任务</li><li>分层决策任务</li><li>离散动作任务</li><li>《我的世界》3D构造任务，这个任务主要用来测试SR的可扩展性</li></ol><p>任务的细节设置以及实验结果详情请看原论文。</p><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/experiment1.png" alt=""></p><p>在这两个实验中，可以看出SR比ICM和HER的效果都要好，任务完成的成功率也必将高，且方差比较小。</p><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/experiment2.png" alt=""></p><p>在这个离散的任务中，从图像看出纯DQN即可以解决，但是却没有看到蓝色的纯PPO曲线，按道理来说PPO应该也能解决。</p><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/experiment3.png" alt=""></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><blockquote><p>We introduce Sibling Rivalry, a simple and effective method for learning goal-reaching tasks from a generic class of distance-based shaped rewards. Sibling Rivalry makes use of sibling rollouts and self-balancing rewards to prevent the learning dynamics from stabilizing around local optima. By leveraging the distance metric used to deﬁne the underlying sparse reward, our technique enables robust learning from shaped rewards without relying on carefully-designed, problem-speciﬁc reward functions. We demonstrate the applicability of our method across a variety of goal-reaching tasks where naive distance-to-goal reward shaping consistently fails and techniques to learn from sparse rewards struggle to explore properly and/or generalize from failed rollouts. Our experiments show that Sibling Rivalry can be readily applied to both continuous and discrete domains, incorporated into hierarchical RL, and scaled to demanding environments.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇论文介绍了一个简单有效的model-free方法——&lt;strong&gt;Sibling Rivalry&lt;/strong&gt;(同胞对抗？)，用于解决稀疏奖励问题。该方法特定于“以达到某个目标状态(goal-oriented)”为任务的问题，并且从塑性的距离目标相关奖励(distance-to-goal rewards)中学习。&lt;/p&gt;
&lt;p&gt;推荐：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;self-balancing 奖励机制&lt;/li&gt;
&lt;li&gt;基于奖励函数的创新，比较有趣&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>关于安装一些库的问题</title>
    <link href="http://StepNeverStop.github.io/some-issues-of-install-packages.html"/>
    <id>http://StepNeverStop.github.io/some-issues-of-install-packages.html</id>
    <published>2020-03-19T06:30:16.000Z</published>
    <updated>2020-03-19T06:39:10.648Z</updated>
    
    <content type="html"><![CDATA[<p>本篇博客主要用以记录在各种环境安装各种库可能会遇到的问题及其解决方案，以便以后应急查询。</p><a id="more"></a><h1 id="pip"><a href="#pip" class="headerlink" title="pip"></a>pip</h1><h2 id="由于网速不好，导致TimeOut"><a href="#由于网速不好，导致TimeOut" class="headerlink" title="由于网速不好，导致TimeOut"></a>由于网速不好，导致TimeOut</h2><p>这种情况一般是由于pip要安装的库的默认源在国外服务器，导致网速只有几kb。可以考虑使用国内镜像站来安装或更新库。</p><p>比如<code>pip install tensorflow -i [国内镜像站的地址]</code></p><p>更新时也可以用<code>pip install --upgrade tensorflow -i [国内镜像站的地址]</code></p><p>常用的国内镜像站地址有：</p><ul><li>阿里云 <a href="https://mirrors.aliyun.com/pypi/simple/" rel="external nofollow" target="_blank">https://mirrors.aliyun.com/pypi/simple/</a></li><li>中科大 <a href="https://pypi.mirrors.ustc.edu.cn/simple/" rel="external nofollow" target="_blank">https://pypi.mirrors.ustc.edu.cn/simple/</a></li><li>清华 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/" rel="external nofollow" target="_blank">https://pypi.tuna.tsinghua.edu.cn/simple/</a></li><li>豆瓣 <a href="https://pypi.douban.com/simple/" rel="external nofollow" target="_blank">https://pypi.douban.com/simple/</a></li></ul><p>如果报错，可以试试如下命令：</p><p><code>pip install tensorflow -i [国内镜像站的地址] --trusted-host [国内镜像站官网]</code></p><p>比如：</p><p><code>pip install tensorflow -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇博客主要用以记录在各种环境安装各种库可能会遇到的问题及其解决方案，以便以后应急查询。&lt;/p&gt;
    
    </summary>
    
      <category term="小知识" scheme="http://StepNeverStop.github.io/categories/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="pip" scheme="http://StepNeverStop.github.io/tags/pip/"/>
    
  </entry>
  
  <entry>
    <title>配置阿里云上的服务器</title>
    <link href="http://StepNeverStop.github.io/config-alios.html"/>
    <id>http://StepNeverStop.github.io/config-alios.html</id>
    <published>2020-01-13T01:22:24.000Z</published>
    <updated>2020-01-16T18:04:24.216Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录了在阿里云服务器上配置自己训练环境的过程。</p><a id="more"></a><h1 id="安装Miniconda"><a href="#安装Miniconda" class="headerlink" title="安装Miniconda"></a>安装Miniconda</h1><p><code>wget https://repo.continuum.io/miniconda/Miniconda3-4.6.14-Linux-x86_64.sh</code></p><p><code>sh Miniconda3-4.6.14-Linux-x86_64.sh</code><br>一路<code>ENTER</code>和<code>yes</code></p><p><code>source ~/.bashrc</code></p><p>更新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda update conda</span><br><span class="line">conda update --all</span><br><span class="line">apt-get update</span><br></pre></td></tr></table></figure><h1 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apt-get install git</span><br><span class="line">mkdir ~/keavnn</span><br><span class="line">cd ~/keavnn/</span><br><span class="line">git clone https://github.com/StepNeverStop/RLt.git</span><br><span class="line">conda create -n tf2 python=3.6</span><br><span class="line">conda activate tf2</span><br><span class="line">conda install -y docopt numpy pillow yaml pyyaml pandas openpyxl</span><br><span class="line"></span><br><span class="line">apt-get install apt-file</span><br><span class="line">apt-file update</span><br><span class="line">apt-file search libSM.so.6</span><br><span class="line">apt-get install libsm6</span><br><span class="line">apt-file search libXrender.so.1</span><br><span class="line">apt-get install libxrender1</span><br><span class="line"></span><br><span class="line">pip install protobuf grpcio grpcio-tools tensorflow tensorflow_probability</span><br></pre></td></tr></table></figure><h1 id="安装MuJoCo"><a href="#安装MuJoCo" class="headerlink" title="安装MuJoCo"></a>安装MuJoCo</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/.mujoco</span><br><span class="line">cd .mujoco/</span><br><span class="line">wget url https://www.roboti.us/download/mujoco200_linux.zip</span><br><span class="line">apt-get install zip</span><br><span class="line">unzip mujoco200_linux.zip</span><br><span class="line">wget url https://www.roboti.us/getid/getid_linux</span><br><span class="line">chmod +x getid_linux</span><br><span class="line">./getid_linux</span><br></pre></td></tr></table></figure><p>接着把获取到的计算机ID用于注册30天免费的Mujoco引擎<a href="https://www.roboti.us/license.html" rel="external nofollow" target="_blank">https://www.roboti.us/license.html</a></p><p>把邮箱里的mjkey.txt通过ssh连接或者sftp软件File Zilla放置云服务器上</p><p>放在<code>~/.mujoco</code>与<code>~/.mujoco/mujoco200_linux/bin</code>文件夹下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mv ~/.mujoco/mujoco200_linux ~/.mujoco/mujoco200</span><br><span class="line">apt-get install nano</span><br><span class="line">nano ~/.bashrc</span><br></pre></td></tr></table></figure><p>在尾部添加<code>export LD_LIBRARY_PATH=$HOME/.mujoco/mujoco200/bin</code>，<code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin</code>然后保存退出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br><span class="line">apt install python3-pip</span><br><span class="line">pip install --upgrade pip</span><br></pre></td></tr></table></figure><p>注意：在配置完Git之后：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">conda activate tf2</span><br><span class="line">apt install libosmesa6-dev</span><br><span class="line">apt-get install python3 python-dev python3-dev \</span><br><span class="line">     build-essential libssl-dev libffi-dev \</span><br><span class="line">     libxml2-dev libxslt1-dev zlib1g-dev \</span><br><span class="line">     python-pip libgl1-mesa-dev patchelf libglfw3 libglfw3-dev</span><br><span class="line">pip install fasteners</span><br><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -U &apos;mujoco-py&lt;2.1,&gt;=2.0&apos;</span><br><span class="line"></span><br><span class="line">cd ~/keavnn/RLt/gym</span><br><span class="line">pip install -e &apos;.[all]&apos;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Successfully built mujoco-py</span><br><span class="line">Installing collected packages: mujoco-py</span><br><span class="line">Successfully installed mujoco-py-2.0.2.9</span><br></pre></td></tr></table></figure><p>测试一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ python3</span><br><span class="line">import mujoco_py</span><br><span class="line">import os</span><br><span class="line">mj_path, _ = mujoco_py.utils.discover_mujoco()</span><br><span class="line">xml_path = os.path.join(mj_path, &apos;model&apos;, &apos;humanoid.xml&apos;)</span><br><span class="line">model = mujoco_py.load_model_from_path(xml_path)</span><br><span class="line">sim = mujoco_py.MjSim(model)</span><br><span class="line"></span><br><span class="line">print(sim.data.qpos)</span><br><span class="line"># [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line"></span><br><span class="line">sim.step()</span><br><span class="line">print(sim.data.qpos)</span><br><span class="line"># [-2.09531783e-19  2.72130735e-05  6.14480786e-22 -3.45474715e-06</span><br><span class="line">#   7.42993721e-06 -1.40711141e-04 -3.04253586e-04 -2.07559344e-04</span><br><span class="line">#   8.50646247e-05 -3.45474715e-06  7.42993721e-06 -1.40711141e-04</span><br><span class="line">#  -3.04253586e-04 -2.07559344e-04 -8.50646247e-05  1.11317030e-04</span><br><span class="line">#  -7.03465386e-05 -2.22862221e-05 -1.11317030e-04  7.03465386e-05</span><br><span class="line">#  -2.22862221e-05]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文记录了在阿里云服务器上配置自己训练环境的过程。&lt;/p&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://StepNeverStop.github.io/categories/Docker/"/>
    
    
      <category term="docker" scheme="http://StepNeverStop.github.io/tags/docker/"/>
    
      <category term="conda" scheme="http://StepNeverStop.github.io/tags/conda/"/>
    
  </entry>
  
  <entry>
    <title>在Windows 10系统上安装gym等环境</title>
    <link href="http://StepNeverStop.github.io/install-atari-and-box2d-on-win10.html"/>
    <id>http://StepNeverStop.github.io/install-atari-and-box2d-on-win10.html</id>
    <published>2019-10-17T07:04:58.000Z</published>
    <updated>2019-10-17T14:27:02.384Z</updated>
    
    <content type="html"><![CDATA[<p>在Win 10系统安装gym，atari，Box2D等环境</p><a id="more"></a><h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><ul><li>win 10 专业版</li><li>python 3.6</li></ul><h1 id="安装gym"><a href="#安装gym" class="headerlink" title="安装gym"></a>安装gym</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/openai/gym</span><br><span class="line"><span class="built_in">cd</span> gym</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><h1 id="安装atari"><a href="#安装atari" class="headerlink" title="安装atari"></a>安装atari</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py</span><br></pre></td></tr></table></figure><h1 id="安装box2d"><a href="#安装box2d" class="headerlink" title="安装box2d"></a>安装box2d</h1><p>在<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#pybox2d" rel="external nofollow" target="_blank">https://www.lfd.uci.edu/~gohlke/pythonlibs/#pybox2d</a>下载相应的<code>.whl</code>文件</p><p><img src="./install-atari-and-box2d-on-win10/box2d.png" alt=""></p><p><em>注意分清python版本与操作系统位数</em></p><p>之后再<code>cmd</code>中输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> download_dir</span><br><span class="line">pip install [name].whl</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Win 10系统安装gym，atari，Box2D等环境&lt;/p&gt;
    
    </summary>
    
      <category term="小知识" scheme="http://StepNeverStop.github.io/categories/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
      <category term="gym" scheme="http://StepNeverStop.github.io/tags/gym/"/>
    
  </entry>
  
  <entry>
    <title>Reinforcement Learning with Deep Energy-Based Policies</title>
    <link href="http://StepNeverStop.github.io/rl-with-deep-energy-based-policies.html"/>
    <id>http://StepNeverStop.github.io/rl-with-deep-energy-based-policies.html</id>
    <published>2019-06-26T07:12:39.000Z</published>
    <updated>2019-07-07T10:19:13.227Z</updated>
    
    <content type="html"><![CDATA[<p>本文提出了一个算法，用于学习连续空间下基于能量的策略：SQL，不是数据库的SQL，而是soft Q-Learning。该算法应用了最大熵理论，并且使用能量模型（EBM，Energy-Based Model）作为决策模型。</p><p>推荐阅读该论文：</p><ul><li>公式复杂，但详尽吃透可以学习到SVGD、EBM等概念与算法</li><li>文章充实，可以继续阅读后续算法SAC</li><li>拓展在强化学习与熵进行结合方面的知识</li></ul><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="https://arxiv.org/abs/1702.08165" rel="external nofollow" target="_blank">https://arxiv.org/abs/1702.08165</a></p><p>源代码：<a href="https://github.com/rail-berkeley/softlearning" rel="external nofollow" target="_blank">https://github.com/rail-berkeley/softlearning</a></p><p>该论文与2017年发于第34次ICML会议上，本文对v2版本进行分析。该论文作者为Tuomas Haarnoja，是伯克利大学BAIR实验室的博士生，SAC算法也是他的杰作。</p><p>传统的RL方法主要是用分布拟合单峰分布，即</p><p><img src="./rl-with-deep-energy-based-policies/unimodal-policy.png" alt=""></p><p>也有许多算法想要根据Q函数的值拟合出多峰分布，即</p><p><img src="./rl-with-deep-energy-based-policies/multimodal-policy.png" alt=""></p><p>本文中就是针对拟合多峰分布提出了算法SQL。</p><p>为什么要拟合多峰分布呢？当我们考虑最优控制和概率推理之间的联系时，随机策略才是最优解。</p><blockquote><p>As discussed in prior work, a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference.</p></blockquote><p>随机策略有一些优点：</p><ul><li>如果可以全面地学习给定任务中的目标策略，那么结果策略可以作为很好的初始化策略，微调后以学习更高级的策略</li><li>这种随机的探索机制，可以更好地寻求多峰任务中的最佳决策模型</li><li>更好的鲁棒性，环境有干扰或者噪音时，有多种完成目标的行动可以选择，可以从干扰中“脱身”</li></ul><h2 id="算法效果"><a href="#算法效果" class="headerlink" title="算法效果"></a>算法效果</h2><blockquote><p>The applications of training such stochastic policies include improved exploration in the case of multimodal objectives and compositionality via pretraining general-purpose stochastic policies that can then be efficiently finetuned into task-specific behaviors. </p></blockquote><p>在多峰目标任务中训练随机策略可以提升探索，也可以预训练出通用目的的随机策略以微调后运用至指定任务中进行训练（迁移学习、元学习）。</p><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><h2 id="标准强化学习的最优策略"><a href="#标准强化学习的最优策略" class="headerlink" title="标准强化学习的最优策略"></a>标准强化学习的最优策略</h2><script type="math/tex; mode=display">\pi_{\mathrm{std}}^{*}=\arg \max _{\pi} \sum_{t} \mathbb{E}_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \rho_{\pi}}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \tag{1}</script><ul><li><p><code>std</code>下标代表标准的意思：standard，星号$\ast$代表最优</p></li><li><p>$\rho_{\pi}$代表策略$\pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)$下的迹分布，$\rho_{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$代表状态-行动对的边缘分布。</p></li></ul><blockquote><p>We will also use $\rho_{\pi}\left(\mathbf{s}_{t}\right)$ and $\rho_{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$ to denote the state and state-action marginals of the trajectory distribution induced by a policy $\pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)$. </p></blockquote><h2 id="最大熵强化学习的最优策略"><a href="#最大熵强化学习的最优策略" class="headerlink" title="最大熵强化学习的最优策略"></a>最大熵强化学习的最优策略</h2><p>最大熵强化学习在标准RL的目标函数上加入了一个关于状态下可选动作分布熵的项，这种目标希望智能体不仅能以获得最大奖励的方式完成目标，而且能够决策地尽可能随机。因为通过这种目标函数学到的策略<strong>更具鲁棒性</strong>，可以更好适用于环境的突然变化，或者从前没有遇到过得场景。</p><script type="math/tex; mode=display">\pi_{\mathrm{MaxEnt}}^{*}=\arg \max _{\pi} \sum_{t} \mathbb{E}_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \rho_{\pi}}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\color{red}{\alpha \mathcal{H}\left(\pi\left(\cdot | \mathbf{s}_{t}\right)\right)}\right]\tag{2}</script><ul><li><code>MaxEnt</code>下标代表最大熵的意思：Maximum entropy</li><li>式中的系数$\alpha$可以用来调节奖励项与熵值项的重要性比率。<strong>一般将$\alpha$表示为奖励范围（reward scale）的倒数，但在实际中通常将其作为超参数手动调节。</strong>SAC算法中有介绍在训练过程中自动调节该系数的方法。</li><li>本文中的SQL算法也是为了优化该目标函数</li></ul><h2 id="最大熵目标的优点"><a href="#最大熵目标的优点" class="headerlink" title="最大熵目标的优点"></a>最大熵目标的优点</h2><ul><li>在多峰（即一个状态下有多个最优动作选择）问题中提升探索能力</li><li>可以用于迁移学习，因为其“预训练”模型更好地适应之后的任务</li></ul><h2 id="soft-值函数"><a href="#soft-值函数" class="headerlink" title="soft 值函数"></a>soft 值函数</h2><p>文中，<strong><em>定义</em></strong>了最大熵RL下的Q函数与V函数，注意，是定义，不是推导出来的。</p><p>soft Q函数定义如下：</p><script type="math/tex; mode=display">{Q_{\text { soft }}^{*}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r_{t}+}  {\mathbb{E}_{\left(\mathbf{s}_{t+1}, \ldots\right) \sim \rho_{\pi}}\left[\sum_{l=1}^{\infty} \gamma^{l}\left(r_{t+l}+\alpha \mathcal{H}\left(\pi_{\text { MaxEnt }}^{*}\left(\cdot | \mathbf{s}_{t+l}\right)\right)\right)\right]}\tag{3}</script><p>soft V函数定义如下：</p><script type="math/tex; mode=display">V_{\mathrm{soft}}^{*}\left(\mathbf{s}_{t}\right)=\alpha \log \int_{\mathcal{A}} \exp \left(\frac{1}{\alpha} Q_{\mathrm{soft}}^{*}\left(\mathbf{s}_{t}, \mathbf{a}^{\prime}\right)\right) d \mathbf{a}^{\prime}\tag{4}</script><p>乍一看这个值函数$V_{\mathrm{soft}}^{*}\left(\mathbf{s}_{t}\right)$的形式定义的很奇怪，的确很奇怪，严格来说，它的真实意义并不是为了构造状态值函数，而是构造一个配分函数使得后面推导最优策略时可以化简过程。当然，算法中也不需要用它的值去衡量状态的价值，只是作为计算的中间过程。</p><p>作者说，值函数满足soft 贝尔曼方程，即</p><script type="math/tex; mode=display">Q_{\mathrm{soft}}^{*}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r_{t}+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p_{\mathbf{s}}}\left[V_{\mathrm{soft}}^{*}\left(\mathbf{s}_{t+1}\right)\right]\tag{5}</script><h2 id="能量模型与策略"><a href="#能量模型与策略" class="headerlink" title="能量模型与策略"></a>能量模型与策略</h2><p>文中提出能量模型（Energy-Based Models）的初衷是之前很多人在研究中使用了多项式分布（discrete multinomial distributions）、高斯分布（Gaussian distributions）来表示策略，这样的分布通常用来表示动作价值分布是单峰（unimodal）的情况，而且最终收敛结果往往是接近确定性（near-deterministic）的。即使拓展出多峰的形式，也各自有或多或少的不足。基于此，作者想使用更广泛、通用的分布用来表示复杂、多峰的动作选择。</p><p>所以，作者选择使用基于能量的通用策略：</p><script type="math/tex; mode=display">\pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) \propto \exp \left(-\mathcal{E}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\tag{6}</script><ul><li><p>$\mathcal{E}$是字母E的花体形式，代表能量函数，其可以被深度神经网络表示，如果使用通用值函数近似来表示能量函数，那么可以表示任意策略$\pi\left(\boldsymbol{a}_{t} | \mathbf{s}_{t}\right)$</p></li><li><blockquote><p>where $\mathcal{E}$ is an energy function that could be represented, for example, by a deep neural network. If we use a universal function approximator for $\mathcal{E}$, we can represent any distribution $\pi\left(\boldsymbol{a}_{t} | \mathbf{s}_{t}\right)$. </p></blockquote></li><li><p>文中将该能量函数设置为</p><script type="math/tex; mode=display">\mathcal{E}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=-\frac{1}{\alpha} Q_{\operatorname{soft}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\tag{7}</script><p>其实也很容易就能理解，将负号抵消掉之后，Q值大的动作能量高嘛，指数分布又能更好的放大较大的值，使Q值大的动作更为突出，这样完全可以作为选择动作的策略</p></li></ul><p>但是有一个问题是，不能使能量无限大呀，假如超过了计算能力那就不好了，当然这种情况几乎不会发生。于是，可以将能量给归一化，即</p><script type="math/tex; mode=display">\begin{aligned} \pi_{\text { MaxEnt }}^{*}\left(a_{t} | s_{t}\right) &=\frac{\exp \left(\frac{1}{\alpha} Q_{\text { soft }}^{*}\left(s_{t}, a_{t}\right)\right)}{\int_{\mathcal{A}} \exp \left(\frac{1}{\alpha} Q_{\text { soft }}^{*}\left(s_{t}, a_{t}\right)\right) \mathrm{d} a^{\prime}} \\ &=\frac{\exp \left(\frac{1}{\alpha} Q_{\text { soft }}^{*}\left(s_{t}, a_{t}\right)\right)}{\exp \left(\frac{1}{\alpha} V_{\text { soft }}^{*}\left(s_{t}\right)\right)} \\&=\frac{\exp \left(\frac{1}{\alpha} Q_{\text { soft }}^{*}\left(s_{t}, a_{t}\right)\right)}{\color{blue}{\exp \log} \exp \left(\frac{1}{\alpha} V_{\text { soft }}^{*}\left(s_{t}\right)\right)} \\&=\color{red}{\exp \left(\frac{1}{\alpha}\left(Q_{\text { soft }}^{*}\left(s_{t}, a_{t}\right)-V_{\text { soft }}^{*}\left(s_{t}\right)\right)\right)}\end{aligned}\tag{8}</script><p></p><p align="center" style="color:blue"><a href="https://bluefisher.github.io/2018/11/13/Reinforcement-Learning-with-Deep-Energy-Based-Policies/" rel="external nofollow" target="_blank">BlueFisher's Blog</a></p><br>论文中只给出了红色字体的部分，其实这才是作者想要表达的意思，文中就是基于此定义了状态值函数$V_{\mathrm{soft}}^{*}\left(\mathbf{s}_{t}\right)$的形式。<p></p><p>在这个公式中就可以看出，策略是对动作值函数Q进行了一个softmax操作，这也是文中soft的含义。</p><h2 id="使用SQL优化目标函数"><a href="#使用SQL优化目标函数" class="headerlink" title="使用SQL优化目标函数"></a>使用SQL优化目标函数</h2><p>像使用Q-Learning对网格世界问题进行优化求解一样，我们也可以使用迭代的方式进行优化，交互计算两个值函数，使其各自收敛，就可以导出最优策略。</p><p>于是，作者定义了soft Q-Iteration。</p><h3 id="Soft-Q-Iteration"><a href="#Soft-Q-Iteration" class="headerlink" title="Soft Q-Iteration"></a>Soft Q-Iteration</h3><p>先要假设值函数$Q_{\mathrm{soft}}(\cdot, \cdot)$、$V_{\text { soft }}(\cdot)$有界，</p><script type="math/tex; mode=display">\int_{\mathcal{A}} \exp \left(\frac{1}{\alpha} Q_{\mathrm{soft}}\left(\cdot, \mathbf{a}^{\prime}\right)\right) d \mathbf{a}^{\prime}<\infty \ ，\ Q_{\mathrm{soft}}^{*}<\infty\tag{9}</script><p>文中定义的交互迭代至收敛的方式其实跟SARSA算法比较像：</p><script type="math/tex; mode=display">\begin{array}{c}{Q_{\text { soft }}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \leftarrow r_{t}+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p_{\mathbf{s}}}\left[V_{\text { soft }}\left(\mathbf{s}_{t+1}\right)\right], \forall \mathbf{s}_{t}, \mathbf{a}_{t}} \\ {V_{\text { soft }}\left(\mathbf{s}_{t}\right) \leftarrow \alpha \log \int_{\mathcal{A}} \exp \left(\frac{1}{\alpha} Q_{\text { soft }}\left(\mathbf{s}_{t}, \mathbf{a}^{\prime}\right)\right) d \mathbf{a}^{\prime}, \forall \mathbf{s}_{t}}\end{array}\tag{10}</script><p>这种优化方式在理论上是可行的，但是在实际应用中存在两个问题：</p><ol><li>连续空间无法求期望，或者计算不准确。<strong>解决方案是重要性采样，使用采样多次后计算来代替积分，在初期进行随机均匀采样，后期根据policy来采样。</strong></li><li>迭代过程需要不断选择动作，问题是式（8）的分布形式无法进行采样。<strong>解决方案是使用SVGD算法拟合后验分布，并输出采样的动作。</strong></li></ol><h3 id="Soft-Q-Learning"><a href="#Soft-Q-Learning" class="headerlink" title="Soft Q-Learning"></a>Soft Q-Learning</h3><p>文中在这一部分引用了重要性采样，解决了上面提到的第一个问题，即，使用分布$q_{\mathrm{a}^{\prime}}$来代替真实策略分布</p><script type="math/tex; mode=display">\exp \left(\frac{1}{\alpha}\left(Q_{\text { soft }}^{*}\left(s_{t}, a_{t}\right)-V_{\text { soft }}^{*}\left(s_{t}\right)\right)\right)</script><p>进行采样。</p><script type="math/tex; mode=display">V_{\mathrm{soft}}^{\theta}\left(\mathbf{s}_{t}\right)=\alpha \log \mathbb{E}_{\color{red}{q_{\mathrm{a}^{\prime}}}}\left[\frac{\exp \left(\frac{1}{\alpha} Q_{\mathrm{soft}}^{\theta}\left(\mathbf{s}_{t}, \mathbf{a}^{\prime}\right)\right)}{q_{\mathrm{a}^{\prime}}\left(\mathbf{a}^{\prime}\right)}\right]\tag{11}</script><p>采样分布$q_{\mathrm{a}^{\prime}}$可以使用任意的分布，但是由于重要性采样的性质，采样分布与原分布越接近，效果越好。式子中的$\theta$为Q神经网络的参数。</p><p>因为在训练初期，我们估计的真实分布是偏差很大的，几乎可以说是错误的，因此在训练初期将采样分布设置为均匀分布比较合理，在训练一段时间之后，可以将采样分布设置为接近原分布，甚至是原分布（如果原分布可以采样，如，使用神经网络等“黑匣子”进行表示）</p><p>由此，可以定义Q神经网络的损失函数为：</p><script type="math/tex; mode=display">J_{Q}(\theta)=\mathbb{E}_{\mathbf{s}_{t} \sim q_{\mathbf{s}_{t}}, \mathbf{a}_{t} \sim q_{\mathbf{a}_{t}}}\left[\frac{1}{2}\left(\hat{Q}_{\mathrm{soft}}^{\overline{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-Q_{\mathrm{soft}}^{\theta}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)^{2}\right]\tag{12}</script><p>上式中期望的下标为环境和真实策略分布，$\overline{\theta}$代表target网络的参数，目标是最小化这个损失函数，其中，</p><script type="math/tex; mode=display">\hat{Q}_{\mathrm{soft}}^{\overline{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r_{t}+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p_{\mathbf{s}}}\left[V_{\mathrm{soft}}^{\overline{\theta}}\left(\mathbf{s}_{t+1}\right)\right]\tag{13}</script><h3 id="近似采样与SVGD"><a href="#近似采样与SVGD" class="headerlink" title="近似采样与SVGD"></a>近似采样与SVGD</h3><p>SVGD：Stein Vairational Gradient Descent，SVGD是一种确定性的、基于梯度的近似推理采样算法。</p><p>文中在这一部分引用了SVGD的优化算法，并且使用SVGD近似策略的后验分布以进行采样，解决了上文提到的第二个问题。在百度上完全搜不到关于SVGD算法的信息，但是了解了这个算法之后，感觉它的能力还是很强的，最终，搜集了Google、Bing的检索结果，发现了原作者在SVGD方法上的一些资源分享，<a href="https://www.cs.utexas.edu/~lqiang/stein.html" rel="external nofollow" target="_blank">Stein’s Method for Practical Machine Learning</a></p><p>对于基于能量的模型、分布，有两类采样方式：</p><ol><li>MCMC采样，即马尔科夫链蒙特卡洛采样</li><li>学习一个采样网络去近似采样出符合目标分布的样本</li></ol><p>在需要不断更新策略的在线学习任务中，使用MCMC采样是不可行的，于是作者使用了基于SVGD和Amortized SVGD的采样网络。</p><p>SVGD论文：<a href="https://arxiv.org/abs/1608.04471" rel="external nofollow" target="_blank">https://arxiv.org/abs/1608.04471</a></p><p>Amortized SVGD论文：<a href="https://arxiv.org/abs/1707.06626" rel="external nofollow" target="_blank">https://arxiv.org/abs/1707.06626</a></p><p><img src="./rl-with-deep-energy-based-policies/1dgmm.gif" alt=""><br><img src="./rl-with-deep-energy-based-policies/vp.gif" alt=""></p><p>Amortized SVGD有一些有趣的性质：</p><ul><li>可以训练随机采样网络非常快地采样</li><li>可以准确收敛至EBM能量模型的后验估计分布</li><li>文中结合了Amortized SVGD后，算法形式很像A-C模式</li></ul><p>SVGD算法的更新形式是这样的，</p><script type="math/tex; mode=display">x_{i} \leftarrow x_{i}+\frac{\epsilon}{n} \sum_{j=1}^{n}\left[k\left(x_{j}, x_{i}\right) \nabla_{x_{j}} \log p\left(x_{j}\right)+\nabla_{x_{j}} k\left(x_{j}, x_{i}\right)\right], \qquad \forall i=1, \ldots, n\tag{14}</script><ul><li>$\epsilon$代表学习率</li><li>$k\left(x_{j}, x_{i}\right)$代表正定核，如径向基（RBF，Radial Basis Function）函数$k\left(x, x^{\prime}\right)=\exp \left(-\frac{1}{h}\left|x-x^{\prime}\right|_{2}^{2}\right)$，它可以被认为是变量之间的相似性度量</li><li>包含对数项$\log p\left(x_{j}\right)$的梯度驱使采样器朝着$p(x)$分布中高概率区域进行采样</li><li>第二项核函数梯度驱使样本点之间产生间隙，相当于用一个排斥力使样本点尽可能分散开</li><li>对数项梯度不依赖分布$p(x)$的归一化常数，使SVGD易于应用于图模型、贝叶斯推理和深层生成模型中出现的难以处理的分布。</li></ul><p>先定义我们采样网络（其实就是Actor）的目标函数：</p><script type="math/tex; mode=display">J_{\pi}\left(\phi ; \mathbf{s}_{t}\right)=D_{\mathrm{KL}}\left(\pi^{\phi}\left(\cdot | \mathbf{s}_{t}\right) \| \exp \left(\frac{1}{\alpha}\left(Q_{\mathrm{soft}}^{\theta}\left(\mathbf{s}_{t}, \cdot\right)-V_{\mathrm{soft}}^{\theta}\right)\right)\right)\tag{15}</script><ul><li>$\phi$表示采样网络中的参数</li><li>将产生动作的函数简写成$\mathbf{a}_{t}^{(i)}=f^{\phi}\left(\xi^{(i)} ; \mathbf{s}_{t}\right)$，也就是说神经网络的输入分为两部分，一部分是状态$s$，一部分是噪声扰乱“perturb”$\xi$，一般从标准正态分布中采样，而且最好使噪声的维度与动作的维度一致</li></ul><p>计算梯度方向：</p><script type="math/tex; mode=display">\begin{aligned}\Delta f^{\phi}\left(\cdot ; \mathbf{s}_{t}\right)=& \mathbb{E}_{\mathbf{a}_{t} \sim \pi^{\phi}}\left[\kappa\left(\mathbf{a}_{t}, f^{\phi}\left(\cdot ; \mathbf{s}_{t}\right)\right) \nabla_{\mathbf{a}^{\prime}} Q_{\mathrm{soft}}^{\theta}\left.\left(\mathbf{s}_{t}, \mathbf{a}^{\prime}\right)\right|_{\mathbf{a}^{\prime}=\mathbf{a}_{t}}\right.\\ &+\alpha \nabla_{\mathbf{a}^{\prime}} \kappa\left.\left(\mathbf{a}^{\prime}, f^{\phi}\left(\cdot ; \mathbf{s}_{t}\right)\right)\right|_{\mathbf{a}^{\prime}=\mathbf{a}_{t}} ] \end{aligned}\tag{16}</script><ul><li><p>严格来说，$\Delta f^{\phi}$是希尔伯特空间的最优梯度方向，并不是Actor目标函数$J_{\pi}$的梯度</p></li><li><blockquote><p>To be precise, $\Delta f^{\phi}$ is the optimal direction in the reproducing kernel Hilbert space of $\kappa$, and is thus not strictly speaking the gradient of $J_{\pi}\left(\phi ; \mathbf{s}_{t}\right)$ </p></blockquote></li></ul><p>根据链式法则，Stein变分梯度SVG为</p><script type="math/tex; mode=display">\frac{\partial J_{\pi}\left(\phi ; \mathbf{s}_{t}\right)}{\partial \phi} \propto \mathbb{E}_{\xi}\left[\Delta f^{\phi}\left(\xi ; \mathbf{s}_{t}\right) \frac{\partial f^{\phi}\left(\xi ; \mathbf{s}_{t}\right)}{\partial \phi}\right]\tag{17}</script><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><p><img src="./rl-with-deep-energy-based-policies/pseudo.png" alt=""></p><p>解析：</p><p>算法中更新Actor网络时，其实是使用了如下梯度公式：</p><script type="math/tex; mode=display">\hat{\nabla}_{\phi} J_{\pi}\left(\phi ; \mathbf{s}_{t}\right)=\frac{1}{K M} \sum_{j=1}^{K} \sum_{i=1}^{M}\left(\kappa\left(\mathbf{a}_{t}^{(i)}, \tilde{\mathbf{a}}_{t}^{(j)}\right) \nabla_{\mathbf{a}^{\prime}} Q_{\mathrm{soft}}\left.\left(\mathbf{s}_{t}, \mathbf{a}^{\prime}\right)\right|_{\mathbf{a}^{\prime}=\mathbf{a}_{i}^{(i)}}+\nabla_{\mathbf{a}^{\prime}} \kappa\left.\left(\mathbf{a}^{\prime}, \tilde{\mathbf{a}}_{t}^{(j)}\right)\right|_{\mathbf{a}^{\prime}=\mathbf{a}_{i}^{(i)}}\right) \nabla_{\phi} f^{\phi}\left(\tilde{\xi}^{(j)} ; \mathbf{s}_{t}\right)\tag{18}</script><p>更新方向为mini-batch经验的梯度平均值，而不是累加和</p><ul><li>伪代码中定义了Actor的target网络，参数为$\overline{\theta}$。但是伪代码中并没有显示出其在何处使用，我<strong>猜测</strong>该网络代表采样分布$q_{\mathbf{a}^{\prime}}$<ul><li>$q_{\mathbf{a}^{\prime}}$在训练初期使用均匀分布</li><li>$q_{\mathbf{a}^{\prime}}$在一段时间之后使用Actor真实分布，我猜测这里使用的就是Actor目标网络</li></ul></li><li>噪音$\xi$从多维标准正态分布中采样，维度最好与动作空间维度一致</li><li>$\left\{\mathbf{a}^{(i, j)}\right\}_{j=0}^{M} \sim q_{\mathbf{a}^{\prime}}$其中的M用于设置采样多少个样本，以使用公式（11）计算V值，使用的网络为Q目标网络</li><li>在更新Q网络时，使用了从经验池采样到的真实执行过的动作</li><li>$\left\{\xi^{(i, j)}\right\}_{j=0}^{M} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})$中其实少写了一个参数$K$，但实际上$K=M$，在这一步中需要采样两组噪音，当然也可以采样一组，使用两次。</li><li>在更新Actor网络时，没有使用经验池中采样到的动作，而模拟采样了两组动作，即根据两组噪音生成的动作，用它们来计算梯度并更新。</li><li>Q网络的输入为状态与动作的连接，$(s||a)$，输出为Q值</li><li>Actor网络的输入为状态与噪音的连接，$(s||\xi)$，输出为动作$a$</li><li>伪代码中的式(10)、(11)、(13)、(14)分别代表本文中的式(11)、(12)、(16)、(17)</li></ul><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li>比较算法：DDPG vs SQL</li><li>Actor和Q网络使用Adam优化器</li><li>Actor学习率为0.0001，Q网络学习率为0.001</li><li>经验池大小为100W</li><li>经验池填充1W条经验后开始训练</li><li>batch_size=64</li><li>Actor和Q网络都是2层隐藏层，每层200个隐藏节点，激活函数为ReLU</li><li><p>DDPG和SQL都使用了Ornstein-Uhlenbeck随机过程产生噪音来增加探索，它是一种序贯相关的随机过程，$\theta=0.15 \ , \ \sigma=0.3$</p><ul><li><p>OU随机过程可以在序贯模型中添加与时间相关的随机噪音，而且噪音也满足强马尔可夫性</p></li><li><p>形式为$d x_{t}=\theta\left(\mu-x_{t}\right) d t+\sigma d W_{t}$，是一个具有均值恢复属性的随机过程</p></li><li><p>$\theta$表示变量$x$以多大幅度、多块恢复到平均值，$\mu$代表平均值，$\sigma$代表波动程度，$d W_{t}$代表维纳过程，一般通过高斯分布实现</p></li><li><p>OU随机过程产生的噪音只与上一次产生的噪音相关，它可以用于增加探索，也能够柔顺控制。比如在相邻的两个决策动作，一个为10，一个为-10，反复如此，智能体会产生震荡。在此使用OU过程可以使智能体在一个方向保持一定时间，不会瞬间过大地改变智能体的状态，相当于增加了时滞性。</p></li><li><p>代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = 10</span><br><span class="line">dx = theta * (mu - x) + sigma * numpy.random.randn(len(x))</span><br><span class="line">x = x + dx</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>参考：</p><ul><li><a href="https://www.quora.com/Why-do-we-use-the-Ornstein-Uhlenbeck-Process-in-the-exploration-of-DDPG" rel="external nofollow" target="_blank">https://www.quora.com/Why-do-we-use-the-Ornstein-Uhlenbeck-Process-in-the-exploration-of-DDPG</a></li><li><a href="https://github.com/floodsung/DDPG-tensorflow/blob/master/ou_noise.py" rel="external nofollow" target="_blank">https://github.com/floodsung/DDPG-tensorflow/blob/master/ou_noise.py</a></li><li><a href="https://zhuanlan.zhihu.com/p/51333694" rel="external nofollow" target="_blank">https://zhuanlan.zhihu.com/p/51333694</a></li><li>核函数使用了径向基函数RBF，$\kappa\left(\mathbf{a}, \mathbf{a}^{\prime}\right)=\exp \left(-\frac{1}{h}\left|\mathbf{a}-\mathbf{a}^{\prime}\right|_{2}^{2}\right)$，其中，$h=\frac{d}{2 \log (M+1)}$，$d$为各变量对之间距离的中位数</li><li>目标网络的更新采样硬覆盖的模式</li><li>超参数$\alpha$根据任务设置为10，0.1等等</li><li>训练的epoch、步长、系数$\alpha$，采样动作的数量$K 和 M$根据任务（多目标，单目标，微调）的不同而不同，具体请看原论文附录D部分</li></ul></li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>未完待续</p><h1 id="个人感想"><a href="#个人感想" class="headerlink" title="个人感想"></a>个人感想</h1><p>虽然文中用大量公式、篇幅结合最大熵进行介绍、推理，但是在伪代码以及目标函数中似乎并没有看到关于熵的影子，包含熵项的Q值也是通过Q网络的输出将熵的值包含在内，并没有显式地计算它。</p><p>虽然算法的名字为soft Q-Learning，但其实它跟传统的Q-Learning算法思想并不相同，如果说有一点相同，那也是都是想使Q值收敛以推导出最优策略，但是这个优化过程也跟SARSA算法比较像，并没有使用传统Q-Learning中贪婪的选择最有价值下一个动作以自举的方法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文提出了一个算法，用于学习连续空间下基于能量的策略：SQL，不是数据库的SQL，而是soft Q-Learning。该算法应用了最大熵理论，并且使用能量模型（EBM，Energy-Based Model）作为决策模型。&lt;/p&gt;
&lt;p&gt;推荐阅读该论文：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;公式复杂，但详尽吃透可以学习到SVGD、EBM等概念与算法&lt;/li&gt;
&lt;li&gt;文章充实，可以继续阅读后续算法SAC&lt;/li&gt;
&lt;li&gt;拓展在强化学习与熵进行结合方面的知识&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Maximum Entropy-Regularized Multi-Goal Reinforcement-Learning</title>
    <link href="http://StepNeverStop.github.io/maximum-entropy-regularized-multi-goal-reinforcement-learning.html"/>
    <id>http://StepNeverStop.github.io/maximum-entropy-regularized-multi-goal-reinforcement-learning.html</id>
    <published>2019-06-12T12:49:32.000Z</published>
    <updated>2019-07-07T10:19:13.194Z</updated>
    
    <content type="html"><![CDATA[<p>这篇论文将强化学习的目标与最大熵结合了起来，提出了简称为MEP的经验池机制。许多将熵与强化学习结合的方法都是考虑可选动作分布的熵，该篇论文很新颖的使用的是“迹”的熵。</p><p>推荐程度中等偏下：</p><ul><li>有些地方解释的不是很清楚</li><li>熵的结合方式特殊，可以一看</li><li>有些公式推导过于复杂，难懂</li><li>有些参考文献标注不准，如A3C算法的论文并没有使用熵的概念，却在熵相关的语句进行了标注</li></ul><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="https://arxiv.org/pdf/1905.08786.pdf" rel="external nofollow" target="_blank">https://arxiv.org/pdf/1905.08786.pdf</a></p><p>原作者代码地址：<a href="https://github.com/ruizhaogit/mep.git" rel="external nofollow" target="_blank">https://github.com/ruizhaogit/mep.git</a></p><p>该文章发于2019年的ICML，与之前写过的《Energy-Based Hindsight Experience Prioritization》为同一作者。</p><p>本文主要做了三个贡献：</p><ol><li>修改了目标函数，提出了最大熵正则化多目标强化学习(Maximum Entropy-Regularized Multi-Goal RL)的想法</li><li>推导出替代(surrogate)目标函数，是第1步目标函数的一个下界，可以使算法稳定优化</li><li>提出了Maximum Entropy-based Prioritization(MEP)的经验池框架</li></ol><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><p>文中目标函数的构造主要受Guiasu于1971年提出的加权熵(Weighted Entropy)启发。加权熵表示为：</p><script type="math/tex; mode=display">\mathcal{H}_{p}^{w}=-\sum_{k=1}^{K} w_{k} p_{k} \log p_{k}</script><p>$w_{k}$表示权重。</p><h2 id="多目标强化学习的符号表示"><a href="#多目标强化学习的符号表示" class="headerlink" title="多目标强化学习的符号表示"></a>多目标强化学习的符号表示</h2><p>用$p\left(\boldsymbol{\tau} | g^{e}, \boldsymbol{\theta}\right)$表示一个迹出现的概率：</p><ul><li>$\boldsymbol{\tau}=s_{1}, a_{1}, s_{2}, a_{2}, \ldots, s_{T-1}, a_{T-1}, s_{T}$代表迹</li><li>$g^{e}$代表一个真实的目标，即不是“事后诸葛亮”假定的目标，$g^{e} \in \operatorname{Val}\left(G^{e}\right)$，后一项为目标空间，一般情况下，可以视为状态空间$\mathcal{S}$的子集</li><li>$\theta$代表策略参数</li></ul><p>展开来写，一个迹在策略$\theta$被采样到的概率为</p><script type="math/tex; mode=display">p\left(\boldsymbol{\tau} | g^{e}, \boldsymbol{\theta}\right)=p\left(s_{1}\right) \prod_{t=1}^{T-1} p\left(a_{t} | s_{t}, g^{e}, \boldsymbol{\theta}\right) p\left(s_{t+1} | s_{t}, a_{t}\right)</script><p>由此定义策略$\theta$下的期望奖励回报，也就是目标函数，表示为</p><script type="math/tex; mode=display">\begin{aligned} \eta(\boldsymbol{\theta}) &=\mathbb{E}\left[\sum_{t=1}^{T} r\left(S_{t}, G^{e}\right) | \boldsymbol{\theta}\right] \\ &=\sum_{g^{e}} p\left(g^{e}\right) \sum_{\boldsymbol{\tau}} p\left(\boldsymbol{\tau} | g^{e}, \boldsymbol{\theta}\right) \sum_{t=1}^{T} r\left(s_{t}, g^{e}\right) \end{aligned}</script><p>很容易理解，也就是在传统的目标函数前边加了一项关于每个目标求积分的步骤。</p><p>如果使用off-policy算法且用经验池机制来提升采样效率，那么目标函数为</p><script type="math/tex; mode=display">\eta^{\mathcal{R}}(\boldsymbol{\theta})=\sum_{\boldsymbol{\tau}, g^{e}} p_{\mathcal{R}}\left(\boldsymbol{\tau}, g^{e} | \boldsymbol{\theta}\right) \sum_{t=1}^{T} r\left(s_{t}, g^{e}\right)</script><p>其中$\mathcal{R}$表示经验池。注意，$\eta^{\mathcal{R}}(\boldsymbol{\theta})$将$\eta(\boldsymbol{\theta})$的前两个积分项合在一起写了，所以是联合概率而不是条件概率。</p><h2 id="最大熵正则化目标函数"><a href="#最大熵正则化目标函数" class="headerlink" title="最大熵正则化目标函数"></a>最大熵正则化目标函数</h2><p>将前文提到的$\eta(\boldsymbol{\theta})$与加权熵结合起来，就构造出了本文中新的目标函数，即</p><script type="math/tex; mode=display">\begin{aligned} \eta^{\mathcal{H}}(\boldsymbol{\theta}) &=\mathcal{H}_{p}^{w}\left(\mathcal{T}^{g}\right) \\ &=\mathbb{E}_{p}\left[\color{red} {\log \frac{1}{p\left(\boldsymbol{\tau}^{g}\right)}} \sum_{t=1}^{T} r\left(S_{t}, G^{e}\right) | \boldsymbol{\theta}\right] \end{aligned}</script><p>$\color{red} {p\left(\boldsymbol{\tau}^{g}\right)}$代表$\sum_{g^{e}} p_{\mathcal{R}}\left(\tau^{g}, g^{e} | \boldsymbol{\theta} \right )$，期望右下角的$p$也是$p\left(\boldsymbol{\tau}^{g}\right)$的意思。</p><p><strong>与传统的结合方式不同的是，这种结合方式并没有将熵作为一个加和的项，而是相乘。</strong></p><p>仔细想一下，如果将这个式子视为加权熵，那么权重系数是累计奖励$\sum_{t=1}^{T} r\left(s_{t}, g^{e}\right)$，这样做的直观解释是：<strong>对于各种各样的迹，给与累计回报大的以更多权重，使算法直到要朝哪个迹的方向优化。</strong></p><p>反而，如果将前一个对数项视为传统强化学习目标函数的权重系数，那么这样的直观解释是：<strong>迹出现的概率越低，就越新颖，反而要使其权重增加，从而驱使算法向探索的方向优化。</strong></p><h2 id="替代目标函数"><a href="#替代目标函数" class="headerlink" title="替代目标函数"></a>替代目标函数</h2><p>文中指出，$\eta^{\mathcal{H}}(\boldsymbol{\theta}) $中的$\log \frac{1}{p\left(\boldsymbol{\tau}^{g}\right)}$这一项是无界的，即取值范围为$[0,+\infty]$，这会导致通用值函数近似的训练不稳定，因此提出了可靠地替代目标函数$\eta^{\mathcal{L}}(\boldsymbol{\theta})$，这个目标函数是原目标函数的一个下界。</p><blockquote><p>the weight, $\log \left(1 / p\left(\boldsymbol{\tau}^{g}\right)\right)$, is unbounded, which makes the training of the universal function approximator unstable. </p></blockquote><p>表示为</p><script type="math/tex; mode=display">\eta^{\mathcal{L}}(\boldsymbol{\theta})=Z \cdot \mathbb{E}_{\color{red}{q}}\left[\sum_{t=1}^{T} r\left(S_{t}, G^{e}\right) | \boldsymbol{\theta}\right]</script><p>注意，期望积分的是迹分布函数$q$，而不是经验池中真实的迹分布函数$p$。</p><p>那么，新的迹分布函数$q\left(\boldsymbol{\tau}^{g}\right)$是怎么得来的？</p><p>首先，使用Latent Varibale Model(LVM)对$p\left(\boldsymbol{\tau}^{g}\right)$的潜在分布进行建模，因为LVM适合于对复杂的分布进行建模。</p><blockquote><p>We use a Latent Variable Model(LVM) (Murphy, 2012) to model the underlying distribution of $p\left(\boldsymbol{\tau}^{g}\right)$, since LVM is suitable for modeling complex distributions.</p></blockquote><p>将分布用混合高斯模型MoG表示，</p><script type="math/tex; mode=display">p\left(\boldsymbol{\tau}^{g} | \boldsymbol{\phi}\right)=\frac{1}{Z} \sum_{i=k}^{K} c_{k} \mathcal{N}\left(\boldsymbol{\tau}^{g} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)</script><p>其中，</p><ul><li>$K$为隐变量的个数</li><li>$\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}$分布为均值与协方差矩阵</li><li>$c_{k}$为混合系数</li><li>$Z$为配分函数（归一化系数）</li><li>$\phi$为模型参数，包含所有的均值、协方差矩阵和混合系数</li></ul><p>文中接着使用$p\left(\boldsymbol{\tau}^{g} | \boldsymbol{\phi}\right)$的补作为经验重放的优先级，即</p><script type="math/tex; mode=display">\overline{p}\left(\boldsymbol{\tau}^{g} | \boldsymbol{\phi}\right) \propto 1-p\left(\boldsymbol{\tau}^{g} | \boldsymbol{\phi}\right)</script><p>补越大，就代表迹出现的概率越低，那么就对该迹赋予更大的优先级。作者想通过过采样这些迹来增大训练时迹分布的熵。直观的解释就是，概率分布不均匀，那么让概率大的出现次数少，概率小的出现次数多，这样就会使采样的分布朝着均匀分布的方向移动，从而使熵值增加。</p><p>文中由此引出了新的迹分布$q\left(\boldsymbol{\tau}^{g}\right)$，它表示为原始分布与其补的联合分布，</p><script type="math/tex; mode=display">\begin{aligned} q\left(\boldsymbol{\tau}^{g}\right) & \propto \overline{p}\left(\boldsymbol{\tau}^{g} | \boldsymbol{\phi}\right) p\left(\boldsymbol{\tau}^{g}\right) \\ & \propto\left(1-p\left(\boldsymbol{\tau}^{g} | \boldsymbol{\phi}\right)\right) p\left(\boldsymbol{\tau}^{g}\right) \\ & \approx p\left(\boldsymbol{\tau}^{g}\right)-p\left(\boldsymbol{\tau}^{g}\right)^{2} \end{aligned}</script><p>文中将正比的比例设置为$\color{blue}{\frac{1}{Z}}$，即$q\left(\boldsymbol{\tau}^{g}\right)=\frac{1}{Z} p\left(\boldsymbol{\tau}^{g}\right)\left(1-p\left(\boldsymbol{\tau}^{g}\right)\right)$，接下来就可以证明$\eta^{\mathcal{L}}(\boldsymbol{\theta})$为$\eta^{\mathcal{H}}(\boldsymbol{\theta}) $的下界：</p><script type="math/tex; mode=display">\begin{aligned} \eta^{\mathcal{L}}(\boldsymbol{\theta}) &=Z \cdot \mathbb{E}_{q}\left[\sum_{t=1}^{T} r\left(S_{t}, G^{e}\right) | \boldsymbol{\theta}\right] \\ &=\sum_{\boldsymbol{\tau}^{g}} Z \cdot q\left(\boldsymbol{\tau}^{g}\right) \sum_{t=1}^{T} r\left(s_{t}, g^{e}\right) \\&=\sum_{\tau^{g}} \frac{Z}{Z} p\left(\tau^{g}\right)\left(1-p\left(\tau^{g}\right)\right) \sum_{t=1}^{T} r\left(s_{t}, g^{e}\right) \\&<\sum_{\tau^{g}}-p\left(\boldsymbol{\tau}^{g}\right) \log p\left(\boldsymbol{\tau}^{g}\right) \sum_{t=1}^{T} r\left(s_{t}, g^{e}\right) \\&=\mathbb{E}_{p}\left[\log \frac{1}{p\left(\boldsymbol{\tau}^{g}\right)} \sum_{t=1}^{T} r\left(S_{t}, G^{e}\right) | \boldsymbol{\theta}\right] \\&=\mathcal{H}_{p}^{w}\left(\mathcal{T}^{g}\right) \\&=\eta^{\mathcal{H}}(\boldsymbol{\theta})\end{aligned}</script><p>在小于号不等式那一步，使用了函数的性质：$\log x&lt;x-1$。很容易可以画出$f(x)=\ln{x}-x+1$在区间[0,1]上的图像：</p><p><img src="./maximum-entropy-regularized-multi-goal-reinforcement-learning/inequality.png" alt=""></p><p>文中进一步证明了$q$分布的熵比$p$分布的熵更大，因此，由$q$分布来进行采样可以使采样更均匀（= =！那么直接使用均匀分布采样不更好吗？），使学习的目标更多样化。证明过程篇幅过长，详见原论文附录，</p><script type="math/tex; mode=display">p\left(\boldsymbol{\tau}^{g}\right), \text { where } p\left(\boldsymbol{\tau}_{i}^{g}\right) \in(0,1) \text { and } \sum_{i=1}^{N} p\left(\boldsymbol{\tau}_{i}^{g}\right)=1</script><script type="math/tex; mode=display">q\left(\boldsymbol{\tau}_{i}^{g}\right)=\frac{1}{Z} p\left(\boldsymbol{\tau}_{i}^{g}\right)\left(1-p\left(\boldsymbol{\tau}_{i}^{g}\right)\right), \text { where } \sum_{i=1}^{N} q\left(\boldsymbol{\tau}_{i}^{g}\right)=1</script><script type="math/tex; mode=display">\mathcal{H}_{q}\left(\mathcal{T}^{g}\right)-\mathcal{H}_{p}\left(\mathcal{T}^{g}\right) \geq 0</script><h2 id="基于最大熵的优先级"><a href="#基于最大熵的优先级" class="headerlink" title="基于最大熵的优先级"></a>基于最大熵的优先级</h2><p>文中说是基于最大熵的优先经验回放，但是除了使用$q\left(\boldsymbol{\tau}^{g}\right)$分布之外，在这一部分没有体现出熵的影子，优先级的设置为</p><script type="math/tex; mode=display">q\left(\boldsymbol{\tau}_{i}^{g}\right)=\frac{\operatorname{rank}\left(q\left(\boldsymbol{\tau}_{i}^{g}\right)\right)}{\sum_{n=1}^{N} \operatorname{rank}\left(q\left(\boldsymbol{\tau}_{n}^{g}\right)\right)}</script><p>使用排序作为衡量优先级的标准是因为这种方式更具鲁棒性，对异常值不敏感。</p><p>之前文中说想要使出现概率低的迹以更高的概率被从经验池中采样到，如果是按照新的$q$分布来定义迹出现的概率，那么到这里是有些说不通的，因为，前文提到$q\left(\boldsymbol{\tau}^{g}\right) \approx p\left(\boldsymbol{\tau}^{g}\right)-p\left(\boldsymbol{\tau}^{g}\right)^{2} $，那么画出$f(x)=x-x^2$在区间[0，1]上的图像为：</p><p><img src="./maximum-entropy-regularized-multi-goal-reinforcement-learning/x-x^2.png" alt=""></p><p>由此可见，q值小的地方为p值小与p值大的地方，这就会导致出现概率最大、概率最小的迹被经验池重复的次数多，与前文所讲不同。</p><p>而如果考虑以真实的p分布来定义迹出现的概率，那么此处不应该使用$\operatorname{rank}\left(q\left(\tau_{i}^{g}\right)\right)$作为排序的标准，而应该是$\operatorname{rank}\overline{p}\left(\boldsymbol{\tau}^{g} | \boldsymbol{\phi}\right)$，或者$\operatorname{rank}p\left(\boldsymbol{\tau}^{g}\right)$。</p><p>总之，这里关于优先级的解释不是很清楚。</p><h2 id="流程示意图"><a href="#流程示意图" class="headerlink" title="流程示意图"></a>流程示意图</h2><p><img src="./maximum-entropy-regularized-multi-goal-reinforcement-learning/MEP.png" alt=""></p><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><p><img src="./maximum-entropy-regularized-multi-goal-reinforcement-learning/pseudo.png" alt=""></p><p>解析：</p><ul><li>每一次迭代，都重新构造优先采样分布$q\left(\tau^{g}\right)$</li></ul><h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p><img src="./maximum-entropy-regularized-multi-goal-reinforcement-learning/env.png" alt=""></p><ul><li>算法：DDPG</li><li>5个随机种子进行实验，取最好的结果</li><li>19个CPU</li><li>训练200个epoch</li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="./maximum-entropy-regularized-multi-goal-reinforcement-learning/mean-success.png" alt=""></p><ul><li><strong>以训练的epoch为标准</strong>，使用了MEP的收敛速度更快</li></ul><p><img src="./maximum-entropy-regularized-multi-goal-reinforcement-learning/training-time.png" alt=""></p><p>可以看到，结合MEP的效果最好，但是我觉得这种对比一点都不严谨，从数据上看，不使用MEP比使用MEP的训练时间更短，且效果也差不多，那么如果训练相同的时间，说不定使用MEP的效果并没有不使用的好。</p><p><img src="./maximum-entropy-regularized-multi-goal-reinforcement-learning/sample-efficiency.png" alt=""></p><p>固定成功率，在机器人实验中，提升了1.95倍采样效率，即使用更少的样本训练相同的效果。</p><h1 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h1><p>前文提到原目标函数中的对数项会使训练值函数不稳定，为什么？</p><p>为什么构造新的迹分布$q\left(\boldsymbol{\tau}^{g}\right)$为那样的形式？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇论文将强化学习的目标与最大熵结合了起来，提出了简称为MEP的经验池机制。许多将熵与强化学习结合的方法都是考虑可选动作分布的熵，该篇论文很新颖的使用的是“迹”的熵。&lt;/p&gt;
&lt;p&gt;推荐程度中等偏下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有些地方解释的不是很清楚&lt;/li&gt;
&lt;li&gt;熵的结合方式特殊，可以一看&lt;/li&gt;
&lt;li&gt;有些公式推导过于复杂，难懂&lt;/li&gt;
&lt;li&gt;有些参考文献标注不准，如A3C算法的论文并没有使用熵的概念，却在熵相关的语句进行了标注&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>强化学习论文浅读集合</title>
    <link href="http://StepNeverStop.github.io/rl-rough-reading.html"/>
    <id>http://StepNeverStop.github.io/rl-rough-reading.html</id>
    <published>2019-06-10T07:55:29.000Z</published>
    <updated>2020-04-11T04:00:03.946Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录了一些粗读的强化学习相关的论文。</p><a id="more"></a><h1 align="center" style="color:blue" id="Gorila">[DeepMind]Massively Parallel Methods for Deep Reinforcement Learning[Gorila]</h1><p>本文提出了一个分布式强化学习训练的架构：Gorila(General Reinforcement Learning Architecture)。2015年发于ICML，本文使用DQN算法进行分布式实现。</p><p>论文地址：<a href="https://arxiv.org/pdf/1507.04296.pdf" rel="external nofollow" target="_blank">https://arxiv.org/pdf/1507.04296.pdf</a></p><h2 id="模型示意图"><a href="#模型示意图" class="headerlink" title="模型示意图"></a>模型示意图</h2><p><img src="./rl-rough-reading/gorila.png" alt=""></p><p>解析：</p><ul><li>shard代表参数分片的意思，即模型过大、参数过多，需要将参数分片放置多台机器上</li><li>Bundled Mode模式指的是Actor中的Q网络与Learner中的Q网络一样，但是Learner比Actor多了一个目标Q网络，用于计算梯度</li></ul><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul><li>并行Actor采数据</li><li>并行Learner计算梯度，<strong>不更新Learner中的模型</strong></li><li>中心参数服务器，用于维持最新的网络模型。如果模型太大、参数过多，可以分片将网络模型放置多个参数服务器，每个参数服务器中的参数独立不关联，根据learner传的梯度更新相应的变量</li><li>经验池机制，分为local与global两种<ul><li>local，即每个actor节点一个经验池</li><li>global，将所有actor节点的经验存至一个分布式数据库中，这个<strong>需要网络通信开销</strong></li></ul></li></ul><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><p><img src="./rl-rough-reading/gorila-pseudo.png" alt=""></p><p>解析：</p><ul><li><p>伪代码中为一个actor节点的流程</p></li><li><p>注意伪代码中出现两次<code>Update θ from parameters θ+ of the parameter server</code>，这句话的意思为从中心参数服务器拉取模型到actor和learner，拉取的时间点为：</p><ul><li>每个episode开始前</li><li>每次执行动作$a_{t}$后，但是在计算梯度并将梯度传递至参数服务器之前</li></ul></li><li><p>伪代码中<code>equation 2</code>，代表$g_{i}=\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s, a ; \theta_{i}\right)\right) \nabla_{\theta_{i}} Q(s, a ; \theta)$，这是DQN中的损失函数</p></li><li><p>注意，与传统DQN不同的是，<strong>该分布式DQN中给Learner中的目标Q网络赋值时，是直接将更新N次的中心参数服务器中的模型进行拉取覆盖，而不是使用Learner中的Q网络</strong></p></li><li><p>中心参数服务器中的参数梯度更新需要累计多个learner传来的梯度后进行更新，使用异步SGD即ASGD方法进行梯度下降。</p><ul><li><blockquote><p>The parameter server then applies the updates that are accumulated from many learners. </p></blockquote></li></ul></li><li><p>因为每个actor都是阶段更新自己的模型，即从参数服务器中拉取。所以每个actor中的行为策略（采样策略）都不完全相同，事实上，每个actor节点可以采取不同的探索机制，这样可以更有效地探索环境</p></li></ul><h2 id="稳定性"><a href="#稳定性" class="headerlink" title="稳定性"></a>稳定性</h2><p>为了应对节点退出、网速慢、节点机器运行慢等问题，该文章中指出使用了一个超参数用来控制actor和server之间最大延时。</p><ul><li><p>过时的梯度（低于时间阈值）将会被丢弃</p><ul><li><blockquote><p>All gradients older than the threshold are discarded by the parameter server. </p></blockquote></li></ul></li><li><p>过高或过低的梯度也将被丢弃</p><ul><li><blockquote><p>each actor/learner keeps a running average and standard deviation of the absolute DQN loss for the data it sees and discards gradients with absolute loss higher than the mean plus several standard deviations. </p></blockquote></li></ul></li><li><p>使用AdaGrad更新规则</p></li></ul><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p>采用于提出DQN的论文中一样的网络结构，具体请见论文中第5部分。</p><p>在Atari 2600 49个游戏中，41个明显优于单GPU DQN。</p><p>Gorila进一步实现了DRL的希望：一个可伸缩的架构，随着计算和内存的增加，它的性能会越来越好</p><h1 align="center" style="color:blue" id="MB-MPO">[UCB/OpenAI]Model-Based Reinforcement Learning via Meta-Policy Optimization[MB-MPO]</h1><p>论文地址：<a href="https://arxiv.org/pdf/1809.05214.pdf" rel="external nofollow" target="_blank">https://arxiv.org/pdf/1809.05214.pdf</a></p><p>本文2018年发布于CoRL，提出了一个基于模型的元强化学习算法MB-MPO。相比于一般的元强化学习是从多个MDPs任务中学习一个通用模型加速以后特定任务的模型训练，该文中的方法是将一个model-free的任务学习多个不确定、不完全、不完美的动态模型，即一个模型集合，然后使用这个模型集合学习出该任务的通用模型。因为它有一个从model-free学习动态模型的过程，所以为model-based方法。</p><h2 id="元强化学习"><a href="#元强化学习" class="headerlink" title="元强化学习"></a>元强化学习</h2><script type="math/tex; mode=display">\max _{\theta} \mathbb{E}_{\mathcal{M}_{k} \sim \rho(\mathcal{M}),\boldsymbol{s}_{t+1} \sim p_{k},\boldsymbol{a}_{t} \sim \pi_{\boldsymbol{\theta}^{\prime}}\left(\boldsymbol{a}_{t} | \boldsymbol{s}_{t}\right)}\left[\sum_{t=0}^{H-1} r_{k}\left(s_{t}, a_{t}\right)\right] \\ s.t.:\boldsymbol{\theta}^{\prime}=\boldsymbol{\theta}+\alpha\nabla_{\boldsymbol{\theta}} \mathbb{E}_{\boldsymbol{s}_{t+1} \sim p_{k},\boldsymbol{a}_{t} \sim \pi_{\boldsymbol{\theta}}\left(\boldsymbol{a}_{t} | \boldsymbol{s}_{t}\right)}\left[\sum_{t=0}^{H-1} r_{k}\left(s_{t}, a_{t}\right)\right]</script><p>$\mathcal{M}$为一系列MDP，共享相同的状态空间$\mathcal{S}$与动作空间$\mathcal{A}$，但是奖励函数可以不同</p><h2 id="学习环境动态模型"><a href="#学习环境动态模型" class="headerlink" title="学习环境动态模型"></a>学习环境动态模型</h2><script type="math/tex; mode=display">\min _{\boldsymbol{\phi}_{k}} \frac{1}{\left|\mathcal{D}_{k}\right|} \sum_{\left(\boldsymbol{s}_{t}, \boldsymbol{a}_{t}, \boldsymbol{s}_{t+1}\right) \in \mathcal{D}_{k}}\left\|\boldsymbol{s}_{t+1}-\hat{f}_{\boldsymbol{\phi}_{k}}\left(\boldsymbol{s}_{t}, \boldsymbol{a}_{t}\right)\right\|_{2}^{2}</script><p>解析：</p><ul><li><p>$\mathcal{D}_{k}$为第k个学习模型采样的“经验”</p></li><li><p>$\phi$为用神经网络表示的环境模型的参数</p></li><li><p>$\hat{f}_{\boldsymbol{\phi}_{k}}\left(\boldsymbol{s}_{t}, \boldsymbol{a}_{t}\right)$为第k个学习模型针对状态$s_{t}$执行动作$a_{t}$后转移状态的预测，其中，神经网络的输出不直接是预测的状态$\color{red}{s_{t+1}}$，而是$\color{red}{\Delta s=s_{t+1}-s_{t}}$，所以$\hat{f}_{\boldsymbol{\phi}_{k}}\left(\boldsymbol{s}_{t}, \boldsymbol{a}_{t}\right)=s_{t}+\Delta s$</p><ul><li><blockquote><p>We follow the standard practice in model-based RL of training the neural network to predict the change in state $\Delta s=s_{t+1}-s_{t}$ (rather than the next state $s_{t+1}$) </p></blockquote></li></ul></li></ul><p>为了防止过拟合，文中使用了3个trick：</p><ol><li>早停</li><li>归一化神经网络输入与输出</li><li>权重归一化</li></ol><h2 id="基于环境动态模型的元强化学习"><a href="#基于环境动态模型的元强化学习" class="headerlink" title="基于环境动态模型的元强化学习"></a>基于环境动态模型的元强化学习</h2><p>假设学到了K个近似模型$\left\{\hat{f}_{\phi_{1}}, \hat{f}_{\phi_{2}}, \ldots, \hat{f}_{\phi_{K}}\right\}$，把每个模型转换成一个MDP过程，即$\mathcal{M}_{k}=\left(S, A, \hat{f}_{\phi_{k}}, r, \gamma, p_{0}\right)$，其中，<strong>奖励函数相同</strong></p><p>由此给每个学习到的动态模型分配的行为策略目标函数为：</p><script type="math/tex; mode=display">J_{k}(\boldsymbol{\theta})=\mathbb{E}_{\boldsymbol{a}_{t} \sim \pi_{\boldsymbol{\theta}}\left(\boldsymbol{a}_{t} | s_{t}\right)}\left[\sum_{t=0}^{H-1} r\left(\boldsymbol{s}_{t}, \boldsymbol{a}_{t}\right) | \boldsymbol{s}_{t+1}=\hat{f}_{\boldsymbol{\phi}_{k}}\left(\boldsymbol{s}_{t}, \boldsymbol{a}_{t}\right)\right]</script><p>定义MB=MPO的最终目标函数为：</p><script type="math/tex; mode=display">\max _{\boldsymbol{\theta}} \frac{1}{K} \sum_{k=0}^{K} J_{k}\left(\boldsymbol{\theta}_{k}^{\prime}\right) \quad \text { s.t.: } \quad \boldsymbol{\theta}_{k}^{\prime}=\boldsymbol{\theta}+\alpha \nabla_{\boldsymbol{\theta}} J_{k}(\boldsymbol{\theta})</script><p>小写k代表第k个学到的模型，大写K代表模型的总数。</p><p>注意看，这里公式后边使用的是$\color{red}{\theta’_{k}}$，而不是$\theta$。这里并没有写错，我起初以为写错了，具体请看后边的伪代码解释。</p><h2 id="伪代码-1"><a href="#伪代码-1" class="headerlink" title="伪代码"></a>伪代码</h2><p><img src="./rl-rough-reading/mb-mpo-pseudo.png" alt=""></p><p>解析：</p><ul><li>MB-MPO分为两部分更新，第一部分更新每个模型分配的行为策略，第二部分更新元策略。<strong>注意：行为策略的更新是不连贯的，即不是自身迭代，而是不断使用元策略进行稍加修改然后替换，所以叫做adapted policy</strong></li><li>上一项提到的两次更新都是对元策略的参数$\theta$进行更新，区别是，第一次更新将更新后的参数赋值给了行为策略，未更改元策略本身，第二次更新直接更新元策略本身</li><li>$\alpha, \beta$为两部分更新的学习率</li><li>行为策略使用VPG，即传统策略梯度算法进行优化，元策略使用TRPO算法进行优化</li><li>伪代码中的大致流程如下：<ol><li>初始化策略$\pi_{\theta}$并将其复制K份$\pi_{\theta_{1}^{\prime}}, \dots, \pi_{\boldsymbol{\theta}_{K}^{\prime}}$</li><li>使用$\pi_{\theta_{1}^{\prime}}, \dots, \pi_{\boldsymbol{\theta}_{K}^{\prime}}$对<strong>真实的环境模型进行采样（这一步是实际交互，即真实数据）</strong>，将数据存入经验池</li><li>根据经验池训练K个环境模型，即使用<code>学习环境动态模型</code>部分的公式</li><li>对于每个更新后的环境模型，用元策略$\color{red}{\pi_{\theta}}$进行<strong>虚拟采样（这一步是预测采样，即不实际进行交互）</strong>，采样到$\mathcal{T}_{k}$以适应性修改行为策略$\boldsymbol{\theta}_{k}^{\prime}$。这里也是前边提到的行为策略更新是不连贯的原因。</li><li>再用适应性策略$\boldsymbol{\theta}_{k}^{\prime}$进行<strong>虚拟采样</strong>，采样到$\mathcal{T}_{k}^{\prime}$以更新元策略$\pi_{\theta}$</li><li>跳向第2步</li></ol></li><li>伪代码中虽然没有明确指出，但是其实使用了baseline的trick用来减少方差</li></ul><h2 id="流程示意图"><a href="#流程示意图" class="headerlink" title="流程示意图"></a>流程示意图</h2><p><img src="./rl-rough-reading/mb-mpo-visio.png" alt=""></p><h2 id="效果-1"><a href="#效果-1" class="headerlink" title="效果"></a>效果</h2><ol><li>比之前的model-based方法效果好、收敛快</li><li>可以达到model-free算法的渐进性能</li><li>需要更少的经验，低采样复杂性。其实是使用了虚拟采样，提高了数据效率，减少了交互采样的代价。</li><li>对于模型偏差（model-bias，即环境模型没学到位）的情况，之前的算法不能有效处理，该算法对不完美、不完全、不完整的模型具有很好地鲁棒性</li></ol><h1 align="center" style="color:blue" id="DIAYN">[UCB/Google AI]Diversity is All Your Need: Learning Skills Without a Reward Function[DIAYN]</h1><p>论文地址：<a href="https://arxiv.org/pdf/1802.06070.pdf" rel="external nofollow" target="_blank">https://arxiv.org/pdf/1802.06070.pdf</a></p><p>Google网页：<a href="https://sites.google.com/view/diayn/home" rel="external nofollow" target="_blank">https://sites.google.com/view/diayn/home</a></p><p>Github项目：<a href="https://github.com/ben-eysenbach/sac/blob/master/DIAYN.md" rel="external nofollow" target="_blank">https://github.com/ben-eysenbach/sac/blob/master/DIAYN.md</a></p><p>这篇文章使用信息论中最大熵的方法来构造强化学习的学习目标，<strong>期望学习到具有多样性的技能（skills）</strong>。</p><p>个人认为，此文章中所提的方法虽然很新颖，但是不能作为优化一项任务的可用算法，因为虽然其可以学到以各种花样完成目标，但是没有奖励函数的控制使得无法规范、指引智能体“解题”过程的效果，如柔顺性、实用性、实际可行性等。从另一方面来讲，将这样虽然不规划、不严谨决策行为的策略用于元策略的预训练模式还是可用的。</p><h2 id="技能"><a href="#技能" class="headerlink" title="技能"></a>技能</h2><p>技能的定义在文中有如下表述：</p><blockquote><p>A skill is a latent-conditioned policy that alters that state of the environment in a consistent<br>way.<br>we refer to a/the policy conditioned on a fixed Z as a “skill” .</p></blockquote><p>意思是，设定一个隐变量，以（状态$S$，隐变量$Z$）为条件进行动作选择，即为技能——skill。</p><p>DIAYN就好像是要给每个状态赋予各个不同技能的概率，并且使其中一个技能的概率最大，这样就使得在整个状态空间中，不同的技能“占领”着状态空间的不同部分，每个技能在各自偏好的局部状态空间中作用，但是作者同样希望每个技能在各自的状态空间中尽可能随机决策。</p><p><img src="./rl-rough-reading/skill.png" alt=""></p><p>假设以不同的颜色代表不同的技能skill，每个方格代表一个状态，那么每个状态对于每个技能到达此状态的“偏好”概率是不同的。总的来说，作者希望技能之间的重合度尽可能小，但每个技能在各自的领域内尽可能随机地完成目标。</p><h2 id="亮点与作用"><a href="#亮点与作用" class="headerlink" title="亮点与作用"></a>亮点与作用</h2><ol><li>去掉了奖励函数</li><li>修改了目标函数，$\mathcal{F}(\theta)\triangleq \mathcal{G}(\color{red}{\theta, \phi})$<ul><li>$\color{red}{\theta}$代表Actor网络中的参数</li><li>$\color{red}{\phi}$代表Critic网络中的参数</li></ul></li><li>学习到的技能可以用于<em>分层强化学习、迁移学习、模仿学习</em></li></ol><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><script type="math/tex; mode=display">\begin{aligned} \mathcal{F}(\theta) & \triangleq \color{red}{I(S ; Z)+\mathcal{H}[A | S]-I(A ; Z | S)} \\ &=(\mathcal{H}[Z]-\mathcal{H}[Z | S])+\mathcal{H}[A | S]-(\mathcal{H}[A | S]-\mathcal{H}[A | S, Z]) \\ &=\color{blue}{\mathcal{H}[Z]-\mathcal{H}[Z | S]+\mathcal{H}[A | S, Z]} \\&=\mathcal{H}[A | S, Z]+\mathbb{E}_{z \sim p(z), s \sim \pi(z)}[\log p(z | s)]-\mathbb{E}_{z \sim p(z)}[\log p(z)] \\&{ \color{orange}{\geq} \mathcal{H}[A | S, Z]+\mathbb{E}_{z \sim p(z), s \sim \pi(z)}\left[\log q_{\phi}(z | s)-\log p(z)\right] \\ \triangleq \mathcal{G}(\theta, \phi)}\end{aligned}</script><p>解析：</p><ul><li><p>互信息，离散下为$I(X ; Y)=\sum_{y \in Y} \sum_{x \in X} p(x, y) \log \left(\frac{p(x, y)}{p(x) p(y)}\right)$，连续下为$I(X ; Y)=\int_{Y} \int_{X} p(x, y) \log \left(\frac{p(x, y)}{p(x) p(y)}\right) d x d y$</p></li><li><p>信息熵表示为$H(X, Y)=-\sum_{x, y} p(x, y) \log p(x, y)=-\sum_{i=1}^{n} \sum_{j=1}^{m} p\left(x_{i}, y_{i}\right) \log p\left(x_{i}, y_{i}\right)$</p></li><li><p>推到中频繁使用了性质$I(X,Y)=H(X)-H(X | Y)$</p></li><li><p>式中对数的底为自然指数$e$</p></li><li><p>看红色部分，化简之前：</p><ul><li><strong>增大</strong>：$I(S ; Z)$代表状态$S$与策略隐变量$Z$之间的互信息。因为作者希望可以通过策略所能到达的状态来判别其属于哪个技能，即将技能与状态挂钩。作者给出一个直观的解释：因为在有些状态下可以执行很多动作，但是却不改变环境（至少不明显改变），就像用机械手臂夹紧一个物体时，可使用力的大小、方向等都是很多的，不同技能选择不同动作导致的效果可能相同，所以作者不希望从动作的选择来区分学到的技能，而是通过可以明显观察到、数值化的状态$S$来作为区别不同技能的标准。<strong>互信息$I(X,Y)$有一个直观的性质就是，它可以衡量两个随机变量的“相关性”，也就是说，互信息越大，代表知道$X$后对$Y$的不确定性减少，即知道其一可以加深对另一个的了解。</strong>所以，目标函数希望最大化互信息$I(S ; Z)$，以将状态和技能相关联，使技能尽可能根据状态可以区分。</li><li><strong>增大</strong>：$\mathcal{H}[A | S]$代表策略（不以隐变量$Z$区分技能，混合所有技能即为策略）的熵值。与SAC算法中想要使用熵增来使得动作的选择更加随机一样，作者希望随机性的动作同样可以完成目标，所以希望尽可能增大这一项。</li><li><strong>减小</strong>：$I(A ; Z | S)$代表动作$A$与策略隐变量$Z$在给定状态$S$时之间的互信息。为了避免歧义，应该写作为$I[(A ; Z) | S]$。作者希望技能根据状态可区分，而不是根据动作，所以需要最小化这一项。</li></ul></li><li><p>看蓝色部分，化简之后：</p><ul><li><strong>固定最大，为$\ln n$</strong>：$\mathcal{H}[Z]$代表技能分布的不确定性，既然要最大化这个项，不如就固定它，使得技能从其中均匀采样，使熵为最大值。</li><li><strong>减小</strong>：$\mathcal{H}[Z | S]$代表状态$S$条件下技能的不确定性，我们知道，熵越大，不确定性越大；熵越小，不确定性越小。作者希望技能根据状态可区分，可以需要使这一项最小，以减小给定状态下所属技能的不确定性，使其尽可能接近概率1。</li><li><strong>增大</strong>：$\mathcal{H [ A | S}, Z ]$代表给定技能$(S,Z)$下动作的不确定性。因为作者希望动作的选择尽可能随机但又可以完成目标，所以需要最大化这一项。</li></ul></li><li><p>看橘色部分，使用Jensen不等式：</p><ul><li><p>这一步推导使用了论文<a href="https://pdfs.semanticscholar.org/f586/4b47b1d848e4426319a8bb28efeeaf55a52a.pdf" rel="external nofollow" target="_blank">《The IM Algorithm : A variational approach to Information Maximization》</a>中的推导公式</p><script type="math/tex; mode=display">I(\mathbf{x}, \mathbf{y}) \geq \underbrace{H(\mathbf{x})}_{\text { ‘‘entrop’’ }}+\underbrace{\langle\log q(\mathbf{x} | \mathbf{y})\rangle_{p(\mathbf{x}, \mathbf{y})}}_{\text { ‘‘energy’’ }} \stackrel{\mathrm{def}}{=} \tilde{I}(\mathbf{x}, \mathbf{y})</script></li><li><p><img src="./rl-rough-reading/Agakov.png" alt=""></p></li><li><p>蓝色公式中，有$I(Z;S) = \mathcal{H}[Z]-\mathcal{H}[Z | S]$，可以应用上述性质进行推导，将真实分布$p(z | s)$替换为任意变分分布(variational distribution)$q(z | s)$</p></li><li><p>最后使用变分下界$\mathcal{G}(\theta, \phi)$代替目标函数$\mathcal{F}(\theta)$</p></li></ul></li><li><p>至此，思路已经十分清晰。Actor网络以变量$\theta$参数化，并使用SAC算法($\alpha=0.1$)最大化$\mathcal{G}(\theta, \phi)$中$\mathcal{H}[A | S, Z]$部分；Critic网络以变量$\phi$参数化，并最大化后半个期望部分。文中将期望内的元素定义为“伪奖励”：</p><script type="math/tex; mode=display">r_{z}(s, a) \triangleq \log q_{\phi}(z | s)-\log p(z)</script><p>由于$p(z)$为均匀分布，是固定的；对数函数不改变原函数单调性，所以只需最大化$q_{\phi}(z | s)$即可。</p></li></ul><h2 id="伪代码-2"><a href="#伪代码-2" class="headerlink" title="伪代码"></a>伪代码</h2><p><img src="./rl-rough-reading/diayn-pseudo.png" alt=""></p><p>解析：</p><ul><li>每个episode都重新采样隐变量$z$</li><li>Actor网络的输入为$(S||Z)$，即状态与隐变量的连结(我猜的= =)</li><li>Critic网络的输入为状态$S$</li></ul><h2 id="模型示意图-1"><a href="#模型示意图-1" class="headerlink" title="模型示意图"></a>模型示意图</h2><p><img src="./rl-rough-reading/diayn.png" alt=""></p><p>解析：</p><ul><li>隐变量分布$p(z)$是固定的</li></ul><h1 align="center" style="color:blue" id="CDP">Curiosity-Driven Experience Prioritization via Density Estimation[CDP]</h1>论文地址：[https://arxiv.org/pdf/1902.08039.pdf](https://arxiv.org/pdf/1902.08039.pdf)这篇文章发于2018年的NIPS，作者为赵瑞，之前读过他的两篇论文，并写了博客，可以在论文精读里找，此处不贴链接了，分别是基于能量的HER和最大熵正则化多目标RL。这篇文章总的来说提出了**基于迹密度的优先经验回放**，人类的好奇心机制驱动他有了这样的想法，文中说受有监督学习使用过采样和降采样解决训练集样本不平衡问题的启发，想在强化学习中解决经验池中迹“不足（under-represented）”的问题。说起来也挺佩服这个作者的，目前（2019年6月21日14:59:05）总共发了三篇关于强化学习的论文，但都有很好地结果：1. 基于迹能量的优先级，发了CoRL2. 基于迹密度的优先级，也就是这篇，发了NIPS3. 基于迹最大熵的优先级，发了ICML我个人道行尚浅，对于几篇论文中的深奥精髓有些不能尽数参透，由于先验知识不足，对于文中内容也不敢完全苟同，但是从这几篇阅读总结下来，发得了这种高级别论文有以下几个“加分性”要求：1. 数学要好，这是必然的，数学公式写的越华丽，数学模型越复杂，当然越具有吸引力2. 工作要专一且连续，看这三篇论文虽然不是递进关系，但都是在解决经验池优先相关的工作，所以找准一个领域内的小角度也是很重要的3. 实验部分要做好，三篇都没用完整地、细节地比较各个算法，但是却新奇地比较了采样复杂性、数据利用率等等，总之，一定要用实验表明自己的方法在某方面有用4. 其他秘密因素## 流程这篇论文的方法流程如下：1. 计算**迹密度**$\rho$2. 计算迹密度的补$\overline{\rho} \propto 1-\rho$3. 根据补排序，并设置优先级，补越大优先级越大4. 使用HER补充经验，设置相同的优先级和迹密度5. 优化算法## 迹密度的计算这一部分没有看太懂，主要是本人数学功底比较薄弱，感兴趣的可以亲自查看论文中2.4与3.2.1、3.2.2部分。根据文中的意思，思想大致如下：1. 用GMM（高斯混合模型）来估计迹密度   $$   \rho(\mathbf{x})=\sum_{k=1}^{K} c_{k} \mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)   $$2. 在每个epoch，使用V-GMM（GMM的一个变体）+EM算法推断GMM参数($\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}$)的后验分布3. 在每个episode，使用如下公式计算迹密度   $$   \rho=\mathrm{V}-\operatorname{GMM}(\mathcal{T})=\sum_{k=1}^{K} c_{k} \mathcal{N}\left(\mathcal{T} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)   $$   其中，$\mathcal{T}=\left(s_{0}\left\|s_{1}\right\| \ldots \| s_{T}\right)$，**每个迹的长度相同**，中间的符号代表连结操作的意思，然后进行归一化   $$   \rho_{i}=\frac{\rho_{i}}{\sum_{n=1}^{N} \rho_{n}}   $$*注：我猜想上边符号表示的$s$其实包含了智能体的所在状态和要达到的真实目标，也就是$(s,g)$，文中有一段可能解释了这一部分，但是我没有太理解。*![](./rl-rough-reading/cdp-sg.png)## 优先级的设定作者说使用rank-based方法来设置优先级，因为其受异常点影响小而具有良好的鲁棒性。先计算迹密度的补$$\overline{\rho} \propto 1-\rho$$将补从小到大排序，并根据排名计算优先级，排名从0开始，即$$\operatorname{rank}(\cdot) \in\{0,1, \ldots, N-1\}$$$$p\left(\mathcal{T}_{i}\right)=\frac{\operatorname{rank}\left(\overline{\rho}\left(\mathcal{T}_{i}\right)\right)}{\sum_{n=1}^{N} \operatorname{rank}\left(\left(\overline{\rho}\left(\mathcal{T}_{n}\right)\right)\right.}$$## 伪代码![](./rl-rough-reading/cdp-pseudo.png)解析：- 每个epoch根据经验池中的样本数据重新拟合一次密度模型，也就是GMM中的参数- 每个episode都计算其迹密度- 红色框中的公式数字编号分别代表之间部分中关于计算迹密度和迹优先级的公式- 采样迹、采样经验转换之后，需要采样目标并存入经验池，重构后的经验其优先级及迹密度与真正目标下迹的相同## 优点实验部分的比较详见论文。1. 可以适用于任何Off-Policy算法2. 不使用TD-error计算优先级，而使用迹密度，减少了计算时间3. 提升了采样效率两倍左右4. 算法性能超过最新算法9%（这个结果看看即可，不必放在心上）<h1 align="center" style="color:blue" id="NAF">[Google]Continuous Deep Q-Learning with Model-based Acceleration[NAF]</h1>论文地址：[https://arxiv.org/abs/1603.00748](https://arxiv.org/abs/1603.00748)本文介绍了标准化优势函数Normalized Advantage Function——NAF算法，该算法简化了A-C架构，将Q-Learning的思想应用于高维连续空间。本文的主要贡献是：1. 提出NAF，简化了Actor-Critic架构2. 将Q-Learning推广至高维连续空间3. 提出新的与已学模型结合的方法，提升了采样复杂性（也就是降低）和学习效率，不牺牲策略的最优性。原文中翻译意思是，评估了几种将已学习模型与Q-Learning结合的方案，提出将局部线性模型与局部On-Policy想定推演结合以加速Q-Learning算法在model-free、连续问题下的学习## 伪代码![](./rl-rough-reading/naf-pseudo.png)解析：- 待写<h1 align="center" style="color:blue" id="ERE">[NYU]Boosting Soft Actor-Critic: Emphasizing Recent Experience without Forgetting the Past[ERE]</h1><p>论文地址：<a href="http://arxiv.org/abs/1906.04009" rel="external nofollow" target="_blank">http://arxiv.org/abs/1906.04009</a></p><p>代码仓库：<a href="https://github.com/BY571/Soft-Actor-Critic-and-Extensions" rel="external nofollow" target="_blank">https://github.com/BY571/Soft-Actor-Critic-and-Extensions</a>(不确定是否为论文原作者的仓库，该仓库PER的实现没有用sum-tree，2020年04月02日13:42:44)</p><p>这篇论文主要是对经验池机制的扩展，SAC是被应用的算法。</p><p>ERE是Emphasizing Recent Experience的缩写，从名字即可以看出，该经验池机制的侧重点是近期经验。简单说一下ERE的思想及流程：</p><ol><li>强调近期观测到的数据，同时不遗忘过去学到的知识；</li><li>在更新神经网络参数时，比如要连续更新$K$次，即要从经验池中循环取$K$个mini-batch的数据，那么，在第一次更新时，从整个经验池进行采样，后续更新时，逐渐缩小经验池的可采样范围，也就是收缩，使得在间隔内更新的次数也多，近期经验被采样出的概率越大</li></ol><p>论文中的原句：</p><blockquote><p>We propose Emphasizing Recent Experience (ERE), a simple but powerful off-policy sampling technique, which emphasizes recently observed data while not forgetting the past. The ERE algorithm samples more aggressively from recent experience, and also orders the updates to ensure that updates from old data do not overwrite updates from new data.</p></blockquote><h2 id="经验池逐渐缩放的原理"><a href="#经验池逐渐缩放的原理" class="headerlink" title="经验池逐渐缩放的原理"></a>经验池逐渐缩放的原理</h2><p>核心思想是，在训练阶段（也就是先收集好一批轨迹，然后再更新$K$次的阶段），第一个mini-batch数据从整个经验池范围采样，在后续次序的更新中，我们逐渐减小经验池的可采样范围，使得mini-batch数据中包含越来越多的近期经验。</p><p>举个例子：假如经验池共十个位置，即0到9，0放置最新的经验，9放置最旧的经验，batch-size为2，那么在第一次更新时，从0-9中采样两条经验。在第二次更新时，从0-8中采样两条经验。……以此类推。</p><p>论文中提出了下边这个公式：</p><script type="math/tex; mode=display">c_{k}=\max \left\{N \cdot \eta^{k \frac{1000}{K}}, c_{\min }\right\}</script><p>在这个公式中，$N$代表经验池的总容量；$\eta \in(0,1]$是引入的一个超参数，用来决定<strong>对近期数据的重视程度</strong>，当$\eta=1$时等同于均匀采样，当$\eta&lt;1$时，$c_k$随着更新次数逐渐减小。作者通过实验发现$\eta \in(0.994,0.999)$时效果不错；$c_{min}$是为$c_k$设置的一个下界，防止从一个很小的范围内采样数据，可能会导致过拟合现象。大写的$K$表示更新的册数，小写的$k$，$1\leq k \leq K$，表示当前是第几次更新。</p><p>虽然作者说只引入了$\eta$这一个超参数，但其实我觉得，更新次数$K$与基数$1000$也属于可调的超参数，虽然在实验中往往将$K$设置为一条轨迹的步长，即有多少步就更新多少次，这样在更新次数上与一步一更新是一致的，但是说到底它也是个可变的参数。</p><p>文中还提到ERE可以与PER相结合，先决定采样区间，再按照区间内经验的权重选择批数据进行更新。通过代码没有看到在ERE与PER结合时使用sum-tree结构，可能在sum-tree结构上ERE不能很好适用，因为区间的选择也会改变树根节点的位置，为各种功能的计算都引入不便。</p><script type="math/tex; mode=display">P(i)=\frac{p_{i}^{\alpha}}{\sum_{j} p_{j}^{\alpha}}, i, j \in D_{c_{k}}</script><p>$D_{c_k}$即为经验池中前$c_k$个近期的经验数据。</p><h2 id="伪代码-3"><a href="#伪代码-3" class="headerlink" title="伪代码"></a>伪代码</h2><p><img src="./rl-rough-reading/ere-pseudo.png" alt=""></p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>优点：</p><ol><li>简单有效</li><li>可用于任何使用经验池机制的off-policy算法中，通用性强</li><li>引入的额外计算损耗可以忽略不计</li><li>只引入一个超参数$\eta$，用来控制经验池可采样范围缩减的速率，容易调节（我觉得公式中的1000以及$c_{min}$应该也算是需要手动设置的超参数）</li><li>可以结合PER使用</li></ol><p>缺点：</p><ol><li>传统off-policy算法的更新方式是走一步更新一次参数，像baseline等许多仓库都是这么做的，但是OpenAI的Spinning Up仓库却采用先采样一条轨迹，然后按照轨迹的步长为更新次数，循环更新网络。该论文提出的ERE也是使用先采样轨迹再更新的方式，目前看来不能应用在一步一更新的优化方式中，因为各个时间步的经验池是略有不同的</li><li>创新很小，实验上也没有看出明显的提升。而且，增加重放近期经验的次数的本质原因也不清楚，似乎只是实验效果不错，所以就这样错了，没有看到比较透彻的数学分析</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>该论文的实验图见原文。虽然在许多环境中都产生了1.收敛速度更快一点，2.最终性能得分更高一些，但是总感觉不那么make sense。我个人觉得这个ERE可用可不用，它更像是一种实验得出来有效的trick，而不像是为了解决某一问题而特定研究出来的方法。该方法可以当做是扩展眼界的trick。</p><h1 align="center" style="color:blue" id="6SAN">Reinforcement Learning with Attention that Works: A Self-Supervised Approach</h1><p>论文地址：<a href="http://arxiv.org/abs/1904.03367" rel="external nofollow" target="_blank">http://arxiv.org/abs/1904.03367</a></p><p>这篇文章是将self-attention结合进强化学习，并在PPO算法上进行了验证，使用了Arcade Learning Environment的10个环境，分3个随机种子进行实验。</p><p>由于论文中没有明确的self-attention在RL中的运算过程，所以目前不太清楚中间的计算细节。</p><h2 id="提出的方法"><a href="#提出的方法" class="headerlink" title="提出的方法"></a>提出的方法</h2><p><img src="./rl-rough-reading/6san.png" alt=""></p><p>上图为论文提出的Self-Attention在RL中应用的总体架构，其中$H_1,H_2,H_3$均为卷积层。文中将自注意力层加在了卷积层中间，而不是像DARQN一样将注意力层放置在卷积层之后。文中并未对Self-Attention模块中$F_1,F_2,G_1,Y$的运算进行解释。</p><p>作者对self-attention的可能实现做了深入的探索，提出并实验了六种结构：</p><ol><li><p>SAN: Self-Attending Network 在$H_1,H_2$最底层网络间加入自注意力机制</p></li><li><p>SSAN: Strong Self-Attending Network 意思似乎是将自注意力机制的输出$Y$乘以因子2，以增强注意力在网络中的影响</p><blockquote><p>Multiplying the output of the last convolutional layer in the self-attention component (’Y’) by a factor of two (thereby increasing the inﬂuence of attention on the network).</p></blockquote></li><li><p>SADN: Self-Attending Double Network 加入两个自注意力层，分别在$H_1, H_2$与$H_2, H_3$之间</p></li><li><p>SSADN: Strong Self-Attending Double Network 两注意力层的输出均乘2</p></li><li><p>PSAN: Pure Self-Attending Network 只使用注意力层的输出作为特征表示</p><blockquote><p>Passing only the output of the selfattention forward, removing the addition of the previous convolutional layer output.</p></blockquote></li><li><p>PSADN: Pure Self-Attending Double Network 与上相同</p></li></ol><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>实验结果与分析见论文原文。</p><p>此篇文章略读思想即可，文章中没有自注意力机制的完整计算流程，也没有开源源代码，因此无法了解到具体细节。而且，文章中提出了六种自注意力机制结构并进行了实验，个人感觉这六种结构不全是必要的，比如SSAN与PSAN这两类的完全没有get到这么设置的意义，给人一种为了做对比试验而设置的感觉。本篇文章创新性也不高。</p><h1 align="center" style="color:blue" id="MANet">[SNU]Multi-focus Attention Network for Efficient Deep Reinforcement Learning[MANet]</h1><p>论文地址：<a href="http://arxiv.org/abs/1712.04603" rel="external nofollow" target="_blank">http://arxiv.org/abs/1712.04603</a></p><p>这篇文章17年发表在AAAI上，提出了MANet算法(多焦点注意力+DQN)，它的创新点主要是将图像分割成若干区域，在这些区域中并行计算注意力权重，最终加权得到新的状态特征。作者展示了MANet算法可以应用在<strong>单智能体图像输</strong>入的环境中，也可以扩展模型应用<strong>在多智能体合作任务</strong>中。</p><p>作者提出MANet针对的问题是：当前深度强化学习模型无法利用感知数据中实体与实体之间的关系，因此需要大量的采样经验去学习。作者希望通过多焦点注意力模型将感知数据中局部范围内实体之间的关系给embed到状态特征之中，加速策略模型学习并且提升模型性能。</p><blockquote><p>current DRL models connect the entire low-level sensory input to the state-action values rather than exploiting the relationship between and among entities that constitute the sensory input.</p></blockquote><p>最终实验结果简报是：</p><ol><li>与DQN做比较时，单智能体场景中使用更少经验取得最高得分</li><li>在多智能体合作任务中，相较于SOTA算法(18年以前)，MANet加速学习20%</li></ol><blockquote><p>MANet attains highest scores with significantly less experience samples. Additionally, the model shows higher performance compared to the Deep Q-network and the single attention model as benchmarks. Furthermore, we extend our model to attentive communication model for performing multi-agent cooperative tasks. In multi-agent cooperative task experiments, our model shows 20% faster learning than existing state-of-the-art model.</p></blockquote><h2 id="模型讲解"><a href="#模型讲解" class="headerlink" title="模型讲解"></a>模型讲解</h2><p><img src="./rl-rough-reading/MANet-structure.png" alt=""></p><p>上图为论文中提出的MANet的结构示意图。MANet主要由四个模块组成：</p><ul><li>输入分割模块(Input Segmentation)</li><li>特征提取模块(Feature Extraction)</li><li>并行注意力模块(Parallel Attentions)</li><li>状态-动作值估计模块(State-action Value Estimation)</li></ul><p><strong>在单智能体任务中</strong>：</p><ol><li><p>输入分割模块</p><p>在这个阶段主要将底层传感器输入分割成多个块/区域，将每一个块的状态称为局部状态。作者使用了最简单的均匀格子切分方法，就是每个局部状态格子大小相同。作者也提到可以使用其他分割方式，可能由于实现困难或者切分后图像大小不一致使处理困难等因素而没有采用。</p><blockquote><p>We believe that we can apply more sophisticated methods like super-pixel segmentation (Achanta et al. 2010) or spatial transformer networks (Jaderberg, Simonyan, and Zisserman 2015).</p></blockquote></li><li><p>特征提取模块</p><p>这个模块主要从每个局部状态提取键(Key)特征和值(Value)特征。</p><p>其中，键特征主要用于决定模型应该注意到的位置</p><blockquote><p>The key features are used to determine where the model should attend.</p></blockquote><p>值特征主要用于编码用于评估Q值的信息</p><blockquote><p>The value features are used to encode information for state-action value estimation.</p></blockquote><p>这一模块的计算过程是这样的，首先提取局部状态的通用特征：</p><script type="math/tex; mode=display">c_{i}=f_{f}\left(s_{i}\right) \text { for all } i \in(0,1, \ldots, K)</script><p>其中，$K$为第一阶段切分的局部状态的个数，$s_i$表示第$i$个局部状态，$c_i$表示其通用特征，$f_f$是特征提取函数（作者使用深度卷积网络）。</p><p>然后，将通用特征$c_i$与局部状态的索引$i$拼接后，再计算键特征与值特征：</p><script type="math/tex; mode=display">\begin{aligned}&K e y_{i}=W_{k e y} * c_{i}\\&V a l_{i}=f_{v}\left(W_{v a l} * c_{i}\right)\end{aligned}</script><p>其中，$Key_i$与$Val_i$分别表示第$i$个局部状态的键、值特征。$f_v$表示非线性激活函数（作者使用的是leaky ReLU），$W_{key},W_{val}$是权重矩阵。</p></li><li><p>并行注意力模块</p><p>这一模块主要是区分各个局部状态的重要性，按重要性的不同加权表示特征。</p><script type="math/tex; mode=display">A_{i}^{n}=\frac{\exp \left(a_{n} * Key_{i}^{T}\right)}{\sum_{i^{\prime}} \exp \left(a_{n^{*} Key_{i^{\prime}}^{T}}\right)} \text { for all } n \in(0,1, \ldots, N)</script><p>其中，$N$是上面结构图中注意力层的数量，$A_i^n$表示第$n$层注意力的向量的第$i$个元素，$i^{\prime} \in\{0,1, \ldots, \mathrm{K}\}$，$a_n$是第$n$个选择向量（图中的selector，像神经网络参数一样可以被训练）。上述公式其实是softmax的形式。</p><p>由于$a$的随机初始化基本上相似，所以计算得来的注意力权重也必将相近，作者不希望所有的注意力层都注意某一局部区域（或者相似区域），比如说第5个局部区域，这样的话就很低效。作者希望多个注意力层可以皆可能地注意到不同的局部状态，以达到多焦点（multi-focus）的效果，作者探索了两种正则化方式来鼓励这种行为。</p><p>第一种正则化方法是熵正则化：</p><script type="math/tex; mode=display">R_{e}=\lambda_{e} * \sum_{n}\left\|A^{n} * \log A^{n}\right\|</script><p>这个式子的熵的负数形式，值越小，越随机（越雨露均沾），趋向于均匀分布，也就是越不集中注意力。</p><p>第二种正则化方法是距离正则化：</p><script type="math/tex; mode=display">R_{d}=\lambda_{d} * \exp \left(-\sum_{n, m}\left(A^{n}-A^{m}\right)^{2}\right)</script><p>这个式子为$e$的指数性质，值越小，说明两个注意力层注意关注的区域越不相同，也就是越集中注意力且注意不同的局部状态。</p><p><em>由于论文没有开源源代码，所以目前不确定熵正则化项的具体应用方式，不知道是最大化熵正则化项以鼓励不同注意力层注意不同区域，还是最小化熵正则化项以trade-off距离正则化项，放置距离正则化项过多的注意不同区域。</em></p></li><li><p>状态-动作值估计模块</p><p>这一模块综合每个注意力层的输出特征，并且输入到剩余网络以估计状态-动作值。作者将基于注意力的加权值特征定义为：</p><script type="math/tex; mode=display">h_{n}=\sum_{i} V a l_{i} * A_{i}^{n}</script><p>其中，$h_n$是由第$n$个注意力层的注意力权重$A^n$加权的值特征。最终联合特征和Q值表示为：</p><script type="math/tex; mode=display">g=\left\{h_{0}, h_{1}, \ldots, h_{N}\right\}</script><script type="math/tex; mode=display">Q=f_{q}(g)</script></li></ol><p><strong>在多智能体合作任务中</strong>：</p><p>​    在这种任务中，MANet主要将其它智能体的相关信息加权到某个智能体的状态特征中，相当于特征融合。</p><ol><li><p>输入分割模块</p><p>每一个智能体即是局部状态，因此不用分割，故不需要该模块。</p></li><li><p>特征提取模块</p><script type="math/tex; mode=display">\begin{aligned}&c_{i}=f_{f}\left(s_{i}\right) \text { for all } i \in(0,1, \ldots, K)\\&K e y_{i}=W_{k e y} * c_{i}\\&\color{red}{a_{i}=W_{a} * c_{i}}\\&V a l_{i}=f_{v}\left(W_{v a l} * c_{i}\right)\end{aligned}</script><p>$K$表示任务中智能体的个数。与单智能体不同的是，这里的selector是由通用特征$c_i$与权重矩阵$W_a$计算得来的，如上述公式红色字体所示。</p></li><li><p>注意力交流模块（Attentive Communication）</p><script type="math/tex; mode=display">A_{j}^{i}=\frac{\exp \left(a^{i} * \operatorname{Key}_{j}^{T}\right)}{\sum_{j^{\prime}} \exp \left(a^{i} * \operatorname{Ke} y_{j^{\prime}}^{T}\right)} \quad i, j \in(0,1, \ldots, K)</script><p>$A_j^i$表示智能体$j$对智能体$i$的注意力权重，值越大则$j$的信息对$i$越重要。</p></li><li><p>状态-动作值估计模块</p><script type="math/tex; mode=display">\begin{array}{c}h_{i}=\sum_{j} V a l_{j} * A_{j}^{i} \quad j \in(0,1, \ldots, K) \\g_{i}=\left\{V a l_{i}, h_{i}\right\} \\Q_{i}=f\left(g_{i}\right)\end{array}</script><p>$h_i$为第$i$个智能体的交流特征——从其他智能体的特征中基于注意力加权得来的。</p><p>在多智能体任务中，由于selector $a$与智能体的通用特征相关，因此之间不太相似，所以不需要熵和距离正则化项（作者认为的）。</p><p>作者添加了另一个正则化项$R=\lambda \cdot \left(a \cdot Key^{T} \right)$以解决值容易发散的问题。<strong><em>我目前还没有理解到这个正则化项的作用及意义。</em></strong></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文记录了一些粗读的强化学习相关的论文。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Universal Value Function Approximators</title>
    <link href="http://StepNeverStop.github.io/universal-value-function-approximators.html"/>
    <id>http://StepNeverStop.github.io/universal-value-function-approximators.html</id>
    <published>2019-06-02T01:47:47.000Z</published>
    <updated>2019-06-02T03:15:27.597Z</updated>
    
    <content type="html"><![CDATA[<p>本文中的方法简称UVFA，即通用值函数逼近器，主要是用于将只能表示同一任务单目标的值函数表示成通用的多目标值函数。很多论文如HER都引用了这篇论文中提出的方法。</p><p>推荐程度中等：</p><ul><li>文中理论说明很多，很晦涩，可以不看，直接跳至正文部分即可</li><li>思想简单，了解一下即可</li></ul><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://proceedings.mlr.press/v37/schaul15.pdf" rel="external nofollow" target="_blank">http://proceedings.mlr.press/v37/schaul15.pdf</a></p><blockquote><p>Our main idea is to represent a large set of optimal value functions by a single, unified function approximator that generalises over both states and goals.</p></blockquote><p>主要思想是通过一个统一的函数逼近器来表示一大组最优值函数，该函数逼近器可以概括状态和目标。</p><p>单目标中值函数这么表示：</p><script type="math/tex; mode=display">V_{g, \pi}(s) :=\mathbb{E}\left[\sum_{t=0}^{\infty} R_{g}\left(s_{t+1}, a_{t}, s_{t}\right) \prod_{k=0}^{t} \gamma_{g}\left(s_{k}\right) | s_{0}=s\right]</script><p>动作值函数这么表示：</p><script type="math/tex; mode=display">Q_{g, \pi}(s, a) :=\mathbb{E}_{s^{\prime}}\left[R_{g}\left(s, a, s^{\prime}\right)+\gamma_{g}\left(s^{\prime}\right) \cdot V_{g, \pi}\left(s^{\prime}\right)\right]</script><p>最优策略：</p><script type="math/tex; mode=display">\pi_{g}^{*}(s) :=\arg \max _{a} Q_{\pi, g}(s, a)</script><p>相应的最优值函数：</p><script type="math/tex; mode=display">V_{g}^{*} :=V_{g, \pi_{g}^{*}} \ , \ Q_{g}^{*} :=Q_{g, \pi_{g}^{*}}</script><p>本文中是想将一个单目标的最优值函数逼近改造成多目标的最优值函数表示，形象一点来说，就是把用向量表示状态值函数$V(s; \theta)$变成矩阵表示$V(s, g ; \theta)$，行列分别是状态$s$和目标$g$；把用矩阵表示动作值函数$Q(s, a; \theta)$变成三维Tensor表示$Q(s, a, g ; \theta)$，行列不变，增加维度-深度表示目标$g$。其中满足：</p><script type="math/tex; mode=display">V(s, g ; \theta) \approx V_{q}^{*}(s) \ , \ Q(s, a, g ; \theta) \approx Q_{a}^{*}(s, a)</script><h1 id="文中正文"><a href="#文中正文" class="headerlink" title="文中正文"></a>文中正文</h1><p>该方法拟解决的问题：</p><ul><li>如何通用表示各种问题的值函数逼近器？</li></ul><p>主要思想：</p><ul><li>用单个函数逼近器表示多目标的最优值函数</li></ul><p>实现方法：</p><ul><li>算法的输入由状态$s$扩展为状态-目标$\lt s,g \gt$，假设原状态表示向量$s$不包含目标信息</li></ul><p>有两种实现形式：</p><ul><li>直接用$||$连接，即$\left ( s||g \right )$，然后通过非线性函数逼近器(如MLPs)得到最终输出结果$V(s, g)$</li><li>分别embedding，并将嵌入后的表示通过运算得到<strong>最终输出结果</strong>(文中使用的是点积)，$h : \mathbb{R}^{n} \times \mathbb{R}^{n} \mapsto \mathbb{R}$，然后$V(s, g) :=h(\phi(s), \psi(g))$</li></ul><p><img src="./universal-value-function-approximators/sg.png" alt=""></p><p>左图为连结模式，中间图表示分别embedding并通过函数运算得到标量输出$h : \mathbb{R}^{n} \times \mathbb{R}^{n} \mapsto \mathbb{R}$，右图为中间图的细化。</p><p>文中指出，对于第二种形式，可以使$\phi$网络和$\psi$网络共享几层参数，因为一般来说，目标的向量表示形式与状态的向量表示形式相同，即$\mathcal{G} \subseteq \mathcal{S}$。如果对于对称问题，即奖励函数是$s$和$g$的距离(平方差)等形式，那么有特点：</p><script type="math/tex; mode=display">V_{g}^{*}(s)=V_{s}^{*}(g) \ , \ \forall s, g</script><p>这个时候可以使$\phi$网络和$\psi$网络相同。</p><p><strong><em>注：文中提到有使用低秩因式分解分别表示$\hat{\phi}_{t}$和$\hat{\psi}_{g}$，并使用有监督学习训练两个网络对$s$和$g$的embediing($\phi_{t}$和$\psi_{t}$)进行训练的方法，但是这需要假设问题可进行因式分解，即有限的状态和目标，但这在实际应用中往往是不成立的，所以不对这部分做过多的关注。</em></strong></p><p>优点：</p><ul><li>UVFA可用于同任务多目标的迁移学习中，可以比随机值初始化更快地学习解决新任务。<em>这是显而易见的作用，毕竟本身就是多目标的通用函数逼近器。</em></li><li>可以用于特征表示。<em>这也说，= =，强行增加字数。。。。。。</em></li><li>UVFA有效地提供了一个通用决策模型，代表（近似）朝向任何目标$g \in \mathcal{G}$的最佳行为。<em>这个优点与现在提出的元强化学习中的slow部分如出一辙。</em></li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>UVFA把$V(s)$变成$V(s,g)$，一个可表示<strong>单任务、多目标</strong>的通用值函数近似，扩展对任务的知识表达。(通用即知识，哈哈)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文中的方法简称UVFA，即通用值函数逼近器，主要是用于将只能表示同一任务单目标的值函数表示成通用的多目标值函数。很多论文如HER都引用了这篇论文中提出的方法。&lt;/p&gt;
&lt;p&gt;推荐程度中等：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文中理论说明很多，很晦涩，可以不看，直接跳至正文部分即可&lt;/li&gt;
&lt;li&gt;思想简单，了解一下即可&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04配置Shadowsocks服务器</title>
    <link href="http://StepNeverStop.github.io/ss.html"/>
    <id>http://StepNeverStop.github.io/ss.html</id>
    <published>2019-06-01T14:03:10.000Z</published>
    <updated>2019-06-01T15:00:01.275Z</updated>
    
    <content type="html"><![CDATA[<p>科学上网，你懂的。</p><a id="more"></a><h1 id="购买VPS"><a href="#购买VPS" class="headerlink" title="购买VPS"></a>购买VPS</h1><p><a href="https://www.vultr.com/" rel="external nofollow" target="_blank">VULTR</a></p><p>纽约节点，3.5$一个月，ubuntu 16.04，enable IPV6</p><p>更改root密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo passwd</span><br></pre></td></tr></table></figure><h1 id="shadowsocks-服务器"><a href="#shadowsocks-服务器" class="headerlink" title="shadowsocks 服务器"></a>shadowsocks 服务器</h1><p>更新软件源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure><p>安装PIP</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python-pip</span><br><span class="line">sudo apt-get install python3-pip</span><br></pre></td></tr></table></figure><p>安装shadowsocks</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install https://github.com/shadowsocks/shadowsocks/archive/master.zip</span><br></pre></td></tr></table></figure><p>查看shadowsocks版本，显示”Shadowsocks 3.0.0”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ssserver --version</span><br></pre></td></tr></table></figure><p>创建配置文件夹及文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /etc/shadowsocks</span><br><span class="line">sudo nano /etc/shadowsocks/config.json</span><br></pre></td></tr></table></figure><p>复制并修改配置内容，然后<code>ctrl+x</code>，<code>y</code>，回车，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;server&quot;:&quot;::&quot;,</span><br><span class="line">    &quot;port_password&quot;: &#123;</span><br><span class="line">        &quot;端口1&quot;: &quot;密码1&quot;,</span><br><span class="line">        &quot;端口2&quot;: &quot;密码2&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;timeout&quot;:300,</span><br><span class="line">    &quot;method&quot;:&quot;rc4-md5&quot;,</span><br><span class="line">    &quot;fast_open&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>赋予权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod 755 /etc/shadowsocks/config.json</span><br></pre></td></tr></table></figure><p>为了支持这些加密方式，也许需要安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python-dev</span><br><span class="line">sudo apt-get install python–m2crypto</span><br></pre></td></tr></table></figure><p>服务端后台启停</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo ssserver -c /etc/shadowsocks/config.json -d start</span><br><span class="line">sudo ssserver -c /etc/shadowsocks/config.json -d stop</span><br></pre></td></tr></table></figure><p>配置Systemd管理Shadowsocks，新建Shadowsocks管理文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/systemd/system/shadowsocks-server.service</span><br></pre></td></tr></table></figure><p>复制粘贴，<code>ctrl+x</code>，<code>y</code>，回车</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Shadowsocks Server</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/usr/local/bin/ssserver -c /etc/shadowsocks/config.json</span><br><span class="line">Restart=on-abort</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>启动Shadowsocks</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start shadowsocks-server</span><br></pre></td></tr></table></figure><p>设置开机自启动Shadowsocks</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl enable shadowsocks-server</span><br></pre></td></tr></table></figure><p>查看运行状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl status shadowsocks-server</span><br></pre></td></tr></table></figure><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>查看linux内核</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">uname -r</span><br><span class="line">如果其显示版本在4.9.0之下，则需要升级Linux内核，否则请忽略下文</span><br><span class="line"></span><br><span class="line">sudo apt update</span><br><span class="line">查看可用的Linux内核版本</span><br><span class="line">sudo apt-cache showpkg linux-image</span><br><span class="line">找到一个你想要升级的Linux内核版本，如“linux-image-4.10.0-22-generic”</span><br><span class="line">sudo apt install linux-image-4.10.0-22-generic</span><br><span class="line">重启</span><br><span class="line">sudo reboot</span><br><span class="line">删除旧的内核</span><br><span class="line">sudo purge-old-kernels</span><br></pre></td></tr></table></figure><p>开启BBR</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">modprobe tcp_bbr</span><br><span class="line">echo &quot;tcp_bbr&quot; &gt;&gt; /etc/modules-load.d/modules.conf</span><br><span class="line">echo &quot;net.core.default_qdisc=fq&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo &quot;net.ipv4.tcp_congestion_control=bbr&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><p>运行下两句，均有”bbr”则开启BBR成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sysctl net.ipv4.tcp_available_congestion_control</span><br><span class="line">sysctl net.ipv4.tcp_congestion_control</span><br></pre></td></tr></table></figure><p>优化吞吐量，新建配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/sysctl.d/local.conf</span><br></pre></td></tr></table></figure><p>复制粘贴，<code>ctrl+x</code>，<code>y</code>，回车</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># max open files</span><br><span class="line">fs.file-max = 51200</span><br><span class="line"># max read buffer</span><br><span class="line">net.core.rmem_max = 67108864</span><br><span class="line"># max write buffer</span><br><span class="line">net.core.wmem_max = 67108864</span><br><span class="line"># default read buffer</span><br><span class="line">net.core.rmem_default = 65536</span><br><span class="line"># default write buffer</span><br><span class="line">net.core.wmem_default = 65536</span><br><span class="line"># max processor input queue</span><br><span class="line">net.core.netdev_max_backlog = 4096</span><br><span class="line"># max backlog</span><br><span class="line">net.core.somaxconn = 4096</span><br><span class="line"></span><br><span class="line"># resist SYN flood attacks</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line"># reuse timewait sockets when safe</span><br><span class="line">net.ipv4.tcp_tw_reuse = 1</span><br><span class="line"># turn off fast timewait sockets recycling</span><br><span class="line">net.ipv4.tcp_tw_recycle = 0</span><br><span class="line"># short FIN timeout</span><br><span class="line">net.ipv4.tcp_fin_timeout = 30</span><br><span class="line"># short keepalive time</span><br><span class="line">net.ipv4.tcp_keepalive_time = 1200</span><br><span class="line"># outbound port range</span><br><span class="line">net.ipv4.ip_local_port_range = 10000 65000</span><br><span class="line"># max SYN backlog</span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 4096</span><br><span class="line"># max timewait sockets held by system simultaneously</span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 5000</span><br><span class="line"># turn on TCP Fast Open on both client and server side</span><br><span class="line">net.ipv4.tcp_fastopen = 3</span><br><span class="line"># TCP receive buffer</span><br><span class="line">net.ipv4.tcp_rmem = 4096 87380 67108864</span><br><span class="line"># TCP write buffer</span><br><span class="line">net.ipv4.tcp_wmem = 4096 65536 67108864</span><br><span class="line"># turn on path MTU discovery</span><br><span class="line">net.ipv4.tcp_mtu_probing = 1</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_congestion_control = bbr</span><br></pre></td></tr></table></figure><p>运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl --system</span><br></pre></td></tr></table></figure><p>编辑之前的shadowsocks-server.service文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/systemd/system/shadowsocks-server.service</span><br></pre></td></tr></table></figure><p>在<code>ExecStart</code>前插入一行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ExecStartPre=/bin/sh -c &apos;ulimit -n 51200&apos;</span><br></pre></td></tr></table></figure><p>重载shadowsocks-server.service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl daemon-reload</span><br></pre></td></tr></table></figure><p>重启Shadowsocks</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart shadowsocks-server</span><br></pre></td></tr></table></figure><p>开启TCP Fast Open，降低Shadowsocks服务器和客户端的延迟，将<code>fast_open</code>的值由<code>false</code>修改为<code>true</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/shadowsocks/config.json</span><br></pre></td></tr></table></figure><p>重启服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart shadowsocks-server</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;科学上网，你懂的。&lt;/p&gt;
    
    </summary>
    
      <category term="小知识" scheme="http://StepNeverStop.github.io/categories/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="note" scheme="http://StepNeverStop.github.io/tags/note/"/>
    
      <category term="ubuntu" scheme="http://StepNeverStop.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>RL^2|Fast Reinforcement Learning vis Slow Reinforcement Learning</title>
    <link href="http://StepNeverStop.github.io/rl2.html"/>
    <id>http://StepNeverStop.github.io/rl2.html</id>
    <published>2019-05-31T03:26:39.000Z</published>
    <updated>2019-06-02T01:19:23.071Z</updated>
    
    <content type="html"><![CDATA[<p>本文引用了元学习在深度学习领域的思想，在多任务中训练一个通用模型——slow，用这个通用模型拓展到其他任务进行训练就会快很多，得到新模型——fast。本文中的模型使用RNN作为训练模型。</p><p>推荐程度：中等偏下</p><ul><li>可以拓宽知识面，了解众家思想</li><li>我个人认为，这样的元学习并没有达到让机器”学会如何学习的学习方法“的目的，即“learning to learn”</li></ul><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="https://arxiv.org/pdf/1611.02779.pdf" rel="external nofollow" target="_blank">https://arxiv.org/pdf/1611.02779.pdf</a></p><p>深度强化学习被成功运用在许多复杂行为学习当中，但是需要很多很多很多试错才能得出一个较好的模型，但是动物学习知识只需要一小会儿，就像骑自行车和骑电瓶车，人可能学会骑自行车后5分钟就可以学会骑电瓶车，但是机器却要重头学起，1小时也未必学得会、学得好，这就是因为动物具有先验知识指导，而且可以利用先验知识，机器却不可以。</p><p>把先验知识融入到强化学习算法中过去已经被探索了很多次，而且也有几种不同的形式：</p><ul><li>自动调超参数，学习率等</li><li>使用分层贝叶斯方法在动力学模型上保持后验，并根据后验应用Thompson采样</li><li>许多分层强化学习的工作都提出从以前的任务中提取可重用的技能，以加快对新任务的探索</li></ul><p>本文中的RNN即充当智能体的元学习者，也充当决策者，即生成策略</p><p>以往强化学习算法把学习一个策略当成要解决的问题，本文却把学习强化学习算法当成要解决的问题。很绕对吧，其实本质上就是使用RNN表示策略，加了点深度学习中元学习的思想。</p><p>本文提出的方法是：</p><blockquote><p>we view the learning process of the agent itself as an objective, which can be optimized using standard reinforcement learning algorithms. The objective is averaged across all possible MDPs according to a specific distribution, which reflects the prior that we would like to distill into the agent.  </p></blockquote><p>即，智能体的学习过程看做目标，用标准强化学习算法进行优化。然后使用RNN处理多个MDP问题，提取先验知识到智能体。</p><p><em>注：我个人觉得这根本不能算是真正意义上的元学习，机器还是个傻子。</em></p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>训练的流程图如下：</p><p><img src="./rl2/meta.png" alt=""></p><p>解析：</p><ul><li>在这种训练方式下，需要来回切换任务，即切换MDP过程</li><li>每个trail代表一个MDP过程</li><li>MDP需要从MDPs分布$\rho_{\mathcal{M}} : \mathcal{M} \rightarrow \mathbb{R}_{+}$中采样，至于$\mathbb{R}_{+}$是什么，我不知道</li><li>在每个trial中训练多个episode，个数用$n$表示，图示表示$n=2$</li><li>$h$代表RNN网络中的知识参数</li><li>同一个trial中$h$可以传承，<strong>但是</strong>，当切换MDP问题，即进行下一个trail时，参数$h$需要重新初始化</li><li>输入不是单纯的状态$s$，而是$\left ( s_{t+1},a_{t},r_{t},d_{t} \right )$，其中，$d_{t}$代表episode结束的标志，输出为动作$a_{t+1}$。（输入往往需要embed为$\phi(s, a, r, d)$）</li><li>训练过程的目标<strong>不是最大化一个episode的累计奖励，而是最大化一个trial的累计奖励</strong></li></ul><p>深度学习中元学习有K-shot N-class问题，即N个类，每类K个样本，（本文）强化学习中元学习可以表示为N-episode M-MDPs问题，即共有M个trial，每个trial训练N个episode</p><hr><p>策略表示：</p><ul><li>RNN使用GRUs（Gated Recurrent Units）</li><li>输入为$\phi(s, a, r, d)$</li></ul><p>策略优化：</p><ul><li>TRPO，原因：性能出色，不需要大量调参</li><li>使用baseline减小方差</li><li>考虑使用了GAE</li></ul><p>测试结果：</p><ul><li>MDP问题：在多臂老虎机和网格MDPs任务中，与理论上合理的算法（没有指出是哪些算法）性能相当</li><li>POMDP问题：图像输入的导航任务中，实验表明，智能体能够有效地利用学习到的视觉信息和以往情景中获得的短期信息。实验结果视频：<a href="https://goo.gl/rDDBpb" rel="external nofollow" target="_blank">https://goo.gl/rDDBpb</a></li></ul><h1 id="个人见解"><a href="#个人见解" class="headerlink" title="个人见解"></a>个人见解</h1><p>我认为这样的元强化学习根本不能实现提出元学习的初衷。说到底，这种方式只是让一个“一面白纸”的模型可以学习成多种任务的通用基础模型（slow），然后在使用这个基础模型对其他任务进行训练时可以快速训练出结果（fast）。这种方式可以加速学习，但是却并不能使机器学会去学习，学会如何进行学习的方法，虽然意义上说让智能体学到了强化学习算法的过程，其实本质上是使智能体学到了多种MDP任务中的经验，寻找一个易于根据MDP任务最优化的通用模型，并把它转换为“记忆，而没有教会机器”记忆“这个过程。</p><p>（WTF？RNN训练智能体+多任务切换+更改网络输入+任务级目标函数 这就是元强化学习啦？？？）</p><p><img src="./rl2/meta-rl.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文引用了元学习在深度学习领域的思想，在多任务中训练一个通用模型——slow，用这个通用模型拓展到其他任务进行训练就会快很多，得到新模型——fast。本文中的模型使用RNN作为训练模型。&lt;/p&gt;
&lt;p&gt;推荐程度：中等偏下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以拓宽知识面，了解众家思想&lt;/li&gt;
&lt;li&gt;我个人认为，这样的元学习并没有达到让机器”学会如何学习的学习方法“的目的，即“learning to learn”&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Asynchronous Methods for Deep Reinforcement Learning</title>
    <link href="http://StepNeverStop.github.io/asynchronous-methods-for-drl.html"/>
    <id>http://StepNeverStop.github.io/asynchronous-methods-for-drl.html</id>
    <published>2019-05-30T07:22:13.000Z</published>
    <updated>2019-05-31T16:01:49.826Z</updated>
    
    <content type="html"><![CDATA[<p>本文提出了A3C模型，即Asynchronous Advantage Actor-Critic，是A2C的异步版本，使用CPU多核而不用GPU进行训练，文中说效果比使用GPU反而更好。</p><p>推荐：</p><ul><li>并行梯度优化的佳作</li><li>通俗易懂</li></ul><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="https://arxiv.org/pdf/1602.01783.pdf" rel="external nofollow" target="_blank">https://arxiv.org/pdf/1602.01783.pdf</a></p><p>本文提出了一个概念简单、异步梯度优化的轻量级深度强化学习训练框架。提出该框架的初衷是占用少的资源稳定深度神经网络的学习。</p><p>该框架可适用于：</p><ul><li>基于值与基于策略的方法</li><li>on-policy与off-policy的方法</li><li>离散与连续问题</li></ul><p>效果：</p><ul><li>稳定训练</li><li>在Atari游戏上使用DQN算法，一个16核CPU比Nvidia K40 GPU快，使用A3C算法可以快一倍</li><li>成功适用于很多连续运动学控制问题，例如图像输入的3D迷宫</li></ul><blockquote><p>the sequence of observed data encountered by an online RL agent is non-stationary, and on-line RL updates are strongly correlated. By storing the agent’s data in an experience replay memory, the data can be batched or randomly sampled from different time-steps. Aggregating over memory in this way reduces non-stationarity and decorrelates updates, but at the same time limits the methods to off-policy reinforcement learning algorithms </p></blockquote><p>文中指出，on-policy方法训练不稳定，数据相关性很强，off-policy机制结合经验池机制减轻了训练的不稳定性和数据相关性。</p><p>经验池的缺点：</p><ol><li>占用内存，增加计算量</li><li>需要off-policy算法</li></ol><p>本文为深度强化学习提供了一个非常不同的范例，不使用经验池机制，而使用异步并行的方法在多个<strong>相同环境</strong>中执行多个智能体。（同时训练多个不同环境没有进行描述和实验）</p><p>这种异步并行方式的优点是：</p><ol><li>实现减轻数据相关性的效果，使训练稳定</li><li>可用于大范围on-policy算法，如Sarsa，n-step方法，A-C方法，也可用于off-policy方法，如Q-learning</li><li>利用深度神经网络设计算法，保持鲁棒性与有效性（不予置评）</li><li>不使用GPU，只使用多核CPU，反而训练时间短</li><li>比大规模分布式需要更少的资源占用（内存、算力等）</li><li><strong>相比于<a href="https://arxiv.org/pdf/1507.04296.pdf" rel="external nofollow" target="_blank">Gorila</a>异步训练方式，本文中的训练方式不需要中心服务器，其使用的是共享内存模式</strong></li><li>单机运行减少了通信（梯度和超参数）的开销</li></ol><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><p>由于本文思想非常简单，所以精简描述论文精华</p><p>本文使用<strong><a href="https://arxiv.org/pdf/1106.5730.pdf" rel="external nofollow" target="_blank">Hogwild!</a></strong>方式进行异步梯度下降</p><p>Hogwild! 这个方法的提出也很偶然，容我步步道来：</p><p>我们使用函数对样本集进行拟合如下图所示</p><p><img src="./asynchronous-methods-for-drl/regression.png" alt=""></p><p>当我们想要判断函数是否拟合的不错，我们往往使用损失函数来衡量，损失越小，则代表函数拟合得越好（但，过拟合不是我们想要的）。</p><p><img src="./asynchronous-methods-for-drl/lossfunction.png" alt=""></p><p>为了减小损失，我们常用梯度下降算法来优化。标准的梯度下降原理如下图所示，如果函数为凸函数，且更新步长很小，那么在有限步长内总可以下降至函数最小点，即，获得使损失函数最小的参数$\theta^{\ast}$</p><p><img src="./asynchronous-methods-for-drl/gd.png" alt=""></p><p>之后出现了SGD，也就是随机梯度下降，这种方法差不多在60年代提出，由于思想过于简单，迭代次数很长，一直不被主流优化算法接受。但是，当大数据时代到来时，SGD变成了很普遍的优化方法。</p><p>SGD的算法流程如下：</p><ul><li>选一个初始参数向量$\theta$和正步长$\alpha$</li><li>循环直到满足结束条件：<ul><li>从训练集中随机选择一个样本$x_{i}$</li><li>更新参数$\theta \leftarrow\left(\theta-\alpha \nabla L\left(f_{\theta}\left(x_{i}\right), y_{i}\right)\right)$</li></ul></li></ul><p>SGD的优点：</p><ol><li>少内存占用。SGD不需要所有的样本集进行计算梯度，只需要从样本集中抽取一个样本进行训练。频繁的采样操作可以使用高速缓存来加速训练。</li><li>收敛至<strong>可接受的解</strong>速度很快。其实我们不希望看到过拟合，当然也不希望看到欠拟合，SGD正好是这两个极端的trade-off。SGD可以很快的收敛到一个较好的解，相对于样本集较好的解比相对于样本集最好的解的泛化能力可要强得多。下图展示了SGD与标准梯度下降的损失函数曲线比较。</li></ol><p><img src="./asynchronous-methods-for-drl/sgdvsgd.png" alt=""></p><p>看图像可能会觉得SGD并没有严格下降，有时会有损失上升的倾向，但是，总体来看，这种方法最终也是可以收敛到最小值的。总体上，它使损失进行了下降。</p><p>对于熟悉并行编程的人来说，如果让他们设计并行随机梯度下降，他们一定会像这样设计：</p><ul><li>每个线程从训练集随机抽取一个样本$x_{i}$<ul><li>锁参数$\theta$</li><li>线程读参数$\theta$</li><li>线程更新参数$\theta \leftarrow\left(\theta-\alpha \nabla L\left(f_{\theta}\left(x_{i}\right), y_{i}\right)\right)$</li><li>解锁</li></ul></li></ul><p>更新前锁定参数，更新后解锁参数。对于许多问题，更新这一步骤耗时在微秒级，而锁参数耗时在毫秒级，这意味着锁参数要比更新多占用1000多倍的时间。虽然有一些其他方法可以对该过程进行优化，但差距还是很明显。</p><p>如果，<strong>注释掉关于锁的代码呢？</strong>这真是一个大胆的想法，但是Hogwild!就是这么做的。（这都是一个叫Feng Niu的人搞出来的，不知是出于好奇还是在Debug，他在研究加速SGD的时候注释掉了锁的代码，算法不仅有效，还提升了一百多倍。所以说，多试试总是好的。。。）流程如下：</p><ul><li>每个线程从训练集随机抽取一个样本$x_{i}$<ul><li><del>锁参数$\theta $</del>​</li><li>线程读参数$\theta$</li><li>线程更新参数$\theta \leftarrow\left(\theta-\alpha \nabla L\left(f_{\theta}\left(x_{i}\right), y_{i}\right)\right)$</li><li><del>解锁</del></li></ul></li></ul><blockquote><p>In a sentence, the main idea of Hogwild! is — “<strong>Remove all thread locks from parallel SGD code</strong>.” In Hogwild!, threads can overwrite each other by writing at the same time and compute gradients using a stale version of the “current solution.” </p></blockquote><p>我们不禁都有一个疑问，这么做，真的可以吗？还真别说，经过实验表明，该方法取得了多线程的益处却没有数学效率上的负面影响。</p><p>本文中的异步更新就是用这种方法，唯一有问题的可能就是两个线程同写，但是即便是同写，写入还是有先后的，最多也就是把前一个线程写入的给覆盖掉，丢弃一个线程的数据更新而已，无伤大雅，至于多个线程写后读、读后写倒都没有大的影响，不影响参数更新与收敛。</p><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><p><strong>本文中所有伪代码都使用Hogwild!方式进行梯度更新。</strong></p><p>以下伪代码都是单个线程中actor-learning的操作。每个线程中的actor可以使用不同的探索机制，实验证明不同的探索机制可以提高算法的鲁棒性和性能效果。</p><p>首先介绍异步one-step Q-Learning的训练模式</p><p><img src="./asynchronous-methods-for-drl/a1stepq.png" alt=""></p><p>解析：</p><ul><li>相比后两个伪代码，该代码中各线程是不需要复制用于选择动作的训练网络（因为各方对target network的定义不同，有些人认为等待赋值的是目标网络，有些人认为需要训练的是目标网络，因此，此处不使用目标网络的术语）的，即每次选择动作，都使用其他线程可能更新过的Q网络进行决策。这是因为不需要使用同一个决策模型向后看多步</li><li>$\theta$是动态变化的，即在该线程的训练过程中，其他线程也可能对参数$\theta$进行了更新</li><li>$I_{target}$代表双Q学习赋值的间隔</li><li>$I_{AsyncUpdate}$代表单线程对共享参数$\theta$更新的间隔</li></ul><hr><p>接下来是异步n-step Q-Learning的训练模式：</p><p>相比于n-step，one-step方法中获得的立即奖励$r$只影响导致其产生的$Q(s,a)$，从而通过$Q(s,a)$间接影响其他的动作值，这会使训练过程很慢，因为需要多次更新才能将奖励传播到前面的相关状态和动作。使奖励传播更快的一个方法就是使用n-step回报。</p><script type="math/tex; mode=display">G_{t}=r_{t}+\gamma r_{t+1}+\cdots+\gamma^{n-1} r_{t+n-1}+\max _{a} \gamma^{n} Q\left(s_{t+n}, a\right)</script><p><img src="./asynchronous-methods-for-drl/anstepq.png" alt=""></p><p>解析：</p><ul><li>相比于one-step，该算法为每个线程配置了一个备份网络$\color{red}{\theta’}$</li><li>$t_{max}$为n-step中的$n$</li></ul><hr><p>最后是异步A2C，即A3C的训练模式：</p><p><img src="./asynchronous-methods-for-drl/a3c.png" alt=""></p><p>解析：</p><ul><li>$t_{max}$为n-step向前看的步数</li><li>文中针对该模型将A-C网络架构共享了部分神经网络参数，并且对actor网络的损失函数公式进行了改造：$\nabla_{\theta^{\prime}} \log \pi\left(a_{t} | s_{t} ; \theta^{\prime}\right)\left(R_{t}-V\left(s_{t} ; \theta_{v}\right)\right)+\color{red}{\beta \nabla_{\theta^{\prime}} H\left(\pi\left(s_{t} ; \theta^{\prime}\right)\right)}$，即添加了熵正则化项，它的作用是增加探索，避免网络过早地收敛至局部最优，$\beta$为超参数</li></ul><p>正态分布的熵可以表示为$-\frac{1}{2}\left(\log \left(2 \pi \sigma^{2}\right)+1\right)$</p><h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><p>实验结果视频：</p><ul><li><a href="https://youtu.be/0xo1Ldx3L5Q" rel="external nofollow" target="_blank">TORCS A3C训练驾驶汽车</a></li><li><a href="https://youtu.be/Ajjc08-iPx8" rel="external nofollow" target="_blank">MuJoco 一些训练效果</a></li><li><a href="https://youtu.be/nMR5mjCFZCw" rel="external nofollow" target="_blank">Labyrinth 3D迷宫</a></li></ul><p>Atari 2600 实验结果：</p><p><img src="./asynchronous-methods-for-drl/table1.png" alt=""></p><p>不同线程数的加速效果：</p><p><img src="./asynchronous-methods-for-drl/table2.png" alt=""></p><p>文中比较了三种优化函数的性能，分别是：</p><ul><li>动量SGD，Momentum SGD</li><li>RMSProp</li><li>Shared RMSProp</li></ul><p>RMSProp是这样更新的：</p><script type="math/tex; mode=display">g=\alpha g+(1-\alpha) \Delta \theta^{2}</script><script type="math/tex; mode=display">\theta \leftarrow \theta-\eta \frac{\Delta \theta}{\sqrt{g+\epsilon}}</script><p>$\eta$为学习率，$\alpha$为RMSProp折扣因子</p><p>RMSProp与Shared RMSProp的差别就是：</p><ul><li>Shared RMSProp各线程共享参数$g$，且无锁异步更新</li><li>RMSProp各线程独立一个参数$g$</li></ul><p>实验结果如下：</p><p>测试每种算法50次试验，得分从高到低排列，算法为n-step Q-Learning和A3C</p><p><strong>综合来看，三种优化方式效果差别不大，但是Shared RMSProp&gt;RMSProp&gt;Momentum SGD</strong></p><p><img src="./asynchronous-methods-for-drl/threeoptimizer.png" alt=""></p><hr><p>实验结果太多，懒得贴了，总之，这种异步框架方法的优点是：</p><ul><li>使用离线在线策略，基于值基于策略方法，离散连续问题</li><li>稳定训练</li><li>加速训练</li><li>比经验池少消耗资源</li></ul><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><blockquote><p><a href="https://medium.com/@krishna_srd/parallel-machine-learning-with-hogwild-f945ad7e48a4" rel="external nofollow" target="_blank">Parallel Machine Learning with Hogwild!</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文提出了A3C模型，即Asynchronous Advantage Actor-Critic，是A2C的异步版本，使用CPU多核而不用GPU进行训练，文中说效果比使用GPU反而更好。&lt;/p&gt;
&lt;p&gt;推荐：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;并行梯度优化的佳作&lt;/li&gt;
&lt;li&gt;通俗易懂&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Energy-Based Hindsight Experience Prioritization</title>
    <link href="http://StepNeverStop.github.io/energy-based-hindsight-experience-prioritization.html"/>
    <id>http://StepNeverStop.github.io/energy-based-hindsight-experience-prioritization.html</id>
    <published>2019-05-30T00:58:58.000Z</published>
    <updated>2019-05-30T09:52:24.500Z</updated>
    
    <content type="html"><![CDATA[<p>本文是对HER“事后”经验池机制的一个扩展，它结合了物理学的能量知识以及优先经验回放PER对HER进行提升。简称：EBP</p><p>推荐：</p><ul><li>创新虽不多，但是基于能量的创意可以拓宽在机器人领域训练的视野</li><li>通俗易懂</li></ul><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="https://arxiv.org/pdf/1810.01363.pdf" rel="external nofollow" target="_blank">https://arxiv.org/pdf/1810.01363.pdf</a></p><p>这篇论文由慕尼黑大学博三学生<a href="https://ruizhaogit.github.io" rel="external nofollow" target="_blank">赵瑞</a>和他的导师Volker Tresp发于2018年的CoRL会议。</p><p><strong>本文提出了一个简单高效的、基于能量的方法去优先回放“事后经验”。Energy+HER+PER</strong></p><p>在HER中，智能体从它可完成的“虚拟”目标中进行大量学习，虚拟目标就是我们使用“事后诸葛亮”方法所调整的经验中的目标。</p><p>本文针对原始HER提出了一个稍有不足的地方：经验回放是完全随机的，即没有优先级，没有考虑哪些episode哪些经验对学习更有价值，其实这个问题与PER相对于传统经验池机制也是一样的。</p><p>本文中使用的功能定理（work-energy principle）来计算能量。</p><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><p>相比于传统的PER优先经验回放使用TD-error作为衡量优先级的度量，本文中使用“迹能量”作为其度量。</p><p>迹能量是这么定义的：</p><ul><li><blockquote><p>We define a trajectory energy function as the sum of the transition energy of the target object over the trajectory. </p></blockquote></li><li><p>迹能量是一个episode中transition energy（不知道怎么翻译合适，过渡能量？经验能量？转换能量？）的总和</p></li></ul><p>接下来介绍一下能量在本文中是如何体现的。</p><h2 id="经验能量差-Transition-Energy"><a href="#经验能量差-Transition-Energy" class="headerlink" title="经验能量差 Transition Energy"></a>经验能量差 Transition Energy</h2><p>我就直接拿论文中实验场景所用到的能力来说明这个能量差。简言之，在本文的实验中主要是操作机械手臂移动物体的水平位置和垂直高度，所以物体的能量基本包含三种：</p><ol><li><strong>势能 Potential Energy</strong> $E_{p}(s_{t})$</li><li><strong>动能 Kinetic Energy</strong> $E_{k}(s_{t})$</li><li><strong>转动能，也叫角动能 Rotational Energy</strong> $E_{r}(s_{t})$</li></ol><p>一个物体的能量由这三部分之和组成：</p><script type="math/tex; mode=display">E\left(s_{t}\right)=E_{p}\left(s_{t}\right)+E_{k}\left(s_{t}\right)+E_{r}\left(s_{t}\right)</script><p>经验能量差指的就是相邻状态转移之间的能量差值，表示为：</p><script type="math/tex; mode=display">E_{t r a n}\left(s_{t-1}, s_{t}\right)=\operatorname{clip}\left(E\left(s_{t}\right)-E\left(s_{t-1}\right), 0, E_{t r a n}^{\max }\right)</script><p>其中，</p><ul><li><p>将差值clip到0是因为我们只对由机器人做功导致物体的能量增值感兴趣</p></li><li><p>将差值clip到$E_{t r a n}^{\max }$是想减缓某些特别大的能量差值的影响，使<strong>训练更稳定</strong></p></li></ul><p><em>注：其实我觉得文中加这个clip操作完全是想多使用一个trick，让文章看起来更饱满一点，我个人认为不使用这个clip，或者只对下界进行clip，对算法性能是没有影响的。有待验证。</em></p><h3 id="势能-Potential-Energy"><a href="#势能-Potential-Energy" class="headerlink" title="势能 Potential Energy"></a>势能 Potential Energy</h3><p>物理学中学过，物体的重力势能公式为：$E=mgh$</p><p>本文中这样书写：</p><script type="math/tex; mode=display">E_{p}(s_{t})=mgz_{t}</script><ul><li>$m$代表物体的质量</li><li>$g$代表地球的重力系数，$g \approx 9.81 \mathrm{m} / \mathrm{s}^{2}$</li><li>$z_{t}$代表物体在$t$时刻的高度$h$</li></ul><h3 id="动能-Kinetic-Energy"><a href="#动能-Kinetic-Energy" class="headerlink" title="动能 Kinetic Energy"></a>动能 Kinetic Energy</h3><p>物理学中学过，物体的动能公式为：</p><script type="math/tex; mode=display">E=\frac{1}{2} mv^{2}=\frac{1}{2} m\left [ \frac{\sqrt{v_{x}^{2}+v_{y}^{2}+v_{z}^{2}}}{\Delta t} \right ]^{2}</script><p>本文中这样书写：</p><script type="math/tex; mode=display">E_{k}\left(s_{t}\right)=\frac{1}{2} m v_{x, t}^{2}+\frac{1}{2} m v_{y, t}^{2}+\frac{1}{2} m v_{z, t}^{2} \approx \frac{m\left(\left(x_{t}-x_{t-1}\right)^{2}+\left(y_{t}-y_{t-1}\right)^{2}+\left(z_{t}-z_{t-1}\right)^{2}\right)}{2 \Delta t^{2}}</script><ul><li>$v_{x, t} \approx\left(x_{t}-x_{t-1}\right) / \Delta t$</li><li>$v_{y, t} \approx\left(y_{t}-y_{t-1}\right) / \Delta t$</li><li>$v_{z, t} \approx\left(z_{t}-z_{t-1}\right) / \Delta t$</li><li>$\Delta t$表示相邻两个状态之间的时间间隔，假设我们在模拟器中，1秒60帧，即每帧16.67ms，我们如果每帧执行一次动作，那么$\Delta t=16.67ms$，如果每60帧执行一次动作，那么$\Delta t=1s$</li></ul><h3 id="转动能-Rotational-Energy"><a href="#转动能-Rotational-Energy" class="headerlink" title="转动能 Rotational Energy"></a>转动能 Rotational Energy</h3><p>物理学中学过，物体的转动能公式为：$K=\frac{1}{2} I \cdot \omega^{2}$，注意，中间的点代表点乘，$I$代表物体的惯性矩，$\omega$代表物体的角速度</p><p>本文中这样书写：</p><script type="math/tex; mode=display">\left[ \begin{array}{c}{\phi} \\ {\theta} \\ {\psi}\end{array}\right]=\left[ \begin{array}{c}{\arctan \frac{2(a b+c d)}{1-2\left(b^{2}+c^{2}\right)}} \\ {\arcsin (2(a c-d b))} \\ {\arcsin \frac{2(a d+b c)}{1-2\left(c^{2}+d^{2}\right)}}\end{array}\right]=\left[ \begin{array}{c}{\operatorname{atan} 2\left(2(a b+c d), 1-2\left(b^{2}+c^{2}\right)\right)} \\ {\operatorname{asin}(2(a c-d b))} \\ {\operatorname{atan} 2\left(2(a d+b c), 1-2\left(c^{2}+d^{2}\right)\right)}\end{array}\right]</script><script type="math/tex; mode=display">E_{r}\left(s_{t}\right)=\frac{1}{2} I_{x} \omega_{x, t}^{2}+\frac{1}{2} I_{y} \omega_{y, t}^{2}+\frac{1}{2} I_{z} \omega_{z, t}^{2} \approx \frac{I_{x}\left(\phi_{t}-\phi_{t-1}\right)^{2}+I_{y}\left(\theta_{t}-\theta_{t-1}\right)^{2}+I_{z}\left(\psi_{t}-\psi_{t-1}\right)^{2}}{2 \Delta t^{2}}</script><p>其中$a,b,c,d$为旋转四元组，其知识可以百度或google自行了解。</p><script type="math/tex; mode=display">q=a+b \imath+c \jmath+d k</script><p>$\phi, \theta, \psi$代表$x,y,z$轴方向的旋转角度</p><ul><li>$\omega_{x, t} \approx\left(\phi_{t}-\phi_{t-1}\right) / \Delta_{t}$</li><li>$\omega_{y, t} \approx\left(\theta_{t}-\theta_{t-1}\right) / \Delta_{t}$</li><li>$\omega_{z, t} \approx\left(\psi_{t}-\psi_{t-1}\right) / \Delta_{t}$</li><li>$\Delta t$与上文解释相同</li></ul><p><strong>$m,I_{x},I_{y},I_{z}$可以设置为常量，本文实验中设置$m=I_{x}=I_{y}=I_{z}=1$</strong></p><h2 id="迹能量-Trajectory-Energy"><a href="#迹能量-Trajectory-Energy" class="headerlink" title="迹能量 Trajectory Energy"></a>迹能量 Trajectory Energy</h2><p>给定一个回合中所有的经验能量差，迹能量可以表示为这个回合中所有经验能量差之和：</p><script type="math/tex; mode=display">E_{t r a j}(\mathcal{T})=E_{t r a j}\left(s_{0}, s_{1}, \ldots, s_{T}\right)=\sum_{t=1}^{T} E_{t r a n}\left(s_{t-1}, s_{t}\right)</script><h2 id="基于能量的优先级"><a href="#基于能量的优先级" class="headerlink" title="基于能量的优先级"></a>基于能量的优先级</h2><p>首先计算迹能量，然后对迹能量高的迹（episode）优先进行回放。</p><p>根据迹能量计算迹的优先级为：</p><script type="math/tex; mode=display">p\left(\mathcal{T}_{i}\right)=\frac{E_{t r a j}\left(\mathcal{T}_{i}\right)}{\sum_{n=1}^{N} E_{t r a j}\left(\mathcal{T}_{n}\right)}</script><p>$N$代表经验池中迹的总数量</p><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><p><img src="./energy-based-hindsight-experience-prioritization/pseudo.png" alt=""></p><p>解析：</p><ul><li>以本文实验为例，状态$s$由七元组$\left[x_{t}, y_{t}, z_{t}, a_{t}, b_{t}, c_{t}, d_{t}\right]$表示，其中前三个代表物体的位置，后三个代表物体旋转的四元组。</li><li>目标$g$与状态$s$的表示相同</li><li>$||$操作符为连结的意思，即<code>tf.concat(a,b)</code></li><li>向经验池中存入的不仅仅有$(s,a,r,s’)$，还有优先级$p$与迹能量$E_{traj}$，<strong>其实我感觉这样很多余，如果使用sum-tree结构的，存其一即可</strong></li><li>文中所使用的HER是<strong>future模式</strong></li></ul><p><strong>注意：</strong></p><p>我认为伪代码中有两行很有问题，即</p><p><img src="./energy-based-hindsight-experience-prioritization/issue.png" alt=""></p><p>我不明白为什么把原始经验$\left(s_{t}\left|g, a_{t}, r_{t}, s_{t+1}\right| g, p, E_{t r a j}\right)$存入经验池之后，需要根据优先级采样一个迹，再从采样到的迹中采样出一个经验$\left(s_{t}, a_{t}, s_{t+1}\right)$</p><p>起初我是这么认为的，它想对经验池中迹能量高的episode进行大概率抽取，并对其中的经验进行多次扩充，由此对迹能量小的episode更加忽视，突出迹能量高的episode</p><p>但是，看到下一行<img src="./energy-based-hindsight-experience-prioritization/issue2.png" alt="">我有一个疑问：如果根据优先级采样出的迹$\mathcal{T}$与当前所操作的迹$\mathcal{T}_{current}$不同，那么，为什么还要为不同迹中的经验存入相同的优先级和迹能量呢？即$\left(s_{t}\left|g^{\prime}, a_{t}, r_{t}^{\prime}, s_{t+1}\right| g^{\prime}, p, E_{t r a j}\right)$</p><p>这样肯定是不行的，那么只有一个答案，采样迹这一步多余的，或者说，不应该出现在这里，而应该放在最后一个循环的开始，即</p><p><img src="./energy-based-hindsight-experience-prioritization/issue3.png" alt=""></p><p>也就是说，应该把采样迹，从迹中采样经验的步骤放在minibatch之前，这样就合情合理了。</p><p>这是我自己的一个疑问，如果读者有其他见解，欢迎置评讨论。</p><h2 id="EBP的总结"><a href="#EBP的总结" class="headerlink" title="EBP的总结"></a>EBP的总结</h2><p>EBP与PER的不同点：</p><ul><li>EBP使用物理学中的能量</li><li>PER使用TD-error</li></ul><p>相比于将HER与PER结合而使用TD-error作为衡量优先级的方法，使用迹能量较少了计算量，因为PER每次回放经验都必须重新计算使用经验的新的TD-error，并存回经验池。（其实，如果使用sum-tree来构建PER，这个劣势其实很小）</p><p>文中通过实验发现：比较PER与EBP的时间复杂性，显示EBP提升了算法的性能效果（performance）但是却不增加额外的计算量。PER则提升较少，计算量也增加了。</p><p>EBP的优点：</p><ul><li>可结合任意off-policy算法</li><li>结合了物理知识，使其可以应用于现实世界的问题</li><li>提升采样效率进两倍</li><li>相比最先进的（state-of-the-art）算法，不增加计算时间的情况下，算法效果提升了4个百分点。（此条可以忽略，因为其未必做了充分的实验来进行对比）</li><li>适用于任何机器人操作任务</li><li>适用于多目标算法</li></ul><h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><p>文中实验结果：<a href="https://youtu.be/jtsF2tTeUGQ" rel="external nofollow" target="_blank">https://youtu.be/jtsF2tTeUGQ</a></p><p>代码地址：<a href="https://github.com/ruizhaogit/EnergyBasedPrioritization" rel="external nofollow" target="_blank">https://github.com/ruizhaogit/EnergyBasedPrioritization</a></p><p>实验部分的完整细节请参考论文原文。</p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul><li>OpenAI Gym与MuJoCo物理引擎</li><li>一个7自由度的机械手臂，与HER中一样；一个24自由度的机器手</li><li>四项任务：pick &amp; place，机器手操作方块、蛋、笔</li></ul><p><img src="./energy-based-hindsight-experience-prioritization/env.png" alt=""></p><ul><li>使用稀疏奖励，二分奖励，完成容忍度内目标为0，否则为-1</li></ul><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ul><li>文中没有说明具体使用什么算法作对比，只有伪代码中提到了DPG、DDPG</li><li>文中亦没有对算法中的超参数设置、网络结构进行说明</li><li>19个CPU</li><li>器械臂场景$E_{t r a n}^{\max }=0.5$，机械手场景$E_{t r a n}^{\max }=2.5$</li><li>文中主要比较了HER、HER+PER、HER+EBP</li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="./energy-based-hindsight-experience-prioritization/meansuccessrate.png" alt=""></p><ul><li>横坐标是训练的轮数，应该是指episode的意思</li><li>纵坐标是5个随机种子实验的平均成功率</li><li>蓝色代表HER+EBP，橘色代表HER，绿色代表HER+PER</li></ul><p><img src="./energy-based-hindsight-experience-prioritization/trainingtime.png" alt=""></p><p>结果：</p><ul><li>从上图可以看出，四项任务中，HER+EBP比其他两种方法收敛速度都快，效果也更好一点</li><li>从上表可以看出，HER+EBP与HER的训练时间基本相同，而HER+PER要消耗10倍的时间</li></ul><hr><p><img src="./energy-based-hindsight-experience-prioritization/finalmeanrate.png" alt=""></p><p>结果：</p><ul><li>训练结束后，HER+EBP在四项任务中效果都最好</li><li>HER+EBP比HER提高了1-5个百分点，平均提升了3.75个百分点</li></ul><blockquote><p>We can see that EBP is a simple yet effective method, without increasing computational time, but still, improves current state-of-the-art methods. </p></blockquote><p><img src="./energy-based-hindsight-experience-prioritization/sampleefficiency.png" alt=""></p><p>结果：</p><ul><li>采样效率方面，总体来看，EBP+HER比HER提升了2倍</li></ul><hr><p>最后，作者比较了迹能量与TD-error的pearson相关系数</p><ul><li>系数为1，即正线性相关</li><li>系数为-1，即负线性相关</li><li>系数为0，即不线性相关</li></ul><p><img src="./energy-based-hindsight-experience-prioritization/pearsoncorrelation.png" alt=""></p><p>结果：</p><ul><li>四个实验中，迹能量与TD-error均成正相关</li><li>平均下来pearson系数为0.6，说明迹能量与TD-error呈正线性相关关系，也就是说迹能量可以像TD-error一样表示经验的可学习价值</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是对HER“事后”经验池机制的一个扩展，它结合了物理学的能量知识以及优先经验回放PER对HER进行提升。简称：EBP&lt;/p&gt;
&lt;p&gt;推荐：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;创新虽不多，但是基于能量的创意可以拓宽在机器人领域训练的视野&lt;/li&gt;
&lt;li&gt;通俗易懂&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
</feed>
