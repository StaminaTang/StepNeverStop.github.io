{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/robots.txt","path":"robots.txt","modified":0,"renderable":0},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/Kicon.jpg","path":"images/Kicon.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/images/alipay.jpg","path":"images/alipay.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16.png","path":"images/favicon-16x16.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32.png","path":"images/favicon-32x32.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/wechatpay.jpg","path":"images/wechatpay.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1}],"Cache":[{"_id":"themes/next/debug.log","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1546398377000},{"_id":"source/robots.txt","hash":"2bd0324ddb031aa7eaa53529c9e9e66227d488b4","modified":1557449937104},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1546668615945},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1546668615945},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1546668615946},{"_id":"source/.DS_Store","hash":"e306a5b8de31321aab1138b5403b41ef567f3fce","modified":1546415201000},{"_id":"themes/next/.gitignore","hash":"ee0b13c268cc8695d3883a5da84930af02d4ed08","modified":1546668615950},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1546668615950},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1546668615951},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1546668615951},{"_id":"themes/next/Gemfile","hash":"82e6fee493ac9365ba321b19a14a34f5b381d1c6","modified":1546668615953},{"_id":"themes/next/.DS_Store","hash":"c9c42be7a62ae06aa482d3a2916812fd3fc82734","modified":1546419515000},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1546668615952},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1546668615954},{"_id":"themes/next/.travis.yml","hash":"6674fbdfe0d0c03b8a04527ffb8ab66a94253acd","modified":1546668615953},{"_id":"themes/next/README.cn.md","hash":"23e92a2599725db2f8dbd524fbef2087c6d11c7b","modified":1546668615955},{"_id":"themes/next/README.md","hash":"50abff86ffe4113051a409c1ed9261195d2aead0","modified":1546668615955},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1546668615957},{"_id":"themes/next/package.json","hash":"3963ad558a24c78a3fd4ef23cf5f73f421854627","modified":1546668616026},{"_id":"themes/next/bower.json","hash":"486ebd72068848c97def75f36b71cbec9bb359c5","modified":1546668615957},{"_id":"themes/next/test.md","hash":"b01e235b7eb01d49b8e741b27215954a7662f0d4","modified":1546668616278},{"_id":"source/_data/menu.yml","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1546668615834},{"_id":"themes/next/_config.yml","hash":"f18c7f420cfe734de0c0900d5298035217c45136","modified":1567493292710},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary.md","hash":"863cda6458482a812d7b7a75db8371c817b990af","modified":1557747672603},{"_id":"source/_posts/.DS_Store","hash":"8df1c6f9b9ff204f76344b33ee1490566113fea6","modified":1546412314000},{"_id":"source/_posts/Docker命令学习.md","hash":"29c734b55ad13b121d15a522140b84da4aeb55c5","modified":1570850057976},{"_id":"source/_posts/Git-learn.md","hash":"bbd8d66ad2135982830c2175557f54cd85dfe5ab","modified":1557748031340},{"_id":"source/_posts/Hindsight-Experience-Replay.md","hash":"cff01b716d9e87175b8e3e06141428d1000ba3f8","modified":1559203048345},{"_id":"source/_posts/asynchronous-methods-for-drl.md","hash":"dfdcafca04de8d1fa17b3f8f977e0581d90f4802","modified":1559273116041},{"_id":"source/_posts/conda环境和pip包的转移.md","hash":"8d0f46415a745b1b96c7e31664fa70cecda989f6","modified":1557747739949},{"_id":"source/_posts/MarkDown-Grammar.md","hash":"db23df417f9ed4c04db0c4790458f21e2cbb589e","modified":1557748145906},{"_id":"source/_posts/dynamic-programming.md","hash":"6a79691e9089b7d02e36bdef5cde630b8e610a55","modified":1557751348856},{"_id":"source/_posts/create-sniper-docker-image.md","hash":"0a3601fd24b9a8fcde11d1a28f86630a492cf2f5","modified":1557747874919},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning.md","hash":"e49de7d29e88259340c3cc6768ba70de578579b0","modified":1560391685463},{"_id":"source/_posts/mc-td.md","hash":"be054820650ded85517dba334ce3b7436d8d335c","modified":1557751422482},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization.md","hash":"84089cb72271c898307ab831ba28aef09ef7a87a","modified":1559200806669},{"_id":"source/_posts/rl-rough-reading.md","hash":"54a7751bbdaf2ed52e53e637e6296be3a6a2abf0","modified":1570523431087},{"_id":"source/_posts/rl-with-deep-energy-based-policies.md","hash":"d210bf3f51fae1d66446f34afda70f3de12096f7","modified":1561598999095},{"_id":"source/_posts/rl-classification.md","hash":"a4ae5f03b2e8f3e6807af1946789b08599fc0089","modified":1566711466831},{"_id":"source/_posts/Prioritized-Experience-Replay.md","hash":"8972c542784156d8022f5672ae0ccb0759ef18bc","modified":1571055500252},{"_id":"source/_posts/sarsa-and-q-learning.md","hash":"53b718837e6f1785881b0a6280a076350019c895","modified":1557751597930},{"_id":"source/_posts/Evolution-Strategies-2017.md","hash":"c1c93a5d7b4a0dd3968e28b64b8370128e1602c4","modified":1558525645386},{"_id":"source/_posts/universal-value-function-approximators.md","hash":"b18bd2c2e48c1425b3adc2d469b53ce0f17a76c6","modified":1559461237582},{"_id":"source/_posts/use-conda-env-in-jupyter.md","hash":"7b0d01891aaf6bb16ea40efb8101b08681ca247d","modified":1564448327583},{"_id":"source/_posts/win-rightclick-create-md.md","hash":"13a0a5c524e0f3718eb3261436ac19422474c501","modified":1566810038710},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面.md","hash":"c576db94055c613781d1a92bbf6481b244aa7dd7","modified":1557451498142},{"_id":"source/_posts/ss.md","hash":"0e6eb952853d770eb78e74a473bc0bfdfcb61ed3","modified":1559461237581},{"_id":"source/_posts/创建ML-Agents的Docker镜像.md","hash":"f2d9c8ccd3773e28ef7a2063c26c314e008f09dd","modified":1557451504774},{"_id":"source/_posts/强化学习.md","hash":"f61ccb12553e3eead660f242859336de2a93e269","modified":1561550095394},{"_id":"source/_posts/价值与贝尔曼方程.md","hash":"eb9928dc0423f468367b54ec3c1bb8fa4a5adfde","modified":1557742272683},{"_id":"source/_posts/rl2.md","hash":"a46b4cf63990635de5adbd2fe09acc33b906e06f","modified":1559461237580},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程.md","hash":"3e6ab470e3e8a96c4b1097bba2070f10132e697b","modified":1557709748291},{"_id":"source/categories/.DS_Store","hash":"894a1066d062f778f4734ec041b15051406bc25f","modified":1546414948000},{"_id":"source/categories/index.md","hash":"4a1bc030bc1ac3291951eb91a9c528559cdf9a21","modified":1546668615878},{"_id":"source/about/.DS_Store","hash":"8089fd692eb879a00b8114dae892e0f2dc5dc215","modified":1546415177000},{"_id":"source/about/index.md","hash":"2d44a7c09ff07640294a0828f2cf3093a94c5f45","modified":1567493557483},{"_id":"source/schedule/.DS_Store","hash":"894a1066d062f778f4734ec041b15051406bc25f","modified":1546415196000},{"_id":"source/schedule/index.md","hash":"03b31174f851b6837264c9600ca288a6a8ae8b2f","modified":1546668615878},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5adfad3ef1b870063e621bc0838268eb2c7c697a","modified":1546668615947},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"a0a82dbfabdef9a9d7c17a08ceebfb4052d98d81","modified":1546668615948},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1228506a940114288d61812bfe60c045a0abeac1","modified":1546668615948},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1546668615949},{"_id":"source/tags/.DS_Store","hash":"894a1066d062f778f4734ec041b15051406bc25f","modified":1546414847000},{"_id":"source/tags/index.md","hash":"2b34dd5641ffd25ce2191ffb45d135fe0245cb27","modified":1546668615880},{"_id":"source/sitemap/.DS_Store","hash":"772213df72edfaaabb77a8f34d3bb60937412d35","modified":1546415255000},{"_id":"source/sitemap/index.md","hash":"5849b70a1fbc3ecfa5b2a8f1c750c072f62edcf1","modified":1546668615879},{"_id":"source/_posts/强化学习的里程碑.md","hash":"421eb1d186559973e1e0d293fc59233d97283fca","modified":1557451519932},{"_id":"themes/next/layout/archive.swig","hash":"9a2c14874a75c7085d2bada5e39201d3fc4fd2b4","modified":1546668616022},{"_id":"source/_posts/something-hard-install-docker.md","hash":"aa872782f5a7c1837c299ca40f041109186c3a99","modified":1557748279996},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1546668616022},{"_id":"themes/next/layout/index.swig","hash":"abb86526b6587862c104808cf52cbdf56412a905","modified":1557449937108},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1546668616025},{"_id":"themes/next/layout/page.swig","hash":"e8fcaa641d46930237675d2ad4b56964d9e262e9","modified":1546668616024},{"_id":"themes/next/layout/post.swig","hash":"7a6ce102ca82c3a80f776e555dddae1a9981e1ed","modified":1546668616024},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1546668616026},{"_id":"themes/next/languages/de.yml","hash":"fd02d9c2035798d5dc7c1a96b4c3e24b05b31a47","modified":1546668615958},{"_id":"themes/next/languages/en.yml","hash":"2f4b4776ca1a08cc266a19afb0d1350a3926f42c","modified":1546668615960},{"_id":"themes/next/languages/fr-FR.yml","hash":"efeeb55d5c4add54ad59a612fc0630ee1300388c","modified":1546668615960},{"_id":"themes/next/languages/it.yml","hash":"a215d016146b1bd92cef046042081cbe0c7f976f","modified":1546668615962},{"_id":"themes/next/languages/id.yml","hash":"dccae33e2a5b3c9f11c0e05ec4a7201af1b25745","modified":1546668615961},{"_id":"themes/next/languages/ko.yml","hash":"dc8f3e8c64eb7c4bb2385025b3006b8efec8b31d","modified":1546668615963},{"_id":"themes/next/languages/ja.yml","hash":"37f954e47a3bc669620ca559e3edb3b0072a4be5","modified":1546668615962},{"_id":"themes/next/languages/nl-NL.yml","hash":"213e7a002b82fb265f69dabafbbc382cfd460030","modified":1546668615964},{"_id":"themes/next/languages/pt.yml","hash":"2efcd240c66ab1a122f061505ca0fb1e8819877b","modified":1546668615965},{"_id":"themes/next/languages/pt-BR.yml","hash":"568d494a1f37726a5375b11452a45c71c3e2852d","modified":1546668615964},{"_id":"themes/next/languages/ru.yml","hash":"e33ee44e80f82e329900fc41eb0bb6823397a4d6","modified":1546668615966},{"_id":"themes/next/languages/vi.yml","hash":"a9b89ebd3e5933033d1386c7c56b66c44aca299a","modified":1546668615966},{"_id":"themes/next/languages/zh-hk.yml","hash":"fe0d45807d015082049f05b54714988c244888da","modified":1546668615968},{"_id":"themes/next/languages/zh-tw.yml","hash":"432463b481e105073accda16c3e590e54c8e7b74","modified":1546668615968},{"_id":"themes/next/scripts/merge-configs.js","hash":"38d86aab4fc12fb741ae52099be475196b9db972","modified":1546668616027},{"_id":"themes/next/languages/default.yml","hash":"b3bcd8934327448a43d9bfada5dd11b1b8c1402e","modified":1546668615959},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1546668616028},{"_id":"themes/next/languages/zh-Hans.yml","hash":"1fb60dcb01ae1e59292294275945f347eaf8b1fd","modified":1555233495768},{"_id":"themes/next/source/.DS_Store","hash":"025481e7a0b9714e109ed9fdc8a097c1845b092e","modified":1546426291000},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1546668616279},{"_id":"themes/next/layout/_layout.swig","hash":"8d2beaa32ff0aef2eb9ec8af31e1946781794e31","modified":1557449937106},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1546668616280},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1546668616280},{"_id":"source/_posts/强化学习基本概念.md","hash":"996971b2970cada8acda5d7d856f02062a673aec","modified":1557657539847},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1546668616115},{"_id":"source/_posts/Evolution-Strategies-2017/algorithm1.png","hash":"f4431c520e0ad825bd463cc447323ebe912259c8","modified":1558441934162},{"_id":"source/_posts/Evolution-Strategies-2017/parallelization.png","hash":"1f02681b5fe1680f23b4b48a5ead120f18e07968","modified":1558440335868},{"_id":"source/_posts/Evolution-Strategies-2017/mujoco.png","hash":"231bd199e610864138cde0c912057836a6d8cf16","modified":1558439039063},{"_id":"source/_posts/Hindsight-Experience-Replay/hindsight.png","hash":"d1859e5e2a5fd826f461e4a6d5d57dd38c79c249","modified":1559040646965},{"_id":"source/_posts/asynchronous-methods-for-drl/gd.png","hash":"793f5d0e75c1f42511923da906f93d70b3114727","modified":1559205214867},{"_id":"source/_posts/asynchronous-methods-for-drl/lossfunction.png","hash":"be9353d8c2b1b81beeadf4c004c0abe4348c3792","modified":1559205459855},{"_id":"source/_posts/asynchronous-methods-for-drl/sgdvsgd.png","hash":"83665a9158ebab3a84c4ce770002efb304074dd4","modified":1559206178546},{"_id":"source/_posts/asynchronous-methods-for-drl/regression.png","hash":"b8090fce34ea2ea921bbe44954f499b719d6b232","modified":1559205367526},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_08-30-41.png","hash":"83a2093c01bce4def53352c4c8d71ec27cccaef0","modified":1557449936943},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png","hash":"82d79b39f34388c7c3e1d53d0df9c184aff173b5","modified":1557449936945},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_10-03-30.png","hash":"14d91d4a1075adb4b2c900f86f08469d360eabe2","modified":1557449936945},{"_id":"source/_posts/Hindsight-Experience-Replay/Her.png","hash":"0308d074bc1bd852f8424c8c29173fe8cd13992f","modified":1559049371007},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_10-57-56.png","hash":"2ff022de36958966b11fa2767a8ef352464df350","modified":1557449936948},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_10-11-01.png","hash":"4f0c7c10d84f0a9736119525b8955cecd38698c1","modified":1557449936947},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-32-08.png","hash":"d66fb1805a685b4fd7b35cc769e1ef4b6d406f0e","modified":1557449936955},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-33-27.png","hash":"729ff5819f4896a125b6e78fb7f11bc5755ece9e","modified":1557449936956},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_09-42-07.png","hash":"b95f4dcf0d9e2265cf2061a7e539b2bb9e6656a1","modified":1557449936944},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png","hash":"0f20561591ddeadfe0df86f1855f0d02aa5a8dc2","modified":1557449936947},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_17-12-36.png","hash":"cd606d229c598acb674599866015c25fb4b49491","modified":1557449936960},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_18-11-49.png","hash":"d70c211b0801f50eff38b5d5b4d297ed7b705e0f","modified":1557449936974},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_18-14-21.png","hash":"82c07ac184b8d89aa11e718bcf12293e6554bfb7","modified":1557449936974},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_17-39-07.png","hash":"cc5d08ae28ac589ddd8fc1a32511cd57d2f1e77a","modified":1557449936972},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_18-20-07.png","hash":"6ffc920c6da4731a3466a2aa04a9f86a8589efa5","modified":1557449936975},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_18-31-16.png","hash":"648be7fa6a62443d9da38f531d2e8c4768def0e8","modified":1557449936976},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_18-08-49.png","hash":"bf515ccdeaa1149af9c1d483436cac1e93625fc5","modified":1557449936973},{"_id":"source/_posts/Prioritized-Experience-Replay/visio.vsdx","hash":"82c57a859ab29e92ede4d9d0ee453782232b1899","modified":1558594790532},{"_id":"source/_posts/dynamic-programming/dp.png","hash":"d6f93fd511036ee9557f60a7d1bb614b76c7824f","modified":1557751035613},{"_id":"source/_posts/dynamic-programming/gridworld.png","hash":"985f08ef2435799c50fb2b0552534e77d361f6f1","modified":1557742272676},{"_id":"source/_posts/Prioritized-Experience-Replay/normalized-score1.png","hash":"d57115611a74e0a25c82668678d0d702c2ce0d45","modified":1558595699142},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/issue.png","hash":"5233a522e2346c0734a9ac4a7e8d3d796dbbdcbb","modified":1559182813478},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/finalmeanrate.png","hash":"267198b3a9165098038d037d5041c7f5f087e04c","modified":1559199926933},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/issue3.png","hash":"97b54824c376a59b1596989fc8c45f67e8e0599a","modified":1559183312209},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/issue2.png","hash":"1a616d5bff1afee2e75ea83d63b5bd45a8d211e1","modified":1559183093055},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/trainingtime.png","hash":"00f246434d944d815633a6fa8c659f521a0786b9","modified":1559199823488},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/inequality.png","hash":"0a09a108a132dcf31d16ca6feb18449911eeaa5b","modified":1560389138204},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/x-x^2.png","hash":"71a404f011e958e5e5f2bc28f6ebb794e37185d9","modified":1560389833429},{"_id":"source/_posts/mc-td/dp.png","hash":"d6f93fd511036ee9557f60a7d1bb614b76c7824f","modified":1557751331676},{"_id":"source/_posts/mc-td/mc.png","hash":"05687b207b833daf3801711572bfc9f687d51ceb","modified":1557751323074},{"_id":"source/_posts/mc-td/td.png","hash":"31e9c68a2ff7e27a51141aee7afbbe7f2bb166b0","modified":1557751298654},{"_id":"source/_posts/rl-classification/model-classification.png","hash":"65beb87ac27c5e8a05a8e96a16b92234cec361df","modified":1557749792397},{"_id":"source/_posts/rl-classification/policy-based.png","hash":"1f50811df42bccb176a8d098721458ea32490852","modified":1557749161173},{"_id":"source/_posts/rl-classification/stationary.png","hash":"c316b7092c017c420d2b877c36813f2644775879","modified":1557662608114},{"_id":"source/_posts/rl-with-deep-energy-based-policies/multimodal-policy.png","hash":"6312bf0d05d23b81556bc20505b5bd35d7378010","modified":1561549358746},{"_id":"source/_posts/rl-rough-reading/diayn-pseudo.png","hash":"ed45e55f63b68bb59ead817c0ea2857e3f936d71","modified":1560774647661},{"_id":"source/_posts/rl-rough-reading/diayn.png","hash":"426707f503d64b2dc98bfe5e70d61c2eab5df62b","modified":1560775465587},{"_id":"source/_posts/rl-with-deep-energy-based-policies/unimodal-policy.png","hash":"56ef3ec995d3fd83cc901a91c8bd44d1af991ed3","modified":1561549290259},{"_id":"source/_posts/rl-rough-reading/gorila.png","hash":"beeb25e82565148d03da6f5b7fdca975fb71d8f0","modified":1560160173721},{"_id":"source/_posts/rl-rough-reading/mb-mpo-visio.png","hash":"b9136d3a78ea9f82429e8e1357e18ed1e5514950","modified":1560308073845},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_17-05-30.png","hash":"a89542c74c04189645d7778481277b6d509245f1","modified":1557449936960},{"_id":"source/_posts/rl-classification/value-based.png","hash":"bfdf5b70c3f574985c32035807bc70c0d7f19f34","modified":1557749526466},{"_id":"source/_posts/rl-classification/non-stationary.png","hash":"1f19121d34404e32f3f2da667f9cab1752b1f3ea","modified":1557663218884},{"_id":"source/_posts/rl-rough-reading/visio.vsdx","hash":"822c929a60c77a47bc77ce97a301650487e2b40c","modified":1560827455495},{"_id":"source/_posts/use-conda-env-in-jupyter/1.png","hash":"4aac952f561c9a1f1327b93faa841d003dd6601c","modified":1557449936979},{"_id":"source/_posts/something-hard-install-docker/1.png","hash":"87549df7ed37d6d7007050f9ffaa135624924668","modified":1557449936977},{"_id":"source/_posts/use-conda-env-in-jupyter/3.png","hash":"b83963e3d39d16262fe1a0579fc9ef64a3cff526","modified":1557449936988},{"_id":"source/_posts/rl-classification/图.vsdx","hash":"cbea18d1d388a938c59c1c0221b9d07466dab1d3","modified":1557749513296},{"_id":"source/_posts/use-conda-env-in-jupyter/2.png","hash":"e48c030ee7c801ac51d34cab8ba855f588dcb835","modified":1557449936988},{"_id":"source/_posts/use-conda-env-in-jupyter/4.png","hash":"f7583c736bc02436bf03e99121b69f73fe60ddf5","modified":1557449936989},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/6.png","hash":"47bbca2eb1ddc68d9b6cf156cba85ef210735bd2","modified":1557449937019},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/9.png","hash":"617ce93d753ffa6960bc7806464fd2749231ae95","modified":1557449937023},{"_id":"source/_posts/价值与贝尔曼方程/example2.png","hash":"646cf9c4c0cafeec786e7b26d3138eb23a0c54c0","modified":1557466854099},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/10.png","hash":"064596db0933c2b12490abd6f3989943741a12d1","modified":1557449937007},{"_id":"source/_posts/价值与贝尔曼方程/example6.png","hash":"68d74c5001b82eda81bf49ed063b72c1c9ef2078","modified":1557468425165},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/8.png","hash":"0ce398ad32bc5261fa78eab08b724c8ad96ee56b","modified":1557449937022},{"_id":"source/_posts/价值与贝尔曼方程/example3.png","hash":"019485248ca5fa7e08bbee784ab84fd87e35482e","modified":1557466939972},{"_id":"source/_posts/价值与贝尔曼方程/qsa.jpg","hash":"fff83c05644ae4e30b98188acc7fa7866ea5ded3","modified":1557454482743},{"_id":"source/_posts/价值与贝尔曼方程/example1.png","hash":"7a6334c4fce383da569f77b96683d71de9a587ba","modified":1557466600266},{"_id":"source/_posts/价值与贝尔曼方程/vq.jpg","hash":"ebeb9cd0ae060891fe557f73707dfcf9803c021c","modified":1557454505140},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-44-58.png","hash":"711c592ab41c5c3659c6699b950deb0b064d46d4","modified":1557449937026},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-52-49.png","hash":"1e46bb570e4a512f232955983ea18b2190d17398","modified":1557449937026},{"_id":"source/_posts/价值与贝尔曼方程/v.jpg","hash":"b4346c668d4fc48a82dc2efff7d5365bdd3ab68a","modified":1557454423590},{"_id":"source/_posts/价值与贝尔曼方程/vs.jpg","hash":"0444cdf1b4c9f8fa54c9eb2413ca41007850b1bc","modified":1557454464024},{"_id":"source/_posts/价值与贝尔曼方程/q.jpg","hash":"e6d379acfa4ccc453a63a4095574c5c8519daea3","modified":1557454441195},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-15.png","hash":"5ea2f1add0d50e26e0fdece4e6b31641c5c73e8f","modified":1557449937032},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-01-32.png","hash":"146aabfccfdf3bfb90309df88304564634ab4aa7","modified":1557449937030},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-52.png","hash":"c862d9841a45e1eadcefff91d0ed5f3d65814283","modified":1557449937039},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-02-47.png","hash":"d1d28c63bc96287dc882511a60f0f6a47573836e","modified":1557449937031},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-16-44.png","hash":"5f17cce8addd197a11a86b32875c344e11a8c053","modified":1557449937040},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-25-17.png","hash":"94927046e1aa72159979c02ebae6e6c749a634d4","modified":1557449937042},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-11_15-02-16.png","hash":"52698fed5a3555474d50f06aebc1817c09aafae6","modified":1557449937045},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-18-40.png","hash":"6138514960948cd4fa20fc9ed6b515919af89110","modified":1557449937041},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-11_22-38-42.png","hash":"7a1325c15aca2ef2d32e009e97cca73fa3bb05f4","modified":1557449937046},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-39-26.png","hash":"7811d046eded5e6b1535a4f54657e424ac0e5bff","modified":1557449937044},{"_id":"source/_posts/价值与贝尔曼方程/example5.png","hash":"32caae8ff14e681b53102503a73ee657f40b4265","modified":1557468310322},{"_id":"source/_posts/win-rightclick-create-md/1546050455.jpg","hash":"a08a8362d1de59e1e8e5751b28a3fff0d87c47c1","modified":1557449936995},{"_id":"source/_posts/win-rightclick-create-md/20181229103503.png","hash":"836a0cc95e251f2b72d69fbb1690424b3ad37963","modified":1557449936997},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-35-56.png","hash":"feecbf78f4e1264cc735fb38a2a2886cfaf4a036","modified":1557449937043},{"_id":"source/_posts/win-rightclick-create-md/20181229105300.png","hash":"8ca42182e6814c4a2fc27b57fff1dbfb89e7c949","modified":1557449937000},{"_id":"source/_posts/强化学习的里程碑/GKBattleWithDeepBlue.jpeg","hash":"49cd9889ae2ff006e19d2f24b1d3948fafc46f9c","modified":1557449937102},{"_id":"source/_posts/强化学习的里程碑/KeJieBattleWithAlphaGo.jpeg","hash":"944ea54a8ec7a0d61b08d3548c7ddecf82a6d2d1","modified":1557449937103},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/agent-env.png","hash":"dfe83ed4336f01bbdcdd4e711754ecb5512b16e7","modified":1557452029205},{"_id":"source/_posts/win-rightclick-create-md/20181229103752.png","hash":"6684b234550bfeedbc21a5070450a82a5d4a0086","modified":1557449936998},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1546668615970},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1546668615970},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1546668615990},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1546668615991},{"_id":"source/_posts/强化学习的里程碑/LeeSedolBattleWithAlphaGo.jpeg","hash":"581e4c5538dc4fd621531ae2c368612a90778b3d","modified":1557449937104},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/MPs.jpg","hash":"3e2a1c41c5cfcbf127dbd97d3e7ff8d3c1b9bf11","modified":1557454107615},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4adc65a602d1276615da3b887dcbf2ac68e7382b","modified":1546668615977},{"_id":"themes/next/layout/_partials/footer.swig","hash":"806db4fa1940f3c48a4771c322446a19e32fb09c","modified":1546668615978},{"_id":"themes/next/layout/_partials/head.swig","hash":"24b04658629f56cf54ebcf31dc5b63d09d0fdc1d","modified":1557449937107},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9be624634703be496a5d2535228bc568a8373af9","modified":1546668615996},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1546668615982},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"1634fb887842698e01ff6e632597fe03c75d2d01","modified":1546668615982},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1546668615983},{"_id":"themes/next/layout/_partials/header.swig","hash":"c54b32263bc8d75918688fb21f795103b3f57f03","modified":1546668615981},{"_id":"themes/next/layout/_macro/passage-end-tag.swig","hash":"01eb6a82e6a2f07f4c1288e5a0c6de1b0c9bb99f","modified":1546668615973},{"_id":"themes/next/layout/_macro/my-copyright.swig","hash":"a3192bca82223de12629fb94ce547c2346cefbb1","modified":1546668615972},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1546668615973},{"_id":"themes/next/layout/_macro/post.swig","hash":"a808140d943f47f4981c1e022809e4e7a203ce16","modified":1546668615975},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1546668615977},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"f83befdc740beb8dc88805efd7fbb0fef9ed19be","modified":1546668615974},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1546668616012},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1546668616013},{"_id":"themes/next/layout/_macro/reward.swig","hash":"357d86ec9586705bfbb2c40a8c7d247a407db21a","modified":1546668615975},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"fa882641da3bd83d9a58a8a97f9d4c62a9ee7b5c","modified":1546668616014},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1546668616013},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1546668616015},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1546668616016},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1546668616015},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"ac41ffc2c4cd8059fcf65b7e4d883ab47364bf74","modified":1557709748301},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1546668616030},{"_id":"themes/next/scripts/tags/button.js","hash":"eddbb612c15ac27faf11c59c019ce188f33dec2c","modified":1546668616029},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1546668616031},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1546668616032},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1546668616033},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1546668616030},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1546668616033},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1546668616034},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1546668616035},{"_id":"source/_posts/win-rightclick-create-md/20181229105408.png","hash":"07faaf69e53752c52dd3307aa8716d5f91d2b124","modified":1557449937001},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1546668616116},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1546668616114},{"_id":"themes/next/source/images/Kicon.jpg","hash":"4373f42f4a36dac017399219a33355476cdd515a","modified":1546668616116},{"_id":"themes/next/source/images/alipay.jpg","hash":"fcff318686daaa2de536e3bbc4d0f692ae87eb61","modified":1546668616117},{"_id":"themes/next/source/images/apple-touch-icon.png","hash":"baec5880136ee04093a30b43767a09c3a3ff225a","modified":1546668616118},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1546668616120},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1546668616119},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1546668616120},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1546668616121},{"_id":"themes/next/source/images/favicon-16x16.png","hash":"dbb4ef84d00d2fdf50a12ff69b0c3e988f5888ba","modified":1546668616124},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1546668616122},{"_id":"themes/next/source/css/.DS_Store","hash":"92d674a29dca71261f0e8fb66b187663f3e363da","modified":1546419521000},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1546668616123},{"_id":"themes/next/source/images/favicon-32x32.png","hash":"3376ab0494aaf4584d1b5ebfdb5c6d6b53582410","modified":1546668616125},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1546668616126},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1546668616127},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1546668616124},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1546668616127},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1546668616128},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1546668616128},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1546668616122},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1546668616125},{"_id":"source/_posts/Evolution-Strategies-2017/algorithm2.png","hash":"03ee451c83603dba9a3f7e3febdac0803712c37d","modified":1558441950396},{"_id":"source/_posts/Hindsight-Experience-Replay/rewardshape.png","hash":"d412555c0f7bfa083e620b228a60f7b1c377bbe1","modified":1559048209323},{"_id":"source/_posts/Hindsight-Experience-Replay/tasks.png","hash":"6c3dc1f748102fa8e5e974eb0c6e017f4350287b","modified":1559046908951},{"_id":"source/_posts/Hindsight-Experience-Replay/singlegoal.png","hash":"69f43fc979088448ab910a05113797f35be15aad","modified":1559047747823},{"_id":"source/_posts/asynchronous-methods-for-drl/table1.png","hash":"28f03af54f63ab758819d0dfdc3af35a27d1fa5d","modified":1559271607525},{"_id":"source/_posts/asynchronous-methods-for-drl/table2.png","hash":"3ea357bebc2ba31c1428b11a4f3f40e9014aefdb","modified":1559271709786},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_13-49-28.png","hash":"ad2ac085e8d096290882398ddf0f402843be6f86","modified":1557449936951},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-24-49.png","hash":"f847e0683355dd3b8c2f94070d689ab64eee5b8a","modified":1557449936953},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-44-22.png","hash":"5bacfd1dd4315392a705fba900d540a3baca7ef2","modified":1557449936959},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-40-53.png","hash":"b9985565ba4eb37e3f68440f1266494959b816c7","modified":1557449936958},{"_id":"source/_posts/Evolution-Strategies-2017/frame-skip.png","hash":"d1bdd98e83cd9ea8779a7e95c70a6175f9e641c1","modified":1558441002705},{"_id":"source/_posts/Prioritized-Experience-Replay/sum-tree.png","hash":"1159ede81fafd294e61951cf9907ded8644b89ae","modified":1558593309765},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/env.png","hash":"9c12851f8dca8aee4c24e4679ceceb45b60f13ca","modified":1559198897675},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/meansuccessrate.png","hash":"066db04686ddaca262608d9f8cbe9dba563bbc2a","modified":1559199510326},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/env.png","hash":"da7f5b533d082625d12a83c653763d6f5a0ae79a","modified":1560390793923},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/sampleefficiency.png","hash":"c86d3324bc229fef8adde3d0e8d49a7a638e695b","modified":1559200177076},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/MEP.png","hash":"1095b43f6207f9b9bb294c72b91bfbc0ff28c16d","modified":1560390380401},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/training-time.png","hash":"f77a955f7b43d9db2789b80a1c15d3c485ca60a4","modified":1560391212426},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/pseudo.png","hash":"a12f87c7ddac39d98e41a3aa341e5657dc7523b9","modified":1560346286118},{"_id":"source/_posts/rl2/meta.png","hash":"149f386ffc5a373c8d6cabe90baae11925450186","modified":1559283046108},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/1.png","hash":"af326825623bbd798b64c0d7b0d55e80987a3908","modified":1557449937005},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-36-31.png","hash":"7b20a4cb60606cc964ca46914ef232b402ae2fd4","modified":1557449937029},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-28-19.png","hash":"cfed661288b897840cf511cf5fc55ff09d792ed9","modified":1557449937032},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-29-30.png","hash":"47fd9073f9cba7b1e7772184e1592d888b0e671b","modified":1557449937028},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/MDP.jpg","hash":"3ffd4dca7a616a6e6aeafc66698f32899f225780","modified":1557449937087},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/MP.jpg","hash":"1372f421a8b8fb1c0393b80c2e90fcb1b7cad8b5","modified":1557449937092},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1546668615993},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1546668615994},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1546668616089},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1546668616090},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1546668616092},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1546668616111},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1546668616113},{"_id":"themes/next/source/images/wechatpay.jpg","hash":"0e2196955d6229673c92143a929ff5a54d04a4c7","modified":1546668616129},{"_id":"source/_posts/asynchronous-methods-for-drl/a1stepq.png","hash":"9b07fb37e0c19081accc2bf984bcc037a9382172","modified":1559204692914},{"_id":"source/_posts/asynchronous-methods-for-drl/a3c.png","hash":"e73a84246790f8e0a6a62b276a98c9f5fcefdf4f","modified":1559206658963},{"_id":"source/_posts/asynchronous-methods-for-drl/anstepq.png","hash":"919308ed9841a66cf8fb53792ea1c452136698a4","modified":1559206638700},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-29-58.png","hash":"7593af06e466318684d72dd51e0e7d95f5fae824","modified":1557449936955},{"_id":"source/_posts/dynamic-programming/vi.png","hash":"3e2c0e220045b7ec5354565d20a990f8fb09e0d5","modified":1557742272682},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/sample-efficiency.png","hash":"a4d851bbc8126bf552bbf4c1669decb7dd51a6e7","modified":1560391461255},{"_id":"source/_posts/rl-rough-reading/gorila-pseudo.png","hash":"55db710222283c97d79eb4dee6bc374f90bfeef2","modified":1560160533989},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/4.png","hash":"8f156494735cf0350b426abe850903dc712e08cf","modified":1557449937011},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/M.jpg","hash":"60afd253a45442fa846c2e960a34fe36974f7315","modified":1557449937060},{"_id":"source/_posts/价值与贝尔曼方程/example4.png","hash":"11b071b9e060a8222b23394fc1cf4fad7280f7ea","modified":1557468145102},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1546668615993},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1546668615992},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1546668615980},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1546668615995},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1546668615984},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1546668615985},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1546668615986},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1546668615987},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1546668615981},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1546668615987},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1546668615989},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1546668615997},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1546668615988},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1546668615998},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1546668616000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1546668616000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1546668615998},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"7b11eac3a0685fa1ab2ab6ecff60afc4f15f0d16","modified":1546668615999},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"ccc443b22bd4f8c7ac4145664686c756395b90e0","modified":1546668616001},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1546668616003},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1546668616003},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1546668616001},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1546668616002},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1546668616004},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1546668616005},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1546668616019},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1546668616019},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1546668616021},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1546668616020},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1546668616007},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"493bd5999a1061b981922be92d8277a0f9152447","modified":1546668616009},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1546668616010},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"4dcc3213c033994d342d02b800b6229295433d30","modified":1546668616008},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1546668616009},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"4050553d44ba1396174161c9a6bb0f89fa779eca","modified":1546668616011},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1546668616011},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1546668616006},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1546668616131},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"0289031200c3d4c2bdd801ee10fff13bb2c353e4","modified":1546668616132},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1546668616132},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1546668616133},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1546668616135},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1546668616138},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1546668616134},{"_id":"themes/next/source/js/src/post-details.js","hash":"93a18271b4123dd8f94f09d1439b47c3c19a8712","modified":1546668616136},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1546668616138},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1546668616089},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"7896c3ee107e1a8b9108b6019f1c070600a1e8cc","modified":1546668616090},{"_id":"themes/next/source/js/src/utils.js","hash":"b3e9eca64aba59403334f3fa821f100d98d40337","modified":1546668616139},{"_id":"themes/next/source/css/_common/.DS_Store","hash":"f72fdd48f1f3c3e30582e782ee384fb40c9b1e80","modified":1546419521000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1546668616007},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1546668616091},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"58e7dd5947817d9fc30770712fc39b2f52230d1e","modified":1546668616110},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4069f918ccc312da86db6c51205fc6c6eaabb116","modified":1546668616112},{"_id":"themes/next/source/css/_variables/base.styl","hash":"a82bbe643f8c89a4fb123b538d164e5e4b5f2719","modified":1546668616113},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"c7eda63dacaa3ffa2af3971c0ac665e0d15e0443","modified":1546668616088},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1546668616148},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1546668616191},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1546668616194},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1546668616152},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1546668616228},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"b02737510e9b89aeed6b54f89f602a9c24b06ff2","modified":1546668616153},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"14264a210bf94232d58d7599ea2ba93bfa4fb458","modified":1546668616242},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"e33aa8fa48b6639d8d8b937d13261597dd473b3a","modified":1546668616242},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1546668616197},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1546668616200},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1546668616201},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1546668616111},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1546668616202},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"2ce5f3bf15c523b9bfc97720d8884bb22602a454","modified":1546668616246},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1546668616258},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1546668616259},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1546668616260},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1546668616191},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1546668616192},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1546668616268},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1546668616270},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1546394565000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1546668616277},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1546394565000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1546668616276},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1546668616239},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1546668616230},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1546668616231},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1546668616197},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1546668616232},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1546668616240},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1546668616240},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1546668616248},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1546668616247},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1546668616250},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1546668616250},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1546668616254},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1546668616252},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1546668616251},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1546668616253},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1546668616252},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1546668616254},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1546668616256},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1546668616257},{"_id":"source/_posts/Hindsight-Experience-Replay/finalvsfuture.png","hash":"c7212de7b38806d4a0309910ad8c1ce1184cc1df","modified":1559047324396},{"_id":"source/_posts/Hindsight-Experience-Replay/pseudo.png","hash":"494263d98f77b11524318620f2ccb9531628a373","modified":1559045585105},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1546668616256},{"_id":"source/_posts/Prioritized-Experience-Replay/normalized-score.png","hash":"702b384a45e97ebaf5289a38efe9340767673fb6","modified":1558595669276},{"_id":"source/_posts/Prioritized-Experience-Replay/pseudo.png","hash":"aa580d3591d557b78f9122f979f1e3b08d57fa29","modified":1558525294909},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/pearsoncorrelation.png","hash":"e7e97a270a315cf406bdf37740646e82ac93375e","modified":1559200397448},{"_id":"source/_posts/rl-rough-reading/Agakov.png","hash":"d3c93953485488c6b7e5e28f3962505eafa3bfa7","modified":1560826212691},{"_id":"source/_posts/rl-rough-reading/cdp-pseudo.png","hash":"700e498fb25894be143e019197da28c0ca982bd5","modified":1561102266291},{"_id":"source/_posts/rl-with-deep-energy-based-policies/pseudo.png","hash":"fb069fcc0e0a27003e31ebdbbfc6e52c013e43a6","modified":1561547174474},{"_id":"source/_posts/rl-rough-reading/mb-mpo-pseudo.png","hash":"4ba8ca1132a492540276bafd688eac3397e1eaf3","modified":1560166069089},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/3.png","hash":"fe7dfe4d8603d45162f9d1769931d03386efd1fa","modified":1557449937010},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/7.png","hash":"a588ebe2f2088c621a5260a9a0086b79d40408b9","modified":1557449937021},{"_id":"themes/next/source/js/src/motion.js","hash":"885176ed51d468f662fbf0fc09611f45c7e5a3b1","modified":1546668616135},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1546668616229},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1546668616255},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/16.png","hash":"f40be3c8ef3ef8803c17c9e749547396e3e92e90","modified":1557449936859},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/2.png","hash":"900849540a078f00d12be63bb507a039dd37de7a","modified":1557449936874},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/17.png","hash":"8ab98d82828a90fdbbe9550c6e8f4a2aeaf6c33c","modified":1557449936862},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/4.png","hash":"d80992d6b39c753ac20baec4a50073c907b706e2","modified":1557449936920},{"_id":"source/_posts/Hindsight-Experience-Replay/fourmodel.png","hash":"4e623e54421af73bc241fb49c4e7f167843d0ede","modified":1559048626826},{"_id":"source/_posts/dynamic-programming/iteration.png","hash":"ad52cf8b064058e77a0ffb995c047d9c0c25e411","modified":1557742272679},{"_id":"source/_posts/dynamic-programming/pi.png","hash":"8bb2d97d896e05a2f74dbd44598f19eb871f94ec","modified":1557742272681},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/mean-success.png","hash":"c8ceb00055b96b132be3df14fc424133efa70d8d","modified":1560390949610},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/pseudo.png","hash":"5e7b87272623cc17949a5b37ac0e7b2fc5421bc8","modified":1559182350472},{"_id":"source/_posts/rl-rough-reading/cdp-sg.png","hash":"0596abf360a29555b102daf1ee9553132410c2a1","modified":1561102044213},{"_id":"source/_posts/universal-value-function-approximators/sg.png","hash":"6ead6568bf504ee9a5f7ab3aee112b2417631fda","modified":1559461237585},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/2.png","hash":"249f4af7270f3d1d4d5058938671c596dc66fc5d","modified":1557449937009},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1546668616018},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1546668616017},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1546668616137},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"24ee4b356ff55fc6e58f26a929fa07750002cf29","modified":1546668616083},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1546668616086},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1546668616084},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1546668616084},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1546668616085},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1546668616035},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1546668616082},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1546668616037},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1546668616087},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1546668616038},{"_id":"themes/next/source/css/_common/components/.DS_Store","hash":"992da4de44b0ba9cdbc905fe8a5eca0059b01026","modified":1546420292000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1546668616036},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1546668616037},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1546668616049},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1546668616068},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1546668616101},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1546668616101},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"e695e58f714129ca292c2e54cd62c251aca7f7fe","modified":1546668616102},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1546668616104},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1546668616103},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1546668616106},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1546668616109},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1546668616094},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1546668616108},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1546668616096},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1546668616107},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1546668616106},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1546668616097},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"ad2dcedf393ed1f3f5afd2508d24969c916d02fc","modified":1546668616108},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1546668616095},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"9c99034f8e00d47e978b3959f51eb4a9ded0fcc8","modified":1546668616097},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1546668616095},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1546668616098},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1546668616144},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1546668616147},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1546668616196},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1546668616195},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1546668616204},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1546668616217},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1546668616266},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1546668616267},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1546668616154},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1546668616156},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1546668616156},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1546668616180},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1546668616180},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1546668616181},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1546668616188},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1546668616276},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/5.png","hash":"30f3723a5f63ceb668c0ceb838b27da85c262cc3","modified":1557449936923},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/6.png","hash":"e4c51c5ffb065d86f98621fc3cbc73037b6d516a","modified":1557449936926},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/7.png","hash":"6847e7b98b582d5941ebe109154477ab72be1fba","modified":1557449936929},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/26.png","hash":"d8cf18e3730c405f6d6c187b44b64a21010c7f0f","modified":1557449936904},{"_id":"source/_posts/Evolution-Strategies-2017/atari.png","hash":"4ac985cb7c9e0797f8bcc02e0d482bde90288468","modified":1558439968883},{"_id":"source/_posts/asynchronous-methods-for-drl/threeoptimizer.png","hash":"b5b0926d093cf93a7d02a4fad026a4df19605f21","modified":1559272941995},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/5.png","hash":"b649040df6d1ee3c8aff5dbfbfbe4d55168f5d98","modified":1557449937015},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"60fa84aa7731760f05f52dd7d8f79b5f74ac478d","modified":1546668616093},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1546668616146},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1546668616203},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1546668616189},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1546668616190},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/1.png","hash":"1fb0066b7b6b502080c42ba8d336d41bb0c86138","modified":1557449936834},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/23.png","hash":"72112f7660bdb4d4f7096607745f7dd4760a86ed","modified":1557449936890},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/21.png","hash":"3c45b12dc0e1e4929ea22ca0d14740062ccc2280","modified":1557449936881},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/14.png","hash":"487768f9ce9a801ba04652376a7a1e8e3efbf8ff","modified":1557449936853},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/28.png","hash":"4adaa88692fbacfbd2305513e82b14fda94753d2","modified":1557449936911},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/3.png","hash":"139fafcf0d0ec69aa31336e2f07be08afabc96cf","modified":1557449936915},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/27.png","hash":"1ca67a0009a59aa3ae25ccc21d153a2ceb665fd7","modified":1557449936907},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/11.png","hash":"806bd3d8b1556a72083f5357c8c5220c127cb5ec","modified":1557449936842},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/MRP.png","hash":"08ab5eba51d5be16a0a9709d32e98137cd316187","modified":1557449937096},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"9f73c4696f0907aa451a855444f88fc0698fa472","modified":1546668616039},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1546668616040},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1546668616040},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1546668616041},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1546668616046},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1546668616043},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1546668616047},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1546668616047},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1546668616044},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1546668616048},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1546668616042},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1546668616051},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1546668616049},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1546668616045},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1546668616053},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"50450d9fdc8a2b2be8cfca51e3e1a01ffd636c0b","modified":1546668616044},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1546668616054},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1546668616053},{"_id":"themes/next/source/css/_common/components/post/my-post-copyright.styl","hash":"e954bf95842945b196bbb0de7b8098950a780129","modified":1546668616050},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1546668616052},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"a6c6eb8adba0a090ad1f4b9124e866887f20d10d","modified":1546668616054},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d0d7a5c90d62b685520d2b47fea8ba6019ff5402","modified":1546668616055},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1546668616057},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1546668616058},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"7968343e41f8b94b318c36289dff1196c3eb1791","modified":1546668616059},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1546668616059},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"ccb34c52be8adba5996c6b94f9e723bd07d34c16","modified":1546668616058},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"39f04c4c7237a4e10acd3002331992b79945d241","modified":1546668616061},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1546668616056},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1546668616063},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1546668616063},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1546668616064},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1546668616064},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"3a28364d0df9f7c9b6eecf313d06df2cea163fac","modified":1546668616056},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"26e4e1ed9d5e44a67fc1332a0a1e76bf25dfb343","modified":1546668616060},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"2eea5a53993a24fffe0b91ac205f57c955208764","modified":1546668616062},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1546668616067},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1546668616066},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1546668616077},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"5ef6343835f484a2c0770bd1eb9cc443609e4c39","modified":1546668616066},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1546668616078},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1546668616075},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1546668616076},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"c8fe49a4bc014c24dead05b782a7082411a4abc5","modified":1546668616065},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1546668616080},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1546668616078},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9c8196394a89dfa40b87bf0019e80144365a9c93","modified":1546668616081},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1546668616079},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1546668616069},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1546668616077},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"f825da191816eef69ea8efb498a7f756d5ebb498","modified":1546668616070},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1546668616071},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1546668616069},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1546668616071},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1546668616080},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1546668616072},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1546668616074},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1546668616105},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1546668616073},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1546668616099},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1546668616073},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1546668616100},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1546668616141},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1546668616141},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1546668616142},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1546668616219},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1546668616144},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1546668616143},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1546668616227},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1546668616182},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1546668616183},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1546668616186},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1546668616185},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1546668616187},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1546668616187},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1546668616145},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/12.png","hash":"40ee94e807e5fb8107a389669696df5b88c77f6e","modified":1557449936845},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/8.png","hash":"a09129f8740daa1c9d31dbbbf3e690f1967dec03","modified":1557449936934},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/9.png","hash":"1e1a8203b22f610b8097db6f862f4abe57aa4fa1","modified":1557449936938},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1546668616272},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1546668616221},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1546668616226},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/13.png","hash":"7381a254604ce2061aaf70e3ebc64cc36da5d78c","modified":1557449936848},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/18.png","hash":"c1a2a91e36bed04f9e1e68adf07c2f85edd942d9","modified":1557449936866},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/15.png","hash":"2c612516fef737d86623692f8bdc75f4c687b8cd","modified":1557449936856},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/22.png","hash":"4b16ac8d30269c22ff4c79185ce28c57af303bac","modified":1557449936886},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/20.png","hash":"6702cec99aefcb56a3d87b681a65c06dd2db33ab","modified":1557449936878},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/25.png","hash":"b1e06712ce17fd066ce754d15090c6f0482de641","modified":1557449936899},{"_id":"source/_posts/rl2/meta-rl.png","hash":"17c8eace305bf615ddf9f74d1664a7cce73b961a","modified":1559285566605},{"_id":"source/_posts/rl-rough-reading/skill.png","hash":"330e407c59580b9e28ed124b903b4c28f9073150","modified":1560831274912},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1546668616225},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/19.png","hash":"70457762d1bd3dd19da38c8ef970f889a5c87e34","modified":1557449936871},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1546668616151},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/10.png","hash":"471375e196748ccb29bf050bc737a3b96b678513","modified":1557449936838},{"_id":"source/_posts/dynamic-programming/pivsvi.png","hash":"1fc85aa51fb007eac28c7d89b6a8fce5c7415c49","modified":1557745758717},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/24.png","hash":"f567c2f97adc1279c643338f6783a4308e3f17cf","modified":1557449936894},{"_id":"source/_posts/Prioritized-Experience-Replay/learning-speed.png","hash":"d0473fd72e38609063a631c24e2c440347560da3","modified":1558594995294},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1546668616224},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1546668616264},{"_id":"source/_posts/rl-with-deep-energy-based-policies/1dgmm.gif","hash":"6a8c180515d4754935c24d9f88ad622f720970e8","modified":1561544970909},{"_id":"source/_posts/rl-with-deep-energy-based-policies/vp.gif","hash":"3f528b7cd76f62fc3d802ad1e3ee3d8a335a7035","modified":1561545032365},{"_id":"public/baidusitemap.xml","hash":"da6fc6d14a2674b9724efda666e6e291bc373572","modified":1571232282495},{"_id":"public/atom.xml","hash":"632b4574062312ebc04958af03f717517ab38be8","modified":1571232285709},{"_id":"public/search.xml","hash":"65c72dd8c6d9badec3dbcdfcd32bfdea6defb78b","modified":1571232285497},{"_id":"public/sitemap.xml","hash":"cdd8a4ecd516daeba67e12ef52bba38ce7bdf23a","modified":1571232285709},{"_id":"public/categories/index.html","hash":"491ee2abbdcf00293c09abeb0599d77a5aab30d4","modified":1571232290174},{"_id":"public/sitemap/index.html","hash":"fe4f22a615fb195c3cdd5961f0c4e45532af4535","modified":1571232290361},{"_id":"public/about/index.html","hash":"b9ab20749f81b5b88a413fc88d107674a9e928da","modified":1571232290375},{"_id":"public/schedule/index.html","hash":"af9904e090de5c653901f5a4440a410e74f7d8e1","modified":1571232290374},{"_id":"public/maximum-entropy-regularized-multi-goal-reinforcement-learning.html","hash":"f1b757dcb7f1e42dac619c6ccaddf331a9e95d54","modified":1571232290375},{"_id":"public/rl-rough-reading.html","hash":"f1116c3a0775a72f2093b61a88400ec9455ee291","modified":1571232290375},{"_id":"public/universal-value-function-approximators.html","hash":"f19c01088430feba01322ffd6be957ddcb00bf92","modified":1571232290375},{"_id":"public/ss.html","hash":"07a71d6b92873b0d1dbb4505dc182b41f3a9cc23","modified":1571232290375},{"_id":"public/rl2.html","hash":"c817daf93bb467dab95efad56e024971ab3ad18c","modified":1571232290375},{"_id":"public/asynchronous-methods-for-drl.html","hash":"42ae6a18dc6ac2e20084558a6c2b95c4eba875aa","modified":1571232290376},{"_id":"public/energy-based-hindsight-experience-prioritization.html","hash":"d82cac069d5f50070b40eae74975dcf05a2502ed","modified":1571232290376},{"_id":"public/Hindsight-Experience-Replay.html","hash":"c992409abd84042be5d42cc0f4f46896a7500613","modified":1571232290376},{"_id":"public/Prioritized-Experience-Replay.html","hash":"eb25a03f17d625a09af7d02eaf995460d7e5c6de","modified":1571232290376},{"_id":"public/Evolution-Strategies-2017.html","hash":"be54199d31a91b3bf81493f729e7919db12ff362","modified":1571232290376},{"_id":"public/sarsa-and-q-learning.html","hash":"b87c5bc70303e78e6355e5736714a90939738514","modified":1571232290376},{"_id":"public/mc-td.html","hash":"b6a7444e93bba8c415348df9d77d4292988062fb","modified":1571232290376},{"_id":"public/dynamic-programming.html","hash":"bef90594076015363f483576dc846e3dea864278","modified":1571232290376},{"_id":"public/rl-classification.html","hash":"38a525bffae312eaa82175a986eeffbd6bdbc2a2","modified":1571232290376},{"_id":"public/价值与贝尔曼方程.html","hash":"fc59ee3cb4bb9242bc0cef1108b232f30bfbc123","modified":1571232290376},{"_id":"public/tags/index.html","hash":"1886d01ebb4d3656d717e03e58e27ce83a44e633","modified":1571232290362},{"_id":"public/强化学习.html","hash":"b554057756a16be2501d5f3dfd717585c26484a4","modified":1571232290376},{"_id":"public/强化学习的里程碑.html","hash":"63cbe3ad84b282198fcff58f4f230a9c5c95c2db","modified":1571232290377},{"_id":"public/conda环境和pip包的转移.html","hash":"b1648c4f3027b126594311c4d714f249b46f79e6","modified":1571232290377},{"_id":"public/强化学习基本概念.html","hash":"194bfd35ca31cc3e71536604e4447895e1f62523","modified":1571232290377},{"_id":"public/AnderewNg-deeplearning-note-summary.html","hash":"19f8ff0c9a4852a8add63be0acc499befef1ee4a","modified":1571232290377},{"_id":"public/something-hard-install-docker.html","hash":"c7d683ad5a12f67c193b5343696ec50b045e43de","modified":1571232290377},{"_id":"public/use-conda-env-in-jupyter.html","hash":"b720e12cdf760d26bc8685ccedb2a592aa80aad7","modified":1571232290377},{"_id":"public/为远程Ubuntu服务器安装图像界面.html","hash":"dc58bd55f902e18e84536e4fa07736383d6ce6b1","modified":1571232290384},{"_id":"public/创建ML-Agents的Docker镜像.html","hash":"8814fcc2ec42de6cd4548ee534b2dc42ca8a4104","modified":1571232290378},{"_id":"public/Docker命令学习.html","hash":"c5d31e73f7a945d83c1e3355e6299422e4197295","modified":1571232290378},{"_id":"public/create-sniper-docker-image.html","hash":"2f73ddaed6a99fc496b79b02f7ff21544c33c652","modified":1571232290384},{"_id":"public/Git-learn.html","hash":"2e842335991818e8b464920828cca6015b9c01b3","modified":1571232290384},{"_id":"public/win-rightclick-create-md.html","hash":"2211a57fc0453ff21abc1e37bffc8c4a84ddf581","modified":1571232290384},{"_id":"public/MarkDown-Grammar.html","hash":"45571a0430b1b5420ff1cf75bdeca014a3fe5606","modified":1571232290384},{"_id":"public/archives/index.html","hash":"8f9277e32db1d65bb85980ecfb995fb5f45678ce","modified":1571232290384},{"_id":"public/archives/page/2/index.html","hash":"d6a9f26b92555a994dda8b112287e78876b6df82","modified":1571232290385},{"_id":"public/archives/page/3/index.html","hash":"9d5526e8acc131c94e42711f13ac2a6ea223a3db","modified":1571232290384},{"_id":"public/archives/2018/index.html","hash":"256af683d330d18f8a69288a521cbea3bef22b88","modified":1571232290385},{"_id":"public/archives/2018/12/index.html","hash":"3a59fdb538d1a45e8588926704f0f53dee65d174","modified":1571232290385},{"_id":"public/archives/2019/index.html","hash":"f7333af30487cfdb0f2df63bb2000762fb7bcb37","modified":1571232290385},{"_id":"public/archives/2019/page/2/index.html","hash":"4fcfd8e9919ac0ee9b67da353ee0bafaff648452","modified":1571232290385},{"_id":"public/archives/2019/01/index.html","hash":"bc8ea548c19b52a4442940989eacda747e6d806f","modified":1571232290385},{"_id":"public/archives/2019/page/3/index.html","hash":"9b84b697e75f5bcfc64a97c3f63cc739c18965f7","modified":1571232290385},{"_id":"public/archives/2019/03/index.html","hash":"9222689d4a6af8d91c1e3afdcffa5f0e0d647e1e","modified":1571232290385},{"_id":"public/archives/2019/04/index.html","hash":"c3d7729c6bf16db968bc33e0b3e9b02ea60bf710","modified":1571232290385},{"_id":"public/archives/2019/05/index.html","hash":"5ee4ba955a4c10ad3d27eed42a6a319d2bebdee5","modified":1571232290385},{"_id":"public/archives/2019/05/page/2/index.html","hash":"76343850e0e6119c3365d65ba4efc21b557f57a5","modified":1571232290385},{"_id":"public/archives/2019/06/index.html","hash":"3964f35d092b591a00ce0e17e663fcbd469441f6","modified":1571232290386},{"_id":"public/categories/DeepLearning/index.html","hash":"13fb432cf2f4769323f6480d85b6a0ab818c5c69","modified":1571232290385},{"_id":"public/categories/小知识/index.html","hash":"492e2468da3d39450e7765d5ce17935058773ecf","modified":1571232290385},{"_id":"public/categories/Docker/index.html","hash":"b5188aae67c8a1d72d1305f308360b8cd4805b06","modified":1571232290385},{"_id":"public/categories/ReinforcementLearning/index.html","hash":"33b5e9d0f7a72170b1476012e2f1b68cd29c060d","modified":1571232290385},{"_id":"public/categories/Conda/index.html","hash":"6e621a448637442a5c1c9bc4b18c6476a693d2c4","modified":1571232290386},{"_id":"public/categories/Ubuntu/index.html","hash":"a587204ca3ce53c9a0a9fd7a272bf709b052060e","modified":1571232290386},{"_id":"public/categories/Docker/Unity/index.html","hash":"d9de2e055feeec06613f23a693c973c67be029b9","modified":1571232290386},{"_id":"public/index.html","hash":"2994f5194373e1ac69c9d81b44084d42c946a022","modified":1571232290386},{"_id":"public/page/2/index.html","hash":"a2271b2be829e7a63698a1574e9c631e42e17f9d","modified":1571232290386},{"_id":"public/categories/ReinforcementLearning/page/2/index.html","hash":"3013e16039488852f0b7f6d3fbc412ee3db0c6fc","modified":1571232290386},{"_id":"public/tags/note/index.html","hash":"54ef5b97ad9008562ff34f3e734dc11a60f670b0","modified":1571232290386},{"_id":"public/tags/deeplearning/index.html","hash":"eed177a8b4cdc8d2372347cfe80ee8d55d11c3e1","modified":1571232290386},{"_id":"public/tags/Git/index.html","hash":"dc132af99267c3b672279eaeda36f5521c60bc0f","modified":1571232290387},{"_id":"public/tags/docker/index.html","hash":"e6e9e6a8fd13dea2a263669b7698165050d72d5a","modified":1571232290387},{"_id":"public/tags/markdown/index.html","hash":"ed27c0c3d1c21c25490267c241620ff20403b6a7","modified":1571232290386},{"_id":"public/tags/rl/index.html","hash":"b3ec713f4498b4afa7d7478304db6e7ccc9aad77","modified":1571232290387},{"_id":"public/tags/conda/index.html","hash":"fa1eaebf982de02ed71289e3b03ca5477c4eb91c","modified":1571232290387},{"_id":"public/tags/jupyter-notebook/index.html","hash":"dc89cc33b1bfe2220d894a14c65ca960faac9acc","modified":1571232290387},{"_id":"public/tags/ubuntu/index.html","hash":"25da9ff56acbd425ca54febcb462d8add417d49a","modified":1571232290388},{"_id":"public/tags/x2go/index.html","hash":"d1450619e8d4bc06ba0e665694127a6b43598049","modified":1571232290387},{"_id":"public/tags/mxnet/index.html","hash":"0536027c2aa6331204bc44318f733f00df39817c","modified":1571232290387},{"_id":"public/tags/sniper/index.html","hash":"22c38306c6ccd752607033664d022a127af3ae2a","modified":1571232290387},{"_id":"public/tags/unity/index.html","hash":"20e29bbcfcca2cc8d59e4264e619462cf0b998cd","modified":1571232290387},{"_id":"public/tags/rl/page/2/index.html","hash":"57d12969ec9892bf7fd99120c4b0467719c0a2ca","modified":1571232290387},{"_id":"public/tags/ml-agents/index.html","hash":"e4999265acf985f0ffd472bae94b6490107cd4f1","modified":1571232290388},{"_id":"public/强化学习之MDP马尔科夫决策过程.html","hash":"7ed1a2773d5edd6cc694f0a4a921f513c06b34af","modified":1571232290377},{"_id":"public/rl-with-deep-energy-based-policies.html","hash":"ddb6f7bb749e0b885cd8f9058afc634075744861","modified":1571232290375},{"_id":"public/archives/page/4/index.html","hash":"2c7456bd0b2337f60b3dee000707bcbde288b030","modified":1571232290385},{"_id":"public/live2dw/assets/hijiki.model.json","hash":"feff43bf7498d213982c3736c2c029664e4bcbd2","modified":1561550543250},{"_id":"public/live2dw/assets/mtn/00_idle.mtn","hash":"b224c60e463b9f71ddbfc0c720e430496c175f4f","modified":1561550543250},{"_id":"public/live2dw/assets/mtn/06.mtn","hash":"ad404bd852d276cdd3d054c953e23f90e4e45ae1","modified":1561550543250},{"_id":"public/live2dw/assets/mtn/03.mtn","hash":"f900737c7a98441cbb2e05255427e6260e19ae68","modified":1561550543250},{"_id":"public/live2dw/assets/mtn/08.mtn","hash":"4411c7651ff65195b113d95e7d5ebef8a59a37d9","modified":1561550543250},{"_id":"public/live2dw/assets/mtn/07.mtn","hash":"b7f2e3a9fa4f3ffbb6e64a08f8d9f45ca1868ffb","modified":1561550543250},{"_id":"public/live2dw/lib/L2Dwidget.min.js","hash":"5f1a807437cc723bcadc3791d37add5ceed566a2","modified":1561550543250},{"_id":"public/live2dw/assets/mtn/01.mtn","hash":"fb550833ae22c9954c3e01df37ed29b2d61700f2","modified":1561550543250},{"_id":"public/live2dw/assets/hijiki.pose.json","hash":"81438bf69b32c7c11e311b4fe043730cdc7b7ec2","modified":1561550543250},{"_id":"public/live2dw/assets/mtn/02.mtn","hash":"7eafc52edc73b7cb80ae70d34b43c6ac778fa47b","modified":1561550543250},{"_id":"public/live2dw/assets/mtn/05.mtn","hash":"dd20ad24b5d1830a5d44b9bccb28f922eea5e0e5","modified":1561550543250},{"_id":"public/live2dw/assets/mtn/04.mtn","hash":"c7a25d3c5d783639bae18db2f3cd284b819c3c85","modified":1561550543250},{"_id":"public/rl-with-deep-energy-based-policies/unimodal-policy.png","hash":"56ef3ec995d3fd83cc901a91c8bd44d1af991ed3","modified":1561550543251},{"_id":"public/rl-with-deep-energy-based-policies/multimodal-policy.png","hash":"6312bf0d05d23b81556bc20505b5bd35d7378010","modified":1561550543251},{"_id":"public/live2dw/lib/L2Dwidget.min.js.map","hash":"3290fe2df45f065b51a1cd7b24ec325cbf9bb5ce","modified":1561550543271},{"_id":"public/live2dw/assets/moc/hijiki.moc","hash":"44289e62545a7046e0f5231103a851750b78524e","modified":1561550543381},{"_id":"public/live2dw/lib/L2Dwidget.0.min.js","hash":"35bb5b588b6de25c9be2dd51d3fd331feafac02d","modified":1561550543381},{"_id":"public/rl-with-deep-energy-based-policies/pseudo.png","hash":"fb069fcc0e0a27003e31ebdbbfc6e52c013e43a6","modified":1561550543381},{"_id":"public/live2dw/assets/moc/hijiki.2048/texture_00.png","hash":"66464e0d96439695b5542c5e2f5be60739c29999","modified":1561550543406},{"_id":"public/live2dw/lib/L2Dwidget.0.min.js.map","hash":"35e71cc2a130199efb167b9a06939576602f0d75","modified":1561550543422},{"_id":"public/rl-with-deep-energy-based-policies/1dgmm.gif","hash":"6a8c180515d4754935c24d9f88ad622f720970e8","modified":1561550543422},{"_id":"public/rl-with-deep-energy-based-policies/vp.gif","hash":"3f528b7cd76f62fc3d802ad1e3ee3d8a335a7035","modified":1561550543424},{"_id":"source/_posts/rl-rough-reading/naf-pseudo.png","hash":"694309346a8436db231f565f95ae4979b56d7d28","modified":1562727681947},{"_id":"public/rl-rough-reading/naf-pseudo.png","hash":"694309346a8436db231f565f95ae4979b56d7d28","modified":1565051382469}],"Category":[{"name":"DeepLearning","_id":"cjxd6m9s00004ekveb0b3tima"},{"name":"小知识","_id":"cjxd6m9si000bekvewgsxgaf2"},{"name":"Docker","_id":"cjxd6m9sx000hekvef7j07efs"},{"name":"ReinforcementLearning","_id":"cjxd6m9tt000tekvex8mn3w05"},{"name":"Conda","_id":"cjxd6m9u50010ekve7p1jy6d4"},{"name":"Ubuntu","_id":"cjxd6m9uy001uekveo7yiqflj"},{"name":"Unity","parent":"cjxd6m9sx000hekvef7j07efs","_id":"cjxd6ma400036ekve03rjwdu3"}],"Data":[],"Page":[{"title":"categories","date":"2019-01-02T07:40:45.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2019-01-02 15:40:45\ntype: \"categories\"\ncomments: false\n---\n","updated":"2019-01-05T06:10:15.878Z","path":"categories/index.html","layout":"page","_id":"cjxd6m9rl0001ekvexepcayd7","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"sitemap","date":"2019-01-02T07:46:11.000Z","type":"sitemap","_content":"","source":"sitemap/index.md","raw":"---\ntitle: sitemap\ndate: 2019-01-02 15:46:11\ntype: \"sitemap\"\n---\n","updated":"2019-01-05T06:10:15.879Z","path":"sitemap/index.html","comments":1,"layout":"page","_id":"cjxd6m9rq0003ekvekmh1jpvc","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Tagcloud","date":"2019-01-02T07:32:57.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: Tagcloud\ndate: 2019-01-02 15:32:57\ntype: \"tags\"\ncomments: false\n---\n","updated":"2019-01-05T06:10:15.880Z","path":"tags/index.html","layout":"page","_id":"cjxd6m9s80007ekvee516uljs","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"自我介绍","date":"2019-01-02T07:30:04.000Z","type":"about","comments":0,"_content":"\n​\t\n\n- 上海大学计算机工程与科学学院研究生在读，目前研二\n- 主要研究方向：强化学习\n- 邮箱地址：271668153@qq.com\n- 代码仓库：[https://github.com/StepNeverStop](https://github.com/StepNeverStop)\n\n","source":"about/index.md","raw":"---\ntitle: 自我介绍\ndate: 2019-01-02 15:30:04\ntype: \"about\"\ncomments: false\n---\n\n​\t\n\n- 上海大学计算机工程与科学学院研究生在读，目前研二\n- 主要研究方向：强化学习\n- 邮箱地址：271668153@qq.com\n- 代码仓库：[https://github.com/StepNeverStop](https://github.com/StepNeverStop)\n\n","updated":"2019-09-03T06:52:37.483Z","path":"about/index.html","_id":"cjxd6m9sc0009ekveb2q6ijcw","layout":"page","content":"<p>​    </p>\n<ul>\n<li>上海大学计算机工程与科学学院研究生在读，目前研二</li>\n<li>主要研究方向：强化学习</li>\n<li>邮箱地址：271668153@qq.com</li>\n<li>代码仓库：<a href=\"https://github.com/StepNeverStop\" rel=\"external nofollow\" target=\"_blank\">https://github.com/StepNeverStop</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>​    </p>\n<ul>\n<li>上海大学计算机工程与科学学院研究生在读，目前研二</li>\n<li>主要研究方向：强化学习</li>\n<li>邮箱地址：271668153@qq.com</li>\n<li>代码仓库：<a href=\"https://github.com/StepNeverStop\" rel=\"external nofollow\" target=\"_blank\">https://github.com/StepNeverStop</a></li>\n</ul>\n"},{"title":"schedule","date":"2019-01-02T07:46:05.000Z","type":"schedule","_content":"","source":"schedule/index.md","raw":"---\ntitle: schedule\ndate: 2019-01-02 15:46:05\ntype: \"schedule\"\n---\n","updated":"2019-01-05T06:10:15.878Z","path":"schedule/index.html","comments":1,"layout":"page","_id":"cjxd6m9sm000dekvex6cu608k","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"吴恩达deeplearning.ai课程笔记总结","copyright":true,"top":1,"date":"2019-03-25T10:30:30.000Z","_content":"\n在吴恩达机器学习系列课程完结后不久，一位名叫[Tess Ferrandez](https://www.slideshare.net/TessFerrandez?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview \"Tess Ferrandez小姐姐的主页\")的小姐姐在推特上分享了一套自己的课程笔记，瞬间收获了3k+赞和1k+转发。\n\n不同于满屏公式代码的黑白笔记，这套信息图不仅知识点满满，且行文构图都像插画一样颜值颇高。吴恩达自己也在推特上转发称赞了这一位有诚意的学习者，毕竟他一直倡导学习是一件简单快乐的事情。\n\nLink: [笔记源地址](https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng \"笔记源地址\")\n\n<!--more-->\n\n# 深度学习介绍\n![](./AnderewNg-deeplearning-note-summary/1.png)\n# 逻辑回归\n![](./AnderewNg-deeplearning-note-summary/2.png)\n# 浅层神经网络\n![](./AnderewNg-deeplearning-note-summary/3.png)\n# 深层神经网络\n![](./AnderewNg-deeplearning-note-summary/4.png)\n# 机器学习应用程序设置\n![](./AnderewNg-deeplearning-note-summary/5.png)\n# 正则化——防止过拟合\n![](./AnderewNg-deeplearning-note-summary/6.png)\n# 优化训练\n![](./AnderewNg-deeplearning-note-summary/7.png)\n# 优化算法\n![](./AnderewNg-deeplearning-note-summary/8.png)\n# 超参数调试\n![](./AnderewNg-deeplearning-note-summary/9.png)\n# 机器学习项目构建\n![](./AnderewNg-deeplearning-note-summary/10.png)\n# 错误分析\n![](./AnderewNg-deeplearning-note-summary/11.png)\n# 训练 vs 验证/测试 失配\n![](./AnderewNg-deeplearning-note-summary/12.png)\n# 扩展学习\n![](./AnderewNg-deeplearning-note-summary/13.png)\n# 卷积基础\n![](./AnderewNg-deeplearning-note-summary/14.png)\n# Padding\n![](./AnderewNg-deeplearning-note-summary/15.png)\n# 深层 CNN\n![](./AnderewNg-deeplearning-note-summary/16.png)\n# 典型的 CNN 模型\n![](./AnderewNg-deeplearning-note-summary/17.png)\n# ResNet\n![](./AnderewNg-deeplearning-note-summary/18.png)\n# 实用建议\n![](./AnderewNg-deeplearning-note-summary/19.png)\n# 检测算法\n![](./AnderewNg-deeplearning-note-summary/20.png)\n# 人脸识别\n![](./AnderewNg-deeplearning-note-summary/21.png)\n# 神经风格迁移\n![](./AnderewNg-deeplearning-note-summary/22.png)\n# 循环神经网络\n![](./AnderewNg-deeplearning-note-summary/23.png)\n# 更多 RNN 模型\n![](./AnderewNg-deeplearning-note-summary/24.png)\n# NLP-词嵌入\n![](./AnderewNg-deeplearning-note-summary/25.png)\n# 词嵌入详解\n![](./AnderewNg-deeplearning-note-summary/26.png)\n# 序列到序列基本模型\n![](./AnderewNg-deeplearning-note-summary/27.png)\n# 序列到序列\n![](./AnderewNg-deeplearning-note-summary/28.png)","source":"_posts/AnderewNg-deeplearning-note-summary.md","raw":"---\ntitle: 吴恩达deeplearning.ai课程笔记总结\ncopyright: true\ntop: 1\ndate: 2019-03-25 18:30:30\ncategories: DeepLearning\ntags:\n- note\n- deeplearning\n---\n\n在吴恩达机器学习系列课程完结后不久，一位名叫[Tess Ferrandez](https://www.slideshare.net/TessFerrandez?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview \"Tess Ferrandez小姐姐的主页\")的小姐姐在推特上分享了一套自己的课程笔记，瞬间收获了3k+赞和1k+转发。\n\n不同于满屏公式代码的黑白笔记，这套信息图不仅知识点满满，且行文构图都像插画一样颜值颇高。吴恩达自己也在推特上转发称赞了这一位有诚意的学习者，毕竟他一直倡导学习是一件简单快乐的事情。\n\nLink: [笔记源地址](https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng \"笔记源地址\")\n\n<!--more-->\n\n# 深度学习介绍\n![](./AnderewNg-deeplearning-note-summary/1.png)\n# 逻辑回归\n![](./AnderewNg-deeplearning-note-summary/2.png)\n# 浅层神经网络\n![](./AnderewNg-deeplearning-note-summary/3.png)\n# 深层神经网络\n![](./AnderewNg-deeplearning-note-summary/4.png)\n# 机器学习应用程序设置\n![](./AnderewNg-deeplearning-note-summary/5.png)\n# 正则化——防止过拟合\n![](./AnderewNg-deeplearning-note-summary/6.png)\n# 优化训练\n![](./AnderewNg-deeplearning-note-summary/7.png)\n# 优化算法\n![](./AnderewNg-deeplearning-note-summary/8.png)\n# 超参数调试\n![](./AnderewNg-deeplearning-note-summary/9.png)\n# 机器学习项目构建\n![](./AnderewNg-deeplearning-note-summary/10.png)\n# 错误分析\n![](./AnderewNg-deeplearning-note-summary/11.png)\n# 训练 vs 验证/测试 失配\n![](./AnderewNg-deeplearning-note-summary/12.png)\n# 扩展学习\n![](./AnderewNg-deeplearning-note-summary/13.png)\n# 卷积基础\n![](./AnderewNg-deeplearning-note-summary/14.png)\n# Padding\n![](./AnderewNg-deeplearning-note-summary/15.png)\n# 深层 CNN\n![](./AnderewNg-deeplearning-note-summary/16.png)\n# 典型的 CNN 模型\n![](./AnderewNg-deeplearning-note-summary/17.png)\n# ResNet\n![](./AnderewNg-deeplearning-note-summary/18.png)\n# 实用建议\n![](./AnderewNg-deeplearning-note-summary/19.png)\n# 检测算法\n![](./AnderewNg-deeplearning-note-summary/20.png)\n# 人脸识别\n![](./AnderewNg-deeplearning-note-summary/21.png)\n# 神经风格迁移\n![](./AnderewNg-deeplearning-note-summary/22.png)\n# 循环神经网络\n![](./AnderewNg-deeplearning-note-summary/23.png)\n# 更多 RNN 模型\n![](./AnderewNg-deeplearning-note-summary/24.png)\n# NLP-词嵌入\n![](./AnderewNg-deeplearning-note-summary/25.png)\n# 词嵌入详解\n![](./AnderewNg-deeplearning-note-summary/26.png)\n# 序列到序列基本模型\n![](./AnderewNg-deeplearning-note-summary/27.png)\n# 序列到序列\n![](./AnderewNg-deeplearning-note-summary/28.png)","slug":"AnderewNg-deeplearning-note-summary","published":1,"updated":"2019-05-13T11:41:12.603Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9rb0000ekvez87f60ik","content":"<p>在吴恩达机器学习系列课程完结后不久，一位名叫<a href=\"https://www.slideshare.net/TessFerrandez?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideview\" title=\"Tess Ferrandez小姐姐的主页\" rel=\"external nofollow\" target=\"_blank\">Tess Ferrandez</a>的小姐姐在推特上分享了一套自己的课程笔记，瞬间收获了3k+赞和1k+转发。</p>\n<p>不同于满屏公式代码的黑白笔记，这套信息图不仅知识点满满，且行文构图都像插画一样颜值颇高。吴恩达自己也在推特上转发称赞了这一位有诚意的学习者，毕竟他一直倡导学习是一件简单快乐的事情。</p>\n<p>Link: <a href=\"https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng\" title=\"笔记源地址\" rel=\"external nofollow\" target=\"_blank\">笔记源地址</a></p>\n<a id=\"more\"></a>\n<h1 id=\"深度学习介绍\"><a href=\"#深度学习介绍\" class=\"headerlink\" title=\"深度学习介绍\"></a>深度学习介绍</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/1.png\" alt=\"\"></p>\n<h1 id=\"逻辑回归\"><a href=\"#逻辑回归\" class=\"headerlink\" title=\"逻辑回归\"></a>逻辑回归</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/2.png\" alt=\"\"></p>\n<h1 id=\"浅层神经网络\"><a href=\"#浅层神经网络\" class=\"headerlink\" title=\"浅层神经网络\"></a>浅层神经网络</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/3.png\" alt=\"\"></p>\n<h1 id=\"深层神经网络\"><a href=\"#深层神经网络\" class=\"headerlink\" title=\"深层神经网络\"></a>深层神经网络</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/4.png\" alt=\"\"></p>\n<h1 id=\"机器学习应用程序设置\"><a href=\"#机器学习应用程序设置\" class=\"headerlink\" title=\"机器学习应用程序设置\"></a>机器学习应用程序设置</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/5.png\" alt=\"\"></p>\n<h1 id=\"正则化——防止过拟合\"><a href=\"#正则化——防止过拟合\" class=\"headerlink\" title=\"正则化——防止过拟合\"></a>正则化——防止过拟合</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/6.png\" alt=\"\"></p>\n<h1 id=\"优化训练\"><a href=\"#优化训练\" class=\"headerlink\" title=\"优化训练\"></a>优化训练</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/7.png\" alt=\"\"></p>\n<h1 id=\"优化算法\"><a href=\"#优化算法\" class=\"headerlink\" title=\"优化算法\"></a>优化算法</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/8.png\" alt=\"\"></p>\n<h1 id=\"超参数调试\"><a href=\"#超参数调试\" class=\"headerlink\" title=\"超参数调试\"></a>超参数调试</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/9.png\" alt=\"\"></p>\n<h1 id=\"机器学习项目构建\"><a href=\"#机器学习项目构建\" class=\"headerlink\" title=\"机器学习项目构建\"></a>机器学习项目构建</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/10.png\" alt=\"\"></p>\n<h1 id=\"错误分析\"><a href=\"#错误分析\" class=\"headerlink\" title=\"错误分析\"></a>错误分析</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/11.png\" alt=\"\"></p>\n<h1 id=\"训练-vs-验证-测试-失配\"><a href=\"#训练-vs-验证-测试-失配\" class=\"headerlink\" title=\"训练 vs 验证/测试 失配\"></a>训练 vs 验证/测试 失配</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/12.png\" alt=\"\"></p>\n<h1 id=\"扩展学习\"><a href=\"#扩展学习\" class=\"headerlink\" title=\"扩展学习\"></a>扩展学习</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/13.png\" alt=\"\"></p>\n<h1 id=\"卷积基础\"><a href=\"#卷积基础\" class=\"headerlink\" title=\"卷积基础\"></a>卷积基础</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/14.png\" alt=\"\"></p>\n<h1 id=\"Padding\"><a href=\"#Padding\" class=\"headerlink\" title=\"Padding\"></a>Padding</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/15.png\" alt=\"\"></p>\n<h1 id=\"深层-CNN\"><a href=\"#深层-CNN\" class=\"headerlink\" title=\"深层 CNN\"></a>深层 CNN</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/16.png\" alt=\"\"></p>\n<h1 id=\"典型的-CNN-模型\"><a href=\"#典型的-CNN-模型\" class=\"headerlink\" title=\"典型的 CNN 模型\"></a>典型的 CNN 模型</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/17.png\" alt=\"\"></p>\n<h1 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/18.png\" alt=\"\"></p>\n<h1 id=\"实用建议\"><a href=\"#实用建议\" class=\"headerlink\" title=\"实用建议\"></a>实用建议</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/19.png\" alt=\"\"></p>\n<h1 id=\"检测算法\"><a href=\"#检测算法\" class=\"headerlink\" title=\"检测算法\"></a>检测算法</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/20.png\" alt=\"\"></p>\n<h1 id=\"人脸识别\"><a href=\"#人脸识别\" class=\"headerlink\" title=\"人脸识别\"></a>人脸识别</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/21.png\" alt=\"\"></p>\n<h1 id=\"神经风格迁移\"><a href=\"#神经风格迁移\" class=\"headerlink\" title=\"神经风格迁移\"></a>神经风格迁移</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/22.png\" alt=\"\"></p>\n<h1 id=\"循环神经网络\"><a href=\"#循环神经网络\" class=\"headerlink\" title=\"循环神经网络\"></a>循环神经网络</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/23.png\" alt=\"\"></p>\n<h1 id=\"更多-RNN-模型\"><a href=\"#更多-RNN-模型\" class=\"headerlink\" title=\"更多 RNN 模型\"></a>更多 RNN 模型</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/24.png\" alt=\"\"></p>\n<h1 id=\"NLP-词嵌入\"><a href=\"#NLP-词嵌入\" class=\"headerlink\" title=\"NLP-词嵌入\"></a>NLP-词嵌入</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/25.png\" alt=\"\"></p>\n<h1 id=\"词嵌入详解\"><a href=\"#词嵌入详解\" class=\"headerlink\" title=\"词嵌入详解\"></a>词嵌入详解</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/26.png\" alt=\"\"></p>\n<h1 id=\"序列到序列基本模型\"><a href=\"#序列到序列基本模型\" class=\"headerlink\" title=\"序列到序列基本模型\"></a>序列到序列基本模型</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/27.png\" alt=\"\"></p>\n<h1 id=\"序列到序列\"><a href=\"#序列到序列\" class=\"headerlink\" title=\"序列到序列\"></a>序列到序列</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/28.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"<p>在吴恩达机器学习系列课程完结后不久，一位名叫<a href=\"https://www.slideshare.net/TessFerrandez?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideview\" title=\"Tess Ferrandez小姐姐的主页\" rel=\"external nofollow\" target=\"_blank\">Tess Ferrandez</a>的小姐姐在推特上分享了一套自己的课程笔记，瞬间收获了3k+赞和1k+转发。</p>\n<p>不同于满屏公式代码的黑白笔记，这套信息图不仅知识点满满，且行文构图都像插画一样颜值颇高。吴恩达自己也在推特上转发称赞了这一位有诚意的学习者，毕竟他一直倡导学习是一件简单快乐的事情。</p>\n<p>Link: <a href=\"https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng\" title=\"笔记源地址\" rel=\"external nofollow\" target=\"_blank\">笔记源地址</a></p>","more":"<h1 id=\"深度学习介绍\"><a href=\"#深度学习介绍\" class=\"headerlink\" title=\"深度学习介绍\"></a>深度学习介绍</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/1.png\" alt=\"\"></p>\n<h1 id=\"逻辑回归\"><a href=\"#逻辑回归\" class=\"headerlink\" title=\"逻辑回归\"></a>逻辑回归</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/2.png\" alt=\"\"></p>\n<h1 id=\"浅层神经网络\"><a href=\"#浅层神经网络\" class=\"headerlink\" title=\"浅层神经网络\"></a>浅层神经网络</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/3.png\" alt=\"\"></p>\n<h1 id=\"深层神经网络\"><a href=\"#深层神经网络\" class=\"headerlink\" title=\"深层神经网络\"></a>深层神经网络</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/4.png\" alt=\"\"></p>\n<h1 id=\"机器学习应用程序设置\"><a href=\"#机器学习应用程序设置\" class=\"headerlink\" title=\"机器学习应用程序设置\"></a>机器学习应用程序设置</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/5.png\" alt=\"\"></p>\n<h1 id=\"正则化——防止过拟合\"><a href=\"#正则化——防止过拟合\" class=\"headerlink\" title=\"正则化——防止过拟合\"></a>正则化——防止过拟合</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/6.png\" alt=\"\"></p>\n<h1 id=\"优化训练\"><a href=\"#优化训练\" class=\"headerlink\" title=\"优化训练\"></a>优化训练</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/7.png\" alt=\"\"></p>\n<h1 id=\"优化算法\"><a href=\"#优化算法\" class=\"headerlink\" title=\"优化算法\"></a>优化算法</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/8.png\" alt=\"\"></p>\n<h1 id=\"超参数调试\"><a href=\"#超参数调试\" class=\"headerlink\" title=\"超参数调试\"></a>超参数调试</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/9.png\" alt=\"\"></p>\n<h1 id=\"机器学习项目构建\"><a href=\"#机器学习项目构建\" class=\"headerlink\" title=\"机器学习项目构建\"></a>机器学习项目构建</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/10.png\" alt=\"\"></p>\n<h1 id=\"错误分析\"><a href=\"#错误分析\" class=\"headerlink\" title=\"错误分析\"></a>错误分析</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/11.png\" alt=\"\"></p>\n<h1 id=\"训练-vs-验证-测试-失配\"><a href=\"#训练-vs-验证-测试-失配\" class=\"headerlink\" title=\"训练 vs 验证/测试 失配\"></a>训练 vs 验证/测试 失配</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/12.png\" alt=\"\"></p>\n<h1 id=\"扩展学习\"><a href=\"#扩展学习\" class=\"headerlink\" title=\"扩展学习\"></a>扩展学习</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/13.png\" alt=\"\"></p>\n<h1 id=\"卷积基础\"><a href=\"#卷积基础\" class=\"headerlink\" title=\"卷积基础\"></a>卷积基础</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/14.png\" alt=\"\"></p>\n<h1 id=\"Padding\"><a href=\"#Padding\" class=\"headerlink\" title=\"Padding\"></a>Padding</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/15.png\" alt=\"\"></p>\n<h1 id=\"深层-CNN\"><a href=\"#深层-CNN\" class=\"headerlink\" title=\"深层 CNN\"></a>深层 CNN</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/16.png\" alt=\"\"></p>\n<h1 id=\"典型的-CNN-模型\"><a href=\"#典型的-CNN-模型\" class=\"headerlink\" title=\"典型的 CNN 模型\"></a>典型的 CNN 模型</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/17.png\" alt=\"\"></p>\n<h1 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/18.png\" alt=\"\"></p>\n<h1 id=\"实用建议\"><a href=\"#实用建议\" class=\"headerlink\" title=\"实用建议\"></a>实用建议</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/19.png\" alt=\"\"></p>\n<h1 id=\"检测算法\"><a href=\"#检测算法\" class=\"headerlink\" title=\"检测算法\"></a>检测算法</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/20.png\" alt=\"\"></p>\n<h1 id=\"人脸识别\"><a href=\"#人脸识别\" class=\"headerlink\" title=\"人脸识别\"></a>人脸识别</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/21.png\" alt=\"\"></p>\n<h1 id=\"神经风格迁移\"><a href=\"#神经风格迁移\" class=\"headerlink\" title=\"神经风格迁移\"></a>神经风格迁移</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/22.png\" alt=\"\"></p>\n<h1 id=\"循环神经网络\"><a href=\"#循环神经网络\" class=\"headerlink\" title=\"循环神经网络\"></a>循环神经网络</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/23.png\" alt=\"\"></p>\n<h1 id=\"更多-RNN-模型\"><a href=\"#更多-RNN-模型\" class=\"headerlink\" title=\"更多 RNN 模型\"></a>更多 RNN 模型</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/24.png\" alt=\"\"></p>\n<h1 id=\"NLP-词嵌入\"><a href=\"#NLP-词嵌入\" class=\"headerlink\" title=\"NLP-词嵌入\"></a>NLP-词嵌入</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/25.png\" alt=\"\"></p>\n<h1 id=\"词嵌入详解\"><a href=\"#词嵌入详解\" class=\"headerlink\" title=\"词嵌入详解\"></a>词嵌入详解</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/26.png\" alt=\"\"></p>\n<h1 id=\"序列到序列基本模型\"><a href=\"#序列到序列基本模型\" class=\"headerlink\" title=\"序列到序列基本模型\"></a>序列到序列基本模型</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/27.png\" alt=\"\"></p>\n<h1 id=\"序列到序列\"><a href=\"#序列到序列\" class=\"headerlink\" title=\"序列到序列\"></a>序列到序列</h1><p><img src=\"./AnderewNg-deeplearning-note-summary/28.png\" alt=\"\"></p>"},{"title":"Git命令学习记录","date":"2018-12-29T07:48:51.000Z","updated":"2019-01-02T06:38:24.000Z","_content":"\n本文记录了使用Git过程中一些常用的、复杂的命令。\n\n<!--more-->\n\n- 删除远程仓库的文件,保留本地的文件\n\n  ```\n  git rm -r /path/to/filename\n  git commit -m \"msg\"\n  git push\n  ```\n\n- 删除远程仓库的文件,同时删除本地文件\n\n  ```\n  git rm /path/to/filename\n  git commit -m \"msg\"\n  git push\n  ```\n\n\n- 查看本地所有分支\n\n  ```\n  git branch -a\n  ```\n  \n- 查看本地分支\n\n  ```\n  git branch\n  ```\n\n- 切换分支\n\n  ```\n  git checkout [branchname]\n  ```\n  \n- 查看各个分支当前所指的对象\n\n  ```\n  git log --oneline --decorate\n  ```\n  \n- 如果我同一项目有两个不同的版本，怎么切换某一版本到master分支呢？比如说我有两个分支名字为A和B，目前默认master分支指向A，现在我想把master切换至B，该怎么做呢？\n  \n  ```\n  git branch -m master A\n  把当前的master分支内容放置分支A\n  git branch -m B master\n  将分支B重命名为master\n  git push -f origin master\n  更新master分支\n  ```\n  \n- 如果在本地git仓库下有另外一个clone过来的git仓库，那么当使用`git add .`，然后再`git commit ...`时会报错。并且上传到仓库的文件夹是空的，解决方案如下：\n  1. `cd`到`clone`的仓库目录下，执行`rd /s/q .git`命令，删除`clone`的仓库目录下的`.git`文件夹\n  2. 回到仓库根目录删除仓库中的空文件夹\n\n     2.1 `git rm -r --cached \"themes/[branchname]\"`\n     \n     2.2 `git commit -m \"remove empty folder\"`\n     \n     2.3 `git push origin master`\n     \n  3. 在仓库根目录重新提交代码\n\n     3.1 `git add .`\n     \n     3.2 `git commit -m \"repush\"`\n     \n     3.3 `git push origin master`","source":"_posts/Git-learn.md","raw":"---\ntitle: Git命令学习记录\ndate: 2018-12-29 15:48:51\nupdated: 2019-01-02 14:38:24\ncategories: 小知识\ntags: Git\n---\n\n本文记录了使用Git过程中一些常用的、复杂的命令。\n\n<!--more-->\n\n- 删除远程仓库的文件,保留本地的文件\n\n  ```\n  git rm -r /path/to/filename\n  git commit -m \"msg\"\n  git push\n  ```\n\n- 删除远程仓库的文件,同时删除本地文件\n\n  ```\n  git rm /path/to/filename\n  git commit -m \"msg\"\n  git push\n  ```\n\n\n- 查看本地所有分支\n\n  ```\n  git branch -a\n  ```\n  \n- 查看本地分支\n\n  ```\n  git branch\n  ```\n\n- 切换分支\n\n  ```\n  git checkout [branchname]\n  ```\n  \n- 查看各个分支当前所指的对象\n\n  ```\n  git log --oneline --decorate\n  ```\n  \n- 如果我同一项目有两个不同的版本，怎么切换某一版本到master分支呢？比如说我有两个分支名字为A和B，目前默认master分支指向A，现在我想把master切换至B，该怎么做呢？\n  \n  ```\n  git branch -m master A\n  把当前的master分支内容放置分支A\n  git branch -m B master\n  将分支B重命名为master\n  git push -f origin master\n  更新master分支\n  ```\n  \n- 如果在本地git仓库下有另外一个clone过来的git仓库，那么当使用`git add .`，然后再`git commit ...`时会报错。并且上传到仓库的文件夹是空的，解决方案如下：\n  1. `cd`到`clone`的仓库目录下，执行`rd /s/q .git`命令，删除`clone`的仓库目录下的`.git`文件夹\n  2. 回到仓库根目录删除仓库中的空文件夹\n\n     2.1 `git rm -r --cached \"themes/[branchname]\"`\n     \n     2.2 `git commit -m \"remove empty folder\"`\n     \n     2.3 `git push origin master`\n     \n  3. 在仓库根目录重新提交代码\n\n     3.1 `git add .`\n     \n     3.2 `git commit -m \"repush\"`\n     \n     3.3 `git push origin master`","slug":"Git-learn","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9rp0002ekvepc7g52i8","content":"<p>本文记录了使用Git过程中一些常用的、复杂的命令。</p>\n<a id=\"more\"></a>\n<ul>\n<li><p>删除远程仓库的文件,保留本地的文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rm -r /path/to/filename</span><br><span class=\"line\">git commit -m &quot;msg&quot;</span><br><span class=\"line\">git push</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>删除远程仓库的文件,同时删除本地文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rm /path/to/filename</span><br><span class=\"line\">git commit -m &quot;msg&quot;</span><br><span class=\"line\">git push</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查看本地所有分支</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -a</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>查看本地分支</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>切换分支</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout [branchname]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>查看各个分支当前所指的对象</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git log --oneline --decorate</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>如果我同一项目有两个不同的版本，怎么切换某一版本到master分支呢？比如说我有两个分支名字为A和B，目前默认master分支指向A，现在我想把master切换至B，该怎么做呢？</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -m master A</span><br><span class=\"line\">把当前的master分支内容放置分支A</span><br><span class=\"line\">git branch -m B master</span><br><span class=\"line\">将分支B重命名为master</span><br><span class=\"line\">git push -f origin master</span><br><span class=\"line\">更新master分支</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>如果在本地git仓库下有另外一个clone过来的git仓库，那么当使用<code>git add .</code>，然后再<code>git commit ...</code>时会报错。并且上传到仓库的文件夹是空的，解决方案如下：</p>\n<ol>\n<li><code>cd</code>到<code>clone</code>的仓库目录下，执行<code>rd /s/q .git</code>命令，删除<code>clone</code>的仓库目录下的<code>.git</code>文件夹</li>\n<li><p>回到仓库根目录删除仓库中的空文件夹</p>\n<p>2.1 <code>git rm -r --cached &quot;themes/[branchname]&quot;</code></p>\n<p>2.2 <code>git commit -m &quot;remove empty folder&quot;</code></p>\n<p>2.3 <code>git push origin master</code></p>\n</li>\n<li><p>在仓库根目录重新提交代码</p>\n<p>3.1 <code>git add .</code></p>\n<p>3.2 <code>git commit -m &quot;repush&quot;</code></p>\n<p>3.3 <code>git push origin master</code></p>\n</li>\n</ol>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>本文记录了使用Git过程中一些常用的、复杂的命令。</p>","more":"<ul>\n<li><p>删除远程仓库的文件,保留本地的文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rm -r /path/to/filename</span><br><span class=\"line\">git commit -m &quot;msg&quot;</span><br><span class=\"line\">git push</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>删除远程仓库的文件,同时删除本地文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rm /path/to/filename</span><br><span class=\"line\">git commit -m &quot;msg&quot;</span><br><span class=\"line\">git push</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查看本地所有分支</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -a</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>查看本地分支</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>切换分支</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout [branchname]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>查看各个分支当前所指的对象</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git log --oneline --decorate</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>如果我同一项目有两个不同的版本，怎么切换某一版本到master分支呢？比如说我有两个分支名字为A和B，目前默认master分支指向A，现在我想把master切换至B，该怎么做呢？</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -m master A</span><br><span class=\"line\">把当前的master分支内容放置分支A</span><br><span class=\"line\">git branch -m B master</span><br><span class=\"line\">将分支B重命名为master</span><br><span class=\"line\">git push -f origin master</span><br><span class=\"line\">更新master分支</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>如果在本地git仓库下有另外一个clone过来的git仓库，那么当使用<code>git add .</code>，然后再<code>git commit ...</code>时会报错。并且上传到仓库的文件夹是空的，解决方案如下：</p>\n<ol>\n<li><code>cd</code>到<code>clone</code>的仓库目录下，执行<code>rd /s/q .git</code>命令，删除<code>clone</code>的仓库目录下的<code>.git</code>文件夹</li>\n<li><p>回到仓库根目录删除仓库中的空文件夹</p>\n<p>2.1 <code>git rm -r --cached &quot;themes/[branchname]&quot;</code></p>\n<p>2.2 <code>git commit -m &quot;remove empty folder&quot;</code></p>\n<p>2.3 <code>git push origin master</code></p>\n</li>\n<li><p>在仓库根目录重新提交代码</p>\n<p>3.1 <code>git add .</code></p>\n<p>3.2 <code>git commit -m &quot;repush&quot;</code></p>\n<p>3.3 <code>git push origin master</code></p>\n</li>\n</ol>\n</li>\n</ul>"},{"title":"Docker常用以及组合命令学习","copyright":true,"top":1,"date":"2019-01-03T05:52:04.000Z","_content":"\n本文记录了使用Docker过程中的一些常用的、复杂的命令。\n\n<!--more-->\n\n- 停止并删除正在运行的容器\n`docker rm $(docker stop $(docker ps -aq))`\n如果是在windows操作系统，可能会执行出错，可以切换至powershell或者使用命令：\n`FOR /f \"tokens=*\" %i IN ('command 1') DO [command 2] %i`\n\n- 查看容器的长ID\n`docker inspect -f '{?{.ID}}' [name]`\n**去掉命令中的`?`,因为双括号会转义失败**\n\n- 宿主机向容器内传输文件/文件夹\n`docker cp 本地文件路径 ID全称:容器路径`\n\n- 容器传输文件/文件夹到宿主机\n`docker cp ID全称:容器文件路径 本地路径`\n\n- 修改本地已有镜像的名字\n`docker tag [ImageID] [NewImageNmae]:[tag]`\n\n- 创建新的镜像\n`docker build -t [name]:[tag] -f [dockerfile_path] .`\n\n- 开启一个容器\n`docker run -it(交互模式) -d(后台运行) --name [name] [name]:[tag]`\n如果需要启用`NVIDIA GPU`，可以加上命令参数`--gpus [all or device=0]`\n\n- 进入一个正在运行的容器\n`docker exec -it [name] [command]`\n","source":"_posts/Docker命令学习.md","raw":"---\ntitle: Docker常用以及组合命令学习\ncopyright: true\ntop: 1\ndate: 2019-01-03 13:52:04\ncategories: Docker\ntags:\n- docker\n\n---\n\n本文记录了使用Docker过程中的一些常用的、复杂的命令。\n\n<!--more-->\n\n- 停止并删除正在运行的容器\n`docker rm $(docker stop $(docker ps -aq))`\n如果是在windows操作系统，可能会执行出错，可以切换至powershell或者使用命令：\n`FOR /f \"tokens=*\" %i IN ('command 1') DO [command 2] %i`\n\n- 查看容器的长ID\n`docker inspect -f '{?{.ID}}' [name]`\n**去掉命令中的`?`,因为双括号会转义失败**\n\n- 宿主机向容器内传输文件/文件夹\n`docker cp 本地文件路径 ID全称:容器路径`\n\n- 容器传输文件/文件夹到宿主机\n`docker cp ID全称:容器文件路径 本地路径`\n\n- 修改本地已有镜像的名字\n`docker tag [ImageID] [NewImageNmae]:[tag]`\n\n- 创建新的镜像\n`docker build -t [name]:[tag] -f [dockerfile_path] .`\n\n- 开启一个容器\n`docker run -it(交互模式) -d(后台运行) --name [name] [name]:[tag]`\n如果需要启用`NVIDIA GPU`，可以加上命令参数`--gpus [all or device=0]`\n\n- 进入一个正在运行的容器\n`docker exec -it [name] [command]`\n","slug":"Docker命令学习","published":1,"updated":"2019-10-12T03:14:17.976Z","_id":"cjxd6m9s50006ekve4q655etg","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本文记录了使用Docker过程中的一些常用的、复杂的命令。</p>\n<a id=\"more\"></a>\n<ul>\n<li><p>停止并删除正在运行的容器<br><code>docker rm $(docker stop $(docker ps -aq))</code><br>如果是在windows操作系统，可能会执行出错，可以切换至powershell或者使用命令：<br><code>FOR /f &quot;tokens=*&quot; %i IN (&#39;command 1&#39;) DO [command 2] %i</code></p>\n</li>\n<li><p>查看容器的长ID<br><code>docker inspect -f &#39;{?{.ID}}&#39; [name]</code><br><strong>去掉命令中的<code>?</code>,因为双括号会转义失败</strong></p>\n</li>\n<li><p>宿主机向容器内传输文件/文件夹<br><code>docker cp 本地文件路径 ID全称:容器路径</code></p>\n</li>\n<li><p>容器传输文件/文件夹到宿主机<br><code>docker cp ID全称:容器文件路径 本地路径</code></p>\n</li>\n<li><p>修改本地已有镜像的名字<br><code>docker tag [ImageID] [NewImageNmae]:[tag]</code></p>\n</li>\n<li><p>创建新的镜像<br><code>docker build -t [name]:[tag] -f [dockerfile_path] .</code></p>\n</li>\n<li><p>开启一个容器<br><code>docker run -it(交互模式) -d(后台运行) --name [name] [name]:[tag]</code><br>如果需要启用<code>NVIDIA GPU</code>，可以加上命令参数<code>--gpus [all or device=0]</code></p>\n</li>\n<li><p>进入一个正在运行的容器<br><code>docker exec -it [name] [command]</code></p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>本文记录了使用Docker过程中的一些常用的、复杂的命令。</p>","more":"<ul>\n<li><p>停止并删除正在运行的容器<br><code>docker rm $(docker stop $(docker ps -aq))</code><br>如果是在windows操作系统，可能会执行出错，可以切换至powershell或者使用命令：<br><code>FOR /f &quot;tokens=*&quot; %i IN (&#39;command 1&#39;) DO [command 2] %i</code></p>\n</li>\n<li><p>查看容器的长ID<br><code>docker inspect -f &#39;{?{.ID}}&#39; [name]</code><br><strong>去掉命令中的<code>?</code>,因为双括号会转义失败</strong></p>\n</li>\n<li><p>宿主机向容器内传输文件/文件夹<br><code>docker cp 本地文件路径 ID全称:容器路径</code></p>\n</li>\n<li><p>容器传输文件/文件夹到宿主机<br><code>docker cp ID全称:容器文件路径 本地路径</code></p>\n</li>\n<li><p>修改本地已有镜像的名字<br><code>docker tag [ImageID] [NewImageNmae]:[tag]</code></p>\n</li>\n<li><p>创建新的镜像<br><code>docker build -t [name]:[tag] -f [dockerfile_path] .</code></p>\n</li>\n<li><p>开启一个容器<br><code>docker run -it(交互模式) -d(后台运行) --name [name] [name]:[tag]</code><br>如果需要启用<code>NVIDIA GPU</code>，可以加上命令参数<code>--gpus [all or device=0]</code></p>\n</li>\n<li><p>进入一个正在运行的容器<br><code>docker exec -it [name] [command]</code></p>\n</li>\n</ul>"},{"title":"MarkDown基本语法","date":"2018-12-29T07:23:57.000Z","_content":"\n本文介绍了Markdown的基本语法。\n\n<!--more-->\n\n所有使用Markdown语法标记的符号后要加一个空格`Space`\n\n# 一、 标题\t{{ '{#' }}...}\n\n使用`#`来设置标题级数,一个`#`则代表一级标题,字体大小最大\n\n`#` \n\n# 一级标题\n\n`##`\n\n## 二级标题\n\n`###`\n\n### 三级标题\n\n`####`\n\n#### 四级标题\n\n`#####`\n\n##### 五级标题\n\n`######`\n\n###### 六级标题 \n\n# 二、 列表\n\n## 1. 无序列表\n使用`-`、`+`、`*`三个符号都可以\n\n- 使用`-`\n\n+ 使用`+`\n\n* 使用`*`\n\n如果在列表中想取消下一行的列表性质,需要按下退格`Backspace`删除列表前的圆点后,然后按`Shift`+`Tab`组合键来退回首位.\n\n- 一级列表\n  - 二级列表\n    - 三级列表\n      - 四级列表\n\n共有三级标题\n\n## 2. 有序列表\n\n数字加点加空格,如`1.[Space]`、`2.[Space]`\n\n需要往前挪动请按`Tab`键,往后挪动请按`Shift`+`Tab`组合键\n\n1. 第一级\n2. 第二级\n   1. 第二级第一小节\n   2. 第二级第二小节\n      1. 第二级第二小节第一小小节\n      2. 第二级第二小节第二小小节\n   3. 第二级第三小节\n3. 第三级\n\n# 三、 字体\n\n- *斜体*\n\n  用法:`*[内容]*`或`_[内容]_`,包含在两个`*`星号或两个`_`下划线中间的内容会倾斜\n\n  `*Hello World*`:*Hello World*\n\n  `_Hello World_`:_Hello World_\n\n- **加粗**\n\n  用法:`**[内容]**`,包含在四个`*`星号中间的内容会加粗\n\n  `**Hello World**`:**Hello World**\n\n- ***斜体加粗***\n\n  用法:`***[内容]***`,包含在六个`*`星号中间的内容会加粗并斜体\n\n  `***Hello World***`:***Hello World***\n\n- ~~删除线~~\n\n  用法:`~~[内容]~~`,包含在四个`~`波浪号中间的内容会添加删除线\n\n  `~~Hello World~~`:~~Hello World~~\n\n# 四、 引用\n\n`>`表示引用,与`#`用法相同\n\n`>`\n\n> 一级引用\n\n`>>`\n\n> > 二级引用\n\n`>>>`\n\n> > > 三级引用\n\n退格使用`Shift`+`Tab`\n\n# 五、 分割线\n\n大于等于三个的`-`或`+`或`*`\n\n`---`\n\n---\n\n`+++`\n\n+++\n\n`***`\n\n***\n\n# 六、 图片\n\n语法:`![图片文字](图片地址 \"鼠标放置时显示的信息\")`\n\n例子:\n\n`![大桥](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1546072097132&di=8669a22f7be9af8266cb1580d15c155d&imgtype=0&src=http%3A%2F%2Fd.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2F63d9f2d3572c11dfea8f528e6e2762d0f603c2c5.jpg \"美丽的桥梁\")`\n\n![大桥](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1546072097132&di=8669a22f7be9af8266cb1580d15c155d&imgtype=0&src=http%3A%2F%2Fd.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2F63d9f2d3572c11dfea8f528e6e2762d0f603c2c5.jpg \"美丽的桥梁\")\n\n# 七、 超链接\n\n语法:`[超链接名](超链接地址 \"鼠标放置时显示的信息\")`\n\n例子:`[百度一下,你就知道](www.baidu.com \"我就是百度\")`\n\n[百度一下,你就知道](https://www.baidu.com \"我就是百度\")\n\n# 八、 代码\n\n## 1. 单行\n\n使用<code>&#96;</code>反引号包裹\n\n## 2.多行,代码块\n\n使用三个反引号包裹\n\n<code>&#96;&#96;&#96;</code>\n\n使用<code>&#96;&#96;&#96;</code>+编程语言可以打开代码编辑器\n\n如 <code>&#96;&#96;&#96;</code>+python\n\n```python\nimport sys\n这是一个python语法的编译器\n```\n\n# 九、 表格\n\n每一行都使用`|`隔开\n\n第二行使用`:`设置对齐,两边都加表示文字居中,加在左边表示居左\n\n```\n|标题1|标题2|标题3|\n|-|:-:|:-|\n|1|2|3|\n```\n\n| 标题1 | 标题2 | 标题3 |\n| ----- | :---: | :---- |\n| 1     |   2   | 3     |\n\n\n\n\n\n# 十、 流程图\n\n```\n​```flow\nst=>start: 开始\nop=>operation: My Operation\ncond=>condition: Yes or No?\ne=>end\nst->op->cond\ncond(yes)->e\ncond(no)->op\n​```\n```\n\n```flow\nst=>start: 开始\nop=>operation: My Operation\ncond=>condition: Yes or No?\ne=>end\nst->op->cond\ncond(yes)->e\ncond(no)->op\n```\n\n\n\n","source":"_posts/MarkDown-Grammar.md","raw":"---\ntitle:  MarkDown基本语法\ndate:   2018-12-29 15:23:57\ncategories: 小知识\ntags: markdown\n---\n\n本文介绍了Markdown的基本语法。\n\n<!--more-->\n\n所有使用Markdown语法标记的符号后要加一个空格`Space`\n\n# 一、 标题\t{{ '{#' }}...}\n\n使用`#`来设置标题级数,一个`#`则代表一级标题,字体大小最大\n\n`#` \n\n# 一级标题\n\n`##`\n\n## 二级标题\n\n`###`\n\n### 三级标题\n\n`####`\n\n#### 四级标题\n\n`#####`\n\n##### 五级标题\n\n`######`\n\n###### 六级标题 \n\n# 二、 列表\n\n## 1. 无序列表\n使用`-`、`+`、`*`三个符号都可以\n\n- 使用`-`\n\n+ 使用`+`\n\n* 使用`*`\n\n如果在列表中想取消下一行的列表性质,需要按下退格`Backspace`删除列表前的圆点后,然后按`Shift`+`Tab`组合键来退回首位.\n\n- 一级列表\n  - 二级列表\n    - 三级列表\n      - 四级列表\n\n共有三级标题\n\n## 2. 有序列表\n\n数字加点加空格,如`1.[Space]`、`2.[Space]`\n\n需要往前挪动请按`Tab`键,往后挪动请按`Shift`+`Tab`组合键\n\n1. 第一级\n2. 第二级\n   1. 第二级第一小节\n   2. 第二级第二小节\n      1. 第二级第二小节第一小小节\n      2. 第二级第二小节第二小小节\n   3. 第二级第三小节\n3. 第三级\n\n# 三、 字体\n\n- *斜体*\n\n  用法:`*[内容]*`或`_[内容]_`,包含在两个`*`星号或两个`_`下划线中间的内容会倾斜\n\n  `*Hello World*`:*Hello World*\n\n  `_Hello World_`:_Hello World_\n\n- **加粗**\n\n  用法:`**[内容]**`,包含在四个`*`星号中间的内容会加粗\n\n  `**Hello World**`:**Hello World**\n\n- ***斜体加粗***\n\n  用法:`***[内容]***`,包含在六个`*`星号中间的内容会加粗并斜体\n\n  `***Hello World***`:***Hello World***\n\n- ~~删除线~~\n\n  用法:`~~[内容]~~`,包含在四个`~`波浪号中间的内容会添加删除线\n\n  `~~Hello World~~`:~~Hello World~~\n\n# 四、 引用\n\n`>`表示引用,与`#`用法相同\n\n`>`\n\n> 一级引用\n\n`>>`\n\n> > 二级引用\n\n`>>>`\n\n> > > 三级引用\n\n退格使用`Shift`+`Tab`\n\n# 五、 分割线\n\n大于等于三个的`-`或`+`或`*`\n\n`---`\n\n---\n\n`+++`\n\n+++\n\n`***`\n\n***\n\n# 六、 图片\n\n语法:`![图片文字](图片地址 \"鼠标放置时显示的信息\")`\n\n例子:\n\n`![大桥](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1546072097132&di=8669a22f7be9af8266cb1580d15c155d&imgtype=0&src=http%3A%2F%2Fd.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2F63d9f2d3572c11dfea8f528e6e2762d0f603c2c5.jpg \"美丽的桥梁\")`\n\n![大桥](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1546072097132&di=8669a22f7be9af8266cb1580d15c155d&imgtype=0&src=http%3A%2F%2Fd.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2F63d9f2d3572c11dfea8f528e6e2762d0f603c2c5.jpg \"美丽的桥梁\")\n\n# 七、 超链接\n\n语法:`[超链接名](超链接地址 \"鼠标放置时显示的信息\")`\n\n例子:`[百度一下,你就知道](www.baidu.com \"我就是百度\")`\n\n[百度一下,你就知道](https://www.baidu.com \"我就是百度\")\n\n# 八、 代码\n\n## 1. 单行\n\n使用<code>&#96;</code>反引号包裹\n\n## 2.多行,代码块\n\n使用三个反引号包裹\n\n<code>&#96;&#96;&#96;</code>\n\n使用<code>&#96;&#96;&#96;</code>+编程语言可以打开代码编辑器\n\n如 <code>&#96;&#96;&#96;</code>+python\n\n```python\nimport sys\n这是一个python语法的编译器\n```\n\n# 九、 表格\n\n每一行都使用`|`隔开\n\n第二行使用`:`设置对齐,两边都加表示文字居中,加在左边表示居左\n\n```\n|标题1|标题2|标题3|\n|-|:-:|:-|\n|1|2|3|\n```\n\n| 标题1 | 标题2 | 标题3 |\n| ----- | :---: | :---- |\n| 1     |   2   | 3     |\n\n\n\n\n\n# 十、 流程图\n\n```\n​```flow\nst=>start: 开始\nop=>operation: My Operation\ncond=>condition: Yes or No?\ne=>end\nst->op->cond\ncond(yes)->e\ncond(no)->op\n​```\n```\n\n```flow\nst=>start: 开始\nop=>operation: My Operation\ncond=>condition: Yes or No?\ne=>end\nst->op->cond\ncond(yes)->e\ncond(no)->op\n```\n\n\n\n","slug":"MarkDown-Grammar","published":1,"updated":"2019-05-13T11:49:05.906Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9sb0008ekvevjgqf2ss","content":"<p>本文介绍了Markdown的基本语法。</p>\n<a id=\"more\"></a>\n<p>所有使用Markdown语法标记的符号后要加一个空格<code>Space</code></p>\n<h1 id=\"一、-标题-…\"><a href=\"#一、-标题-…\" class=\"headerlink\" title=\"一、 标题    …}\"></a>一、 标题    {#…}</h1><p>使用<code>#</code>来设置标题级数,一个<code>#</code>则代表一级标题,字体大小最大</p>\n<p><code>#</code> </p>\n<h1 id=\"一级标题\"><a href=\"#一级标题\" class=\"headerlink\" title=\"一级标题\"></a>一级标题</h1><p><code>##</code></p>\n<h2 id=\"二级标题\"><a href=\"#二级标题\" class=\"headerlink\" title=\"二级标题\"></a>二级标题</h2><p><code>###</code></p>\n<h3 id=\"三级标题\"><a href=\"#三级标题\" class=\"headerlink\" title=\"三级标题\"></a>三级标题</h3><p><code>####</code></p>\n<h4 id=\"四级标题\"><a href=\"#四级标题\" class=\"headerlink\" title=\"四级标题\"></a>四级标题</h4><p><code>#####</code></p>\n<h5 id=\"五级标题\"><a href=\"#五级标题\" class=\"headerlink\" title=\"五级标题\"></a>五级标题</h5><p><code>######</code></p>\n<h6 id=\"六级标题\"><a href=\"#六级标题\" class=\"headerlink\" title=\"六级标题\"></a>六级标题</h6><h1 id=\"二、-列表\"><a href=\"#二、-列表\" class=\"headerlink\" title=\"二、 列表\"></a>二、 列表</h1><h2 id=\"1-无序列表\"><a href=\"#1-无序列表\" class=\"headerlink\" title=\"1. 无序列表\"></a>1. 无序列表</h2><p>使用<code>-</code>、<code>+</code>、<code>*</code>三个符号都可以</p>\n<ul>\n<li>使用<code>-</code></li>\n</ul>\n<ul>\n<li>使用<code>+</code></li>\n</ul>\n<ul>\n<li>使用<code>*</code></li>\n</ul>\n<p>如果在列表中想取消下一行的列表性质,需要按下退格<code>Backspace</code>删除列表前的圆点后,然后按<code>Shift</code>+<code>Tab</code>组合键来退回首位.</p>\n<ul>\n<li>一级列表<ul>\n<li>二级列表<ul>\n<li>三级列表<ul>\n<li>四级列表</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>共有三级标题</p>\n<h2 id=\"2-有序列表\"><a href=\"#2-有序列表\" class=\"headerlink\" title=\"2. 有序列表\"></a>2. 有序列表</h2><p>数字加点加空格,如<code>1.[Space]</code>、<code>2.[Space]</code></p>\n<p>需要往前挪动请按<code>Tab</code>键,往后挪动请按<code>Shift</code>+<code>Tab</code>组合键</p>\n<ol>\n<li>第一级</li>\n<li>第二级<ol>\n<li>第二级第一小节</li>\n<li>第二级第二小节<ol>\n<li>第二级第二小节第一小小节</li>\n<li>第二级第二小节第二小小节</li>\n</ol>\n</li>\n<li>第二级第三小节</li>\n</ol>\n</li>\n<li>第三级</li>\n</ol>\n<h1 id=\"三、-字体\"><a href=\"#三、-字体\" class=\"headerlink\" title=\"三、 字体\"></a>三、 字体</h1><ul>\n<li><p><em>斜体</em></p>\n<p>用法:<code>*[内容]*</code>或<code>_[内容]_</code>,包含在两个<code>*</code>星号或两个<code>_</code>下划线中间的内容会倾斜</p>\n<p><code>*Hello World*</code>:<em>Hello World</em></p>\n<p><code>_Hello World_</code>:_Hello World_</p>\n</li>\n<li><p><strong>加粗</strong></p>\n<p>用法:<code>**[内容]**</code>,包含在四个<code>*</code>星号中间的内容会加粗</p>\n<p><code>**Hello World**</code>:<strong>Hello World</strong></p>\n</li>\n<li><p><strong><em>斜体加粗</em></strong></p>\n<p>用法:<code>***[内容]***</code>,包含在六个<code>*</code>星号中间的内容会加粗并斜体</p>\n<p><code>***Hello World***</code>:<strong><em>Hello World</em></strong></p>\n</li>\n<li><p><del>删除线</del></p>\n<p>用法:<code>~~[内容]~~</code>,包含在四个<code>~</code>波浪号中间的内容会添加删除线</p>\n<p><code>~~Hello World~~</code>:<del>Hello World</del></p>\n</li>\n</ul>\n<h1 id=\"四、-引用\"><a href=\"#四、-引用\" class=\"headerlink\" title=\"四、 引用\"></a>四、 引用</h1><p><code>&gt;</code>表示引用,与<code>#</code>用法相同</p>\n<p><code>&gt;</code></p>\n<blockquote>\n<p>一级引用</p>\n</blockquote>\n<p><code>&gt;&gt;</code></p>\n<blockquote>\n<blockquote>\n<p>二级引用</p>\n</blockquote>\n</blockquote>\n<p><code>&gt;&gt;&gt;</code></p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>三级引用</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<p>退格使用<code>Shift</code>+<code>Tab</code></p>\n<h1 id=\"五、-分割线\"><a href=\"#五、-分割线\" class=\"headerlink\" title=\"五、 分割线\"></a>五、 分割线</h1><p>大于等于三个的<code>-</code>或<code>+</code>或<code>*</code></p>\n<p><code>---</code></p>\n<hr>\n<p><code>+++</code></p>\n<p>+++</p>\n<p><code>***</code></p>\n<hr>\n<h1 id=\"六、-图片\"><a href=\"#六、-图片\" class=\"headerlink\" title=\"六、 图片\"></a>六、 图片</h1><p>语法:<code>![图片文字](图片地址 &quot;鼠标放置时显示的信息&quot;)</code></p>\n<p>例子:</p>\n<p><code>![大桥](https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1546072097132&amp;di=8669a22f7be9af8266cb1580d15c155d&amp;imgtype=0&amp;src=http%3A%2F%2Fd.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2F63d9f2d3572c11dfea8f528e6e2762d0f603c2c5.jpg &quot;美丽的桥梁&quot;)</code></p>\n<p><img src=\"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1546072097132&amp;di=8669a22f7be9af8266cb1580d15c155d&amp;imgtype=0&amp;src=http%3A%2F%2Fd.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2F63d9f2d3572c11dfea8f528e6e2762d0f603c2c5.jpg\" alt=\"大桥\" title=\"美丽的桥梁\"></p>\n<h1 id=\"七、-超链接\"><a href=\"#七、-超链接\" class=\"headerlink\" title=\"七、 超链接\"></a>七、 超链接</h1><p>语法:<code>[超链接名](超链接地址 &quot;鼠标放置时显示的信息&quot;)</code></p>\n<p>例子:<code>[百度一下,你就知道](www.baidu.com &quot;我就是百度&quot;)</code></p>\n<p><a href=\"https://www.baidu.com\" title=\"我就是百度\" rel=\"external nofollow\" target=\"_blank\">百度一下,你就知道</a></p>\n<h1 id=\"八、-代码\"><a href=\"#八、-代码\" class=\"headerlink\" title=\"八、 代码\"></a>八、 代码</h1><h2 id=\"1-单行\"><a href=\"#1-单行\" class=\"headerlink\" title=\"1. 单行\"></a>1. 单行</h2><p>使用<code>&#96;</code>反引号包裹</p>\n<h2 id=\"2-多行-代码块\"><a href=\"#2-多行-代码块\" class=\"headerlink\" title=\"2.多行,代码块\"></a>2.多行,代码块</h2><p>使用三个反引号包裹</p>\n<p><code>&#96;&#96;&#96;</code></p>\n<p>使用<code>&#96;&#96;&#96;</code>+编程语言可以打开代码编辑器</p>\n<p>如 <code>&#96;&#96;&#96;</code>+python</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">这是一个python语法的编译器</span><br></pre></td></tr></table></figure>\n<h1 id=\"九、-表格\"><a href=\"#九、-表格\" class=\"headerlink\" title=\"九、 表格\"></a>九、 表格</h1><p>每一行都使用<code>|</code>隔开</p>\n<p>第二行使用<code>:</code>设置对齐,两边都加表示文字居中,加在左边表示居左</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">|标题1|标题2|标题3|</span><br><span class=\"line\">|-|:-:|:-|</span><br><span class=\"line\">|1|2|3|</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>标题1</th>\n<th style=\"text-align:center\">标题2</th>\n<th style=\"text-align:left\">标题3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:left\">3</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h1 id=\"十、-流程图\"><a href=\"#十、-流程图\" class=\"headerlink\" title=\"十、 流程图\"></a>十、 流程图</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">​```flow</span><br><span class=\"line\">st=&gt;start: 开始</span><br><span class=\"line\">op=&gt;operation: My Operation</span><br><span class=\"line\">cond=&gt;condition: Yes or No?</span><br><span class=\"line\">e=&gt;end</span><br><span class=\"line\">st-&gt;op-&gt;cond</span><br><span class=\"line\">cond(yes)-&gt;e</span><br><span class=\"line\">cond(no)-&gt;op</span><br><span class=\"line\">​</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">```flow</span><br><span class=\"line\">st=&gt;start: 开始</span><br><span class=\"line\">op=&gt;operation: My Operation</span><br><span class=\"line\">cond=&gt;condition: Yes or No?</span><br><span class=\"line\">e=&gt;end</span><br><span class=\"line\">st-&gt;op-&gt;cond</span><br><span class=\"line\">cond(yes)-&gt;e</span><br><span class=\"line\">cond(no)-&gt;op</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>本文介绍了Markdown的基本语法。</p>","more":"<p>所有使用Markdown语法标记的符号后要加一个空格<code>Space</code></p>\n<h1 id=\"一、-标题-…\"><a href=\"#一、-标题-…\" class=\"headerlink\" title=\"一、 标题    …}\"></a>一、 标题    {#…}</h1><p>使用<code>#</code>来设置标题级数,一个<code>#</code>则代表一级标题,字体大小最大</p>\n<p><code>#</code> </p>\n<h1 id=\"一级标题\"><a href=\"#一级标题\" class=\"headerlink\" title=\"一级标题\"></a>一级标题</h1><p><code>##</code></p>\n<h2 id=\"二级标题\"><a href=\"#二级标题\" class=\"headerlink\" title=\"二级标题\"></a>二级标题</h2><p><code>###</code></p>\n<h3 id=\"三级标题\"><a href=\"#三级标题\" class=\"headerlink\" title=\"三级标题\"></a>三级标题</h3><p><code>####</code></p>\n<h4 id=\"四级标题\"><a href=\"#四级标题\" class=\"headerlink\" title=\"四级标题\"></a>四级标题</h4><p><code>#####</code></p>\n<h5 id=\"五级标题\"><a href=\"#五级标题\" class=\"headerlink\" title=\"五级标题\"></a>五级标题</h5><p><code>######</code></p>\n<h6 id=\"六级标题\"><a href=\"#六级标题\" class=\"headerlink\" title=\"六级标题\"></a>六级标题</h6><h1 id=\"二、-列表\"><a href=\"#二、-列表\" class=\"headerlink\" title=\"二、 列表\"></a>二、 列表</h1><h2 id=\"1-无序列表\"><a href=\"#1-无序列表\" class=\"headerlink\" title=\"1. 无序列表\"></a>1. 无序列表</h2><p>使用<code>-</code>、<code>+</code>、<code>*</code>三个符号都可以</p>\n<ul>\n<li>使用<code>-</code></li>\n</ul>\n<ul>\n<li>使用<code>+</code></li>\n</ul>\n<ul>\n<li>使用<code>*</code></li>\n</ul>\n<p>如果在列表中想取消下一行的列表性质,需要按下退格<code>Backspace</code>删除列表前的圆点后,然后按<code>Shift</code>+<code>Tab</code>组合键来退回首位.</p>\n<ul>\n<li>一级列表<ul>\n<li>二级列表<ul>\n<li>三级列表<ul>\n<li>四级列表</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>共有三级标题</p>\n<h2 id=\"2-有序列表\"><a href=\"#2-有序列表\" class=\"headerlink\" title=\"2. 有序列表\"></a>2. 有序列表</h2><p>数字加点加空格,如<code>1.[Space]</code>、<code>2.[Space]</code></p>\n<p>需要往前挪动请按<code>Tab</code>键,往后挪动请按<code>Shift</code>+<code>Tab</code>组合键</p>\n<ol>\n<li>第一级</li>\n<li>第二级<ol>\n<li>第二级第一小节</li>\n<li>第二级第二小节<ol>\n<li>第二级第二小节第一小小节</li>\n<li>第二级第二小节第二小小节</li>\n</ol>\n</li>\n<li>第二级第三小节</li>\n</ol>\n</li>\n<li>第三级</li>\n</ol>\n<h1 id=\"三、-字体\"><a href=\"#三、-字体\" class=\"headerlink\" title=\"三、 字体\"></a>三、 字体</h1><ul>\n<li><p><em>斜体</em></p>\n<p>用法:<code>*[内容]*</code>或<code>_[内容]_</code>,包含在两个<code>*</code>星号或两个<code>_</code>下划线中间的内容会倾斜</p>\n<p><code>*Hello World*</code>:<em>Hello World</em></p>\n<p><code>_Hello World_</code>:_Hello World_</p>\n</li>\n<li><p><strong>加粗</strong></p>\n<p>用法:<code>**[内容]**</code>,包含在四个<code>*</code>星号中间的内容会加粗</p>\n<p><code>**Hello World**</code>:<strong>Hello World</strong></p>\n</li>\n<li><p><strong><em>斜体加粗</em></strong></p>\n<p>用法:<code>***[内容]***</code>,包含在六个<code>*</code>星号中间的内容会加粗并斜体</p>\n<p><code>***Hello World***</code>:<strong><em>Hello World</em></strong></p>\n</li>\n<li><p><del>删除线</del></p>\n<p>用法:<code>~~[内容]~~</code>,包含在四个<code>~</code>波浪号中间的内容会添加删除线</p>\n<p><code>~~Hello World~~</code>:<del>Hello World</del></p>\n</li>\n</ul>\n<h1 id=\"四、-引用\"><a href=\"#四、-引用\" class=\"headerlink\" title=\"四、 引用\"></a>四、 引用</h1><p><code>&gt;</code>表示引用,与<code>#</code>用法相同</p>\n<p><code>&gt;</code></p>\n<blockquote>\n<p>一级引用</p>\n</blockquote>\n<p><code>&gt;&gt;</code></p>\n<blockquote>\n<blockquote>\n<p>二级引用</p>\n</blockquote>\n</blockquote>\n<p><code>&gt;&gt;&gt;</code></p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>三级引用</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<p>退格使用<code>Shift</code>+<code>Tab</code></p>\n<h1 id=\"五、-分割线\"><a href=\"#五、-分割线\" class=\"headerlink\" title=\"五、 分割线\"></a>五、 分割线</h1><p>大于等于三个的<code>-</code>或<code>+</code>或<code>*</code></p>\n<p><code>---</code></p>\n<hr>\n<p><code>+++</code></p>\n<p>+++</p>\n<p><code>***</code></p>\n<hr>\n<h1 id=\"六、-图片\"><a href=\"#六、-图片\" class=\"headerlink\" title=\"六、 图片\"></a>六、 图片</h1><p>语法:<code>![图片文字](图片地址 &quot;鼠标放置时显示的信息&quot;)</code></p>\n<p>例子:</p>\n<p><code>![大桥](https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1546072097132&amp;di=8669a22f7be9af8266cb1580d15c155d&amp;imgtype=0&amp;src=http%3A%2F%2Fd.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2F63d9f2d3572c11dfea8f528e6e2762d0f603c2c5.jpg &quot;美丽的桥梁&quot;)</code></p>\n<p><img src=\"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1546072097132&amp;di=8669a22f7be9af8266cb1580d15c155d&amp;imgtype=0&amp;src=http%3A%2F%2Fd.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2F63d9f2d3572c11dfea8f528e6e2762d0f603c2c5.jpg\" alt=\"大桥\" title=\"美丽的桥梁\"></p>\n<h1 id=\"七、-超链接\"><a href=\"#七、-超链接\" class=\"headerlink\" title=\"七、 超链接\"></a>七、 超链接</h1><p>语法:<code>[超链接名](超链接地址 &quot;鼠标放置时显示的信息&quot;)</code></p>\n<p>例子:<code>[百度一下,你就知道](www.baidu.com &quot;我就是百度&quot;)</code></p>\n<p><a href=\"https://www.baidu.com\" title=\"我就是百度\" rel=\"external nofollow\" target=\"_blank\">百度一下,你就知道</a></p>\n<h1 id=\"八、-代码\"><a href=\"#八、-代码\" class=\"headerlink\" title=\"八、 代码\"></a>八、 代码</h1><h2 id=\"1-单行\"><a href=\"#1-单行\" class=\"headerlink\" title=\"1. 单行\"></a>1. 单行</h2><p>使用<code>&#96;</code>反引号包裹</p>\n<h2 id=\"2-多行-代码块\"><a href=\"#2-多行-代码块\" class=\"headerlink\" title=\"2.多行,代码块\"></a>2.多行,代码块</h2><p>使用三个反引号包裹</p>\n<p><code>&#96;&#96;&#96;</code></p>\n<p>使用<code>&#96;&#96;&#96;</code>+编程语言可以打开代码编辑器</p>\n<p>如 <code>&#96;&#96;&#96;</code>+python</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">这是一个python语法的编译器</span><br></pre></td></tr></table></figure>\n<h1 id=\"九、-表格\"><a href=\"#九、-表格\" class=\"headerlink\" title=\"九、 表格\"></a>九、 表格</h1><p>每一行都使用<code>|</code>隔开</p>\n<p>第二行使用<code>:</code>设置对齐,两边都加表示文字居中,加在左边表示居左</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">|标题1|标题2|标题3|</span><br><span class=\"line\">|-|:-:|:-|</span><br><span class=\"line\">|1|2|3|</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>标题1</th>\n<th style=\"text-align:center\">标题2</th>\n<th style=\"text-align:left\">标题3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:left\">3</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h1 id=\"十、-流程图\"><a href=\"#十、-流程图\" class=\"headerlink\" title=\"十、 流程图\"></a>十、 流程图</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">​```flow</span><br><span class=\"line\">st=&gt;start: 开始</span><br><span class=\"line\">op=&gt;operation: My Operation</span><br><span class=\"line\">cond=&gt;condition: Yes or No?</span><br><span class=\"line\">e=&gt;end</span><br><span class=\"line\">st-&gt;op-&gt;cond</span><br><span class=\"line\">cond(yes)-&gt;e</span><br><span class=\"line\">cond(no)-&gt;op</span><br><span class=\"line\">​</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">```flow</span><br><span class=\"line\">st=&gt;start: 开始</span><br><span class=\"line\">op=&gt;operation: My Operation</span><br><span class=\"line\">cond=&gt;condition: Yes or No?</span><br><span class=\"line\">e=&gt;end</span><br><span class=\"line\">st-&gt;op-&gt;cond</span><br><span class=\"line\">cond(yes)-&gt;e</span><br><span class=\"line\">cond(no)-&gt;op</span><br></pre></td></tr></table></figure>"},{"title":"Monte Carlo and Temporal-Difference","copyright":true,"mathjax":true,"password":123,"top":1,"date":"2019-05-13T11:11:58.000Z","keywords":null,"description":null,"_content":"\n本位介绍了强化学习中解决Model-Free问题的两个基本解决思路：蒙特卡洛Monte Carlo与时间差分Temporal-Difference。\n\n<!--more-->\n\n![](./mc-td/dp.png)\n\n![](./mc-td/mc.png)\n\n![](./mc-td/td.png)\n\n\n\n# 蒙特卡洛方法 Monte Carlo Methods\n\n\n\n# 时间差分学习 Temporal-Difference Learning\n\n","source":"_posts/mc-td.md","raw":"---\ntitle: Monte Carlo and Temporal-Difference\ncopyright: true\nmathjax: true\npassword: 123\ntop: 1\ndate: 2019-05-13 19:11:58\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n本位介绍了强化学习中解决Model-Free问题的两个基本解决思路：蒙特卡洛Monte Carlo与时间差分Temporal-Difference。\n\n<!--more-->\n\n![](./mc-td/dp.png)\n\n![](./mc-td/mc.png)\n\n![](./mc-td/td.png)\n\n\n\n# 蒙特卡洛方法 Monte Carlo Methods\n\n\n\n# 时间差分学习 Temporal-Difference Learning\n\n","slug":"mc-td","published":1,"updated":"2019-05-13T12:43:42.482Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9sg000aekveiwixg3ik","content":"<p>本位介绍了强化学习中解决Model-Free问题的两个基本解决思路：蒙特卡洛Monte Carlo与时间差分Temporal-Difference。</p>\n<a id=\"more\"></a>\n<p><img src=\"./mc-td/dp.png\" alt=\"\"></p>\n<p><img src=\"./mc-td/mc.png\" alt=\"\"></p>\n<p><img src=\"./mc-td/td.png\" alt=\"\"></p>\n<h1 id=\"蒙特卡洛方法-Monte-Carlo-Methods\"><a href=\"#蒙特卡洛方法-Monte-Carlo-Methods\" class=\"headerlink\" title=\"蒙特卡洛方法 Monte Carlo Methods\"></a>蒙特卡洛方法 Monte Carlo Methods</h1><h1 id=\"时间差分学习-Temporal-Difference-Learning\"><a href=\"#时间差分学习-Temporal-Difference-Learning\" class=\"headerlink\" title=\"时间差分学习 Temporal-Difference Learning\"></a>时间差分学习 Temporal-Difference Learning</h1>","site":{"data":{}},"excerpt":"<p>本位介绍了强化学习中解决Model-Free问题的两个基本解决思路：蒙特卡洛Monte Carlo与时间差分Temporal-Difference。</p>","more":"<p><img src=\"./mc-td/dp.png\" alt=\"\"></p>\n<p><img src=\"./mc-td/mc.png\" alt=\"\"></p>\n<p><img src=\"./mc-td/td.png\" alt=\"\"></p>\n<h1 id=\"蒙特卡洛方法-Monte-Carlo-Methods\"><a href=\"#蒙特卡洛方法-Monte-Carlo-Methods\" class=\"headerlink\" title=\"蒙特卡洛方法 Monte Carlo Methods\"></a>蒙特卡洛方法 Monte Carlo Methods</h1><h1 id=\"时间差分学习-Temporal-Difference-Learning\"><a href=\"#时间差分学习-Temporal-Difference-Learning\" class=\"headerlink\" title=\"时间差分学习 Temporal-Difference Learning\"></a>时间差分学习 Temporal-Difference Learning</h1>"},{"title":"conda环境和pip包的转移","copyright":true,"top":1,"date":"2019-04-14T08:52:41.000Z","_content":"\n本文记录了如何导出、导入自己的conda环境，对于pip安装的包如何导出、导入。\n\n<!--more-->\n\n# Conda 环境\n\n1. 激活环境\n`conda activate [env_name]`\n2. 导出环境\n`conda env export > [env_filename].yaml`\n当前环境将被保存在定义的`.yaml`文件中\n3. 导入环境\n`conda env create -f [env_filename].yaml`\n移植过来的conda环境只安装了原环境中使用`conda install`等命令安装的包, 使用`pip`命令安装的包需要重新安装\n\n# pip包\n\n1. 导出\n`pip freeze > requirements.txt`\n2. 导入\n`pip install -r requirements.txt`\n","source":"_posts/conda环境和pip包的转移.md","raw":"---\ntitle: conda环境和pip包的转移\ncopyright: true\ntop: 1\ndate: 2019-04-14 16:52:41\ncategories: Conda\ntags:\n- conda\n---\n\n本文记录了如何导出、导入自己的conda环境，对于pip安装的包如何导出、导入。\n\n<!--more-->\n\n# Conda 环境\n\n1. 激活环境\n`conda activate [env_name]`\n2. 导出环境\n`conda env export > [env_filename].yaml`\n当前环境将被保存在定义的`.yaml`文件中\n3. 导入环境\n`conda env create -f [env_filename].yaml`\n移植过来的conda环境只安装了原环境中使用`conda install`等命令安装的包, 使用`pip`命令安装的包需要重新安装\n\n# pip包\n\n1. 导出\n`pip freeze > requirements.txt`\n2. 导入\n`pip install -r requirements.txt`\n","slug":"conda环境和pip包的转移","published":1,"updated":"2019-05-13T11:42:19.949Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9sr000eekvez6rytift","content":"<p>本文记录了如何导出、导入自己的conda环境，对于pip安装的包如何导出、导入。</p>\n<a id=\"more\"></a>\n<h1 id=\"Conda-环境\"><a href=\"#Conda-环境\" class=\"headerlink\" title=\"Conda 环境\"></a>Conda 环境</h1><ol>\n<li>激活环境<br><code>conda activate [env_name]</code></li>\n<li>导出环境<br><code>conda env export &gt; [env_filename].yaml</code><br>当前环境将被保存在定义的<code>.yaml</code>文件中</li>\n<li>导入环境<br><code>conda env create -f [env_filename].yaml</code><br>移植过来的conda环境只安装了原环境中使用<code>conda install</code>等命令安装的包, 使用<code>pip</code>命令安装的包需要重新安装</li>\n</ol>\n<h1 id=\"pip包\"><a href=\"#pip包\" class=\"headerlink\" title=\"pip包\"></a>pip包</h1><ol>\n<li>导出<br><code>pip freeze &gt; requirements.txt</code></li>\n<li>导入<br><code>pip install -r requirements.txt</code></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>本文记录了如何导出、导入自己的conda环境，对于pip安装的包如何导出、导入。</p>","more":"<h1 id=\"Conda-环境\"><a href=\"#Conda-环境\" class=\"headerlink\" title=\"Conda 环境\"></a>Conda 环境</h1><ol>\n<li>激活环境<br><code>conda activate [env_name]</code></li>\n<li>导出环境<br><code>conda env export &gt; [env_filename].yaml</code><br>当前环境将被保存在定义的<code>.yaml</code>文件中</li>\n<li>导入环境<br><code>conda env create -f [env_filename].yaml</code><br>移植过来的conda环境只安装了原环境中使用<code>conda install</code>等命令安装的包, 使用<code>pip</code>命令安装的包需要重新安装</li>\n</ol>\n<h1 id=\"pip包\"><a href=\"#pip包\" class=\"headerlink\" title=\"pip包\"></a>pip包</h1><ol>\n<li>导出<br><code>pip freeze &gt; requirements.txt</code></li>\n<li>导入<br><code>pip install -r requirements.txt</code></li>\n</ol>"},{"title":"强化学习的类别","copyright":true,"top":1,"date":"2019-05-10T07:18:23.000Z","mathjax":true,"keywords":null,"description":null,"_content":"\n本文讲述了强化学习中各种算法、问题的分类规则。\n\n<!--more-->\n\n# Stationary or not\n\n根据环境十分稳定、可以将强化学习问题分为stationary、non-stationary。\n\n如果状态转移**和**奖励函数是确定的，即选择动作$a$后执行它的结果是确定的，那么这个环境就是stationary。\n\n![](./rl-classification/stationary.png)\n\n如果状态转移**或**奖励函数是不确定的，即选择动作$a$后执行它的结果是不确定的，那么这个环境就是non-stationary。\n\n![](./rl-classification/non-stationary.png)\n\n# Model Based-or-Free\n\n一直对这个问题的认识不清晰，直到最近（2019年5月12日19:13:53）才有了清晰的认识。\n\n需要注意的是，无论是Model-Based还是Model-Free都不是对强化学习问题的分类，而是对算法的分类。之前一直理解的是状态空间$\\mathcal{S}$、动作空间$\\mathcal{A}$的都是离散的，转移概率矩阵$\\mathcal{P}$是确定的，这样即是Model-Based，如果状态空间$\\mathcal{S}$、动作空间$\\mathcal{A}$或转移概率矩阵$\\mathcal{P}$是不确定的，则是Model-Free，其实这只是对Model的分类，并不是Model-Based与Model-Free的真实含义，Model-Based与Model-Free是对算法求解过程的分类，理解这个可以在阅读国外文献、实验环境时更清晰，提升自己对强化学习算法的理解深度。\n\nModel-Based：\n\n- 智能体Agent在已知模型（$\\mathcal{S,A,R,P}$有限且确定）或者先学习一个模型（使用有监督对状态转移、奖励函数进行学习而得到），并在这个模型中使用**planning**（预测所有状态转移可能）方法来计算解决方案\n\n> Now if we know what all those elements of an MDP are, we can just compute the solution before ever actually executing an action in the environment. In AI, we typically call computing the solution to a decision-making problem before executing an actual decision *planning*. Some classic planning algorithms for MDPs include Value Iteration, Policy Iteration, and whole lot more.\n\nModel-Free:\n\n- 智能体在模型（$\\mathcal{S,A,R,P}$可能确定但没有使用planning方式解决，也可能不确定）中试错，并且使用**learning**（不预测全部可能性）方法来产生最佳策略\n\n> But the RL problem isn’t so kind to us. What makes a problem an RL problem, rather than a planning problem, is the agent does *not* know all the elements of the MDP, precluding it from being able to plan a solution. Specifically, the agent does not know how the world will change in response to its actions (the transition function TT), nor what immediate reward it will receive for doing so (the reward function RR). The agent will simply have to try taking actions in the environment, observe what happens, and somehow, find a good policy from doing so.\n\n根据Model-Based、Model-Free对算法、解决方法进行分类：\n\nModel-Based：DP、Policy Iteration、Value Iteration……\n\nModel-Free：SARSA、Q-Learning、PG……\n\n---\n\n> if you want a way to check if an RL algorithm is model-based or model-free, ask yourself this question: after learning, can the agent make predictions about what the next state and reward will be before it takes each action? If it can, then it’s a model-based RL algorithm. if it cannot, it’s a model-free algorithm.\n\n**使用算法学成策略之后，智能体可以在执行动作前判断该动作的后果，即是Model-Based，反之则是Model-Free**\n\n---\n\n根据Model-Based和Model-Free可以将强化学习算法分类，图片摘自OpenAI Spinning Up，如图所示：\n\n![](./rl-classification/model-classification.png)\n\n>[What is the difference between model-based and model-free reinforcement learning?](https://www.quora.com/What-is-the-difference-between-model-based-and-model-free-reinforcement-learning)\n>\n>[OpenAI Spinning Up : A Taxonomy of RL Algorithms](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms)\n\n# Policy or Value\n\n强化学习的目的是找到最优策略使得累积期望回报最大化，获得最优策略的方法有直接与间接之分。直接获取策略的图示为：\n\n![](./rl-classification/policy-based.png)\n\n间接获得策略为从值函数中提取最优策略，图示为：\n\n![](./rl-classification/value-based.png)\n\n直接获取策略的方式即为Policy-Based，常见的算法有：\n\n- Policy Gradient\n- PPO\n- SAC\n- ……\n\n间接获得策略的方式即为Value-Based，常见的算法有：\n\n- SARSA\n- Q-Learning\n- DQN\n- ……\n\n# On-policy or Off-policy\n\n在机器学习中，提到On跟Off这两个词我们最容易想到的是On-line Learning与Off-line Learning，那么强化学习与On-line、Off-line有什么关系呢？\n\n---\n\n网上对于On-line Learning与Off-line Learning有不同的解释，按热度排序为下面三种：\n\n对于On-line Learning：\n\n1. 单样本学习，样本用完即丢，样本连续不断输入，非数据集，而是数据流\n2. 单样本的（SGD）\n3. 单样本或批样本学习，样本连续不断输入，非数据集，而是数据流\n\n相应对于Off-line Learning：\n\n1. 批样本或全样本学习多次，静态样本集\n2. 批样本学习\n3. 全样本学习，静态数据集\n\n对于这三种方式，强化学习可以怎样融入呢？\n\n1. 对于第一种，强化学习不属于On-line Learning也不属于Off-line Learning，不属于Off-line Learning是因为样本非静态、非固定，不属于On-line Learning是因为对于Q-Learning、Sarsa、PG、PPO等算法样本用完即丢，对于DQN、TD3等算法样本重复利用。\n2. 对于第二种，强化学习包括On-line Learning及On-line Learning\n3. 低于第三种，强化学习属于On-line Learning\n\n---\n\nOn-policy、Off-policy与On-line、Off-line之间有关系吗？\n\n好像没有关系。虽然它们都是关于样本进行的划分，不过On-Off line learning针对的是样本的使用，而On-Off policy针对的是样本的生成。\n\n---\n\n学习On-policy、Off-policy之前首先需要理解什么是行为策略与目标策略。\n\n行为策略$\\mathcal{Behavior Policy}$：\n\n- 采样时间序列$S_{0},A_{0},R_{0},S_{1},A_{1},R_{2},...,S_{n},A_{n},R_{n}$的策略\n- 官话：指导个体产生与环境进行实际交互行为的策略\n- 未必由一个模型表示\n\n目标策略$\\mathcal{TargetPolicy}$:\n\n- 待优化的策略\n- 官话：用来评价状态或行为价值的策略或者待优化的策略称为目标策略\n\n\n\n同步策略学习$\\mathcal{On-Policy}$:\n\n- 简言之，边采样边学习\n- 官话：如果个体在学习过程中优化的策略与自己的行为策略是同一个策略时，这种学习方式称为**同步策略学习（on-policy learning）**\n- 行为策略与目标策略是同一个\n\n异步策略学习$\\mathcal{Off-Policy}$:\n\n- 简言之，你采样我学习\n- 官话：如果个体在学习过程中优化的策略与自己的行为策略是不同的策略时，这种学习方式称为**异步策略学习（off-policy learning）**\n- 行为策略与目标策略不同，行为策略可能是目标策略的“分身”（双网络结构），或者完全是另一个采样的策略\n\n两者的区别简而言之：如果需要估计一个值，用于估计的额外信息和当前信息出自同一策略则为on-policy，否则为off-policy。以SARSA和Q-Learning算法为例，对于Q值的估计，SARSA中$s_{t+1}$的动作由当前策略产生，故为on-policy算法，而Q-Learning中$s_{t+1}$的动作由贪心策略产生，故为off-policy。\n\n例如：\n\n\n\n\n\n|             | SARSA | Q-learning |\n|:-----------:|:-----:|:----------:|\n| Choosing A' |   π   |      π     |\n| Updating Q  |   π   |      μ     |\n\n\n\n> [一个以Q-Learning和Sarsa算法做比较的解释](https://stackoverflow.com/a/41420616)\n\n# Stochastic or Deterministic","source":"_posts/rl-classification.md","raw":"---\ntitle: 强化学习的类别\ncopyright: true\ntop: 1\ndate: 2019-05-10 15:18:23\nmathjax: true\nkeywords: \ndescription: \ncategories: ReinforcementLearning\ntags:\n- rl\n---\n\n本文讲述了强化学习中各种算法、问题的分类规则。\n\n<!--more-->\n\n# Stationary or not\n\n根据环境十分稳定、可以将强化学习问题分为stationary、non-stationary。\n\n如果状态转移**和**奖励函数是确定的，即选择动作$a$后执行它的结果是确定的，那么这个环境就是stationary。\n\n![](./rl-classification/stationary.png)\n\n如果状态转移**或**奖励函数是不确定的，即选择动作$a$后执行它的结果是不确定的，那么这个环境就是non-stationary。\n\n![](./rl-classification/non-stationary.png)\n\n# Model Based-or-Free\n\n一直对这个问题的认识不清晰，直到最近（2019年5月12日19:13:53）才有了清晰的认识。\n\n需要注意的是，无论是Model-Based还是Model-Free都不是对强化学习问题的分类，而是对算法的分类。之前一直理解的是状态空间$\\mathcal{S}$、动作空间$\\mathcal{A}$的都是离散的，转移概率矩阵$\\mathcal{P}$是确定的，这样即是Model-Based，如果状态空间$\\mathcal{S}$、动作空间$\\mathcal{A}$或转移概率矩阵$\\mathcal{P}$是不确定的，则是Model-Free，其实这只是对Model的分类，并不是Model-Based与Model-Free的真实含义，Model-Based与Model-Free是对算法求解过程的分类，理解这个可以在阅读国外文献、实验环境时更清晰，提升自己对强化学习算法的理解深度。\n\nModel-Based：\n\n- 智能体Agent在已知模型（$\\mathcal{S,A,R,P}$有限且确定）或者先学习一个模型（使用有监督对状态转移、奖励函数进行学习而得到），并在这个模型中使用**planning**（预测所有状态转移可能）方法来计算解决方案\n\n> Now if we know what all those elements of an MDP are, we can just compute the solution before ever actually executing an action in the environment. In AI, we typically call computing the solution to a decision-making problem before executing an actual decision *planning*. Some classic planning algorithms for MDPs include Value Iteration, Policy Iteration, and whole lot more.\n\nModel-Free:\n\n- 智能体在模型（$\\mathcal{S,A,R,P}$可能确定但没有使用planning方式解决，也可能不确定）中试错，并且使用**learning**（不预测全部可能性）方法来产生最佳策略\n\n> But the RL problem isn’t so kind to us. What makes a problem an RL problem, rather than a planning problem, is the agent does *not* know all the elements of the MDP, precluding it from being able to plan a solution. Specifically, the agent does not know how the world will change in response to its actions (the transition function TT), nor what immediate reward it will receive for doing so (the reward function RR). The agent will simply have to try taking actions in the environment, observe what happens, and somehow, find a good policy from doing so.\n\n根据Model-Based、Model-Free对算法、解决方法进行分类：\n\nModel-Based：DP、Policy Iteration、Value Iteration……\n\nModel-Free：SARSA、Q-Learning、PG……\n\n---\n\n> if you want a way to check if an RL algorithm is model-based or model-free, ask yourself this question: after learning, can the agent make predictions about what the next state and reward will be before it takes each action? If it can, then it’s a model-based RL algorithm. if it cannot, it’s a model-free algorithm.\n\n**使用算法学成策略之后，智能体可以在执行动作前判断该动作的后果，即是Model-Based，反之则是Model-Free**\n\n---\n\n根据Model-Based和Model-Free可以将强化学习算法分类，图片摘自OpenAI Spinning Up，如图所示：\n\n![](./rl-classification/model-classification.png)\n\n>[What is the difference between model-based and model-free reinforcement learning?](https://www.quora.com/What-is-the-difference-between-model-based-and-model-free-reinforcement-learning)\n>\n>[OpenAI Spinning Up : A Taxonomy of RL Algorithms](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms)\n\n# Policy or Value\n\n强化学习的目的是找到最优策略使得累积期望回报最大化，获得最优策略的方法有直接与间接之分。直接获取策略的图示为：\n\n![](./rl-classification/policy-based.png)\n\n间接获得策略为从值函数中提取最优策略，图示为：\n\n![](./rl-classification/value-based.png)\n\n直接获取策略的方式即为Policy-Based，常见的算法有：\n\n- Policy Gradient\n- PPO\n- SAC\n- ……\n\n间接获得策略的方式即为Value-Based，常见的算法有：\n\n- SARSA\n- Q-Learning\n- DQN\n- ……\n\n# On-policy or Off-policy\n\n在机器学习中，提到On跟Off这两个词我们最容易想到的是On-line Learning与Off-line Learning，那么强化学习与On-line、Off-line有什么关系呢？\n\n---\n\n网上对于On-line Learning与Off-line Learning有不同的解释，按热度排序为下面三种：\n\n对于On-line Learning：\n\n1. 单样本学习，样本用完即丢，样本连续不断输入，非数据集，而是数据流\n2. 单样本的（SGD）\n3. 单样本或批样本学习，样本连续不断输入，非数据集，而是数据流\n\n相应对于Off-line Learning：\n\n1. 批样本或全样本学习多次，静态样本集\n2. 批样本学习\n3. 全样本学习，静态数据集\n\n对于这三种方式，强化学习可以怎样融入呢？\n\n1. 对于第一种，强化学习不属于On-line Learning也不属于Off-line Learning，不属于Off-line Learning是因为样本非静态、非固定，不属于On-line Learning是因为对于Q-Learning、Sarsa、PG、PPO等算法样本用完即丢，对于DQN、TD3等算法样本重复利用。\n2. 对于第二种，强化学习包括On-line Learning及On-line Learning\n3. 低于第三种，强化学习属于On-line Learning\n\n---\n\nOn-policy、Off-policy与On-line、Off-line之间有关系吗？\n\n好像没有关系。虽然它们都是关于样本进行的划分，不过On-Off line learning针对的是样本的使用，而On-Off policy针对的是样本的生成。\n\n---\n\n学习On-policy、Off-policy之前首先需要理解什么是行为策略与目标策略。\n\n行为策略$\\mathcal{Behavior Policy}$：\n\n- 采样时间序列$S_{0},A_{0},R_{0},S_{1},A_{1},R_{2},...,S_{n},A_{n},R_{n}$的策略\n- 官话：指导个体产生与环境进行实际交互行为的策略\n- 未必由一个模型表示\n\n目标策略$\\mathcal{TargetPolicy}$:\n\n- 待优化的策略\n- 官话：用来评价状态或行为价值的策略或者待优化的策略称为目标策略\n\n\n\n同步策略学习$\\mathcal{On-Policy}$:\n\n- 简言之，边采样边学习\n- 官话：如果个体在学习过程中优化的策略与自己的行为策略是同一个策略时，这种学习方式称为**同步策略学习（on-policy learning）**\n- 行为策略与目标策略是同一个\n\n异步策略学习$\\mathcal{Off-Policy}$:\n\n- 简言之，你采样我学习\n- 官话：如果个体在学习过程中优化的策略与自己的行为策略是不同的策略时，这种学习方式称为**异步策略学习（off-policy learning）**\n- 行为策略与目标策略不同，行为策略可能是目标策略的“分身”（双网络结构），或者完全是另一个采样的策略\n\n两者的区别简而言之：如果需要估计一个值，用于估计的额外信息和当前信息出自同一策略则为on-policy，否则为off-policy。以SARSA和Q-Learning算法为例，对于Q值的估计，SARSA中$s_{t+1}$的动作由当前策略产生，故为on-policy算法，而Q-Learning中$s_{t+1}$的动作由贪心策略产生，故为off-policy。\n\n例如：\n\n\n\n\n\n|             | SARSA | Q-learning |\n|:-----------:|:-----:|:----------:|\n| Choosing A' |   π   |      π     |\n| Updating Q  |   π   |      μ     |\n\n\n\n> [一个以Q-Learning和Sarsa算法做比较的解释](https://stackoverflow.com/a/41420616)\n\n# Stochastic or Deterministic","slug":"rl-classification","published":1,"updated":"2019-08-25T05:37:46.831Z","_id":"cjxd6m9sv000fekvezsrt1s37","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本文讲述了强化学习中各种算法、问题的分类规则。</p>\n<a id=\"more\"></a>\n<h1 id=\"Stationary-or-not\"><a href=\"#Stationary-or-not\" class=\"headerlink\" title=\"Stationary or not\"></a>Stationary or not</h1><p>根据环境十分稳定、可以将强化学习问题分为stationary、non-stationary。</p>\n<p>如果状态转移<strong>和</strong>奖励函数是确定的，即选择动作$a$后执行它的结果是确定的，那么这个环境就是stationary。</p>\n<p><img src=\"./rl-classification/stationary.png\" alt=\"\"></p>\n<p>如果状态转移<strong>或</strong>奖励函数是不确定的，即选择动作$a$后执行它的结果是不确定的，那么这个环境就是non-stationary。</p>\n<p><img src=\"./rl-classification/non-stationary.png\" alt=\"\"></p>\n<h1 id=\"Model-Based-or-Free\"><a href=\"#Model-Based-or-Free\" class=\"headerlink\" title=\"Model Based-or-Free\"></a>Model Based-or-Free</h1><p>一直对这个问题的认识不清晰，直到最近（2019年5月12日19:13:53）才有了清晰的认识。</p>\n<p>需要注意的是，无论是Model-Based还是Model-Free都不是对强化学习问题的分类，而是对算法的分类。之前一直理解的是状态空间$\\mathcal{S}$、动作空间$\\mathcal{A}$的都是离散的，转移概率矩阵$\\mathcal{P}$是确定的，这样即是Model-Based，如果状态空间$\\mathcal{S}$、动作空间$\\mathcal{A}$或转移概率矩阵$\\mathcal{P}$是不确定的，则是Model-Free，其实这只是对Model的分类，并不是Model-Based与Model-Free的真实含义，Model-Based与Model-Free是对算法求解过程的分类，理解这个可以在阅读国外文献、实验环境时更清晰，提升自己对强化学习算法的理解深度。</p>\n<p>Model-Based：</p>\n<ul>\n<li>智能体Agent在已知模型（$\\mathcal{S,A,R,P}$有限且确定）或者先学习一个模型（使用有监督对状态转移、奖励函数进行学习而得到），并在这个模型中使用<strong>planning</strong>（预测所有状态转移可能）方法来计算解决方案</li>\n</ul>\n<blockquote>\n<p>Now if we know what all those elements of an MDP are, we can just compute the solution before ever actually executing an action in the environment. In AI, we typically call computing the solution to a decision-making problem before executing an actual decision <em>planning</em>. Some classic planning algorithms for MDPs include Value Iteration, Policy Iteration, and whole lot more.</p>\n</blockquote>\n<p>Model-Free:</p>\n<ul>\n<li>智能体在模型（$\\mathcal{S,A,R,P}$可能确定但没有使用planning方式解决，也可能不确定）中试错，并且使用<strong>learning</strong>（不预测全部可能性）方法来产生最佳策略</li>\n</ul>\n<blockquote>\n<p>But the RL problem isn’t so kind to us. What makes a problem an RL problem, rather than a planning problem, is the agent does <em>not</em> know all the elements of the MDP, precluding it from being able to plan a solution. Specifically, the agent does not know how the world will change in response to its actions (the transition function TT), nor what immediate reward it will receive for doing so (the reward function RR). The agent will simply have to try taking actions in the environment, observe what happens, and somehow, find a good policy from doing so.</p>\n</blockquote>\n<p>根据Model-Based、Model-Free对算法、解决方法进行分类：</p>\n<p>Model-Based：DP、Policy Iteration、Value Iteration……</p>\n<p>Model-Free：SARSA、Q-Learning、PG……</p>\n<hr>\n<blockquote>\n<p>if you want a way to check if an RL algorithm is model-based or model-free, ask yourself this question: after learning, can the agent make predictions about what the next state and reward will be before it takes each action? If it can, then it’s a model-based RL algorithm. if it cannot, it’s a model-free algorithm.</p>\n</blockquote>\n<p><strong>使用算法学成策略之后，智能体可以在执行动作前判断该动作的后果，即是Model-Based，反之则是Model-Free</strong></p>\n<hr>\n<p>根据Model-Based和Model-Free可以将强化学习算法分类，图片摘自OpenAI Spinning Up，如图所示：</p>\n<p><img src=\"./rl-classification/model-classification.png\" alt=\"\"></p>\n<blockquote>\n<p><a href=\"https://www.quora.com/What-is-the-difference-between-model-based-and-model-free-reinforcement-learning\" rel=\"external nofollow\" target=\"_blank\">What is the difference between model-based and model-free reinforcement learning?</a></p>\n<p><a href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms\" rel=\"external nofollow\" target=\"_blank\">OpenAI Spinning Up : A Taxonomy of RL Algorithms</a></p>\n</blockquote>\n<h1 id=\"Policy-or-Value\"><a href=\"#Policy-or-Value\" class=\"headerlink\" title=\"Policy or Value\"></a>Policy or Value</h1><p>强化学习的目的是找到最优策略使得累积期望回报最大化，获得最优策略的方法有直接与间接之分。直接获取策略的图示为：</p>\n<p><img src=\"./rl-classification/policy-based.png\" alt=\"\"></p>\n<p>间接获得策略为从值函数中提取最优策略，图示为：</p>\n<p><img src=\"./rl-classification/value-based.png\" alt=\"\"></p>\n<p>直接获取策略的方式即为Policy-Based，常见的算法有：</p>\n<ul>\n<li>Policy Gradient</li>\n<li>PPO</li>\n<li>SAC</li>\n<li>……</li>\n</ul>\n<p>间接获得策略的方式即为Value-Based，常见的算法有：</p>\n<ul>\n<li>SARSA</li>\n<li>Q-Learning</li>\n<li>DQN</li>\n<li>……</li>\n</ul>\n<h1 id=\"On-policy-or-Off-policy\"><a href=\"#On-policy-or-Off-policy\" class=\"headerlink\" title=\"On-policy or Off-policy\"></a>On-policy or Off-policy</h1><p>在机器学习中，提到On跟Off这两个词我们最容易想到的是On-line Learning与Off-line Learning，那么强化学习与On-line、Off-line有什么关系呢？</p>\n<hr>\n<p>网上对于On-line Learning与Off-line Learning有不同的解释，按热度排序为下面三种：</p>\n<p>对于On-line Learning：</p>\n<ol>\n<li>单样本学习，样本用完即丢，样本连续不断输入，非数据集，而是数据流</li>\n<li>单样本的（SGD）</li>\n<li>单样本或批样本学习，样本连续不断输入，非数据集，而是数据流</li>\n</ol>\n<p>相应对于Off-line Learning：</p>\n<ol>\n<li>批样本或全样本学习多次，静态样本集</li>\n<li>批样本学习</li>\n<li>全样本学习，静态数据集</li>\n</ol>\n<p>对于这三种方式，强化学习可以怎样融入呢？</p>\n<ol>\n<li>对于第一种，强化学习不属于On-line Learning也不属于Off-line Learning，不属于Off-line Learning是因为样本非静态、非固定，不属于On-line Learning是因为对于Q-Learning、Sarsa、PG、PPO等算法样本用完即丢，对于DQN、TD3等算法样本重复利用。</li>\n<li>对于第二种，强化学习包括On-line Learning及On-line Learning</li>\n<li>低于第三种，强化学习属于On-line Learning</li>\n</ol>\n<hr>\n<p>On-policy、Off-policy与On-line、Off-line之间有关系吗？</p>\n<p>好像没有关系。虽然它们都是关于样本进行的划分，不过On-Off line learning针对的是样本的使用，而On-Off policy针对的是样本的生成。</p>\n<hr>\n<p>学习On-policy、Off-policy之前首先需要理解什么是行为策略与目标策略。</p>\n<p>行为策略$\\mathcal{Behavior Policy}$：</p>\n<ul>\n<li>采样时间序列$S_{0},A_{0},R_{0},S_{1},A_{1},R_{2},…,S_{n},A_{n},R_{n}$的策略</li>\n<li>官话：指导个体产生与环境进行实际交互行为的策略</li>\n<li>未必由一个模型表示</li>\n</ul>\n<p>目标策略$\\mathcal{TargetPolicy}$:</p>\n<ul>\n<li>待优化的策略</li>\n<li>官话：用来评价状态或行为价值的策略或者待优化的策略称为目标策略</li>\n</ul>\n<p>同步策略学习$\\mathcal{On-Policy}$:</p>\n<ul>\n<li>简言之，边采样边学习</li>\n<li>官话：如果个体在学习过程中优化的策略与自己的行为策略是同一个策略时，这种学习方式称为<strong>同步策略学习（on-policy learning）</strong></li>\n<li>行为策略与目标策略是同一个</li>\n</ul>\n<p>异步策略学习$\\mathcal{Off-Policy}$:</p>\n<ul>\n<li>简言之，你采样我学习</li>\n<li>官话：如果个体在学习过程中优化的策略与自己的行为策略是不同的策略时，这种学习方式称为<strong>异步策略学习（off-policy learning）</strong></li>\n<li>行为策略与目标策略不同，行为策略可能是目标策略的“分身”（双网络结构），或者完全是另一个采样的策略</li>\n</ul>\n<p>两者的区别简而言之：如果需要估计一个值，用于估计的额外信息和当前信息出自同一策略则为on-policy，否则为off-policy。以SARSA和Q-Learning算法为例，对于Q值的估计，SARSA中$s_{t+1}$的动作由当前策略产生，故为on-policy算法，而Q-Learning中$s_{t+1}$的动作由贪心策略产生，故为off-policy。</p>\n<p>例如：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th style=\"text-align:center\">SARSA</th>\n<th style=\"text-align:center\">Q-learning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Choosing A’</td>\n<td style=\"text-align:center\">π</td>\n<td style=\"text-align:center\">π</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Updating Q</td>\n<td style=\"text-align:center\">π</td>\n<td style=\"text-align:center\">μ</td>\n</tr>\n</tbody>\n</table>\n</div>\n<blockquote>\n<p><a href=\"https://stackoverflow.com/a/41420616\" rel=\"external nofollow\" target=\"_blank\">一个以Q-Learning和Sarsa算法做比较的解释</a></p>\n</blockquote>\n<h1 id=\"Stochastic-or-Deterministic\"><a href=\"#Stochastic-or-Deterministic\" class=\"headerlink\" title=\"Stochastic or Deterministic\"></a>Stochastic or Deterministic</h1>","site":{"data":{}},"excerpt":"<p>本文讲述了强化学习中各种算法、问题的分类规则。</p>","more":"<h1 id=\"Stationary-or-not\"><a href=\"#Stationary-or-not\" class=\"headerlink\" title=\"Stationary or not\"></a>Stationary or not</h1><p>根据环境十分稳定、可以将强化学习问题分为stationary、non-stationary。</p>\n<p>如果状态转移<strong>和</strong>奖励函数是确定的，即选择动作$a$后执行它的结果是确定的，那么这个环境就是stationary。</p>\n<p><img src=\"./rl-classification/stationary.png\" alt=\"\"></p>\n<p>如果状态转移<strong>或</strong>奖励函数是不确定的，即选择动作$a$后执行它的结果是不确定的，那么这个环境就是non-stationary。</p>\n<p><img src=\"./rl-classification/non-stationary.png\" alt=\"\"></p>\n<h1 id=\"Model-Based-or-Free\"><a href=\"#Model-Based-or-Free\" class=\"headerlink\" title=\"Model Based-or-Free\"></a>Model Based-or-Free</h1><p>一直对这个问题的认识不清晰，直到最近（2019年5月12日19:13:53）才有了清晰的认识。</p>\n<p>需要注意的是，无论是Model-Based还是Model-Free都不是对强化学习问题的分类，而是对算法的分类。之前一直理解的是状态空间$\\mathcal{S}$、动作空间$\\mathcal{A}$的都是离散的，转移概率矩阵$\\mathcal{P}$是确定的，这样即是Model-Based，如果状态空间$\\mathcal{S}$、动作空间$\\mathcal{A}$或转移概率矩阵$\\mathcal{P}$是不确定的，则是Model-Free，其实这只是对Model的分类，并不是Model-Based与Model-Free的真实含义，Model-Based与Model-Free是对算法求解过程的分类，理解这个可以在阅读国外文献、实验环境时更清晰，提升自己对强化学习算法的理解深度。</p>\n<p>Model-Based：</p>\n<ul>\n<li>智能体Agent在已知模型（$\\mathcal{S,A,R,P}$有限且确定）或者先学习一个模型（使用有监督对状态转移、奖励函数进行学习而得到），并在这个模型中使用<strong>planning</strong>（预测所有状态转移可能）方法来计算解决方案</li>\n</ul>\n<blockquote>\n<p>Now if we know what all those elements of an MDP are, we can just compute the solution before ever actually executing an action in the environment. In AI, we typically call computing the solution to a decision-making problem before executing an actual decision <em>planning</em>. Some classic planning algorithms for MDPs include Value Iteration, Policy Iteration, and whole lot more.</p>\n</blockquote>\n<p>Model-Free:</p>\n<ul>\n<li>智能体在模型（$\\mathcal{S,A,R,P}$可能确定但没有使用planning方式解决，也可能不确定）中试错，并且使用<strong>learning</strong>（不预测全部可能性）方法来产生最佳策略</li>\n</ul>\n<blockquote>\n<p>But the RL problem isn’t so kind to us. What makes a problem an RL problem, rather than a planning problem, is the agent does <em>not</em> know all the elements of the MDP, precluding it from being able to plan a solution. Specifically, the agent does not know how the world will change in response to its actions (the transition function TT), nor what immediate reward it will receive for doing so (the reward function RR). The agent will simply have to try taking actions in the environment, observe what happens, and somehow, find a good policy from doing so.</p>\n</blockquote>\n<p>根据Model-Based、Model-Free对算法、解决方法进行分类：</p>\n<p>Model-Based：DP、Policy Iteration、Value Iteration……</p>\n<p>Model-Free：SARSA、Q-Learning、PG……</p>\n<hr>\n<blockquote>\n<p>if you want a way to check if an RL algorithm is model-based or model-free, ask yourself this question: after learning, can the agent make predictions about what the next state and reward will be before it takes each action? If it can, then it’s a model-based RL algorithm. if it cannot, it’s a model-free algorithm.</p>\n</blockquote>\n<p><strong>使用算法学成策略之后，智能体可以在执行动作前判断该动作的后果，即是Model-Based，反之则是Model-Free</strong></p>\n<hr>\n<p>根据Model-Based和Model-Free可以将强化学习算法分类，图片摘自OpenAI Spinning Up，如图所示：</p>\n<p><img src=\"./rl-classification/model-classification.png\" alt=\"\"></p>\n<blockquote>\n<p><a href=\"https://www.quora.com/What-is-the-difference-between-model-based-and-model-free-reinforcement-learning\" rel=\"external nofollow\" target=\"_blank\">What is the difference between model-based and model-free reinforcement learning?</a></p>\n<p><a href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms\" rel=\"external nofollow\" target=\"_blank\">OpenAI Spinning Up : A Taxonomy of RL Algorithms</a></p>\n</blockquote>\n<h1 id=\"Policy-or-Value\"><a href=\"#Policy-or-Value\" class=\"headerlink\" title=\"Policy or Value\"></a>Policy or Value</h1><p>强化学习的目的是找到最优策略使得累积期望回报最大化，获得最优策略的方法有直接与间接之分。直接获取策略的图示为：</p>\n<p><img src=\"./rl-classification/policy-based.png\" alt=\"\"></p>\n<p>间接获得策略为从值函数中提取最优策略，图示为：</p>\n<p><img src=\"./rl-classification/value-based.png\" alt=\"\"></p>\n<p>直接获取策略的方式即为Policy-Based，常见的算法有：</p>\n<ul>\n<li>Policy Gradient</li>\n<li>PPO</li>\n<li>SAC</li>\n<li>……</li>\n</ul>\n<p>间接获得策略的方式即为Value-Based，常见的算法有：</p>\n<ul>\n<li>SARSA</li>\n<li>Q-Learning</li>\n<li>DQN</li>\n<li>……</li>\n</ul>\n<h1 id=\"On-policy-or-Off-policy\"><a href=\"#On-policy-or-Off-policy\" class=\"headerlink\" title=\"On-policy or Off-policy\"></a>On-policy or Off-policy</h1><p>在机器学习中，提到On跟Off这两个词我们最容易想到的是On-line Learning与Off-line Learning，那么强化学习与On-line、Off-line有什么关系呢？</p>\n<hr>\n<p>网上对于On-line Learning与Off-line Learning有不同的解释，按热度排序为下面三种：</p>\n<p>对于On-line Learning：</p>\n<ol>\n<li>单样本学习，样本用完即丢，样本连续不断输入，非数据集，而是数据流</li>\n<li>单样本的（SGD）</li>\n<li>单样本或批样本学习，样本连续不断输入，非数据集，而是数据流</li>\n</ol>\n<p>相应对于Off-line Learning：</p>\n<ol>\n<li>批样本或全样本学习多次，静态样本集</li>\n<li>批样本学习</li>\n<li>全样本学习，静态数据集</li>\n</ol>\n<p>对于这三种方式，强化学习可以怎样融入呢？</p>\n<ol>\n<li>对于第一种，强化学习不属于On-line Learning也不属于Off-line Learning，不属于Off-line Learning是因为样本非静态、非固定，不属于On-line Learning是因为对于Q-Learning、Sarsa、PG、PPO等算法样本用完即丢，对于DQN、TD3等算法样本重复利用。</li>\n<li>对于第二种，强化学习包括On-line Learning及On-line Learning</li>\n<li>低于第三种，强化学习属于On-line Learning</li>\n</ol>\n<hr>\n<p>On-policy、Off-policy与On-line、Off-line之间有关系吗？</p>\n<p>好像没有关系。虽然它们都是关于样本进行的划分，不过On-Off line learning针对的是样本的使用，而On-Off policy针对的是样本的生成。</p>\n<hr>\n<p>学习On-policy、Off-policy之前首先需要理解什么是行为策略与目标策略。</p>\n<p>行为策略$\\mathcal{Behavior Policy}$：</p>\n<ul>\n<li>采样时间序列$S_{0},A_{0},R_{0},S_{1},A_{1},R_{2},…,S_{n},A_{n},R_{n}$的策略</li>\n<li>官话：指导个体产生与环境进行实际交互行为的策略</li>\n<li>未必由一个模型表示</li>\n</ul>\n<p>目标策略$\\mathcal{TargetPolicy}$:</p>\n<ul>\n<li>待优化的策略</li>\n<li>官话：用来评价状态或行为价值的策略或者待优化的策略称为目标策略</li>\n</ul>\n<p>同步策略学习$\\mathcal{On-Policy}$:</p>\n<ul>\n<li>简言之，边采样边学习</li>\n<li>官话：如果个体在学习过程中优化的策略与自己的行为策略是同一个策略时，这种学习方式称为<strong>同步策略学习（on-policy learning）</strong></li>\n<li>行为策略与目标策略是同一个</li>\n</ul>\n<p>异步策略学习$\\mathcal{Off-Policy}$:</p>\n<ul>\n<li>简言之，你采样我学习</li>\n<li>官话：如果个体在学习过程中优化的策略与自己的行为策略是不同的策略时，这种学习方式称为<strong>异步策略学习（off-policy learning）</strong></li>\n<li>行为策略与目标策略不同，行为策略可能是目标策略的“分身”（双网络结构），或者完全是另一个采样的策略</li>\n</ul>\n<p>两者的区别简而言之：如果需要估计一个值，用于估计的额外信息和当前信息出自同一策略则为on-policy，否则为off-policy。以SARSA和Q-Learning算法为例，对于Q值的估计，SARSA中$s_{t+1}$的动作由当前策略产生，故为on-policy算法，而Q-Learning中$s_{t+1}$的动作由贪心策略产生，故为off-policy。</p>\n<p>例如：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th style=\"text-align:center\">SARSA</th>\n<th style=\"text-align:center\">Q-learning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Choosing A’</td>\n<td style=\"text-align:center\">π</td>\n<td style=\"text-align:center\">π</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Updating Q</td>\n<td style=\"text-align:center\">π</td>\n<td style=\"text-align:center\">μ</td>\n</tr>\n</tbody>\n</table>\n</div>\n<blockquote>\n<p><a href=\"https://stackoverflow.com/a/41420616\" rel=\"external nofollow\" target=\"_blank\">一个以Q-Learning和Sarsa算法做比较的解释</a></p>\n</blockquote>\n<h1 id=\"Stochastic-or-Deterministic\"><a href=\"#Stochastic-or-Deterministic\" class=\"headerlink\" title=\"Stochastic or Deterministic\"></a>Stochastic or Deterministic</h1>"},{"title":"SARSA and Q-Learning","copyright":true,"mathjax":true,"password":123,"top":1,"date":"2019-05-13T12:44:45.000Z","keywords":null,"description":null,"_content":"\n本文介绍了两个强化学习中解决Model-Free问题的最经典算法：SARSA和Q-Learning，这两个算法也是On-Policy与Off-Policy的分水岭。\n\n<!--more-->\n\n","source":"_posts/sarsa-and-q-learning.md","raw":"---\ntitle: SARSA and Q-Learning\ncopyright: true\nmathjax: true\npassword: 123\ntop: 1\ndate: 2019-05-13 20:44:45\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n本文介绍了两个强化学习中解决Model-Free问题的最经典算法：SARSA和Q-Learning，这两个算法也是On-Policy与Off-Policy的分水岭。\n\n<!--more-->\n\n","slug":"sarsa-and-q-learning","published":1,"updated":"2019-05-13T12:46:37.930Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9t2000jekve7czuq25g","content":"<p>本文介绍了两个强化学习中解决Model-Free问题的最经典算法：SARSA和Q-Learning，这两个算法也是On-Policy与Off-Policy的分水岭。</p>\n<a id=\"more\"></a>\n","site":{"data":{}},"excerpt":"<p>本文介绍了两个强化学习中解决Model-Free问题的最经典算法：SARSA和Q-Learning，这两个算法也是On-Policy与Off-Policy的分水岭。</p>","more":""},{"title":"在Jupyter Notebook中使用本机的conda环境","copyright":true,"top":1,"date":"2019-03-22T05:16:02.000Z","_content":"\n本文介绍了如何在Jupyter Notebook中使用本机的conda环境进行开发\n\n<!--more-->\n\n# Jupyter下conda多环境管理\n\n## 1. 手撸命令\n\n1. 在`base`环境下安装内核管理工具\n    `pip install ipykernel`\n\n2. 将环境内核添加到`jupyter kernel`中\n    `python -m ipykernel install --user --name [env_name] --display-name \"[show name in jupyter]\"`\n\n3. 在各个环境中先`activate [env_name]`激活，再安装`ipykernel`\n`conda install ipykernel`\n\n4. 不用第二步，在各个环境内操作内核\n`python -m ipykernel install --name [show name in jupyter]`\n\n5. 查看已在`jupyter`中创建的虚拟环境内核\n    `jupyter kernelspec list`\n\n6. 删除内核\n    `jupyter kernelspec uninstall [env_name]`\n\n## 2. 使用插件\n\n简单粗暴, 在`base`环境下使用命令\n`conda install nb_conda`\n![](./use-conda-env-in-jupyter/1.png)\n![](./use-conda-env-in-jupyter/2.png)\n接下来, 看看jupyter中能不能显示`conda`环境\n`jupyter notebook`\n![](./use-conda-env-in-jupyter/3.png)\n在文件内部也可以很方便的切换环境\n![](./use-conda-env-in-jupyter/4.png)\n\n","source":"_posts/use-conda-env-in-jupyter.md","raw":"---\ntitle: 在Jupyter Notebook中使用本机的conda环境\ncopyright: true\ntop: 1\ndate: 2019-03-22 13:16:02\ncategories: Conda\ntags:\n- conda\n- jupyter notebook\n---\n\n本文介绍了如何在Jupyter Notebook中使用本机的conda环境进行开发\n\n<!--more-->\n\n# Jupyter下conda多环境管理\n\n## 1. 手撸命令\n\n1. 在`base`环境下安装内核管理工具\n    `pip install ipykernel`\n\n2. 将环境内核添加到`jupyter kernel`中\n    `python -m ipykernel install --user --name [env_name] --display-name \"[show name in jupyter]\"`\n\n3. 在各个环境中先`activate [env_name]`激活，再安装`ipykernel`\n`conda install ipykernel`\n\n4. 不用第二步，在各个环境内操作内核\n`python -m ipykernel install --name [show name in jupyter]`\n\n5. 查看已在`jupyter`中创建的虚拟环境内核\n    `jupyter kernelspec list`\n\n6. 删除内核\n    `jupyter kernelspec uninstall [env_name]`\n\n## 2. 使用插件\n\n简单粗暴, 在`base`环境下使用命令\n`conda install nb_conda`\n![](./use-conda-env-in-jupyter/1.png)\n![](./use-conda-env-in-jupyter/2.png)\n接下来, 看看jupyter中能不能显示`conda`环境\n`jupyter notebook`\n![](./use-conda-env-in-jupyter/3.png)\n在文件内部也可以很方便的切换环境\n![](./use-conda-env-in-jupyter/4.png)\n\n","slug":"use-conda-env-in-jupyter","published":1,"updated":"2019-07-30T00:58:47.583Z","_id":"cjxd6m9t6000lekve2s6k1efv","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本文介绍了如何在Jupyter Notebook中使用本机的conda环境进行开发</p>\n<a id=\"more\"></a>\n<h1 id=\"Jupyter下conda多环境管理\"><a href=\"#Jupyter下conda多环境管理\" class=\"headerlink\" title=\"Jupyter下conda多环境管理\"></a>Jupyter下conda多环境管理</h1><h2 id=\"1-手撸命令\"><a href=\"#1-手撸命令\" class=\"headerlink\" title=\"1. 手撸命令\"></a>1. 手撸命令</h2><ol>\n<li><p>在<code>base</code>环境下安装内核管理工具<br> <code>pip install ipykernel</code></p>\n</li>\n<li><p>将环境内核添加到<code>jupyter kernel</code>中<br> <code>python -m ipykernel install --user --name [env_name] --display-name &quot;[show name in jupyter]&quot;</code></p>\n</li>\n<li><p>在各个环境中先<code>activate [env_name]</code>激活，再安装<code>ipykernel</code><br><code>conda install ipykernel</code></p>\n</li>\n<li><p>不用第二步，在各个环境内操作内核<br><code>python -m ipykernel install --name [show name in jupyter]</code></p>\n</li>\n<li><p>查看已在<code>jupyter</code>中创建的虚拟环境内核<br> <code>jupyter kernelspec list</code></p>\n</li>\n<li><p>删除内核<br> <code>jupyter kernelspec uninstall [env_name]</code></p>\n</li>\n</ol>\n<h2 id=\"2-使用插件\"><a href=\"#2-使用插件\" class=\"headerlink\" title=\"2. 使用插件\"></a>2. 使用插件</h2><p>简单粗暴, 在<code>base</code>环境下使用命令<br><code>conda install nb_conda</code><br><img src=\"./use-conda-env-in-jupyter/1.png\" alt=\"\"><br><img src=\"./use-conda-env-in-jupyter/2.png\" alt=\"\"><br>接下来, 看看jupyter中能不能显示<code>conda</code>环境<br><code>jupyter notebook</code><br><img src=\"./use-conda-env-in-jupyter/3.png\" alt=\"\"><br>在文件内部也可以很方便的切换环境<br><img src=\"./use-conda-env-in-jupyter/4.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"<p>本文介绍了如何在Jupyter Notebook中使用本机的conda环境进行开发</p>","more":"<h1 id=\"Jupyter下conda多环境管理\"><a href=\"#Jupyter下conda多环境管理\" class=\"headerlink\" title=\"Jupyter下conda多环境管理\"></a>Jupyter下conda多环境管理</h1><h2 id=\"1-手撸命令\"><a href=\"#1-手撸命令\" class=\"headerlink\" title=\"1. 手撸命令\"></a>1. 手撸命令</h2><ol>\n<li><p>在<code>base</code>环境下安装内核管理工具<br> <code>pip install ipykernel</code></p>\n</li>\n<li><p>将环境内核添加到<code>jupyter kernel</code>中<br> <code>python -m ipykernel install --user --name [env_name] --display-name &quot;[show name in jupyter]&quot;</code></p>\n</li>\n<li><p>在各个环境中先<code>activate [env_name]</code>激活，再安装<code>ipykernel</code><br><code>conda install ipykernel</code></p>\n</li>\n<li><p>不用第二步，在各个环境内操作内核<br><code>python -m ipykernel install --name [show name in jupyter]</code></p>\n</li>\n<li><p>查看已在<code>jupyter</code>中创建的虚拟环境内核<br> <code>jupyter kernelspec list</code></p>\n</li>\n<li><p>删除内核<br> <code>jupyter kernelspec uninstall [env_name]</code></p>\n</li>\n</ol>\n<h2 id=\"2-使用插件\"><a href=\"#2-使用插件\" class=\"headerlink\" title=\"2. 使用插件\"></a>2. 使用插件</h2><p>简单粗暴, 在<code>base</code>环境下使用命令<br><code>conda install nb_conda</code><br><img src=\"./use-conda-env-in-jupyter/1.png\" alt=\"\"><br><img src=\"./use-conda-env-in-jupyter/2.png\" alt=\"\"><br>接下来, 看看jupyter中能不能显示<code>conda</code>环境<br><code>jupyter notebook</code><br><img src=\"./use-conda-env-in-jupyter/3.png\" alt=\"\"><br>在文件内部也可以很方便的切换环境<br><img src=\"./use-conda-env-in-jupyter/4.png\" alt=\"\"></p>"},{"title":"Universal Value Function Approximators","copyright":true,"mathjax":true,"top":1,"date":"2019-06-02T01:47:47.000Z","keywords":null,"description":null,"_content":"\n本文中的方法简称UVFA，即通用值函数逼近器，主要是用于将只能表示同一任务单目标的值函数表示成通用的多目标值函数。很多论文如HER都引用了这篇论文中提出的方法。\n\n推荐程度中等：\n\n- 文中理论说明很多，很晦涩，可以不看，直接跳至正文部分即可\n- 思想简单，了解一下即可\n\n<!--more-->\n\n# 简介\n\n论文地址：[http://proceedings.mlr.press/v37/schaul15.pdf](http://proceedings.mlr.press/v37/schaul15.pdf)\n\n> Our main idea is to represent a large set of optimal value functions by a single, unified function approximator that generalises over both states and goals.\n\n主要思想是通过一个统一的函数逼近器来表示一大组最优值函数，该函数逼近器可以概括状态和目标。\n\n单目标中值函数这么表示：\n$$\nV_{g, \\pi}(s) :=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} R_{g}\\left(s_{t+1}, a_{t}, s_{t}\\right) \\prod_{k=0}^{t} \\gamma_{g}\\left(s_{k}\\right) | s_{0}=s\\right]\n$$\n动作值函数这么表示：\n$$\nQ_{g, \\pi}(s, a) :=\\mathbb{E}_{s^{\\prime}}\\left[R_{g}\\left(s, a, s^{\\prime}\\right)+\\gamma_{g}\\left(s^{\\prime}\\right) \\cdot V_{g, \\pi}\\left(s^{\\prime}\\right)\\right]\n$$\n最优策略：\n$$\n\\pi_{g}^{*}(s) :=\\arg \\max _{a} Q_{\\pi, g}(s, a)\n$$\n相应的最优值函数：\n$$\nV_{g}^{*} :=V_{g, \\pi_{g}^{*}} \\ , \\ Q_{g}^{*} :=Q_{g, \\pi_{g}^{*}}\n$$\n本文中是想将一个单目标的最优值函数逼近改造成多目标的最优值函数表示，形象一点来说，就是把用向量表示状态值函数$V(s; \\theta)$变成矩阵表示$V(s, g ; \\theta)$，行列分别是状态$s$和目标$g$；把用矩阵表示动作值函数$Q(s, a; \\theta)$变成三维Tensor表示$Q(s, a, g ; \\theta)$，行列不变，增加维度-深度表示目标$g$。其中满足：\n$$\nV(s, g ; \\theta) \\approx V_{q}^{*}(s) \\ , \\ Q(s, a, g ; \\theta) \\approx Q_{a}^{*}(s, a)\n$$\n\n# 文中正文\n\n该方法拟解决的问题：\n\n- 如何通用表示各种问题的值函数逼近器？\n\n主要思想：\n\n- 用单个函数逼近器表示多目标的最优值函数\n\n实现方法：\n\n- 算法的输入由状态$s$扩展为状态-目标$\\lt s,g \\gt$，假设原状态表示向量$s$不包含目标信息\n\n有两种实现形式：\n\n- 直接用$||$连接，即$\\left ( s||g \\right )$，然后通过非线性函数逼近器(如MLPs)得到最终输出结果$V(s, g)$\n- 分别embedding，并将嵌入后的表示通过运算得到**最终输出结果**(文中使用的是点积)，$h : \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\mapsto \\mathbb{R}$，然后$V(s, g) :=h(\\phi(s), \\psi(g))$\n\n![](./universal-value-function-approximators/sg.png)\n\n左图为连结模式，中间图表示分别embedding并通过函数运算得到标量输出$h : \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\mapsto \\mathbb{R}$，右图为中间图的细化。\n\n文中指出，对于第二种形式，可以使$\\phi$网络和$\\psi$网络共享几层参数，因为一般来说，目标的向量表示形式与状态的向量表示形式相同，即$\\mathcal{G} \\subseteq \\mathcal{S}$。如果对于对称问题，即奖励函数是$s$和$g$的距离(平方差)等形式，那么有特点：\n$$\nV_{g}^{*}(s)=V_{s}^{*}(g) \\ , \\ \\forall s, g\n$$\n这个时候可以使$\\phi$网络和$\\psi$网络相同。\n\n***注：文中提到有使用低秩因式分解分别表示$\\hat{\\phi}_{t}$和$\\hat{\\psi}_{g}$，并使用有监督学习训练两个网络对$s$和$g$的embediing($\\phi_{t}$和$\\psi_{t}$)进行训练的方法，但是这需要假设问题可进行因式分解，即有限的状态和目标，但这在实际应用中往往是不成立的，所以不对这部分做过多的关注。***\n\n优点：\n\n- UVFA可用于同任务多目标的迁移学习中，可以比随机值初始化更快地学习解决新任务。*这是显而易见的作用，毕竟本身就是多目标的通用函数逼近器。*\n- 可以用于特征表示。*这也说，= =，强行增加字数。。。。。。*\n- UVFA有效地提供了一个通用决策模型，代表（近似）朝向任何目标$g \\in \\mathcal{G}$的最佳行为。*这个优点与现在提出的元强化学习中的slow部分如出一辙。*\n\n# 总结\n\nUVFA把$V(s)$变成$V(s,g)$，一个可表示**单任务、多目标**的通用值函数近似，扩展对任务的知识表达。(通用即知识，哈哈)","source":"_posts/universal-value-function-approximators.md","raw":"---\ntitle: Universal Value Function Approximators\ncopyright: true\nmathjax: true\ntop: 1\ndate: 2019-06-02 09:47:47\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n本文中的方法简称UVFA，即通用值函数逼近器，主要是用于将只能表示同一任务单目标的值函数表示成通用的多目标值函数。很多论文如HER都引用了这篇论文中提出的方法。\n\n推荐程度中等：\n\n- 文中理论说明很多，很晦涩，可以不看，直接跳至正文部分即可\n- 思想简单，了解一下即可\n\n<!--more-->\n\n# 简介\n\n论文地址：[http://proceedings.mlr.press/v37/schaul15.pdf](http://proceedings.mlr.press/v37/schaul15.pdf)\n\n> Our main idea is to represent a large set of optimal value functions by a single, unified function approximator that generalises over both states and goals.\n\n主要思想是通过一个统一的函数逼近器来表示一大组最优值函数，该函数逼近器可以概括状态和目标。\n\n单目标中值函数这么表示：\n$$\nV_{g, \\pi}(s) :=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} R_{g}\\left(s_{t+1}, a_{t}, s_{t}\\right) \\prod_{k=0}^{t} \\gamma_{g}\\left(s_{k}\\right) | s_{0}=s\\right]\n$$\n动作值函数这么表示：\n$$\nQ_{g, \\pi}(s, a) :=\\mathbb{E}_{s^{\\prime}}\\left[R_{g}\\left(s, a, s^{\\prime}\\right)+\\gamma_{g}\\left(s^{\\prime}\\right) \\cdot V_{g, \\pi}\\left(s^{\\prime}\\right)\\right]\n$$\n最优策略：\n$$\n\\pi_{g}^{*}(s) :=\\arg \\max _{a} Q_{\\pi, g}(s, a)\n$$\n相应的最优值函数：\n$$\nV_{g}^{*} :=V_{g, \\pi_{g}^{*}} \\ , \\ Q_{g}^{*} :=Q_{g, \\pi_{g}^{*}}\n$$\n本文中是想将一个单目标的最优值函数逼近改造成多目标的最优值函数表示，形象一点来说，就是把用向量表示状态值函数$V(s; \\theta)$变成矩阵表示$V(s, g ; \\theta)$，行列分别是状态$s$和目标$g$；把用矩阵表示动作值函数$Q(s, a; \\theta)$变成三维Tensor表示$Q(s, a, g ; \\theta)$，行列不变，增加维度-深度表示目标$g$。其中满足：\n$$\nV(s, g ; \\theta) \\approx V_{q}^{*}(s) \\ , \\ Q(s, a, g ; \\theta) \\approx Q_{a}^{*}(s, a)\n$$\n\n# 文中正文\n\n该方法拟解决的问题：\n\n- 如何通用表示各种问题的值函数逼近器？\n\n主要思想：\n\n- 用单个函数逼近器表示多目标的最优值函数\n\n实现方法：\n\n- 算法的输入由状态$s$扩展为状态-目标$\\lt s,g \\gt$，假设原状态表示向量$s$不包含目标信息\n\n有两种实现形式：\n\n- 直接用$||$连接，即$\\left ( s||g \\right )$，然后通过非线性函数逼近器(如MLPs)得到最终输出结果$V(s, g)$\n- 分别embedding，并将嵌入后的表示通过运算得到**最终输出结果**(文中使用的是点积)，$h : \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\mapsto \\mathbb{R}$，然后$V(s, g) :=h(\\phi(s), \\psi(g))$\n\n![](./universal-value-function-approximators/sg.png)\n\n左图为连结模式，中间图表示分别embedding并通过函数运算得到标量输出$h : \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\mapsto \\mathbb{R}$，右图为中间图的细化。\n\n文中指出，对于第二种形式，可以使$\\phi$网络和$\\psi$网络共享几层参数，因为一般来说，目标的向量表示形式与状态的向量表示形式相同，即$\\mathcal{G} \\subseteq \\mathcal{S}$。如果对于对称问题，即奖励函数是$s$和$g$的距离(平方差)等形式，那么有特点：\n$$\nV_{g}^{*}(s)=V_{s}^{*}(g) \\ , \\ \\forall s, g\n$$\n这个时候可以使$\\phi$网络和$\\psi$网络相同。\n\n***注：文中提到有使用低秩因式分解分别表示$\\hat{\\phi}_{t}$和$\\hat{\\psi}_{g}$，并使用有监督学习训练两个网络对$s$和$g$的embediing($\\phi_{t}$和$\\psi_{t}$)进行训练的方法，但是这需要假设问题可进行因式分解，即有限的状态和目标，但这在实际应用中往往是不成立的，所以不对这部分做过多的关注。***\n\n优点：\n\n- UVFA可用于同任务多目标的迁移学习中，可以比随机值初始化更快地学习解决新任务。*这是显而易见的作用，毕竟本身就是多目标的通用函数逼近器。*\n- 可以用于特征表示。*这也说，= =，强行增加字数。。。。。。*\n- UVFA有效地提供了一个通用决策模型，代表（近似）朝向任何目标$g \\in \\mathcal{G}$的最佳行为。*这个优点与现在提出的元强化学习中的slow部分如出一辙。*\n\n# 总结\n\nUVFA把$V(s)$变成$V(s,g)$，一个可表示**单任务、多目标**的通用值函数近似，扩展对任务的知识表达。(通用即知识，哈哈)","slug":"universal-value-function-approximators","published":1,"updated":"2019-06-02T07:40:37.582Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9th000qekvezc5xzoew","content":"<p>本文中的方法简称UVFA，即通用值函数逼近器，主要是用于将只能表示同一任务单目标的值函数表示成通用的多目标值函数。很多论文如HER都引用了这篇论文中提出的方法。</p>\n<p>推荐程度中等：</p>\n<ul>\n<li>文中理论说明很多，很晦涩，可以不看，直接跳至正文部分即可</li>\n<li>思想简单，了解一下即可</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"http://proceedings.mlr.press/v37/schaul15.pdf\" rel=\"external nofollow\" target=\"_blank\">http://proceedings.mlr.press/v37/schaul15.pdf</a></p>\n<blockquote>\n<p>Our main idea is to represent a large set of optimal value functions by a single, unified function approximator that generalises over both states and goals.</p>\n</blockquote>\n<p>主要思想是通过一个统一的函数逼近器来表示一大组最优值函数，该函数逼近器可以概括状态和目标。</p>\n<p>单目标中值函数这么表示：</p>\n<script type=\"math/tex; mode=display\">\nV_{g, \\pi}(s) :=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} R_{g}\\left(s_{t+1}, a_{t}, s_{t}\\right) \\prod_{k=0}^{t} \\gamma_{g}\\left(s_{k}\\right) | s_{0}=s\\right]</script><p>动作值函数这么表示：</p>\n<script type=\"math/tex; mode=display\">\nQ_{g, \\pi}(s, a) :=\\mathbb{E}_{s^{\\prime}}\\left[R_{g}\\left(s, a, s^{\\prime}\\right)+\\gamma_{g}\\left(s^{\\prime}\\right) \\cdot V_{g, \\pi}\\left(s^{\\prime}\\right)\\right]</script><p>最优策略：</p>\n<script type=\"math/tex; mode=display\">\n\\pi_{g}^{*}(s) :=\\arg \\max _{a} Q_{\\pi, g}(s, a)</script><p>相应的最优值函数：</p>\n<script type=\"math/tex; mode=display\">\nV_{g}^{*} :=V_{g, \\pi_{g}^{*}} \\ , \\ Q_{g}^{*} :=Q_{g, \\pi_{g}^{*}}</script><p>本文中是想将一个单目标的最优值函数逼近改造成多目标的最优值函数表示，形象一点来说，就是把用向量表示状态值函数$V(s; \\theta)$变成矩阵表示$V(s, g ; \\theta)$，行列分别是状态$s$和目标$g$；把用矩阵表示动作值函数$Q(s, a; \\theta)$变成三维Tensor表示$Q(s, a, g ; \\theta)$，行列不变，增加维度-深度表示目标$g$。其中满足：</p>\n<script type=\"math/tex; mode=display\">\nV(s, g ; \\theta) \\approx V_{q}^{*}(s) \\ , \\ Q(s, a, g ; \\theta) \\approx Q_{a}^{*}(s, a)</script><h1 id=\"文中正文\"><a href=\"#文中正文\" class=\"headerlink\" title=\"文中正文\"></a>文中正文</h1><p>该方法拟解决的问题：</p>\n<ul>\n<li>如何通用表示各种问题的值函数逼近器？</li>\n</ul>\n<p>主要思想：</p>\n<ul>\n<li>用单个函数逼近器表示多目标的最优值函数</li>\n</ul>\n<p>实现方法：</p>\n<ul>\n<li>算法的输入由状态$s$扩展为状态-目标$\\lt s,g \\gt$，假设原状态表示向量$s$不包含目标信息</li>\n</ul>\n<p>有两种实现形式：</p>\n<ul>\n<li>直接用$||$连接，即$\\left ( s||g \\right )$，然后通过非线性函数逼近器(如MLPs)得到最终输出结果$V(s, g)$</li>\n<li>分别embedding，并将嵌入后的表示通过运算得到<strong>最终输出结果</strong>(文中使用的是点积)，$h : \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\mapsto \\mathbb{R}$，然后$V(s, g) :=h(\\phi(s), \\psi(g))$</li>\n</ul>\n<p><img src=\"./universal-value-function-approximators/sg.png\" alt=\"\"></p>\n<p>左图为连结模式，中间图表示分别embedding并通过函数运算得到标量输出$h : \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\mapsto \\mathbb{R}$，右图为中间图的细化。</p>\n<p>文中指出，对于第二种形式，可以使$\\phi$网络和$\\psi$网络共享几层参数，因为一般来说，目标的向量表示形式与状态的向量表示形式相同，即$\\mathcal{G} \\subseteq \\mathcal{S}$。如果对于对称问题，即奖励函数是$s$和$g$的距离(平方差)等形式，那么有特点：</p>\n<script type=\"math/tex; mode=display\">\nV_{g}^{*}(s)=V_{s}^{*}(g) \\ , \\ \\forall s, g</script><p>这个时候可以使$\\phi$网络和$\\psi$网络相同。</p>\n<p><strong><em>注：文中提到有使用低秩因式分解分别表示$\\hat{\\phi}_{t}$和$\\hat{\\psi}_{g}$，并使用有监督学习训练两个网络对$s$和$g$的embediing($\\phi_{t}$和$\\psi_{t}$)进行训练的方法，但是这需要假设问题可进行因式分解，即有限的状态和目标，但这在实际应用中往往是不成立的，所以不对这部分做过多的关注。</em></strong></p>\n<p>优点：</p>\n<ul>\n<li>UVFA可用于同任务多目标的迁移学习中，可以比随机值初始化更快地学习解决新任务。<em>这是显而易见的作用，毕竟本身就是多目标的通用函数逼近器。</em></li>\n<li>可以用于特征表示。<em>这也说，= =，强行增加字数。。。。。。</em></li>\n<li>UVFA有效地提供了一个通用决策模型，代表（近似）朝向任何目标$g \\in \\mathcal{G}$的最佳行为。<em>这个优点与现在提出的元强化学习中的slow部分如出一辙。</em></li>\n</ul>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>UVFA把$V(s)$变成$V(s,g)$，一个可表示<strong>单任务、多目标</strong>的通用值函数近似，扩展对任务的知识表达。(通用即知识，哈哈)</p>\n","site":{"data":{}},"excerpt":"<p>本文中的方法简称UVFA，即通用值函数逼近器，主要是用于将只能表示同一任务单目标的值函数表示成通用的多目标值函数。很多论文如HER都引用了这篇论文中提出的方法。</p>\n<p>推荐程度中等：</p>\n<ul>\n<li>文中理论说明很多，很晦涩，可以不看，直接跳至正文部分即可</li>\n<li>思想简单，了解一下即可</li>\n</ul>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"http://proceedings.mlr.press/v37/schaul15.pdf\" rel=\"external nofollow\" target=\"_blank\">http://proceedings.mlr.press/v37/schaul15.pdf</a></p>\n<blockquote>\n<p>Our main idea is to represent a large set of optimal value functions by a single, unified function approximator that generalises over both states and goals.</p>\n</blockquote>\n<p>主要思想是通过一个统一的函数逼近器来表示一大组最优值函数，该函数逼近器可以概括状态和目标。</p>\n<p>单目标中值函数这么表示：</p>\n<script type=\"math/tex; mode=display\">\nV_{g, \\pi}(s) :=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} R_{g}\\left(s_{t+1}, a_{t}, s_{t}\\right) \\prod_{k=0}^{t} \\gamma_{g}\\left(s_{k}\\right) | s_{0}=s\\right]</script><p>动作值函数这么表示：</p>\n<script type=\"math/tex; mode=display\">\nQ_{g, \\pi}(s, a) :=\\mathbb{E}_{s^{\\prime}}\\left[R_{g}\\left(s, a, s^{\\prime}\\right)+\\gamma_{g}\\left(s^{\\prime}\\right) \\cdot V_{g, \\pi}\\left(s^{\\prime}\\right)\\right]</script><p>最优策略：</p>\n<script type=\"math/tex; mode=display\">\n\\pi_{g}^{*}(s) :=\\arg \\max _{a} Q_{\\pi, g}(s, a)</script><p>相应的最优值函数：</p>\n<script type=\"math/tex; mode=display\">\nV_{g}^{*} :=V_{g, \\pi_{g}^{*}} \\ , \\ Q_{g}^{*} :=Q_{g, \\pi_{g}^{*}}</script><p>本文中是想将一个单目标的最优值函数逼近改造成多目标的最优值函数表示，形象一点来说，就是把用向量表示状态值函数$V(s; \\theta)$变成矩阵表示$V(s, g ; \\theta)$，行列分别是状态$s$和目标$g$；把用矩阵表示动作值函数$Q(s, a; \\theta)$变成三维Tensor表示$Q(s, a, g ; \\theta)$，行列不变，增加维度-深度表示目标$g$。其中满足：</p>\n<script type=\"math/tex; mode=display\">\nV(s, g ; \\theta) \\approx V_{q}^{*}(s) \\ , \\ Q(s, a, g ; \\theta) \\approx Q_{a}^{*}(s, a)</script><h1 id=\"文中正文\"><a href=\"#文中正文\" class=\"headerlink\" title=\"文中正文\"></a>文中正文</h1><p>该方法拟解决的问题：</p>\n<ul>\n<li>如何通用表示各种问题的值函数逼近器？</li>\n</ul>\n<p>主要思想：</p>\n<ul>\n<li>用单个函数逼近器表示多目标的最优值函数</li>\n</ul>\n<p>实现方法：</p>\n<ul>\n<li>算法的输入由状态$s$扩展为状态-目标$\\lt s,g \\gt$，假设原状态表示向量$s$不包含目标信息</li>\n</ul>\n<p>有两种实现形式：</p>\n<ul>\n<li>直接用$||$连接，即$\\left ( s||g \\right )$，然后通过非线性函数逼近器(如MLPs)得到最终输出结果$V(s, g)$</li>\n<li>分别embedding，并将嵌入后的表示通过运算得到<strong>最终输出结果</strong>(文中使用的是点积)，$h : \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\mapsto \\mathbb{R}$，然后$V(s, g) :=h(\\phi(s), \\psi(g))$</li>\n</ul>\n<p><img src=\"./universal-value-function-approximators/sg.png\" alt=\"\"></p>\n<p>左图为连结模式，中间图表示分别embedding并通过函数运算得到标量输出$h : \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\mapsto \\mathbb{R}$，右图为中间图的细化。</p>\n<p>文中指出，对于第二种形式，可以使$\\phi$网络和$\\psi$网络共享几层参数，因为一般来说，目标的向量表示形式与状态的向量表示形式相同，即$\\mathcal{G} \\subseteq \\mathcal{S}$。如果对于对称问题，即奖励函数是$s$和$g$的距离(平方差)等形式，那么有特点：</p>\n<script type=\"math/tex; mode=display\">\nV_{g}^{*}(s)=V_{s}^{*}(g) \\ , \\ \\forall s, g</script><p>这个时候可以使$\\phi$网络和$\\psi$网络相同。</p>\n<p><strong><em>注：文中提到有使用低秩因式分解分别表示$\\hat{\\phi}_{t}$和$\\hat{\\psi}_{g}$，并使用有监督学习训练两个网络对$s$和$g$的embediing($\\phi_{t}$和$\\psi_{t}$)进行训练的方法，但是这需要假设问题可进行因式分解，即有限的状态和目标，但这在实际应用中往往是不成立的，所以不对这部分做过多的关注。</em></strong></p>\n<p>优点：</p>\n<ul>\n<li>UVFA可用于同任务多目标的迁移学习中，可以比随机值初始化更快地学习解决新任务。<em>这是显而易见的作用，毕竟本身就是多目标的通用函数逼近器。</em></li>\n<li>可以用于特征表示。<em>这也说，= =，强行增加字数。。。。。。</em></li>\n<li>UVFA有效地提供了一个通用决策模型，代表（近似）朝向任何目标$g \\in \\mathcal{G}$的最佳行为。<em>这个优点与现在提出的元强化学习中的slow部分如出一辙。</em></li>\n</ul>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>UVFA把$V(s)$变成$V(s,g)$，一个可表示<strong>单任务、多目标</strong>的通用值函数近似，扩展对任务的知识表达。(通用即知识，哈哈)</p>"},{"title":"为远程Ubuntu服务器安装图像界面","copyright":true,"top":1,"date":"2019-01-09T06:25:43.000Z","_content":"\n# 为远程服务器Ubuntu系统安装图形界面\n\n<!--more-->\n\n## 资源\n\n- [X2GO](https://wiki.x2go.org/doku.php/doc:installation:x2goclient)\n\n## 在服务器上安装X2go服务器\n\n1. 安装这个`add-apt-repository`命令\n\n\t- `apt-get install -y python-software-properties software-properties-common`\n\n2. 添加PPA\n\n\t- `apt-add-repository -y ppa:x2go/stable`\n\t\n3. 更新包列表并安装`x2go`服务器端\n\t- `apt-get update`\n\t- `apt-get install x2goserver x2goserver-xsession`\n\n### 安装XFCE图像界面\n\n在安装`XFCE`桌面环境时，有可能会出错，原因是perl为系统使用`zh_CN.UTF-8`，但系统不知道`zh_CN.UTF-8`是什么东西，所以需要安装一个中文语言，系统就知道`zh_CN.UTF-8`了，这个时候perl就不会报错了\n​\t- `apt-get install language-pack-zh-hans `\n​\t- `apt-get install xfce4`\n\n### 安装GNOME图像界面\n\n`apt-get install -y gnome`\n*没有测试成功,似乎是不兼容的问题*\n\n### 安装MATE图像界面\n\n`apt-get install -y mate`\n![](./为远程Ubuntu服务器安装图像界面/9.png)\n\n### 安装LXDE图像界面\n\n`apt-get install -y xorg lxde`\n![](./为远程Ubuntu服务器安装图像界面/10.png)\n\n### **重要配置**\n开启远程连接时有可能会出现`mesg: ttyname failed: Inappropriate ioctl for device`错误，所以需要修改一下文件\n​\t- `nano /root/.profile`\n​\t- 把`mesg n` 替换成 `tty -s && mesg n`\n\n## 在客户端上安装X2go客户端\n\n### MAC\n\n1. 安装`Xquartz` [XQuartz](https://www.xquartz.org/)\n\n![](./为远程Ubuntu服务器安装图像界面/1.png)\n\n2. 输入命令\n\n`echo \"*VT100.translations: #override Meta <KeyPress> V: insert-selection(PRIMARY, CUT_BUFFER0) \\n\" > ~/.Xdefaults `\n\n3. 安装`X2Go Client` [X2Go Client](https://code.x2go.org/releases/binary-macosx/x2goclient/)\n\n4. 打开客户端，设置连接\n\n![](./为远程Ubuntu服务器安装图像界面/2.png)\n\n5. 检查客户端设置，确保X11被正确引导\n\n![](./为远程Ubuntu服务器安装图像界面/3.png)\n\n6. 开始连接\n\n![](./为远程Ubuntu服务器安装图像界面/4.png)\n\n7. 连接成功\n\n![](./为远程Ubuntu服务器安装图像界面/5.png)\n\n### Windows\n\n1. 安装`x2goclient` [x2goclient](https://code.x2go.org/releases/binary-win32/x2goclient/releases/4.1.2.0-2018.06.22/)\n\n![](./为远程Ubuntu服务器安装图像界面/6.png)\n\n2. 配置好之后连接成功\n\n![](./为远程Ubuntu服务器安装图像界面/7.png)\n\n## Linux下各种图像界面测评\n\n![](./为远程Ubuntu服务器安装图像界面/8.png)","source":"_posts/为远程Ubuntu服务器安装图像界面.md","raw":"---\ntitle: 为远程Ubuntu服务器安装图像界面\ncopyright: true\ntop: 1\ndate: 2019-01-09 14:25:43\ncategories: Ubuntu\ntags: \n- ubuntu\n- x2go\n\n---\n\n# 为远程服务器Ubuntu系统安装图形界面\n\n<!--more-->\n\n## 资源\n\n- [X2GO](https://wiki.x2go.org/doku.php/doc:installation:x2goclient)\n\n## 在服务器上安装X2go服务器\n\n1. 安装这个`add-apt-repository`命令\n\n\t- `apt-get install -y python-software-properties software-properties-common`\n\n2. 添加PPA\n\n\t- `apt-add-repository -y ppa:x2go/stable`\n\t\n3. 更新包列表并安装`x2go`服务器端\n\t- `apt-get update`\n\t- `apt-get install x2goserver x2goserver-xsession`\n\n### 安装XFCE图像界面\n\n在安装`XFCE`桌面环境时，有可能会出错，原因是perl为系统使用`zh_CN.UTF-8`，但系统不知道`zh_CN.UTF-8`是什么东西，所以需要安装一个中文语言，系统就知道`zh_CN.UTF-8`了，这个时候perl就不会报错了\n​\t- `apt-get install language-pack-zh-hans `\n​\t- `apt-get install xfce4`\n\n### 安装GNOME图像界面\n\n`apt-get install -y gnome`\n*没有测试成功,似乎是不兼容的问题*\n\n### 安装MATE图像界面\n\n`apt-get install -y mate`\n![](./为远程Ubuntu服务器安装图像界面/9.png)\n\n### 安装LXDE图像界面\n\n`apt-get install -y xorg lxde`\n![](./为远程Ubuntu服务器安装图像界面/10.png)\n\n### **重要配置**\n开启远程连接时有可能会出现`mesg: ttyname failed: Inappropriate ioctl for device`错误，所以需要修改一下文件\n​\t- `nano /root/.profile`\n​\t- 把`mesg n` 替换成 `tty -s && mesg n`\n\n## 在客户端上安装X2go客户端\n\n### MAC\n\n1. 安装`Xquartz` [XQuartz](https://www.xquartz.org/)\n\n![](./为远程Ubuntu服务器安装图像界面/1.png)\n\n2. 输入命令\n\n`echo \"*VT100.translations: #override Meta <KeyPress> V: insert-selection(PRIMARY, CUT_BUFFER0) \\n\" > ~/.Xdefaults `\n\n3. 安装`X2Go Client` [X2Go Client](https://code.x2go.org/releases/binary-macosx/x2goclient/)\n\n4. 打开客户端，设置连接\n\n![](./为远程Ubuntu服务器安装图像界面/2.png)\n\n5. 检查客户端设置，确保X11被正确引导\n\n![](./为远程Ubuntu服务器安装图像界面/3.png)\n\n6. 开始连接\n\n![](./为远程Ubuntu服务器安装图像界面/4.png)\n\n7. 连接成功\n\n![](./为远程Ubuntu服务器安装图像界面/5.png)\n\n### Windows\n\n1. 安装`x2goclient` [x2goclient](https://code.x2go.org/releases/binary-win32/x2goclient/releases/4.1.2.0-2018.06.22/)\n\n![](./为远程Ubuntu服务器安装图像界面/6.png)\n\n2. 配置好之后连接成功\n\n![](./为远程Ubuntu服务器安装图像界面/7.png)\n\n## Linux下各种图像界面测评\n\n![](./为远程Ubuntu服务器安装图像界面/8.png)","slug":"为远程Ubuntu服务器安装图像界面","published":1,"updated":"2019-05-10T01:24:58.142Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9tr000sekve87bc9g04","content":"<h1 id=\"为远程服务器Ubuntu系统安装图形界面\"><a href=\"#为远程服务器Ubuntu系统安装图形界面\" class=\"headerlink\" title=\"为远程服务器Ubuntu系统安装图形界面\"></a>为远程服务器Ubuntu系统安装图形界面</h1><a id=\"more\"></a>\n<h2 id=\"资源\"><a href=\"#资源\" class=\"headerlink\" title=\"资源\"></a>资源</h2><ul>\n<li><a href=\"https://wiki.x2go.org/doku.php/doc:installation:x2goclient\" rel=\"external nofollow\" target=\"_blank\">X2GO</a></li>\n</ul>\n<h2 id=\"在服务器上安装X2go服务器\"><a href=\"#在服务器上安装X2go服务器\" class=\"headerlink\" title=\"在服务器上安装X2go服务器\"></a>在服务器上安装X2go服务器</h2><ol>\n<li><p>安装这个<code>add-apt-repository</code>命令</p>\n<ul>\n<li><code>apt-get install -y python-software-properties software-properties-common</code></li>\n</ul>\n</li>\n<li><p>添加PPA</p>\n<ul>\n<li><code>apt-add-repository -y ppa:x2go/stable</code></li>\n</ul>\n</li>\n<li><p>更新包列表并安装<code>x2go</code>服务器端</p>\n<ul>\n<li><code>apt-get update</code></li>\n<li><code>apt-get install x2goserver x2goserver-xsession</code></li>\n</ul>\n</li>\n</ol>\n<h3 id=\"安装XFCE图像界面\"><a href=\"#安装XFCE图像界面\" class=\"headerlink\" title=\"安装XFCE图像界面\"></a>安装XFCE图像界面</h3><p>在安装<code>XFCE</code>桌面环境时，有可能会出错，原因是perl为系统使用<code>zh_CN.UTF-8</code>，但系统不知道<code>zh_CN.UTF-8</code>是什么东西，所以需要安装一个中文语言，系统就知道<code>zh_CN.UTF-8</code>了，这个时候perl就不会报错了<br>​    - <code>apt-get install language-pack-zh-hans</code><br>​    - <code>apt-get install xfce4</code></p>\n<h3 id=\"安装GNOME图像界面\"><a href=\"#安装GNOME图像界面\" class=\"headerlink\" title=\"安装GNOME图像界面\"></a>安装GNOME图像界面</h3><p><code>apt-get install -y gnome</code><br><em>没有测试成功,似乎是不兼容的问题</em></p>\n<h3 id=\"安装MATE图像界面\"><a href=\"#安装MATE图像界面\" class=\"headerlink\" title=\"安装MATE图像界面\"></a>安装MATE图像界面</h3><p><code>apt-get install -y mate</code><br><img src=\"./为远程Ubuntu服务器安装图像界面/9.png\" alt=\"\"></p>\n<h3 id=\"安装LXDE图像界面\"><a href=\"#安装LXDE图像界面\" class=\"headerlink\" title=\"安装LXDE图像界面\"></a>安装LXDE图像界面</h3><p><code>apt-get install -y xorg lxde</code><br><img src=\"./为远程Ubuntu服务器安装图像界面/10.png\" alt=\"\"></p>\n<h3 id=\"重要配置\"><a href=\"#重要配置\" class=\"headerlink\" title=\"重要配置\"></a><strong>重要配置</strong></h3><p>开启远程连接时有可能会出现<code>mesg: ttyname failed: Inappropriate ioctl for device</code>错误，所以需要修改一下文件<br>​    - <code>nano /root/.profile</code><br>​    - 把<code>mesg n</code> 替换成 <code>tty -s &amp;&amp; mesg n</code></p>\n<h2 id=\"在客户端上安装X2go客户端\"><a href=\"#在客户端上安装X2go客户端\" class=\"headerlink\" title=\"在客户端上安装X2go客户端\"></a>在客户端上安装X2go客户端</h2><h3 id=\"MAC\"><a href=\"#MAC\" class=\"headerlink\" title=\"MAC\"></a>MAC</h3><ol>\n<li>安装<code>Xquartz</code> <a href=\"https://www.xquartz.org/\" rel=\"external nofollow\" target=\"_blank\">XQuartz</a></li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/1.png\" alt=\"\"></p>\n<ol>\n<li>输入命令</li>\n</ol>\n<p><code>echo &quot;*VT100.translations: #override Meta &lt;KeyPress&gt; V: insert-selection(PRIMARY, CUT_BUFFER0) \\n&quot; &gt; ~/.Xdefaults</code></p>\n<ol>\n<li><p>安装<code>X2Go Client</code> <a href=\"https://code.x2go.org/releases/binary-macosx/x2goclient/\" rel=\"external nofollow\" target=\"_blank\">X2Go Client</a></p>\n</li>\n<li><p>打开客户端，设置连接</p>\n</li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/2.png\" alt=\"\"></p>\n<ol>\n<li>检查客户端设置，确保X11被正确引导</li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/3.png\" alt=\"\"></p>\n<ol>\n<li>开始连接</li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/4.png\" alt=\"\"></p>\n<ol>\n<li>连接成功</li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/5.png\" alt=\"\"></p>\n<h3 id=\"Windows\"><a href=\"#Windows\" class=\"headerlink\" title=\"Windows\"></a>Windows</h3><ol>\n<li>安装<code>x2goclient</code> <a href=\"https://code.x2go.org/releases/binary-win32/x2goclient/releases/4.1.2.0-2018.06.22/\" rel=\"external nofollow\" target=\"_blank\">x2goclient</a></li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/6.png\" alt=\"\"></p>\n<ol>\n<li>配置好之后连接成功</li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/7.png\" alt=\"\"></p>\n<h2 id=\"Linux下各种图像界面测评\"><a href=\"#Linux下各种图像界面测评\" class=\"headerlink\" title=\"Linux下各种图像界面测评\"></a>Linux下各种图像界面测评</h2><p><img src=\"./为远程Ubuntu服务器安装图像界面/8.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"为远程服务器Ubuntu系统安装图形界面\"><a href=\"#为远程服务器Ubuntu系统安装图形界面\" class=\"headerlink\" title=\"为远程服务器Ubuntu系统安装图形界面\"></a>为远程服务器Ubuntu系统安装图形界面</h1>","more":"<h2 id=\"资源\"><a href=\"#资源\" class=\"headerlink\" title=\"资源\"></a>资源</h2><ul>\n<li><a href=\"https://wiki.x2go.org/doku.php/doc:installation:x2goclient\" rel=\"external nofollow\" target=\"_blank\">X2GO</a></li>\n</ul>\n<h2 id=\"在服务器上安装X2go服务器\"><a href=\"#在服务器上安装X2go服务器\" class=\"headerlink\" title=\"在服务器上安装X2go服务器\"></a>在服务器上安装X2go服务器</h2><ol>\n<li><p>安装这个<code>add-apt-repository</code>命令</p>\n<ul>\n<li><code>apt-get install -y python-software-properties software-properties-common</code></li>\n</ul>\n</li>\n<li><p>添加PPA</p>\n<ul>\n<li><code>apt-add-repository -y ppa:x2go/stable</code></li>\n</ul>\n</li>\n<li><p>更新包列表并安装<code>x2go</code>服务器端</p>\n<ul>\n<li><code>apt-get update</code></li>\n<li><code>apt-get install x2goserver x2goserver-xsession</code></li>\n</ul>\n</li>\n</ol>\n<h3 id=\"安装XFCE图像界面\"><a href=\"#安装XFCE图像界面\" class=\"headerlink\" title=\"安装XFCE图像界面\"></a>安装XFCE图像界面</h3><p>在安装<code>XFCE</code>桌面环境时，有可能会出错，原因是perl为系统使用<code>zh_CN.UTF-8</code>，但系统不知道<code>zh_CN.UTF-8</code>是什么东西，所以需要安装一个中文语言，系统就知道<code>zh_CN.UTF-8</code>了，这个时候perl就不会报错了<br>​    - <code>apt-get install language-pack-zh-hans</code><br>​    - <code>apt-get install xfce4</code></p>\n<h3 id=\"安装GNOME图像界面\"><a href=\"#安装GNOME图像界面\" class=\"headerlink\" title=\"安装GNOME图像界面\"></a>安装GNOME图像界面</h3><p><code>apt-get install -y gnome</code><br><em>没有测试成功,似乎是不兼容的问题</em></p>\n<h3 id=\"安装MATE图像界面\"><a href=\"#安装MATE图像界面\" class=\"headerlink\" title=\"安装MATE图像界面\"></a>安装MATE图像界面</h3><p><code>apt-get install -y mate</code><br><img src=\"./为远程Ubuntu服务器安装图像界面/9.png\" alt=\"\"></p>\n<h3 id=\"安装LXDE图像界面\"><a href=\"#安装LXDE图像界面\" class=\"headerlink\" title=\"安装LXDE图像界面\"></a>安装LXDE图像界面</h3><p><code>apt-get install -y xorg lxde</code><br><img src=\"./为远程Ubuntu服务器安装图像界面/10.png\" alt=\"\"></p>\n<h3 id=\"重要配置\"><a href=\"#重要配置\" class=\"headerlink\" title=\"重要配置\"></a><strong>重要配置</strong></h3><p>开启远程连接时有可能会出现<code>mesg: ttyname failed: Inappropriate ioctl for device</code>错误，所以需要修改一下文件<br>​    - <code>nano /root/.profile</code><br>​    - 把<code>mesg n</code> 替换成 <code>tty -s &amp;&amp; mesg n</code></p>\n<h2 id=\"在客户端上安装X2go客户端\"><a href=\"#在客户端上安装X2go客户端\" class=\"headerlink\" title=\"在客户端上安装X2go客户端\"></a>在客户端上安装X2go客户端</h2><h3 id=\"MAC\"><a href=\"#MAC\" class=\"headerlink\" title=\"MAC\"></a>MAC</h3><ol>\n<li>安装<code>Xquartz</code> <a href=\"https://www.xquartz.org/\" rel=\"external nofollow\" target=\"_blank\">XQuartz</a></li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/1.png\" alt=\"\"></p>\n<ol>\n<li>输入命令</li>\n</ol>\n<p><code>echo &quot;*VT100.translations: #override Meta &lt;KeyPress&gt; V: insert-selection(PRIMARY, CUT_BUFFER0) \\n&quot; &gt; ~/.Xdefaults</code></p>\n<ol>\n<li><p>安装<code>X2Go Client</code> <a href=\"https://code.x2go.org/releases/binary-macosx/x2goclient/\" rel=\"external nofollow\" target=\"_blank\">X2Go Client</a></p>\n</li>\n<li><p>打开客户端，设置连接</p>\n</li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/2.png\" alt=\"\"></p>\n<ol>\n<li>检查客户端设置，确保X11被正确引导</li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/3.png\" alt=\"\"></p>\n<ol>\n<li>开始连接</li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/4.png\" alt=\"\"></p>\n<ol>\n<li>连接成功</li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/5.png\" alt=\"\"></p>\n<h3 id=\"Windows\"><a href=\"#Windows\" class=\"headerlink\" title=\"Windows\"></a>Windows</h3><ol>\n<li>安装<code>x2goclient</code> <a href=\"https://code.x2go.org/releases/binary-win32/x2goclient/releases/4.1.2.0-2018.06.22/\" rel=\"external nofollow\" target=\"_blank\">x2goclient</a></li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/6.png\" alt=\"\"></p>\n<ol>\n<li>配置好之后连接成功</li>\n</ol>\n<p><img src=\"./为远程Ubuntu服务器安装图像界面/7.png\" alt=\"\"></p>\n<h2 id=\"Linux下各种图像界面测评\"><a href=\"#Linux下各种图像界面测评\" class=\"headerlink\" title=\"Linux下各种图像界面测评\"></a>Linux下各种图像界面测评</h2><p><img src=\"./为远程Ubuntu服务器安装图像界面/8.png\" alt=\"\"></p>"},{"title":"Ubuntu16.04配置Shadowsocks服务器","copyright":true,"mathjax":false,"top":1,"date":"2019-06-01T14:03:10.000Z","password":"ss","keywords":null,"description":null,"_content":"\n科学上网，你懂的。\n\n<!--more-->\n\n# 购买VPS\n\n[VULTR](https://www.vultr.com/)\n\n纽约节点，3.5$一个月，ubuntu 16.04，enable IPV6\n\n更改root密码\n\n```\nsudo passwd\n```\n\n# shadowsocks 服务器\n\n更新软件源\n\n```\nsudo apt-get update\n```\n\n安装PIP\n\n```\nsudo apt-get install python-pip\nsudo apt-get install python3-pip\n```\n\n安装shadowsocks\n\n```\npip3 install https://github.com/shadowsocks/shadowsocks/archive/master.zip\n```\n\n查看shadowsocks版本，显示\"Shadowsocks 3.0.0\"\n\n```\nsudo ssserver --version\n```\n\n创建配置文件夹及文件\n\n```\nsudo mkdir /etc/shadowsocks\nsudo nano /etc/shadowsocks/config.json\n```\n\n复制并修改配置内容，然后`ctrl+x`，`y`，回车，\n\n```\n{\n    \"server\":\"::\",\n    \"port_password\": {\n        \"端口1\": \"密码1\",\n        \"端口2\": \"密码2\"\n    },\n    \"timeout\":300,\n    \"method\":\"rc4-md5\",\n    \"fast_open\": false\n}\n```\n\n赋予权限\n\n```\nsudo chmod 755 /etc/shadowsocks/config.json\n```\n\n为了支持这些加密方式，也许需要安装\n\n```\nsudo apt-get install python-dev\nsudo apt-get install python–m2crypto\n```\n\n服务端后台启停\n\n```\nsudo ssserver -c /etc/shadowsocks/config.json -d start\nsudo ssserver -c /etc/shadowsocks/config.json -d stop\n```\n\n配置Systemd管理Shadowsocks，新建Shadowsocks管理文件\n\n```\nsudo nano /etc/systemd/system/shadowsocks-server.service\n```\n\n复制粘贴，`ctrl+x`，`y`，回车\n\n```\n[Unit]\nDescription=Shadowsocks Server\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/ssserver -c /etc/shadowsocks/config.json\nRestart=on-abort\n\n[Install]\nWantedBy=multi-user.target\n```\n\n启动Shadowsocks\n\n```\nsudo systemctl start shadowsocks-server\n```\n\n设置开机自启动Shadowsocks\n\n```\nsudo systemctl enable shadowsocks-server\n```\n\n查看运行状态\n\n```\nsudo systemctl status shadowsocks-server\n```\n\n## 优化\n\n查看linux内核\n\n```\nuname -r\n如果其显示版本在4.9.0之下，则需要升级Linux内核，否则请忽略下文\n\nsudo apt update\n查看可用的Linux内核版本\nsudo apt-cache showpkg linux-image\n找到一个你想要升级的Linux内核版本，如“linux-image-4.10.0-22-generic”\nsudo apt install linux-image-4.10.0-22-generic\n重启\nsudo reboot\n删除旧的内核\nsudo purge-old-kernels\n```\n\n开启BBR\n\n```\nmodprobe tcp_bbr\necho \"tcp_bbr\" >> /etc/modules-load.d/modules.conf\necho \"net.core.default_qdisc=fq\" >> /etc/sysctl.conf\necho \"net.ipv4.tcp_congestion_control=bbr\" >> /etc/sysctl.conf\nsysctl -p\n```\n\n运行下两句，均有\"bbr\"则开启BBR成功\n\n```\nsysctl net.ipv4.tcp_available_congestion_control\nsysctl net.ipv4.tcp_congestion_control\n```\n\n优化吞吐量，新建配置文件\n\n```\nsudo nano /etc/sysctl.d/local.conf\n```\n\n复制粘贴，`ctrl+x`，`y`，回车\n\n```\n# max open files\nfs.file-max = 51200\n# max read buffer\nnet.core.rmem_max = 67108864\n# max write buffer\nnet.core.wmem_max = 67108864\n# default read buffer\nnet.core.rmem_default = 65536\n# default write buffer\nnet.core.wmem_default = 65536\n# max processor input queue\nnet.core.netdev_max_backlog = 4096\n# max backlog\nnet.core.somaxconn = 4096\n\n# resist SYN flood attacks\nnet.ipv4.tcp_syncookies = 1\n# reuse timewait sockets when safe\nnet.ipv4.tcp_tw_reuse = 1\n# turn off fast timewait sockets recycling\nnet.ipv4.tcp_tw_recycle = 0\n# short FIN timeout\nnet.ipv4.tcp_fin_timeout = 30\n# short keepalive time\nnet.ipv4.tcp_keepalive_time = 1200\n# outbound port range\nnet.ipv4.ip_local_port_range = 10000 65000\n# max SYN backlog\nnet.ipv4.tcp_max_syn_backlog = 4096\n# max timewait sockets held by system simultaneously\nnet.ipv4.tcp_max_tw_buckets = 5000\n# turn on TCP Fast Open on both client and server side\nnet.ipv4.tcp_fastopen = 3\n# TCP receive buffer\nnet.ipv4.tcp_rmem = 4096 87380 67108864\n# TCP write buffer\nnet.ipv4.tcp_wmem = 4096 65536 67108864\n# turn on path MTU discovery\nnet.ipv4.tcp_mtu_probing = 1\n\nnet.ipv4.tcp_congestion_control = bbr\n```\n\n运行\n\n```\nsysctl --system\n```\n\n编辑之前的shadowsocks-server.service文件\n\n```\nsudo nano /etc/systemd/system/shadowsocks-server.service\n```\n\n在`ExecStart`前插入一行\n\n```\nExecStartPre=/bin/sh -c 'ulimit -n 51200'\n```\n\n重载shadowsocks-server.service\n\n```\nsudo systemctl daemon-reload\n```\n\n重启Shadowsocks\n\n```\nsudo systemctl restart shadowsocks-server\n```\n\n开启TCP Fast Open，降低Shadowsocks服务器和客户端的延迟，将`fast_open`的值由`false`修改为`true`\n\n```\nsudo nano /etc/shadowsocks/config.json\n```\n\n重启服务\n\n```\nsudo systemctl restart shadowsocks-server\n```\n\n","source":"_posts/ss.md","raw":"---\ntitle: Ubuntu16.04配置Shadowsocks服务器\ncopyright: true\nmathjax: false\ntop: 1\ndate: 2019-06-01 22:03:10\ncategories: 小知识\ntags:\n- ubuntu\n- note\npassword: ss\nkeywords:\ndescription:\n---\n\n科学上网，你懂的。\n\n<!--more-->\n\n# 购买VPS\n\n[VULTR](https://www.vultr.com/)\n\n纽约节点，3.5$一个月，ubuntu 16.04，enable IPV6\n\n更改root密码\n\n```\nsudo passwd\n```\n\n# shadowsocks 服务器\n\n更新软件源\n\n```\nsudo apt-get update\n```\n\n安装PIP\n\n```\nsudo apt-get install python-pip\nsudo apt-get install python3-pip\n```\n\n安装shadowsocks\n\n```\npip3 install https://github.com/shadowsocks/shadowsocks/archive/master.zip\n```\n\n查看shadowsocks版本，显示\"Shadowsocks 3.0.0\"\n\n```\nsudo ssserver --version\n```\n\n创建配置文件夹及文件\n\n```\nsudo mkdir /etc/shadowsocks\nsudo nano /etc/shadowsocks/config.json\n```\n\n复制并修改配置内容，然后`ctrl+x`，`y`，回车，\n\n```\n{\n    \"server\":\"::\",\n    \"port_password\": {\n        \"端口1\": \"密码1\",\n        \"端口2\": \"密码2\"\n    },\n    \"timeout\":300,\n    \"method\":\"rc4-md5\",\n    \"fast_open\": false\n}\n```\n\n赋予权限\n\n```\nsudo chmod 755 /etc/shadowsocks/config.json\n```\n\n为了支持这些加密方式，也许需要安装\n\n```\nsudo apt-get install python-dev\nsudo apt-get install python–m2crypto\n```\n\n服务端后台启停\n\n```\nsudo ssserver -c /etc/shadowsocks/config.json -d start\nsudo ssserver -c /etc/shadowsocks/config.json -d stop\n```\n\n配置Systemd管理Shadowsocks，新建Shadowsocks管理文件\n\n```\nsudo nano /etc/systemd/system/shadowsocks-server.service\n```\n\n复制粘贴，`ctrl+x`，`y`，回车\n\n```\n[Unit]\nDescription=Shadowsocks Server\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/ssserver -c /etc/shadowsocks/config.json\nRestart=on-abort\n\n[Install]\nWantedBy=multi-user.target\n```\n\n启动Shadowsocks\n\n```\nsudo systemctl start shadowsocks-server\n```\n\n设置开机自启动Shadowsocks\n\n```\nsudo systemctl enable shadowsocks-server\n```\n\n查看运行状态\n\n```\nsudo systemctl status shadowsocks-server\n```\n\n## 优化\n\n查看linux内核\n\n```\nuname -r\n如果其显示版本在4.9.0之下，则需要升级Linux内核，否则请忽略下文\n\nsudo apt update\n查看可用的Linux内核版本\nsudo apt-cache showpkg linux-image\n找到一个你想要升级的Linux内核版本，如“linux-image-4.10.0-22-generic”\nsudo apt install linux-image-4.10.0-22-generic\n重启\nsudo reboot\n删除旧的内核\nsudo purge-old-kernels\n```\n\n开启BBR\n\n```\nmodprobe tcp_bbr\necho \"tcp_bbr\" >> /etc/modules-load.d/modules.conf\necho \"net.core.default_qdisc=fq\" >> /etc/sysctl.conf\necho \"net.ipv4.tcp_congestion_control=bbr\" >> /etc/sysctl.conf\nsysctl -p\n```\n\n运行下两句，均有\"bbr\"则开启BBR成功\n\n```\nsysctl net.ipv4.tcp_available_congestion_control\nsysctl net.ipv4.tcp_congestion_control\n```\n\n优化吞吐量，新建配置文件\n\n```\nsudo nano /etc/sysctl.d/local.conf\n```\n\n复制粘贴，`ctrl+x`，`y`，回车\n\n```\n# max open files\nfs.file-max = 51200\n# max read buffer\nnet.core.rmem_max = 67108864\n# max write buffer\nnet.core.wmem_max = 67108864\n# default read buffer\nnet.core.rmem_default = 65536\n# default write buffer\nnet.core.wmem_default = 65536\n# max processor input queue\nnet.core.netdev_max_backlog = 4096\n# max backlog\nnet.core.somaxconn = 4096\n\n# resist SYN flood attacks\nnet.ipv4.tcp_syncookies = 1\n# reuse timewait sockets when safe\nnet.ipv4.tcp_tw_reuse = 1\n# turn off fast timewait sockets recycling\nnet.ipv4.tcp_tw_recycle = 0\n# short FIN timeout\nnet.ipv4.tcp_fin_timeout = 30\n# short keepalive time\nnet.ipv4.tcp_keepalive_time = 1200\n# outbound port range\nnet.ipv4.ip_local_port_range = 10000 65000\n# max SYN backlog\nnet.ipv4.tcp_max_syn_backlog = 4096\n# max timewait sockets held by system simultaneously\nnet.ipv4.tcp_max_tw_buckets = 5000\n# turn on TCP Fast Open on both client and server side\nnet.ipv4.tcp_fastopen = 3\n# TCP receive buffer\nnet.ipv4.tcp_rmem = 4096 87380 67108864\n# TCP write buffer\nnet.ipv4.tcp_wmem = 4096 65536 67108864\n# turn on path MTU discovery\nnet.ipv4.tcp_mtu_probing = 1\n\nnet.ipv4.tcp_congestion_control = bbr\n```\n\n运行\n\n```\nsysctl --system\n```\n\n编辑之前的shadowsocks-server.service文件\n\n```\nsudo nano /etc/systemd/system/shadowsocks-server.service\n```\n\n在`ExecStart`前插入一行\n\n```\nExecStartPre=/bin/sh -c 'ulimit -n 51200'\n```\n\n重载shadowsocks-server.service\n\n```\nsudo systemctl daemon-reload\n```\n\n重启Shadowsocks\n\n```\nsudo systemctl restart shadowsocks-server\n```\n\n开启TCP Fast Open，降低Shadowsocks服务器和客户端的延迟，将`fast_open`的值由`false`修改为`true`\n\n```\nsudo nano /etc/shadowsocks/config.json\n```\n\n重启服务\n\n```\nsudo systemctl restart shadowsocks-server\n```\n\n","slug":"ss","published":1,"updated":"2019-06-02T07:40:37.581Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9tx000wekve8q49en92","content":"<p>科学上网，你懂的。</p>\n<a id=\"more\"></a>\n<h1 id=\"购买VPS\"><a href=\"#购买VPS\" class=\"headerlink\" title=\"购买VPS\"></a>购买VPS</h1><p><a href=\"https://www.vultr.com/\" rel=\"external nofollow\" target=\"_blank\">VULTR</a></p>\n<p>纽约节点，3.5$一个月，ubuntu 16.04，enable IPV6</p>\n<p>更改root密码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo passwd</span><br></pre></td></tr></table></figure>\n<h1 id=\"shadowsocks-服务器\"><a href=\"#shadowsocks-服务器\" class=\"headerlink\" title=\"shadowsocks 服务器\"></a>shadowsocks 服务器</h1><p>更新软件源</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get update</span><br></pre></td></tr></table></figure>\n<p>安装PIP</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install python-pip</span><br><span class=\"line\">sudo apt-get install python3-pip</span><br></pre></td></tr></table></figure>\n<p>安装shadowsocks</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 install https://github.com/shadowsocks/shadowsocks/archive/master.zip</span><br></pre></td></tr></table></figure>\n<p>查看shadowsocks版本，显示”Shadowsocks 3.0.0”</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ssserver --version</span><br></pre></td></tr></table></figure>\n<p>创建配置文件夹及文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo mkdir /etc/shadowsocks</span><br><span class=\"line\">sudo nano /etc/shadowsocks/config.json</span><br></pre></td></tr></table></figure>\n<p>复制并修改配置内容，然后<code>ctrl+x</code>，<code>y</code>，回车，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;server&quot;:&quot;::&quot;,</span><br><span class=\"line\">    &quot;port_password&quot;: &#123;</span><br><span class=\"line\">        &quot;端口1&quot;: &quot;密码1&quot;,</span><br><span class=\"line\">        &quot;端口2&quot;: &quot;密码2&quot;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;timeout&quot;:300,</span><br><span class=\"line\">    &quot;method&quot;:&quot;rc4-md5&quot;,</span><br><span class=\"line\">    &quot;fast_open&quot;: false</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>赋予权限</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo chmod 755 /etc/shadowsocks/config.json</span><br></pre></td></tr></table></figure>\n<p>为了支持这些加密方式，也许需要安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install python-dev</span><br><span class=\"line\">sudo apt-get install python–m2crypto</span><br></pre></td></tr></table></figure>\n<p>服务端后台启停</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ssserver -c /etc/shadowsocks/config.json -d start</span><br><span class=\"line\">sudo ssserver -c /etc/shadowsocks/config.json -d stop</span><br></pre></td></tr></table></figure>\n<p>配置Systemd管理Shadowsocks，新建Shadowsocks管理文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nano /etc/systemd/system/shadowsocks-server.service</span><br></pre></td></tr></table></figure>\n<p>复制粘贴，<code>ctrl+x</code>，<code>y</code>，回车</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Unit]</span><br><span class=\"line\">Description=Shadowsocks Server</span><br><span class=\"line\">After=network.target</span><br><span class=\"line\"></span><br><span class=\"line\">[Service]</span><br><span class=\"line\">ExecStart=/usr/local/bin/ssserver -c /etc/shadowsocks/config.json</span><br><span class=\"line\">Restart=on-abort</span><br><span class=\"line\"></span><br><span class=\"line\">[Install]</span><br><span class=\"line\">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>\n<p>启动Shadowsocks</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl start shadowsocks-server</span><br></pre></td></tr></table></figure>\n<p>设置开机自启动Shadowsocks</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl enable shadowsocks-server</span><br></pre></td></tr></table></figure>\n<p>查看运行状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl status shadowsocks-server</span><br></pre></td></tr></table></figure>\n<h2 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h2><p>查看linux内核</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">uname -r</span><br><span class=\"line\">如果其显示版本在4.9.0之下，则需要升级Linux内核，否则请忽略下文</span><br><span class=\"line\"></span><br><span class=\"line\">sudo apt update</span><br><span class=\"line\">查看可用的Linux内核版本</span><br><span class=\"line\">sudo apt-cache showpkg linux-image</span><br><span class=\"line\">找到一个你想要升级的Linux内核版本，如“linux-image-4.10.0-22-generic”</span><br><span class=\"line\">sudo apt install linux-image-4.10.0-22-generic</span><br><span class=\"line\">重启</span><br><span class=\"line\">sudo reboot</span><br><span class=\"line\">删除旧的内核</span><br><span class=\"line\">sudo purge-old-kernels</span><br></pre></td></tr></table></figure>\n<p>开启BBR</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">modprobe tcp_bbr</span><br><span class=\"line\">echo &quot;tcp_bbr&quot; &gt;&gt; /etc/modules-load.d/modules.conf</span><br><span class=\"line\">echo &quot;net.core.default_qdisc=fq&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class=\"line\">echo &quot;net.ipv4.tcp_congestion_control=bbr&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class=\"line\">sysctl -p</span><br></pre></td></tr></table></figure>\n<p>运行下两句，均有”bbr”则开启BBR成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sysctl net.ipv4.tcp_available_congestion_control</span><br><span class=\"line\">sysctl net.ipv4.tcp_congestion_control</span><br></pre></td></tr></table></figure>\n<p>优化吞吐量，新建配置文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nano /etc/sysctl.d/local.conf</span><br></pre></td></tr></table></figure>\n<p>复制粘贴，<code>ctrl+x</code>，<code>y</code>，回车</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># max open files</span><br><span class=\"line\">fs.file-max = 51200</span><br><span class=\"line\"># max read buffer</span><br><span class=\"line\">net.core.rmem_max = 67108864</span><br><span class=\"line\"># max write buffer</span><br><span class=\"line\">net.core.wmem_max = 67108864</span><br><span class=\"line\"># default read buffer</span><br><span class=\"line\">net.core.rmem_default = 65536</span><br><span class=\"line\"># default write buffer</span><br><span class=\"line\">net.core.wmem_default = 65536</span><br><span class=\"line\"># max processor input queue</span><br><span class=\"line\">net.core.netdev_max_backlog = 4096</span><br><span class=\"line\"># max backlog</span><br><span class=\"line\">net.core.somaxconn = 4096</span><br><span class=\"line\"></span><br><span class=\"line\"># resist SYN flood attacks</span><br><span class=\"line\">net.ipv4.tcp_syncookies = 1</span><br><span class=\"line\"># reuse timewait sockets when safe</span><br><span class=\"line\">net.ipv4.tcp_tw_reuse = 1</span><br><span class=\"line\"># turn off fast timewait sockets recycling</span><br><span class=\"line\">net.ipv4.tcp_tw_recycle = 0</span><br><span class=\"line\"># short FIN timeout</span><br><span class=\"line\">net.ipv4.tcp_fin_timeout = 30</span><br><span class=\"line\"># short keepalive time</span><br><span class=\"line\">net.ipv4.tcp_keepalive_time = 1200</span><br><span class=\"line\"># outbound port range</span><br><span class=\"line\">net.ipv4.ip_local_port_range = 10000 65000</span><br><span class=\"line\"># max SYN backlog</span><br><span class=\"line\">net.ipv4.tcp_max_syn_backlog = 4096</span><br><span class=\"line\"># max timewait sockets held by system simultaneously</span><br><span class=\"line\">net.ipv4.tcp_max_tw_buckets = 5000</span><br><span class=\"line\"># turn on TCP Fast Open on both client and server side</span><br><span class=\"line\">net.ipv4.tcp_fastopen = 3</span><br><span class=\"line\"># TCP receive buffer</span><br><span class=\"line\">net.ipv4.tcp_rmem = 4096 87380 67108864</span><br><span class=\"line\"># TCP write buffer</span><br><span class=\"line\">net.ipv4.tcp_wmem = 4096 65536 67108864</span><br><span class=\"line\"># turn on path MTU discovery</span><br><span class=\"line\">net.ipv4.tcp_mtu_probing = 1</span><br><span class=\"line\"></span><br><span class=\"line\">net.ipv4.tcp_congestion_control = bbr</span><br></pre></td></tr></table></figure>\n<p>运行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sysctl --system</span><br></pre></td></tr></table></figure>\n<p>编辑之前的shadowsocks-server.service文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nano /etc/systemd/system/shadowsocks-server.service</span><br></pre></td></tr></table></figure>\n<p>在<code>ExecStart</code>前插入一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ExecStartPre=/bin/sh -c &apos;ulimit -n 51200&apos;</span><br></pre></td></tr></table></figure>\n<p>重载shadowsocks-server.service</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl daemon-reload</span><br></pre></td></tr></table></figure>\n<p>重启Shadowsocks</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl restart shadowsocks-server</span><br></pre></td></tr></table></figure>\n<p>开启TCP Fast Open，降低Shadowsocks服务器和客户端的延迟，将<code>fast_open</code>的值由<code>false</code>修改为<code>true</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nano /etc/shadowsocks/config.json</span><br></pre></td></tr></table></figure>\n<p>重启服务</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl restart shadowsocks-server</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>科学上网，你懂的。</p>","more":"<h1 id=\"购买VPS\"><a href=\"#购买VPS\" class=\"headerlink\" title=\"购买VPS\"></a>购买VPS</h1><p><a href=\"https://www.vultr.com/\" rel=\"external nofollow\" target=\"_blank\">VULTR</a></p>\n<p>纽约节点，3.5$一个月，ubuntu 16.04，enable IPV6</p>\n<p>更改root密码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo passwd</span><br></pre></td></tr></table></figure>\n<h1 id=\"shadowsocks-服务器\"><a href=\"#shadowsocks-服务器\" class=\"headerlink\" title=\"shadowsocks 服务器\"></a>shadowsocks 服务器</h1><p>更新软件源</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get update</span><br></pre></td></tr></table></figure>\n<p>安装PIP</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install python-pip</span><br><span class=\"line\">sudo apt-get install python3-pip</span><br></pre></td></tr></table></figure>\n<p>安装shadowsocks</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 install https://github.com/shadowsocks/shadowsocks/archive/master.zip</span><br></pre></td></tr></table></figure>\n<p>查看shadowsocks版本，显示”Shadowsocks 3.0.0”</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ssserver --version</span><br></pre></td></tr></table></figure>\n<p>创建配置文件夹及文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo mkdir /etc/shadowsocks</span><br><span class=\"line\">sudo nano /etc/shadowsocks/config.json</span><br></pre></td></tr></table></figure>\n<p>复制并修改配置内容，然后<code>ctrl+x</code>，<code>y</code>，回车，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;server&quot;:&quot;::&quot;,</span><br><span class=\"line\">    &quot;port_password&quot;: &#123;</span><br><span class=\"line\">        &quot;端口1&quot;: &quot;密码1&quot;,</span><br><span class=\"line\">        &quot;端口2&quot;: &quot;密码2&quot;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;timeout&quot;:300,</span><br><span class=\"line\">    &quot;method&quot;:&quot;rc4-md5&quot;,</span><br><span class=\"line\">    &quot;fast_open&quot;: false</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>赋予权限</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo chmod 755 /etc/shadowsocks/config.json</span><br></pre></td></tr></table></figure>\n<p>为了支持这些加密方式，也许需要安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install python-dev</span><br><span class=\"line\">sudo apt-get install python–m2crypto</span><br></pre></td></tr></table></figure>\n<p>服务端后台启停</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ssserver -c /etc/shadowsocks/config.json -d start</span><br><span class=\"line\">sudo ssserver -c /etc/shadowsocks/config.json -d stop</span><br></pre></td></tr></table></figure>\n<p>配置Systemd管理Shadowsocks，新建Shadowsocks管理文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nano /etc/systemd/system/shadowsocks-server.service</span><br></pre></td></tr></table></figure>\n<p>复制粘贴，<code>ctrl+x</code>，<code>y</code>，回车</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Unit]</span><br><span class=\"line\">Description=Shadowsocks Server</span><br><span class=\"line\">After=network.target</span><br><span class=\"line\"></span><br><span class=\"line\">[Service]</span><br><span class=\"line\">ExecStart=/usr/local/bin/ssserver -c /etc/shadowsocks/config.json</span><br><span class=\"line\">Restart=on-abort</span><br><span class=\"line\"></span><br><span class=\"line\">[Install]</span><br><span class=\"line\">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>\n<p>启动Shadowsocks</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl start shadowsocks-server</span><br></pre></td></tr></table></figure>\n<p>设置开机自启动Shadowsocks</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl enable shadowsocks-server</span><br></pre></td></tr></table></figure>\n<p>查看运行状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl status shadowsocks-server</span><br></pre></td></tr></table></figure>\n<h2 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h2><p>查看linux内核</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">uname -r</span><br><span class=\"line\">如果其显示版本在4.9.0之下，则需要升级Linux内核，否则请忽略下文</span><br><span class=\"line\"></span><br><span class=\"line\">sudo apt update</span><br><span class=\"line\">查看可用的Linux内核版本</span><br><span class=\"line\">sudo apt-cache showpkg linux-image</span><br><span class=\"line\">找到一个你想要升级的Linux内核版本，如“linux-image-4.10.0-22-generic”</span><br><span class=\"line\">sudo apt install linux-image-4.10.0-22-generic</span><br><span class=\"line\">重启</span><br><span class=\"line\">sudo reboot</span><br><span class=\"line\">删除旧的内核</span><br><span class=\"line\">sudo purge-old-kernels</span><br></pre></td></tr></table></figure>\n<p>开启BBR</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">modprobe tcp_bbr</span><br><span class=\"line\">echo &quot;tcp_bbr&quot; &gt;&gt; /etc/modules-load.d/modules.conf</span><br><span class=\"line\">echo &quot;net.core.default_qdisc=fq&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class=\"line\">echo &quot;net.ipv4.tcp_congestion_control=bbr&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class=\"line\">sysctl -p</span><br></pre></td></tr></table></figure>\n<p>运行下两句，均有”bbr”则开启BBR成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sysctl net.ipv4.tcp_available_congestion_control</span><br><span class=\"line\">sysctl net.ipv4.tcp_congestion_control</span><br></pre></td></tr></table></figure>\n<p>优化吞吐量，新建配置文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nano /etc/sysctl.d/local.conf</span><br></pre></td></tr></table></figure>\n<p>复制粘贴，<code>ctrl+x</code>，<code>y</code>，回车</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># max open files</span><br><span class=\"line\">fs.file-max = 51200</span><br><span class=\"line\"># max read buffer</span><br><span class=\"line\">net.core.rmem_max = 67108864</span><br><span class=\"line\"># max write buffer</span><br><span class=\"line\">net.core.wmem_max = 67108864</span><br><span class=\"line\"># default read buffer</span><br><span class=\"line\">net.core.rmem_default = 65536</span><br><span class=\"line\"># default write buffer</span><br><span class=\"line\">net.core.wmem_default = 65536</span><br><span class=\"line\"># max processor input queue</span><br><span class=\"line\">net.core.netdev_max_backlog = 4096</span><br><span class=\"line\"># max backlog</span><br><span class=\"line\">net.core.somaxconn = 4096</span><br><span class=\"line\"></span><br><span class=\"line\"># resist SYN flood attacks</span><br><span class=\"line\">net.ipv4.tcp_syncookies = 1</span><br><span class=\"line\"># reuse timewait sockets when safe</span><br><span class=\"line\">net.ipv4.tcp_tw_reuse = 1</span><br><span class=\"line\"># turn off fast timewait sockets recycling</span><br><span class=\"line\">net.ipv4.tcp_tw_recycle = 0</span><br><span class=\"line\"># short FIN timeout</span><br><span class=\"line\">net.ipv4.tcp_fin_timeout = 30</span><br><span class=\"line\"># short keepalive time</span><br><span class=\"line\">net.ipv4.tcp_keepalive_time = 1200</span><br><span class=\"line\"># outbound port range</span><br><span class=\"line\">net.ipv4.ip_local_port_range = 10000 65000</span><br><span class=\"line\"># max SYN backlog</span><br><span class=\"line\">net.ipv4.tcp_max_syn_backlog = 4096</span><br><span class=\"line\"># max timewait sockets held by system simultaneously</span><br><span class=\"line\">net.ipv4.tcp_max_tw_buckets = 5000</span><br><span class=\"line\"># turn on TCP Fast Open on both client and server side</span><br><span class=\"line\">net.ipv4.tcp_fastopen = 3</span><br><span class=\"line\"># TCP receive buffer</span><br><span class=\"line\">net.ipv4.tcp_rmem = 4096 87380 67108864</span><br><span class=\"line\"># TCP write buffer</span><br><span class=\"line\">net.ipv4.tcp_wmem = 4096 65536 67108864</span><br><span class=\"line\"># turn on path MTU discovery</span><br><span class=\"line\">net.ipv4.tcp_mtu_probing = 1</span><br><span class=\"line\"></span><br><span class=\"line\">net.ipv4.tcp_congestion_control = bbr</span><br></pre></td></tr></table></figure>\n<p>运行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sysctl --system</span><br></pre></td></tr></table></figure>\n<p>编辑之前的shadowsocks-server.service文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nano /etc/systemd/system/shadowsocks-server.service</span><br></pre></td></tr></table></figure>\n<p>在<code>ExecStart</code>前插入一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ExecStartPre=/bin/sh -c &apos;ulimit -n 51200&apos;</span><br></pre></td></tr></table></figure>\n<p>重载shadowsocks-server.service</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl daemon-reload</span><br></pre></td></tr></table></figure>\n<p>重启Shadowsocks</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl restart shadowsocks-server</span><br></pre></td></tr></table></figure>\n<p>开启TCP Fast Open，降低Shadowsocks服务器和客户端的延迟，将<code>fast_open</code>的值由<code>false</code>修改为<code>true</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nano /etc/shadowsocks/config.json</span><br></pre></td></tr></table></figure>\n<p>重启服务</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl restart shadowsocks-server</span><br></pre></td></tr></table></figure>"},{"title":"强化学习","copyright":true,"top":100,"date":"2019-05-09T08:24:54.000Z","_content":"\n# 基础知识\n\n- [强化学习基本概念](./强化学习基本概念.html)\n- [强化学习的类别](./rl-classification.html)\n- [强化学习之MDP马尔科夫决策过程](./强化学习之MDP马尔科夫决策过程.html)\n- [价值与贝尔曼方程](./价值与贝尔曼方程.html)\n- [动态规划 Dynamic Programming](./dynamic-programming.html)\n- [Monte Carlo and Temporal-Difference](./mc-td.html)\n- [SARSA and Q-Learning](./sarsa-and-q-learning.html)\n\n\n\n# 论文精读\n\n- [Energy-Based Hindsight Experience Prioritization](./energy-based-hindsight-experience-prioritization.html)\n- [Maximum Entropy-Regularized Multi-Goal Reinforcement-Learning](./maximum-entropy-regularized-multi-goal-reinforcement-learning.html)\n- [Reinforcement Learning with Deep Energy-Based Policies](./rl-with-deep-energy-based-policies.html)\n\n## Open AI\n\n- [Evolution Strategies as a Scalable Alternative to Reinforcement Learning](./Evolution-Strategies-2017.html)\n- [Hindsight Experience Replay](./Hindsight-Experience-Replay.html)\n- [RL<sup>2</sup>: Fast Reinforcement Learning via Slow Reinforcement Learning](./rl2.html)\n\n## Deep Mind\n\n- [Prioritized Experience Replay](./Prioritized-Experience-Replay.html)\n- [Universal Value Function Approximators](./universal-value-function-approximators.html)\n- [Asynchronous Methods for Deep Reinforcement Learning](./asynchronous-methods-for-drl.html)\n\n# [论文浅读](./rl-rough-reading.html)\n\n- [Massively Parallel Methods for Deep Reinforcement Learning](./rl-rough-reading.html#Gorila)\n- [Model-Based Reinforcement Learning via Meta-Policy Optimization](./rl-rough-reading.html#MB-MPO)\n- [Diversity is All Your Need: Learning Skills Without a Reward Function](./rl-rough-reading.html#DIAYN)\n- [Curiosity-Driven Experience Prioritization via Density Estimation](./rl-rough-reading.html#CDP)\n\n# 相关信息\n\n- [强化学习的里程碑](./强化学习的里程碑.html)\n\n# 学习资源\n\n- [《Reinforcement Learning : An Introduction 2nd Edition》——Sutton](http://incompleteideas.net/book/RLbook2018.pdf)\n  - 基础必读\n  - 通俗易懂\n  - 数学公式很多，虽然很多不太实用，但对理解RL的精髓很有帮助\n- [Reinforcement Learning Course by David Silver, 2015](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n  - 基础必看，讲述了强化学习的基础算法\n  - 有字幕，通俗易懂\n  - 有课件Slide\n- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/)\n  - 深度强化学习\n  - 算法很多，解析也很清晰\n  - 有代码仓库，可以查看源代码，方便复现\n- [Medium : Reinforcement Learning](https://medium.com/search?q=reinforcement learning)\n  - 博文质量不错，内容涵盖基础与进阶\n  - 缺点：更新少\n- [StackOverflow : Reinforcement Learning](https://stackoverflow.com/questions/tagged/reinforcement-learning)\n  - 进阶必备\n  - 多看多交流可以加深自己的理解","source":"_posts/强化学习.md","raw":"---\ntitle: 强化学习\ncopyright: true\ntop: 100\ndate: 2019-05-09 16:24:54\ncategories: ReinforcementLearning\ntags:\n- rl\n---\n\n# 基础知识\n\n- [强化学习基本概念](./强化学习基本概念.html)\n- [强化学习的类别](./rl-classification.html)\n- [强化学习之MDP马尔科夫决策过程](./强化学习之MDP马尔科夫决策过程.html)\n- [价值与贝尔曼方程](./价值与贝尔曼方程.html)\n- [动态规划 Dynamic Programming](./dynamic-programming.html)\n- [Monte Carlo and Temporal-Difference](./mc-td.html)\n- [SARSA and Q-Learning](./sarsa-and-q-learning.html)\n\n\n\n# 论文精读\n\n- [Energy-Based Hindsight Experience Prioritization](./energy-based-hindsight-experience-prioritization.html)\n- [Maximum Entropy-Regularized Multi-Goal Reinforcement-Learning](./maximum-entropy-regularized-multi-goal-reinforcement-learning.html)\n- [Reinforcement Learning with Deep Energy-Based Policies](./rl-with-deep-energy-based-policies.html)\n\n## Open AI\n\n- [Evolution Strategies as a Scalable Alternative to Reinforcement Learning](./Evolution-Strategies-2017.html)\n- [Hindsight Experience Replay](./Hindsight-Experience-Replay.html)\n- [RL<sup>2</sup>: Fast Reinforcement Learning via Slow Reinforcement Learning](./rl2.html)\n\n## Deep Mind\n\n- [Prioritized Experience Replay](./Prioritized-Experience-Replay.html)\n- [Universal Value Function Approximators](./universal-value-function-approximators.html)\n- [Asynchronous Methods for Deep Reinforcement Learning](./asynchronous-methods-for-drl.html)\n\n# [论文浅读](./rl-rough-reading.html)\n\n- [Massively Parallel Methods for Deep Reinforcement Learning](./rl-rough-reading.html#Gorila)\n- [Model-Based Reinforcement Learning via Meta-Policy Optimization](./rl-rough-reading.html#MB-MPO)\n- [Diversity is All Your Need: Learning Skills Without a Reward Function](./rl-rough-reading.html#DIAYN)\n- [Curiosity-Driven Experience Prioritization via Density Estimation](./rl-rough-reading.html#CDP)\n\n# 相关信息\n\n- [强化学习的里程碑](./强化学习的里程碑.html)\n\n# 学习资源\n\n- [《Reinforcement Learning : An Introduction 2nd Edition》——Sutton](http://incompleteideas.net/book/RLbook2018.pdf)\n  - 基础必读\n  - 通俗易懂\n  - 数学公式很多，虽然很多不太实用，但对理解RL的精髓很有帮助\n- [Reinforcement Learning Course by David Silver, 2015](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n  - 基础必看，讲述了强化学习的基础算法\n  - 有字幕，通俗易懂\n  - 有课件Slide\n- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/)\n  - 深度强化学习\n  - 算法很多，解析也很清晰\n  - 有代码仓库，可以查看源代码，方便复现\n- [Medium : Reinforcement Learning](https://medium.com/search?q=reinforcement learning)\n  - 博文质量不错，内容涵盖基础与进阶\n  - 缺点：更新少\n- [StackOverflow : Reinforcement Learning](https://stackoverflow.com/questions/tagged/reinforcement-learning)\n  - 进阶必备\n  - 多看多交流可以加深自己的理解","slug":"强化学习","published":1,"updated":"2019-06-26T11:54:55.394Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9u4000yekvej9ip68jh","content":"<h1 id=\"基础知识\"><a href=\"#基础知识\" class=\"headerlink\" title=\"基础知识\"></a>基础知识</h1><ul>\n<li><a href=\"./强化学习基本概念.html\">强化学习基本概念</a></li>\n<li><a href=\"./rl-classification.html\">强化学习的类别</a></li>\n<li><a href=\"./强化学习之MDP马尔科夫决策过程.html\">强化学习之MDP马尔科夫决策过程</a></li>\n<li><a href=\"./价值与贝尔曼方程.html\">价值与贝尔曼方程</a></li>\n<li><a href=\"./dynamic-programming.html\">动态规划 Dynamic Programming</a></li>\n<li><a href=\"./mc-td.html\">Monte Carlo and Temporal-Difference</a></li>\n<li><a href=\"./sarsa-and-q-learning.html\">SARSA and Q-Learning</a></li>\n</ul>\n<h1 id=\"论文精读\"><a href=\"#论文精读\" class=\"headerlink\" title=\"论文精读\"></a>论文精读</h1><ul>\n<li><a href=\"./energy-based-hindsight-experience-prioritization.html\">Energy-Based Hindsight Experience Prioritization</a></li>\n<li><a href=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning.html\">Maximum Entropy-Regularized Multi-Goal Reinforcement-Learning</a></li>\n<li><a href=\"./rl-with-deep-energy-based-policies.html\">Reinforcement Learning with Deep Energy-Based Policies</a></li>\n</ul>\n<h2 id=\"Open-AI\"><a href=\"#Open-AI\" class=\"headerlink\" title=\"Open AI\"></a>Open AI</h2><ul>\n<li><a href=\"./Evolution-Strategies-2017.html\">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></li>\n<li><a href=\"./Hindsight-Experience-Replay.html\">Hindsight Experience Replay</a></li>\n<li><a href=\"./rl2.html\">RL<sup>2</sup>: Fast Reinforcement Learning via Slow Reinforcement Learning</a></li>\n</ul>\n<h2 id=\"Deep-Mind\"><a href=\"#Deep-Mind\" class=\"headerlink\" title=\"Deep Mind\"></a>Deep Mind</h2><ul>\n<li><a href=\"./Prioritized-Experience-Replay.html\">Prioritized Experience Replay</a></li>\n<li><a href=\"./universal-value-function-approximators.html\">Universal Value Function Approximators</a></li>\n<li><a href=\"./asynchronous-methods-for-drl.html\">Asynchronous Methods for Deep Reinforcement Learning</a></li>\n</ul>\n<h1 id=\"论文浅读\"><a href=\"#论文浅读\" class=\"headerlink\" title=\"论文浅读\"></a><a href=\"./rl-rough-reading.html\">论文浅读</a></h1><ul>\n<li><a href=\"./rl-rough-reading.html#Gorila\">Massively Parallel Methods for Deep Reinforcement Learning</a></li>\n<li><a href=\"./rl-rough-reading.html#MB-MPO\">Model-Based Reinforcement Learning via Meta-Policy Optimization</a></li>\n<li><a href=\"./rl-rough-reading.html#DIAYN\">Diversity is All Your Need: Learning Skills Without a Reward Function</a></li>\n<li><a href=\"./rl-rough-reading.html#CDP\">Curiosity-Driven Experience Prioritization via Density Estimation</a></li>\n</ul>\n<h1 id=\"相关信息\"><a href=\"#相关信息\" class=\"headerlink\" title=\"相关信息\"></a>相关信息</h1><ul>\n<li><a href=\"./强化学习的里程碑.html\">强化学习的里程碑</a></li>\n</ul>\n<h1 id=\"学习资源\"><a href=\"#学习资源\" class=\"headerlink\" title=\"学习资源\"></a>学习资源</h1><ul>\n<li><a href=\"http://incompleteideas.net/book/RLbook2018.pdf\" rel=\"external nofollow\" target=\"_blank\">《Reinforcement Learning : An Introduction 2nd Edition》——Sutton</a><ul>\n<li>基础必读</li>\n<li>通俗易懂</li>\n<li>数学公式很多，虽然很多不太实用，但对理解RL的精髓很有帮助</li>\n</ul>\n</li>\n<li><a href=\"https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\" rel=\"external nofollow\" target=\"_blank\">Reinforcement Learning Course by David Silver, 2015</a><ul>\n<li>基础必看，讲述了强化学习的基础算法</li>\n<li>有字幕，通俗易懂</li>\n<li>有课件Slide</li>\n</ul>\n</li>\n<li><a href=\"https://spinningup.openai.com/en/latest/\" rel=\"external nofollow\" target=\"_blank\">OpenAI Spinning Up</a><ul>\n<li>深度强化学习</li>\n<li>算法很多，解析也很清晰</li>\n<li>有代码仓库，可以查看源代码，方便复现</li>\n</ul>\n</li>\n<li><a href=\"https://medium.com/search?q=reinforcement learning\" rel=\"external nofollow\" target=\"_blank\">Medium : Reinforcement Learning</a><ul>\n<li>博文质量不错，内容涵盖基础与进阶</li>\n<li>缺点：更新少</li>\n</ul>\n</li>\n<li><a href=\"https://stackoverflow.com/questions/tagged/reinforcement-learning\" rel=\"external nofollow\" target=\"_blank\">StackOverflow : Reinforcement Learning</a><ul>\n<li>进阶必备</li>\n<li>多看多交流可以加深自己的理解</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"基础知识\"><a href=\"#基础知识\" class=\"headerlink\" title=\"基础知识\"></a>基础知识</h1><ul>\n<li><a href=\"./强化学习基本概念.html\">强化学习基本概念</a></li>\n<li><a href=\"./rl-classification.html\">强化学习的类别</a></li>\n<li><a href=\"./强化学习之MDP马尔科夫决策过程.html\">强化学习之MDP马尔科夫决策过程</a></li>\n<li><a href=\"./价值与贝尔曼方程.html\">价值与贝尔曼方程</a></li>\n<li><a href=\"./dynamic-programming.html\">动态规划 Dynamic Programming</a></li>\n<li><a href=\"./mc-td.html\">Monte Carlo and Temporal-Difference</a></li>\n<li><a href=\"./sarsa-and-q-learning.html\">SARSA and Q-Learning</a></li>\n</ul>\n<h1 id=\"论文精读\"><a href=\"#论文精读\" class=\"headerlink\" title=\"论文精读\"></a>论文精读</h1><ul>\n<li><a href=\"./energy-based-hindsight-experience-prioritization.html\">Energy-Based Hindsight Experience Prioritization</a></li>\n<li><a href=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning.html\">Maximum Entropy-Regularized Multi-Goal Reinforcement-Learning</a></li>\n<li><a href=\"./rl-with-deep-energy-based-policies.html\">Reinforcement Learning with Deep Energy-Based Policies</a></li>\n</ul>\n<h2 id=\"Open-AI\"><a href=\"#Open-AI\" class=\"headerlink\" title=\"Open AI\"></a>Open AI</h2><ul>\n<li><a href=\"./Evolution-Strategies-2017.html\">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></li>\n<li><a href=\"./Hindsight-Experience-Replay.html\">Hindsight Experience Replay</a></li>\n<li><a href=\"./rl2.html\">RL<sup>2</sup>: Fast Reinforcement Learning via Slow Reinforcement Learning</a></li>\n</ul>\n<h2 id=\"Deep-Mind\"><a href=\"#Deep-Mind\" class=\"headerlink\" title=\"Deep Mind\"></a>Deep Mind</h2><ul>\n<li><a href=\"./Prioritized-Experience-Replay.html\">Prioritized Experience Replay</a></li>\n<li><a href=\"./universal-value-function-approximators.html\">Universal Value Function Approximators</a></li>\n<li><a href=\"./asynchronous-methods-for-drl.html\">Asynchronous Methods for Deep Reinforcement Learning</a></li>\n</ul>\n<h1 id=\"论文浅读\"><a href=\"#论文浅读\" class=\"headerlink\" title=\"论文浅读\"></a><a href=\"./rl-rough-reading.html\">论文浅读</a></h1><ul>\n<li><a href=\"./rl-rough-reading.html#Gorila\">Massively Parallel Methods for Deep Reinforcement Learning</a></li>\n<li><a href=\"./rl-rough-reading.html#MB-MPO\">Model-Based Reinforcement Learning via Meta-Policy Optimization</a></li>\n<li><a href=\"./rl-rough-reading.html#DIAYN\">Diversity is All Your Need: Learning Skills Without a Reward Function</a></li>\n<li><a href=\"./rl-rough-reading.html#CDP\">Curiosity-Driven Experience Prioritization via Density Estimation</a></li>\n</ul>\n<h1 id=\"相关信息\"><a href=\"#相关信息\" class=\"headerlink\" title=\"相关信息\"></a>相关信息</h1><ul>\n<li><a href=\"./强化学习的里程碑.html\">强化学习的里程碑</a></li>\n</ul>\n<h1 id=\"学习资源\"><a href=\"#学习资源\" class=\"headerlink\" title=\"学习资源\"></a>学习资源</h1><ul>\n<li><a href=\"http://incompleteideas.net/book/RLbook2018.pdf\" rel=\"external nofollow\" target=\"_blank\">《Reinforcement Learning : An Introduction 2nd Edition》——Sutton</a><ul>\n<li>基础必读</li>\n<li>通俗易懂</li>\n<li>数学公式很多，虽然很多不太实用，但对理解RL的精髓很有帮助</li>\n</ul>\n</li>\n<li><a href=\"https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\" rel=\"external nofollow\" target=\"_blank\">Reinforcement Learning Course by David Silver, 2015</a><ul>\n<li>基础必看，讲述了强化学习的基础算法</li>\n<li>有字幕，通俗易懂</li>\n<li>有课件Slide</li>\n</ul>\n</li>\n<li><a href=\"https://spinningup.openai.com/en/latest/\" rel=\"external nofollow\" target=\"_blank\">OpenAI Spinning Up</a><ul>\n<li>深度强化学习</li>\n<li>算法很多，解析也很清晰</li>\n<li>有代码仓库，可以查看源代码，方便复现</li>\n</ul>\n</li>\n<li><a href=\"https://medium.com/search?q=reinforcement learning\" rel=\"external nofollow\" target=\"_blank\">Medium : Reinforcement Learning</a><ul>\n<li>博文质量不错，内容涵盖基础与进阶</li>\n<li>缺点：更新少</li>\n</ul>\n</li>\n<li><a href=\"https://stackoverflow.com/questions/tagged/reinforcement-learning\" rel=\"external nofollow\" target=\"_blank\">StackOverflow : Reinforcement Learning</a><ul>\n<li>进阶必备</li>\n<li>多看多交流可以加深自己的理解</li>\n</ul>\n</li>\n</ul>\n"},{"title":"Windows下右键新建.md文件教程","date":"2018-12-28T16:00:00.000Z","copyright":true,"_content":"\n原本创建.md文件需要首先打开markdown文本编辑器，如Typora，或者新建.txt文件然后修改后缀名，本文介绍了如何在Windows操作系统中添加右键创建.md文件的方法。\n\n<!--more-->\n\n# 环境\n\nwindows10操作系统\nTypora编辑器\n\n# 效果图\n\n![](./win-rightclick-create-md/1546050455.jpg)\n\n# 步骤\n\n## 1. 打开注册表\n1. `CMD+R`，打开运行对话框\n2. 输入`regedit`，打开注册表编辑器\n\n## 2. 修改注册表\n1. 在`计算机>HKEY_CLASSES_ROOT`右键查找，输入`Typora`，勾选项，取消勾选值和数据\n\n![](./win-rightclick-create-md/20181229103503.png)\n\n2. 确认运行的程序名字，我的电脑如图所示，运行文件是`Typora.exe`\n\n![](./win-rightclick-create-md/20181229103752.png)\n如果使用的是markdownpad或者其他编辑器，同理\n\n3. 在磁盘任意位置新建一个文件，后缀为`.reg`\n4. 打开编辑刚刚创建好的注册表文件，写入以下内容：\n```\nWindows Registry Editor Version 5.00\n[HKEY_CLASSES_ROOT\\.md]\n@=\"Typora.exe\"\n[HKEY_CLASSES_ROOT\\.md\\ShellNew]\n\"NullFile\"=\"\"\n[HKEY_CLASSES_ROOT\\Typora.exe]\n@=\"Markdown\"\n```\n\n`@=\"Typora.exe\"` 代表的是指定.md文件的运行程序\n`@=\"Markdown\"` 代表的是右键时默认的文件名字，这样写新建为`新建Markdown.md`文件，而且右键菜单中显示`MarkDown`\n5. 编辑好之后,另存为,设置如图所示:\n\n![](./win-rightclick-create-md/20181229105408.png)\n\n文件名可以随便设置，但是后缀必须是`.reg`文件,保存类型一定要是`文本文档(*.txt)`,编码选择`Unicode`,非常重要!!!!!\n6. 保存文件后,双击运行,修改注册表即可,现在右键即可达到预期效果,如果不行,请重启一下.\n\n## 3. 编辑新建图标(非必须)\n1. 以`Typora`为例,在注册表`Typora.exe`下点击项`DefaultIcon`,右键修改\n2. 将属性修改为想要设置的Markdown文件图标\n\n![](./win-rightclick-create-md/20181229105300.png)\n\n文档有错或转载请联系邮箱`stepneverstop@qq.com`","source":"_posts/win-rightclick-create-md.md","raw":"---\ntitle:  Windows下右键新建.md文件教程\ndate:   2018-12-29 00:00:00\ncategories: 小知识\ntags: markdown\ncopyright: true\n---\n\n原本创建.md文件需要首先打开markdown文本编辑器，如Typora，或者新建.txt文件然后修改后缀名，本文介绍了如何在Windows操作系统中添加右键创建.md文件的方法。\n\n<!--more-->\n\n# 环境\n\nwindows10操作系统\nTypora编辑器\n\n# 效果图\n\n![](./win-rightclick-create-md/1546050455.jpg)\n\n# 步骤\n\n## 1. 打开注册表\n1. `CMD+R`，打开运行对话框\n2. 输入`regedit`，打开注册表编辑器\n\n## 2. 修改注册表\n1. 在`计算机>HKEY_CLASSES_ROOT`右键查找，输入`Typora`，勾选项，取消勾选值和数据\n\n![](./win-rightclick-create-md/20181229103503.png)\n\n2. 确认运行的程序名字，我的电脑如图所示，运行文件是`Typora.exe`\n\n![](./win-rightclick-create-md/20181229103752.png)\n如果使用的是markdownpad或者其他编辑器，同理\n\n3. 在磁盘任意位置新建一个文件，后缀为`.reg`\n4. 打开编辑刚刚创建好的注册表文件，写入以下内容：\n```\nWindows Registry Editor Version 5.00\n[HKEY_CLASSES_ROOT\\.md]\n@=\"Typora.exe\"\n[HKEY_CLASSES_ROOT\\.md\\ShellNew]\n\"NullFile\"=\"\"\n[HKEY_CLASSES_ROOT\\Typora.exe]\n@=\"Markdown\"\n```\n\n`@=\"Typora.exe\"` 代表的是指定.md文件的运行程序\n`@=\"Markdown\"` 代表的是右键时默认的文件名字，这样写新建为`新建Markdown.md`文件，而且右键菜单中显示`MarkDown`\n5. 编辑好之后,另存为,设置如图所示:\n\n![](./win-rightclick-create-md/20181229105408.png)\n\n文件名可以随便设置，但是后缀必须是`.reg`文件,保存类型一定要是`文本文档(*.txt)`,编码选择`Unicode`,非常重要!!!!!\n6. 保存文件后,双击运行,修改注册表即可,现在右键即可达到预期效果,如果不行,请重启一下.\n\n## 3. 编辑新建图标(非必须)\n1. 以`Typora`为例,在注册表`Typora.exe`下点击项`DefaultIcon`,右键修改\n2. 将属性修改为想要设置的Markdown文件图标\n\n![](./win-rightclick-create-md/20181229105300.png)\n\n文档有错或转载请联系邮箱`stepneverstop@qq.com`","slug":"win-rightclick-create-md","published":1,"updated":"2019-08-26T09:00:38.710Z","_id":"cjxd6m9u60011ekve047kui9x","comments":1,"layout":"post","photos":[],"link":"","content":"<p>原本创建.md文件需要首先打开markdown文本编辑器，如Typora，或者新建.txt文件然后修改后缀名，本文介绍了如何在Windows操作系统中添加右键创建.md文件的方法。</p>\n<a id=\"more\"></a>\n<h1 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h1><p>windows10操作系统<br>Typora编辑器</p>\n<h1 id=\"效果图\"><a href=\"#效果图\" class=\"headerlink\" title=\"效果图\"></a>效果图</h1><p><img src=\"./win-rightclick-create-md/1546050455.jpg\" alt=\"\"></p>\n<h1 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h1><h2 id=\"1-打开注册表\"><a href=\"#1-打开注册表\" class=\"headerlink\" title=\"1. 打开注册表\"></a>1. 打开注册表</h2><ol>\n<li><code>CMD+R</code>，打开运行对话框</li>\n<li>输入<code>regedit</code>，打开注册表编辑器</li>\n</ol>\n<h2 id=\"2-修改注册表\"><a href=\"#2-修改注册表\" class=\"headerlink\" title=\"2. 修改注册表\"></a>2. 修改注册表</h2><ol>\n<li>在<code>计算机&gt;HKEY_CLASSES_ROOT</code>右键查找，输入<code>Typora</code>，勾选项，取消勾选值和数据</li>\n</ol>\n<p><img src=\"./win-rightclick-create-md/20181229103503.png\" alt=\"\"></p>\n<ol>\n<li>确认运行的程序名字，我的电脑如图所示，运行文件是<code>Typora.exe</code></li>\n</ol>\n<p><img src=\"./win-rightclick-create-md/20181229103752.png\" alt=\"\"><br>如果使用的是markdownpad或者其他编辑器，同理</p>\n<ol>\n<li>在磁盘任意位置新建一个文件，后缀为<code>.reg</code></li>\n<li>打开编辑刚刚创建好的注册表文件，写入以下内容：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Windows Registry Editor Version 5.00</span><br><span class=\"line\">[HKEY_CLASSES_ROOT\\.md]</span><br><span class=\"line\">@=&quot;Typora.exe&quot;</span><br><span class=\"line\">[HKEY_CLASSES_ROOT\\.md\\ShellNew]</span><br><span class=\"line\">&quot;NullFile&quot;=&quot;&quot;</span><br><span class=\"line\">[HKEY_CLASSES_ROOT\\Typora.exe]</span><br><span class=\"line\">@=&quot;Markdown&quot;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p><code>@=&quot;Typora.exe&quot;</code> 代表的是指定.md文件的运行程序<br><code>@=&quot;Markdown&quot;</code> 代表的是右键时默认的文件名字，这样写新建为<code>新建Markdown.md</code>文件，而且右键菜单中显示<code>MarkDown</code></p>\n<ol>\n<li>编辑好之后,另存为,设置如图所示:</li>\n</ol>\n<p><img src=\"./win-rightclick-create-md/20181229105408.png\" alt=\"\"></p>\n<p>文件名可以随便设置，但是后缀必须是<code>.reg</code>文件,保存类型一定要是<code>文本文档(*.txt)</code>,编码选择<code>Unicode</code>,非常重要!!!!!</p>\n<ol>\n<li>保存文件后,双击运行,修改注册表即可,现在右键即可达到预期效果,如果不行,请重启一下.</li>\n</ol>\n<h2 id=\"3-编辑新建图标-非必须\"><a href=\"#3-编辑新建图标-非必须\" class=\"headerlink\" title=\"3. 编辑新建图标(非必须)\"></a>3. 编辑新建图标(非必须)</h2><ol>\n<li>以<code>Typora</code>为例,在注册表<code>Typora.exe</code>下点击项<code>DefaultIcon</code>,右键修改</li>\n<li>将属性修改为想要设置的Markdown文件图标</li>\n</ol>\n<p><img src=\"./win-rightclick-create-md/20181229105300.png\" alt=\"\"></p>\n<p>文档有错或转载请联系邮箱<code>stepneverstop@qq.com</code></p>\n","site":{"data":{}},"excerpt":"<p>原本创建.md文件需要首先打开markdown文本编辑器，如Typora，或者新建.txt文件然后修改后缀名，本文介绍了如何在Windows操作系统中添加右键创建.md文件的方法。</p>","more":"<h1 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h1><p>windows10操作系统<br>Typora编辑器</p>\n<h1 id=\"效果图\"><a href=\"#效果图\" class=\"headerlink\" title=\"效果图\"></a>效果图</h1><p><img src=\"./win-rightclick-create-md/1546050455.jpg\" alt=\"\"></p>\n<h1 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h1><h2 id=\"1-打开注册表\"><a href=\"#1-打开注册表\" class=\"headerlink\" title=\"1. 打开注册表\"></a>1. 打开注册表</h2><ol>\n<li><code>CMD+R</code>，打开运行对话框</li>\n<li>输入<code>regedit</code>，打开注册表编辑器</li>\n</ol>\n<h2 id=\"2-修改注册表\"><a href=\"#2-修改注册表\" class=\"headerlink\" title=\"2. 修改注册表\"></a>2. 修改注册表</h2><ol>\n<li>在<code>计算机&gt;HKEY_CLASSES_ROOT</code>右键查找，输入<code>Typora</code>，勾选项，取消勾选值和数据</li>\n</ol>\n<p><img src=\"./win-rightclick-create-md/20181229103503.png\" alt=\"\"></p>\n<ol>\n<li>确认运行的程序名字，我的电脑如图所示，运行文件是<code>Typora.exe</code></li>\n</ol>\n<p><img src=\"./win-rightclick-create-md/20181229103752.png\" alt=\"\"><br>如果使用的是markdownpad或者其他编辑器，同理</p>\n<ol>\n<li>在磁盘任意位置新建一个文件，后缀为<code>.reg</code></li>\n<li>打开编辑刚刚创建好的注册表文件，写入以下内容：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Windows Registry Editor Version 5.00</span><br><span class=\"line\">[HKEY_CLASSES_ROOT\\.md]</span><br><span class=\"line\">@=&quot;Typora.exe&quot;</span><br><span class=\"line\">[HKEY_CLASSES_ROOT\\.md\\ShellNew]</span><br><span class=\"line\">&quot;NullFile&quot;=&quot;&quot;</span><br><span class=\"line\">[HKEY_CLASSES_ROOT\\Typora.exe]</span><br><span class=\"line\">@=&quot;Markdown&quot;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p><code>@=&quot;Typora.exe&quot;</code> 代表的是指定.md文件的运行程序<br><code>@=&quot;Markdown&quot;</code> 代表的是右键时默认的文件名字，这样写新建为<code>新建Markdown.md</code>文件，而且右键菜单中显示<code>MarkDown</code></p>\n<ol>\n<li>编辑好之后,另存为,设置如图所示:</li>\n</ol>\n<p><img src=\"./win-rightclick-create-md/20181229105408.png\" alt=\"\"></p>\n<p>文件名可以随便设置，但是后缀必须是<code>.reg</code>文件,保存类型一定要是<code>文本文档(*.txt)</code>,编码选择<code>Unicode</code>,非常重要!!!!!</p>\n<ol>\n<li>保存文件后,双击运行,修改注册表即可,现在右键即可达到预期效果,如果不行,请重启一下.</li>\n</ol>\n<h2 id=\"3-编辑新建图标-非必须\"><a href=\"#3-编辑新建图标-非必须\" class=\"headerlink\" title=\"3. 编辑新建图标(非必须)\"></a>3. 编辑新建图标(非必须)</h2><ol>\n<li>以<code>Typora</code>为例,在注册表<code>Typora.exe</code>下点击项<code>DefaultIcon</code>,右键修改</li>\n<li>将属性修改为想要设置的Markdown文件图标</li>\n</ol>\n<p><img src=\"./win-rightclick-create-md/20181229105300.png\" alt=\"\"></p>\n<p>文档有错或转载请联系邮箱<code>stepneverstop@qq.com</code></p>"},{"title":"RL^2|Fast Reinforcement Learning vis Slow Reinforcement Learning","copyright":true,"mathjax":true,"top":1,"date":"2019-05-31T03:26:39.000Z","keywords":null,"description":null,"_content":"\n本文引用了元学习在深度学习领域的思想，在多任务中训练一个通用模型——slow，用这个通用模型拓展到其他任务进行训练就会快很多，得到新模型——fast。本文中的模型使用RNN作为训练模型。\n\n推荐程度：中等偏下\n\n- 可以拓宽知识面，了解众家思想\n- 我个人认为，这样的元学习并没有达到让机器”学会如何学习的学习方法“的目的，即“learning to learn”\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1611.02779.pdf](https://arxiv.org/pdf/1611.02779.pdf)\n\n深度强化学习被成功运用在许多复杂行为学习当中，但是需要很多很多很多试错才能得出一个较好的模型，但是动物学习知识只需要一小会儿，就像骑自行车和骑电瓶车，人可能学会骑自行车后5分钟就可以学会骑电瓶车，但是机器却要重头学起，1小时也未必学得会、学得好，这就是因为动物具有先验知识指导，而且可以利用先验知识，机器却不可以。\n\n把先验知识融入到强化学习算法中过去已经被探索了很多次，而且也有几种不同的形式：\n\n- 自动调超参数，学习率等\n- 使用分层贝叶斯方法在动力学模型上保持后验，并根据后验应用Thompson采样\n- 许多分层强化学习的工作都提出从以前的任务中提取可重用的技能，以加快对新任务的探索\n\n本文中的RNN即充当智能体的元学习者，也充当决策者，即生成策略\n\n以往强化学习算法把学习一个策略当成要解决的问题，本文却把学习强化学习算法当成要解决的问题。很绕对吧，其实本质上就是使用RNN表示策略，加了点深度学习中元学习的思想。\n\n本文提出的方法是：\n\n>we view the learning process of the agent itself as an objective, which can be optimized using standard reinforcement learning algorithms. The objective is averaged across all possible MDPs according to a specific distribution, which reflects the prior that we would like to distill into the agent.  \n\n即，智能体的学习过程看做目标，用标准强化学习算法进行优化。然后使用RNN处理多个MDP问题，提取先验知识到智能体。\n\n*注：我个人觉得这根本不能算是真正意义上的元学习，机器还是个傻子。*\n\n# 正文\n\n训练的流程图如下：\n\n![](./rl2/meta.png)\n\n解析：\n\n- 在这种训练方式下，需要来回切换任务，即切换MDP过程\n- 每个trail代表一个MDP过程\n- MDP需要从MDPs分布$\\rho_{\\mathcal{M}} : \\mathcal{M} \\rightarrow \\mathbb{R}_{+}$中采样，至于$\\mathbb{R}_{+}$是什么，我不知道\n- 在每个trial中训练多个episode，个数用$n$表示，图示表示$n=2$\n- $h$代表RNN网络中的知识参数\n- 同一个trial中$h$可以传承，**但是**，当切换MDP问题，即进行下一个trail时，参数$h$需要重新初始化\n- 输入不是单纯的状态$s$，而是$\\left ( s_{t+1},a_{t},r_{t},d_{t} \\right )$，其中，$d_{t}$代表episode结束的标志，输出为动作$a_{t+1}$。（输入往往需要embed为$\\phi(s, a, r, d)$）\n- 训练过程的目标**不是最大化一个episode的累计奖励，而是最大化一个trial的累计奖励**\n\n深度学习中元学习有K-shot N-class问题，即N个类，每类K个样本，（本文）强化学习中元学习可以表示为N-episode M-MDPs问题，即共有M个trial，每个trial训练N个episode\n\n---\n\n策略表示：\n\n- RNN使用GRUs（Gated Recurrent Units）\n- 输入为$\\phi(s, a, r, d)$\n\n策略优化：\n\n- TRPO，原因：性能出色，不需要大量调参\n- 使用baseline减小方差\n- 考虑使用了GAE\n\n测试结果：\n\n- MDP问题：在多臂老虎机和网格MDPs任务中，与理论上合理的算法（没有指出是哪些算法）性能相当\n- POMDP问题：图像输入的导航任务中，实验表明，智能体能够有效地利用学习到的视觉信息和以往情景中获得的短期信息。实验结果视频：[https://goo.gl/rDDBpb](https://goo.gl/rDDBpb)\n\n# 个人见解\n\n我认为这样的元强化学习根本不能实现提出元学习的初衷。说到底，这种方式只是让一个“一面白纸”的模型可以学习成多种任务的通用基础模型（slow），然后在使用这个基础模型对其他任务进行训练时可以快速训练出结果（fast）。这种方式可以加速学习，但是却并不能使机器学会去学习，学会如何进行学习的方法，虽然意义上说让智能体学到了强化学习算法的过程，其实本质上是使智能体学到了多种MDP任务中的经验，寻找一个易于根据MDP任务最优化的通用模型，并把它转换为“记忆，而没有教会机器”记忆“这个过程。\n\n（WTF？RNN训练智能体+多任务切换+更改网络输入+任务级目标函数 这就是元强化学习啦？？？）\n\n![](./rl2/meta-rl.png)\n\n","source":"_posts/rl2.md","raw":"---\ntitle: RL^2|Fast Reinforcement Learning vis Slow Reinforcement Learning\ncopyright: true\nmathjax: true\ntop: 1\ndate: 2019-05-31 11:26:39\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n本文引用了元学习在深度学习领域的思想，在多任务中训练一个通用模型——slow，用这个通用模型拓展到其他任务进行训练就会快很多，得到新模型——fast。本文中的模型使用RNN作为训练模型。\n\n推荐程度：中等偏下\n\n- 可以拓宽知识面，了解众家思想\n- 我个人认为，这样的元学习并没有达到让机器”学会如何学习的学习方法“的目的，即“learning to learn”\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1611.02779.pdf](https://arxiv.org/pdf/1611.02779.pdf)\n\n深度强化学习被成功运用在许多复杂行为学习当中，但是需要很多很多很多试错才能得出一个较好的模型，但是动物学习知识只需要一小会儿，就像骑自行车和骑电瓶车，人可能学会骑自行车后5分钟就可以学会骑电瓶车，但是机器却要重头学起，1小时也未必学得会、学得好，这就是因为动物具有先验知识指导，而且可以利用先验知识，机器却不可以。\n\n把先验知识融入到强化学习算法中过去已经被探索了很多次，而且也有几种不同的形式：\n\n- 自动调超参数，学习率等\n- 使用分层贝叶斯方法在动力学模型上保持后验，并根据后验应用Thompson采样\n- 许多分层强化学习的工作都提出从以前的任务中提取可重用的技能，以加快对新任务的探索\n\n本文中的RNN即充当智能体的元学习者，也充当决策者，即生成策略\n\n以往强化学习算法把学习一个策略当成要解决的问题，本文却把学习强化学习算法当成要解决的问题。很绕对吧，其实本质上就是使用RNN表示策略，加了点深度学习中元学习的思想。\n\n本文提出的方法是：\n\n>we view the learning process of the agent itself as an objective, which can be optimized using standard reinforcement learning algorithms. The objective is averaged across all possible MDPs according to a specific distribution, which reflects the prior that we would like to distill into the agent.  \n\n即，智能体的学习过程看做目标，用标准强化学习算法进行优化。然后使用RNN处理多个MDP问题，提取先验知识到智能体。\n\n*注：我个人觉得这根本不能算是真正意义上的元学习，机器还是个傻子。*\n\n# 正文\n\n训练的流程图如下：\n\n![](./rl2/meta.png)\n\n解析：\n\n- 在这种训练方式下，需要来回切换任务，即切换MDP过程\n- 每个trail代表一个MDP过程\n- MDP需要从MDPs分布$\\rho_{\\mathcal{M}} : \\mathcal{M} \\rightarrow \\mathbb{R}_{+}$中采样，至于$\\mathbb{R}_{+}$是什么，我不知道\n- 在每个trial中训练多个episode，个数用$n$表示，图示表示$n=2$\n- $h$代表RNN网络中的知识参数\n- 同一个trial中$h$可以传承，**但是**，当切换MDP问题，即进行下一个trail时，参数$h$需要重新初始化\n- 输入不是单纯的状态$s$，而是$\\left ( s_{t+1},a_{t},r_{t},d_{t} \\right )$，其中，$d_{t}$代表episode结束的标志，输出为动作$a_{t+1}$。（输入往往需要embed为$\\phi(s, a, r, d)$）\n- 训练过程的目标**不是最大化一个episode的累计奖励，而是最大化一个trial的累计奖励**\n\n深度学习中元学习有K-shot N-class问题，即N个类，每类K个样本，（本文）强化学习中元学习可以表示为N-episode M-MDPs问题，即共有M个trial，每个trial训练N个episode\n\n---\n\n策略表示：\n\n- RNN使用GRUs（Gated Recurrent Units）\n- 输入为$\\phi(s, a, r, d)$\n\n策略优化：\n\n- TRPO，原因：性能出色，不需要大量调参\n- 使用baseline减小方差\n- 考虑使用了GAE\n\n测试结果：\n\n- MDP问题：在多臂老虎机和网格MDPs任务中，与理论上合理的算法（没有指出是哪些算法）性能相当\n- POMDP问题：图像输入的导航任务中，实验表明，智能体能够有效地利用学习到的视觉信息和以往情景中获得的短期信息。实验结果视频：[https://goo.gl/rDDBpb](https://goo.gl/rDDBpb)\n\n# 个人见解\n\n我认为这样的元强化学习根本不能实现提出元学习的初衷。说到底，这种方式只是让一个“一面白纸”的模型可以学习成多种任务的通用基础模型（slow），然后在使用这个基础模型对其他任务进行训练时可以快速训练出结果（fast）。这种方式可以加速学习，但是却并不能使机器学会去学习，学会如何进行学习的方法，虽然意义上说让智能体学到了强化学习算法的过程，其实本质上是使智能体学到了多种MDP任务中的经验，寻找一个易于根据MDP任务最优化的通用模型，并把它转换为“记忆，而没有教会机器”记忆“这个过程。\n\n（WTF？RNN训练智能体+多任务切换+更改网络输入+任务级目标函数 这就是元强化学习啦？？？）\n\n![](./rl2/meta-rl.png)\n\n","slug":"rl2","published":1,"updated":"2019-06-02T07:40:37.580Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9u90014ekvepcsrtohs","content":"<p>本文引用了元学习在深度学习领域的思想，在多任务中训练一个通用模型——slow，用这个通用模型拓展到其他任务进行训练就会快很多，得到新模型——fast。本文中的模型使用RNN作为训练模型。</p>\n<p>推荐程度：中等偏下</p>\n<ul>\n<li>可以拓宽知识面，了解众家思想</li>\n<li>我个人认为，这样的元学习并没有达到让机器”学会如何学习的学习方法“的目的，即“learning to learn”</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1611.02779.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1611.02779.pdf</a></p>\n<p>深度强化学习被成功运用在许多复杂行为学习当中，但是需要很多很多很多试错才能得出一个较好的模型，但是动物学习知识只需要一小会儿，就像骑自行车和骑电瓶车，人可能学会骑自行车后5分钟就可以学会骑电瓶车，但是机器却要重头学起，1小时也未必学得会、学得好，这就是因为动物具有先验知识指导，而且可以利用先验知识，机器却不可以。</p>\n<p>把先验知识融入到强化学习算法中过去已经被探索了很多次，而且也有几种不同的形式：</p>\n<ul>\n<li>自动调超参数，学习率等</li>\n<li>使用分层贝叶斯方法在动力学模型上保持后验，并根据后验应用Thompson采样</li>\n<li>许多分层强化学习的工作都提出从以前的任务中提取可重用的技能，以加快对新任务的探索</li>\n</ul>\n<p>本文中的RNN即充当智能体的元学习者，也充当决策者，即生成策略</p>\n<p>以往强化学习算法把学习一个策略当成要解决的问题，本文却把学习强化学习算法当成要解决的问题。很绕对吧，其实本质上就是使用RNN表示策略，加了点深度学习中元学习的思想。</p>\n<p>本文提出的方法是：</p>\n<blockquote>\n<p>we view the learning process of the agent itself as an objective, which can be optimized using standard reinforcement learning algorithms. The objective is averaged across all possible MDPs according to a specific distribution, which reflects the prior that we would like to distill into the agent.  </p>\n</blockquote>\n<p>即，智能体的学习过程看做目标，用标准强化学习算法进行优化。然后使用RNN处理多个MDP问题，提取先验知识到智能体。</p>\n<p><em>注：我个人觉得这根本不能算是真正意义上的元学习，机器还是个傻子。</em></p>\n<h1 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h1><p>训练的流程图如下：</p>\n<p><img src=\"./rl2/meta.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>在这种训练方式下，需要来回切换任务，即切换MDP过程</li>\n<li>每个trail代表一个MDP过程</li>\n<li>MDP需要从MDPs分布$\\rho_{\\mathcal{M}} : \\mathcal{M} \\rightarrow \\mathbb{R}_{+}$中采样，至于$\\mathbb{R}_{+}$是什么，我不知道</li>\n<li>在每个trial中训练多个episode，个数用$n$表示，图示表示$n=2$</li>\n<li>$h$代表RNN网络中的知识参数</li>\n<li>同一个trial中$h$可以传承，<strong>但是</strong>，当切换MDP问题，即进行下一个trail时，参数$h$需要重新初始化</li>\n<li>输入不是单纯的状态$s$，而是$\\left ( s_{t+1},a_{t},r_{t},d_{t} \\right )$，其中，$d_{t}$代表episode结束的标志，输出为动作$a_{t+1}$。（输入往往需要embed为$\\phi(s, a, r, d)$）</li>\n<li>训练过程的目标<strong>不是最大化一个episode的累计奖励，而是最大化一个trial的累计奖励</strong></li>\n</ul>\n<p>深度学习中元学习有K-shot N-class问题，即N个类，每类K个样本，（本文）强化学习中元学习可以表示为N-episode M-MDPs问题，即共有M个trial，每个trial训练N个episode</p>\n<hr>\n<p>策略表示：</p>\n<ul>\n<li>RNN使用GRUs（Gated Recurrent Units）</li>\n<li>输入为$\\phi(s, a, r, d)$</li>\n</ul>\n<p>策略优化：</p>\n<ul>\n<li>TRPO，原因：性能出色，不需要大量调参</li>\n<li>使用baseline减小方差</li>\n<li>考虑使用了GAE</li>\n</ul>\n<p>测试结果：</p>\n<ul>\n<li>MDP问题：在多臂老虎机和网格MDPs任务中，与理论上合理的算法（没有指出是哪些算法）性能相当</li>\n<li>POMDP问题：图像输入的导航任务中，实验表明，智能体能够有效地利用学习到的视觉信息和以往情景中获得的短期信息。实验结果视频：<a href=\"https://goo.gl/rDDBpb\" rel=\"external nofollow\" target=\"_blank\">https://goo.gl/rDDBpb</a></li>\n</ul>\n<h1 id=\"个人见解\"><a href=\"#个人见解\" class=\"headerlink\" title=\"个人见解\"></a>个人见解</h1><p>我认为这样的元强化学习根本不能实现提出元学习的初衷。说到底，这种方式只是让一个“一面白纸”的模型可以学习成多种任务的通用基础模型（slow），然后在使用这个基础模型对其他任务进行训练时可以快速训练出结果（fast）。这种方式可以加速学习，但是却并不能使机器学会去学习，学会如何进行学习的方法，虽然意义上说让智能体学到了强化学习算法的过程，其实本质上是使智能体学到了多种MDP任务中的经验，寻找一个易于根据MDP任务最优化的通用模型，并把它转换为“记忆，而没有教会机器”记忆“这个过程。</p>\n<p>（WTF？RNN训练智能体+多任务切换+更改网络输入+任务级目标函数 这就是元强化学习啦？？？）</p>\n<p><img src=\"./rl2/meta-rl.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"<p>本文引用了元学习在深度学习领域的思想，在多任务中训练一个通用模型——slow，用这个通用模型拓展到其他任务进行训练就会快很多，得到新模型——fast。本文中的模型使用RNN作为训练模型。</p>\n<p>推荐程度：中等偏下</p>\n<ul>\n<li>可以拓宽知识面，了解众家思想</li>\n<li>我个人认为，这样的元学习并没有达到让机器”学会如何学习的学习方法“的目的，即“learning to learn”</li>\n</ul>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1611.02779.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1611.02779.pdf</a></p>\n<p>深度强化学习被成功运用在许多复杂行为学习当中，但是需要很多很多很多试错才能得出一个较好的模型，但是动物学习知识只需要一小会儿，就像骑自行车和骑电瓶车，人可能学会骑自行车后5分钟就可以学会骑电瓶车，但是机器却要重头学起，1小时也未必学得会、学得好，这就是因为动物具有先验知识指导，而且可以利用先验知识，机器却不可以。</p>\n<p>把先验知识融入到强化学习算法中过去已经被探索了很多次，而且也有几种不同的形式：</p>\n<ul>\n<li>自动调超参数，学习率等</li>\n<li>使用分层贝叶斯方法在动力学模型上保持后验，并根据后验应用Thompson采样</li>\n<li>许多分层强化学习的工作都提出从以前的任务中提取可重用的技能，以加快对新任务的探索</li>\n</ul>\n<p>本文中的RNN即充当智能体的元学习者，也充当决策者，即生成策略</p>\n<p>以往强化学习算法把学习一个策略当成要解决的问题，本文却把学习强化学习算法当成要解决的问题。很绕对吧，其实本质上就是使用RNN表示策略，加了点深度学习中元学习的思想。</p>\n<p>本文提出的方法是：</p>\n<blockquote>\n<p>we view the learning process of the agent itself as an objective, which can be optimized using standard reinforcement learning algorithms. The objective is averaged across all possible MDPs according to a specific distribution, which reflects the prior that we would like to distill into the agent.  </p>\n</blockquote>\n<p>即，智能体的学习过程看做目标，用标准强化学习算法进行优化。然后使用RNN处理多个MDP问题，提取先验知识到智能体。</p>\n<p><em>注：我个人觉得这根本不能算是真正意义上的元学习，机器还是个傻子。</em></p>\n<h1 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h1><p>训练的流程图如下：</p>\n<p><img src=\"./rl2/meta.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>在这种训练方式下，需要来回切换任务，即切换MDP过程</li>\n<li>每个trail代表一个MDP过程</li>\n<li>MDP需要从MDPs分布$\\rho_{\\mathcal{M}} : \\mathcal{M} \\rightarrow \\mathbb{R}_{+}$中采样，至于$\\mathbb{R}_{+}$是什么，我不知道</li>\n<li>在每个trial中训练多个episode，个数用$n$表示，图示表示$n=2$</li>\n<li>$h$代表RNN网络中的知识参数</li>\n<li>同一个trial中$h$可以传承，<strong>但是</strong>，当切换MDP问题，即进行下一个trail时，参数$h$需要重新初始化</li>\n<li>输入不是单纯的状态$s$，而是$\\left ( s_{t+1},a_{t},r_{t},d_{t} \\right )$，其中，$d_{t}$代表episode结束的标志，输出为动作$a_{t+1}$。（输入往往需要embed为$\\phi(s, a, r, d)$）</li>\n<li>训练过程的目标<strong>不是最大化一个episode的累计奖励，而是最大化一个trial的累计奖励</strong></li>\n</ul>\n<p>深度学习中元学习有K-shot N-class问题，即N个类，每类K个样本，（本文）强化学习中元学习可以表示为N-episode M-MDPs问题，即共有M个trial，每个trial训练N个episode</p>\n<hr>\n<p>策略表示：</p>\n<ul>\n<li>RNN使用GRUs（Gated Recurrent Units）</li>\n<li>输入为$\\phi(s, a, r, d)$</li>\n</ul>\n<p>策略优化：</p>\n<ul>\n<li>TRPO，原因：性能出色，不需要大量调参</li>\n<li>使用baseline减小方差</li>\n<li>考虑使用了GAE</li>\n</ul>\n<p>测试结果：</p>\n<ul>\n<li>MDP问题：在多臂老虎机和网格MDPs任务中，与理论上合理的算法（没有指出是哪些算法）性能相当</li>\n<li>POMDP问题：图像输入的导航任务中，实验表明，智能体能够有效地利用学习到的视觉信息和以往情景中获得的短期信息。实验结果视频：<a href=\"https://goo.gl/rDDBpb\" rel=\"external nofollow\" target=\"_blank\">https://goo.gl/rDDBpb</a></li>\n</ul>\n<h1 id=\"个人见解\"><a href=\"#个人见解\" class=\"headerlink\" title=\"个人见解\"></a>个人见解</h1><p>我认为这样的元强化学习根本不能实现提出元学习的初衷。说到底，这种方式只是让一个“一面白纸”的模型可以学习成多种任务的通用基础模型（slow），然后在使用这个基础模型对其他任务进行训练时可以快速训练出结果（fast）。这种方式可以加速学习，但是却并不能使机器学会去学习，学会如何进行学习的方法，虽然意义上说让智能体学到了强化学习算法的过程，其实本质上是使智能体学到了多种MDP任务中的经验，寻找一个易于根据MDP任务最优化的通用模型，并把它转换为“记忆，而没有教会机器”记忆“这个过程。</p>\n<p>（WTF？RNN训练智能体+多任务切换+更改网络输入+任务级目标函数 这就是元强化学习啦？？？）</p>\n<p><img src=\"./rl2/meta-rl.png\" alt=\"\"></p>"},{"title":"强化学习的里程碑","copyright":true,"top":1,"date":"2019-05-07T00:26:39.000Z","mathjax":true,"_content":"\n# 强化学习的里程碑\n\n<!--more-->\n\n## Alpha Go\n\n> 阿尔法围棋（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能机器人，由谷歌（Google）旗下**DeepMind**公司戴密斯·哈萨比斯领衔的团队开发。其主要工作原理是“深度学习”。\n\n>> 2016年3月，阿尔法围棋与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜；\n\n>> ![](./强化学习的里程碑/LeeSedolBattleWithAlphaGo.jpeg)\n\n>> 2016年末2017年初，该程序在中国棋类网站上以“大师”（Master）为注册账号与中日韩数十位围棋高手进行快棋对决，连续60局无一败绩；\n\n>> 2017年5月，在中国乌镇围棋峰会上，它与排名世界第一的世界围棋冠军柯洁对战，以3比0的总比分获胜。围棋界公认阿尔法围棋的棋力已经超过人类职业围棋顶尖水平，在GoRatings网站公布的世界职业围棋排名中，其等级分曾超过排名人类第一的棋手柯洁。\n\n>> ![](./强化学习的里程碑/KeJieBattleWithAlphaGo.jpeg)\n\n>> 2017年5月27日，在柯洁与阿尔法围棋的人机大战之后，阿尔法围棋团队宣布阿尔法围棋将不再参加围棋比赛。\n\n>> 2017年10月18日，DeepMind团队公布了最强版阿尔法围棋，代号AlphaGo Zero。\n\n2016年3月机器学习一个重要的时间就是：名为AlphaGo的计算机程序打败了围棋世界冠军李世石，比分4：1。按理来说我们对机器在某项比赛、某些运动中击败人类顶尖选手不会感到大惊小怪，最著名的就是97年IBM的“深蓝（Deep Blue）”计算机程序打败了世界象棋冠军Garry Kasparov。\n\n![](./强化学习的里程碑/GKBattleWithDeepBlue.jpeg)\n\n机器同样是使用强大的算力以数倍、数十倍、数百倍的训练时间去击败人类（通常人类训练十年的时间，机器可以模拟训练几百年），为什么Alpha Go的取胜这么重要、这么引人关注（世界各地媒体疯狂报道，一股狂潮如炒作一般）呢？\n\n原因有两个：\n\n1. AlphaGo解决的围棋问题比之前的都要复杂，西洋双陆棋只有$10^{20}$种不同的“棋位”空间配置，深蓝打败人类的国际象棋有$10^{43}$种不同的“棋位”空间配置，而围棋却有$10^{170}$种不同的“棋位”空间配置，这种量级的数字人类已经无法处理（意思是对于这么多种不同的状态，就是目前算力最强的计算机也无能为力）。举个例子，$10^{170}$这个数字比宇宙中存在的原子数还多。为什么AlphaGo可以在围棋上击败人类就如此重要呢？因为机器如果可以解决这个大的状态空间的问题，那么在机器学习也应该能解决很复杂的现实世界中的问题。这意味着机器真正融入我们的劳动力市场，为我们的日常生活提供便利的日子已经不远啦（真的吗？）！\n2. AlphaGo解决的围棋问题不可能通过纯粹的、暴力计算的方式来学习出很好的模型，这就需要为AlphaGo设计一个更加“智能、聪明”的算法。AlphaGo引起热潮的另一个原因就是，其训练算法是一个通用算法，而不是一个专门为解决某项任务特别设计的算法，这与97年IBM的深蓝计算机程序完全不同，因为深蓝只能用于学习下国际象棋，在中国象棋中就不适于训练。此前，AlphaGo的前身已经能够在Atari 49个不同规则、不同游戏模式中使用相同的通用训练算法训练出比人类还厉害的模型，AlphaGo的成功意味着不仅在虚拟环境可以使用这一套学习方法训练模型，而且可以在不同的现实世界问题中使用这一套学习方法、代码结构。\n\n**有能力解决状态空间非常大的问题**和**通用学习算法**是使AlphaGo警报一时的两个主要原因，这也解释了为什么这场比赛在媒体上引起了轰动。有些人认为李世石的失败是机器占据人类劳动力市场的先兆，也有些人认为这预示着人工智能迎来了黄金时代，实际上我们距离真正的人工智能还有很长的路要走，就算机器可以在某项非常复杂的任务中超过人类的表现能力，其也没有真正的思维方式，不会进行思考，说到底也只是曲线的拟合罢了，但是，只有基础做好了，才能向上研究人工智能。\n\n构建AlphaGo和其前身（应用于Atari游戏）的学习算法的设计思路、计算架构在一系列论文和视频中都可以获得，而没有被Google（收购了英国公司DeepMind）私藏。为什么他不私藏呢？这么厉害的代码、设计思路没必要公开出来嘛，因为Google想把自己打造为基于云的机器学习和大数据的领导者，而它在2016年是全球第三大云服务提供商，排在微软和亚马逊之后，它需要把客户从其他平台引流到自己的平台上。由此可见，大公司们之间的竞争反而可以使我们平民获益。\n\n\n> [The Algorithm Behind the Curtain: How DeepMind Built a Machine that Beat a Go Master (1 of 5)](https://randomant.net/the-algorithm-behind-the-curtain/)","source":"_posts/强化学习的里程碑.md","raw":"---\ntitle: 强化学习的里程碑\ncopyright: true\ntop: 1\ndate: 2019-05-07 08:26:39\nmathjax: true\ncategories: ReinforcementLearning\ntags: rl\n---\n\n# 强化学习的里程碑\n\n<!--more-->\n\n## Alpha Go\n\n> 阿尔法围棋（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能机器人，由谷歌（Google）旗下**DeepMind**公司戴密斯·哈萨比斯领衔的团队开发。其主要工作原理是“深度学习”。\n\n>> 2016年3月，阿尔法围棋与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜；\n\n>> ![](./强化学习的里程碑/LeeSedolBattleWithAlphaGo.jpeg)\n\n>> 2016年末2017年初，该程序在中国棋类网站上以“大师”（Master）为注册账号与中日韩数十位围棋高手进行快棋对决，连续60局无一败绩；\n\n>> 2017年5月，在中国乌镇围棋峰会上，它与排名世界第一的世界围棋冠军柯洁对战，以3比0的总比分获胜。围棋界公认阿尔法围棋的棋力已经超过人类职业围棋顶尖水平，在GoRatings网站公布的世界职业围棋排名中，其等级分曾超过排名人类第一的棋手柯洁。\n\n>> ![](./强化学习的里程碑/KeJieBattleWithAlphaGo.jpeg)\n\n>> 2017年5月27日，在柯洁与阿尔法围棋的人机大战之后，阿尔法围棋团队宣布阿尔法围棋将不再参加围棋比赛。\n\n>> 2017年10月18日，DeepMind团队公布了最强版阿尔法围棋，代号AlphaGo Zero。\n\n2016年3月机器学习一个重要的时间就是：名为AlphaGo的计算机程序打败了围棋世界冠军李世石，比分4：1。按理来说我们对机器在某项比赛、某些运动中击败人类顶尖选手不会感到大惊小怪，最著名的就是97年IBM的“深蓝（Deep Blue）”计算机程序打败了世界象棋冠军Garry Kasparov。\n\n![](./强化学习的里程碑/GKBattleWithDeepBlue.jpeg)\n\n机器同样是使用强大的算力以数倍、数十倍、数百倍的训练时间去击败人类（通常人类训练十年的时间，机器可以模拟训练几百年），为什么Alpha Go的取胜这么重要、这么引人关注（世界各地媒体疯狂报道，一股狂潮如炒作一般）呢？\n\n原因有两个：\n\n1. AlphaGo解决的围棋问题比之前的都要复杂，西洋双陆棋只有$10^{20}$种不同的“棋位”空间配置，深蓝打败人类的国际象棋有$10^{43}$种不同的“棋位”空间配置，而围棋却有$10^{170}$种不同的“棋位”空间配置，这种量级的数字人类已经无法处理（意思是对于这么多种不同的状态，就是目前算力最强的计算机也无能为力）。举个例子，$10^{170}$这个数字比宇宙中存在的原子数还多。为什么AlphaGo可以在围棋上击败人类就如此重要呢？因为机器如果可以解决这个大的状态空间的问题，那么在机器学习也应该能解决很复杂的现实世界中的问题。这意味着机器真正融入我们的劳动力市场，为我们的日常生活提供便利的日子已经不远啦（真的吗？）！\n2. AlphaGo解决的围棋问题不可能通过纯粹的、暴力计算的方式来学习出很好的模型，这就需要为AlphaGo设计一个更加“智能、聪明”的算法。AlphaGo引起热潮的另一个原因就是，其训练算法是一个通用算法，而不是一个专门为解决某项任务特别设计的算法，这与97年IBM的深蓝计算机程序完全不同，因为深蓝只能用于学习下国际象棋，在中国象棋中就不适于训练。此前，AlphaGo的前身已经能够在Atari 49个不同规则、不同游戏模式中使用相同的通用训练算法训练出比人类还厉害的模型，AlphaGo的成功意味着不仅在虚拟环境可以使用这一套学习方法训练模型，而且可以在不同的现实世界问题中使用这一套学习方法、代码结构。\n\n**有能力解决状态空间非常大的问题**和**通用学习算法**是使AlphaGo警报一时的两个主要原因，这也解释了为什么这场比赛在媒体上引起了轰动。有些人认为李世石的失败是机器占据人类劳动力市场的先兆，也有些人认为这预示着人工智能迎来了黄金时代，实际上我们距离真正的人工智能还有很长的路要走，就算机器可以在某项非常复杂的任务中超过人类的表现能力，其也没有真正的思维方式，不会进行思考，说到底也只是曲线的拟合罢了，但是，只有基础做好了，才能向上研究人工智能。\n\n构建AlphaGo和其前身（应用于Atari游戏）的学习算法的设计思路、计算架构在一系列论文和视频中都可以获得，而没有被Google（收购了英国公司DeepMind）私藏。为什么他不私藏呢？这么厉害的代码、设计思路没必要公开出来嘛，因为Google想把自己打造为基于云的机器学习和大数据的领导者，而它在2016年是全球第三大云服务提供商，排在微软和亚马逊之后，它需要把客户从其他平台引流到自己的平台上。由此可见，大公司们之间的竞争反而可以使我们平民获益。\n\n\n> [The Algorithm Behind the Curtain: How DeepMind Built a Machine that Beat a Go Master (1 of 5)](https://randomant.net/the-algorithm-behind-the-curtain/)","slug":"强化学习的里程碑","published":1,"updated":"2019-05-10T01:25:19.932Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6m9ub0017ekveyl93vuxv","content":"<h1 id=\"强化学习的里程碑\"><a href=\"#强化学习的里程碑\" class=\"headerlink\" title=\"强化学习的里程碑\"></a>强化学习的里程碑</h1><a id=\"more\"></a>\n<h2 id=\"Alpha-Go\"><a href=\"#Alpha-Go\" class=\"headerlink\" title=\"Alpha Go\"></a>Alpha Go</h2><blockquote>\n<p>阿尔法围棋（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能机器人，由谷歌（Google）旗下<strong>DeepMind</strong>公司戴密斯·哈萨比斯领衔的团队开发。其主要工作原理是“深度学习”。</p>\n<blockquote>\n<p>2016年3月，阿尔法围棋与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜；</p>\n<p><img src=\"./强化学习的里程碑/LeeSedolBattleWithAlphaGo.jpeg\" alt=\"\"></p>\n<p>2016年末2017年初，该程序在中国棋类网站上以“大师”（Master）为注册账号与中日韩数十位围棋高手进行快棋对决，连续60局无一败绩；</p>\n<p>2017年5月，在中国乌镇围棋峰会上，它与排名世界第一的世界围棋冠军柯洁对战，以3比0的总比分获胜。围棋界公认阿尔法围棋的棋力已经超过人类职业围棋顶尖水平，在GoRatings网站公布的世界职业围棋排名中，其等级分曾超过排名人类第一的棋手柯洁。</p>\n<p><img src=\"./强化学习的里程碑/KeJieBattleWithAlphaGo.jpeg\" alt=\"\"></p>\n<p>2017年5月27日，在柯洁与阿尔法围棋的人机大战之后，阿尔法围棋团队宣布阿尔法围棋将不再参加围棋比赛。</p>\n<p>2017年10月18日，DeepMind团队公布了最强版阿尔法围棋，代号AlphaGo Zero。</p>\n</blockquote>\n</blockquote>\n<p>2016年3月机器学习一个重要的时间就是：名为AlphaGo的计算机程序打败了围棋世界冠军李世石，比分4：1。按理来说我们对机器在某项比赛、某些运动中击败人类顶尖选手不会感到大惊小怪，最著名的就是97年IBM的“深蓝（Deep Blue）”计算机程序打败了世界象棋冠军Garry Kasparov。</p>\n<p><img src=\"./强化学习的里程碑/GKBattleWithDeepBlue.jpeg\" alt=\"\"></p>\n<p>机器同样是使用强大的算力以数倍、数十倍、数百倍的训练时间去击败人类（通常人类训练十年的时间，机器可以模拟训练几百年），为什么Alpha Go的取胜这么重要、这么引人关注（世界各地媒体疯狂报道，一股狂潮如炒作一般）呢？</p>\n<p>原因有两个：</p>\n<ol>\n<li>AlphaGo解决的围棋问题比之前的都要复杂，西洋双陆棋只有$10^{20}$种不同的“棋位”空间配置，深蓝打败人类的国际象棋有$10^{43}$种不同的“棋位”空间配置，而围棋却有$10^{170}$种不同的“棋位”空间配置，这种量级的数字人类已经无法处理（意思是对于这么多种不同的状态，就是目前算力最强的计算机也无能为力）。举个例子，$10^{170}$这个数字比宇宙中存在的原子数还多。为什么AlphaGo可以在围棋上击败人类就如此重要呢？因为机器如果可以解决这个大的状态空间的问题，那么在机器学习也应该能解决很复杂的现实世界中的问题。这意味着机器真正融入我们的劳动力市场，为我们的日常生活提供便利的日子已经不远啦（真的吗？）！</li>\n<li>AlphaGo解决的围棋问题不可能通过纯粹的、暴力计算的方式来学习出很好的模型，这就需要为AlphaGo设计一个更加“智能、聪明”的算法。AlphaGo引起热潮的另一个原因就是，其训练算法是一个通用算法，而不是一个专门为解决某项任务特别设计的算法，这与97年IBM的深蓝计算机程序完全不同，因为深蓝只能用于学习下国际象棋，在中国象棋中就不适于训练。此前，AlphaGo的前身已经能够在Atari 49个不同规则、不同游戏模式中使用相同的通用训练算法训练出比人类还厉害的模型，AlphaGo的成功意味着不仅在虚拟环境可以使用这一套学习方法训练模型，而且可以在不同的现实世界问题中使用这一套学习方法、代码结构。</li>\n</ol>\n<p><strong>有能力解决状态空间非常大的问题</strong>和<strong>通用学习算法</strong>是使AlphaGo警报一时的两个主要原因，这也解释了为什么这场比赛在媒体上引起了轰动。有些人认为李世石的失败是机器占据人类劳动力市场的先兆，也有些人认为这预示着人工智能迎来了黄金时代，实际上我们距离真正的人工智能还有很长的路要走，就算机器可以在某项非常复杂的任务中超过人类的表现能力，其也没有真正的思维方式，不会进行思考，说到底也只是曲线的拟合罢了，但是，只有基础做好了，才能向上研究人工智能。</p>\n<p>构建AlphaGo和其前身（应用于Atari游戏）的学习算法的设计思路、计算架构在一系列论文和视频中都可以获得，而没有被Google（收购了英国公司DeepMind）私藏。为什么他不私藏呢？这么厉害的代码、设计思路没必要公开出来嘛，因为Google想把自己打造为基于云的机器学习和大数据的领导者，而它在2016年是全球第三大云服务提供商，排在微软和亚马逊之后，它需要把客户从其他平台引流到自己的平台上。由此可见，大公司们之间的竞争反而可以使我们平民获益。</p>\n<blockquote>\n<p><a href=\"https://randomant.net/the-algorithm-behind-the-curtain/\" rel=\"external nofollow\" target=\"_blank\">The Algorithm Behind the Curtain: How DeepMind Built a Machine that Beat a Go Master (1 of 5)</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h1 id=\"强化学习的里程碑\"><a href=\"#强化学习的里程碑\" class=\"headerlink\" title=\"强化学习的里程碑\"></a>强化学习的里程碑</h1>","more":"<h2 id=\"Alpha-Go\"><a href=\"#Alpha-Go\" class=\"headerlink\" title=\"Alpha Go\"></a>Alpha Go</h2><blockquote>\n<p>阿尔法围棋（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能机器人，由谷歌（Google）旗下<strong>DeepMind</strong>公司戴密斯·哈萨比斯领衔的团队开发。其主要工作原理是“深度学习”。</p>\n<blockquote>\n<p>2016年3月，阿尔法围棋与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜；</p>\n<p><img src=\"./强化学习的里程碑/LeeSedolBattleWithAlphaGo.jpeg\" alt=\"\"></p>\n<p>2016年末2017年初，该程序在中国棋类网站上以“大师”（Master）为注册账号与中日韩数十位围棋高手进行快棋对决，连续60局无一败绩；</p>\n<p>2017年5月，在中国乌镇围棋峰会上，它与排名世界第一的世界围棋冠军柯洁对战，以3比0的总比分获胜。围棋界公认阿尔法围棋的棋力已经超过人类职业围棋顶尖水平，在GoRatings网站公布的世界职业围棋排名中，其等级分曾超过排名人类第一的棋手柯洁。</p>\n<p><img src=\"./强化学习的里程碑/KeJieBattleWithAlphaGo.jpeg\" alt=\"\"></p>\n<p>2017年5月27日，在柯洁与阿尔法围棋的人机大战之后，阿尔法围棋团队宣布阿尔法围棋将不再参加围棋比赛。</p>\n<p>2017年10月18日，DeepMind团队公布了最强版阿尔法围棋，代号AlphaGo Zero。</p>\n</blockquote>\n</blockquote>\n<p>2016年3月机器学习一个重要的时间就是：名为AlphaGo的计算机程序打败了围棋世界冠军李世石，比分4：1。按理来说我们对机器在某项比赛、某些运动中击败人类顶尖选手不会感到大惊小怪，最著名的就是97年IBM的“深蓝（Deep Blue）”计算机程序打败了世界象棋冠军Garry Kasparov。</p>\n<p><img src=\"./强化学习的里程碑/GKBattleWithDeepBlue.jpeg\" alt=\"\"></p>\n<p>机器同样是使用强大的算力以数倍、数十倍、数百倍的训练时间去击败人类（通常人类训练十年的时间，机器可以模拟训练几百年），为什么Alpha Go的取胜这么重要、这么引人关注（世界各地媒体疯狂报道，一股狂潮如炒作一般）呢？</p>\n<p>原因有两个：</p>\n<ol>\n<li>AlphaGo解决的围棋问题比之前的都要复杂，西洋双陆棋只有$10^{20}$种不同的“棋位”空间配置，深蓝打败人类的国际象棋有$10^{43}$种不同的“棋位”空间配置，而围棋却有$10^{170}$种不同的“棋位”空间配置，这种量级的数字人类已经无法处理（意思是对于这么多种不同的状态，就是目前算力最强的计算机也无能为力）。举个例子，$10^{170}$这个数字比宇宙中存在的原子数还多。为什么AlphaGo可以在围棋上击败人类就如此重要呢？因为机器如果可以解决这个大的状态空间的问题，那么在机器学习也应该能解决很复杂的现实世界中的问题。这意味着机器真正融入我们的劳动力市场，为我们的日常生活提供便利的日子已经不远啦（真的吗？）！</li>\n<li>AlphaGo解决的围棋问题不可能通过纯粹的、暴力计算的方式来学习出很好的模型，这就需要为AlphaGo设计一个更加“智能、聪明”的算法。AlphaGo引起热潮的另一个原因就是，其训练算法是一个通用算法，而不是一个专门为解决某项任务特别设计的算法，这与97年IBM的深蓝计算机程序完全不同，因为深蓝只能用于学习下国际象棋，在中国象棋中就不适于训练。此前，AlphaGo的前身已经能够在Atari 49个不同规则、不同游戏模式中使用相同的通用训练算法训练出比人类还厉害的模型，AlphaGo的成功意味着不仅在虚拟环境可以使用这一套学习方法训练模型，而且可以在不同的现实世界问题中使用这一套学习方法、代码结构。</li>\n</ol>\n<p><strong>有能力解决状态空间非常大的问题</strong>和<strong>通用学习算法</strong>是使AlphaGo警报一时的两个主要原因，这也解释了为什么这场比赛在媒体上引起了轰动。有些人认为李世石的失败是机器占据人类劳动力市场的先兆，也有些人认为这预示着人工智能迎来了黄金时代，实际上我们距离真正的人工智能还有很长的路要走，就算机器可以在某项非常复杂的任务中超过人类的表现能力，其也没有真正的思维方式，不会进行思考，说到底也只是曲线的拟合罢了，但是，只有基础做好了，才能向上研究人工智能。</p>\n<p>构建AlphaGo和其前身（应用于Atari游戏）的学习算法的设计思路、计算架构在一系列论文和视频中都可以获得，而没有被Google（收购了英国公司DeepMind）私藏。为什么他不私藏呢？这么厉害的代码、设计思路没必要公开出来嘛，因为Google想把自己打造为基于云的机器学习和大数据的领导者，而它在2016年是全球第三大云服务提供商，排在微软和亚马逊之后，它需要把客户从其他平台引流到自己的平台上。由此可见，大公司们之间的竞争反而可以使我们平民获益。</p>\n<blockquote>\n<p><a href=\"https://randomant.net/the-algorithm-behind-the-curtain/\" rel=\"external nofollow\" target=\"_blank\">The Algorithm Behind the Curtain: How DeepMind Built a Machine that Beat a Go Master (1 of 5)</a></p>\n</blockquote>"},{"title":"一些在Docker中比较难以安装的库(整理)","copyright":true,"top":1,"date":"2019-03-24T10:57:50.000Z","_content":"\n本文记录了一些在Docker中比较难以安装、频繁报错的库，如cuda、cudnn。\n\n<!--more-->\n\n# 前言\n在配置镜像时, **强烈建议将源更改为国内镜像站**, 因为国外有些镜像站链接速度很慢, 更新也很慢, 很多库无法正确安装\n\n我所使用的镜像站为`sources.list`:\n\n```\ndeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties\ndeb http://archive.canonical.com/ubuntu xenial partner\ndeb-src http://archive.canonical.com/ubuntu xenial partner\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse\n```\n在`Dockerfile`或者在容器内使用命令\n```\nRUN cp /etc/apt/sources.list /etc/apt/sources.list.bak\nCOPY sources.list /etc/apt/sources.list\n```\n将源替换.\n\n# CUDA 9.0 开发者版\n`dockerfile`如下:\n```\nFROM nvidia/cuda:9.0-runtime-ubuntu16.04\nLABEL maintainer \"Keavnn <https://stepneverstop.github.io>\"\n\nRUN apt-get update && apt-get install -y --allow-unauthenticated --no-install-recommends \\\n        cuda-libraries-dev-$CUDA_PKG_VERSION \\\n        cuda-nvml-dev-$CUDA_PKG_VERSION \\\n        cuda-minimal-build-$CUDA_PKG_VERSION \\\n        cuda-command-line-tools-$CUDA_PKG_VERSION \\\n        cuda-core-9-0=9.0.176.3-1 \\\n        cuda-cublas-dev-9-0=9.0.176.4-1 \\\n        libnccl-dev=$NCCL_VERSION-1+cuda9.0 && \\\n    rm -rf /var/lib/apt/lists/*\n\nENV LIBRARY_PATH /usr/local/cuda/lib64/stubs\n```\n**--allow-unauthenticated** 这句命令很重要, 不使用的话很有可能安装失败\n\n# cudnn 7.0.5\n- [https://developer.nvidia.com/rdp/cudnn-archive](https://developer.nvidia.com/rdp/cudnn-archive) 下载cuDNN Libraries for Linux,不要下载 Power 8\n- 把下载好的包上传到FTP服务器, 或者传输到容器内, 或者直接在容器中下载好\n- `cd`到包位置\n- `cp cudnn-9.0-linux-x64-v7.solitairetheme8 cudnn-9.0-linux-x64-v7.tgz`\n- `tar -xvf cudnn-9.0-linux-x64-v7.tgz`\n- `cp include/* /usr/local/cuda-9.0/include`\n- `cp lib64/* /usr/local/cuda-9.0/lib64`\n- `chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn*` **这一步如果cuda是base版本,则没有include文件夹,需要手动创建后再执行**\n- `export PATH=/usr/local/cuda-9.0/bin:$PATH`\n- `cd`到`/usr/local/cuda-9.0/lib64`\n- `nano ~/.bashrc`,关联环境变量\n- 在最后一行加入`export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH`\n- `source ~/.bashrc`\n- `ldconfig -v`\n- 使用`cat /usr/local/cuda-9.0/include/cudnn.h | grep CUDNN_MAJOR -A 2` 查看cudnn版本\n![](./create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png)\n\n# jemalloc\n选择安装`jemalloc`,这个工具可以加速编译,碎片整理,具体请自行谷歌\n- `apt-get install autoconf`\n- `apt-get install automake`\n- `apt-get install libtool`\n- `git clone https://github.com/jemalloc/jemalloc.git`\n- `cd jemalloc`\n- `git checkout 4.5.0`安装4.5.0版本的jemalloc,5.x版本的有坑,深坑\n- `./autogen.sh`\n- `make`\n- `make install_bin install_include install_lib`,之所以不使用`make install`是因为会报错,如下: ![](./create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png)\n\n# Python3.6\n记得`sudo`\n- `apt-get install software-properties-common`\n- `add-apt-repository ppa:jonathonf/python-3.6`, 按`ENTER`\n- `apt-get update && apt-get install python3.6 -y`\n- 修改系统默认的`python`版本为3.6\n- `cd /usr/bin`, 保险起见,建议分两步\n- `rm python`\n- `ln -s python3.6m python`\n- 如需更新,`pip3 install --upgrade pip`, 8.1.1->19.0.3\n- `python -V`\n![](./something-hard-install-docker/1.png)\n\n","source":"_posts/something-hard-install-docker.md","raw":"---\ntitle: 一些在Docker中比较难以安装的库(整理)\ncopyright: true\ntop: 1\ndate: 2019-03-24 18:57:50\ncategories: Docker\ntags:\n- docker\n---\n\n本文记录了一些在Docker中比较难以安装、频繁报错的库，如cuda、cudnn。\n\n<!--more-->\n\n# 前言\n在配置镜像时, **强烈建议将源更改为国内镜像站**, 因为国外有些镜像站链接速度很慢, 更新也很慢, 很多库无法正确安装\n\n我所使用的镜像站为`sources.list`:\n\n```\ndeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties\ndeb http://archive.canonical.com/ubuntu xenial partner\ndeb-src http://archive.canonical.com/ubuntu xenial partner\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse\n```\n在`Dockerfile`或者在容器内使用命令\n```\nRUN cp /etc/apt/sources.list /etc/apt/sources.list.bak\nCOPY sources.list /etc/apt/sources.list\n```\n将源替换.\n\n# CUDA 9.0 开发者版\n`dockerfile`如下:\n```\nFROM nvidia/cuda:9.0-runtime-ubuntu16.04\nLABEL maintainer \"Keavnn <https://stepneverstop.github.io>\"\n\nRUN apt-get update && apt-get install -y --allow-unauthenticated --no-install-recommends \\\n        cuda-libraries-dev-$CUDA_PKG_VERSION \\\n        cuda-nvml-dev-$CUDA_PKG_VERSION \\\n        cuda-minimal-build-$CUDA_PKG_VERSION \\\n        cuda-command-line-tools-$CUDA_PKG_VERSION \\\n        cuda-core-9-0=9.0.176.3-1 \\\n        cuda-cublas-dev-9-0=9.0.176.4-1 \\\n        libnccl-dev=$NCCL_VERSION-1+cuda9.0 && \\\n    rm -rf /var/lib/apt/lists/*\n\nENV LIBRARY_PATH /usr/local/cuda/lib64/stubs\n```\n**--allow-unauthenticated** 这句命令很重要, 不使用的话很有可能安装失败\n\n# cudnn 7.0.5\n- [https://developer.nvidia.com/rdp/cudnn-archive](https://developer.nvidia.com/rdp/cudnn-archive) 下载cuDNN Libraries for Linux,不要下载 Power 8\n- 把下载好的包上传到FTP服务器, 或者传输到容器内, 或者直接在容器中下载好\n- `cd`到包位置\n- `cp cudnn-9.0-linux-x64-v7.solitairetheme8 cudnn-9.0-linux-x64-v7.tgz`\n- `tar -xvf cudnn-9.0-linux-x64-v7.tgz`\n- `cp include/* /usr/local/cuda-9.0/include`\n- `cp lib64/* /usr/local/cuda-9.0/lib64`\n- `chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn*` **这一步如果cuda是base版本,则没有include文件夹,需要手动创建后再执行**\n- `export PATH=/usr/local/cuda-9.0/bin:$PATH`\n- `cd`到`/usr/local/cuda-9.0/lib64`\n- `nano ~/.bashrc`,关联环境变量\n- 在最后一行加入`export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH`\n- `source ~/.bashrc`\n- `ldconfig -v`\n- 使用`cat /usr/local/cuda-9.0/include/cudnn.h | grep CUDNN_MAJOR -A 2` 查看cudnn版本\n![](./create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png)\n\n# jemalloc\n选择安装`jemalloc`,这个工具可以加速编译,碎片整理,具体请自行谷歌\n- `apt-get install autoconf`\n- `apt-get install automake`\n- `apt-get install libtool`\n- `git clone https://github.com/jemalloc/jemalloc.git`\n- `cd jemalloc`\n- `git checkout 4.5.0`安装4.5.0版本的jemalloc,5.x版本的有坑,深坑\n- `./autogen.sh`\n- `make`\n- `make install_bin install_include install_lib`,之所以不使用`make install`是因为会报错,如下: ![](./create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png)\n\n# Python3.6\n记得`sudo`\n- `apt-get install software-properties-common`\n- `add-apt-repository ppa:jonathonf/python-3.6`, 按`ENTER`\n- `apt-get update && apt-get install python3.6 -y`\n- 修改系统默认的`python`版本为3.6\n- `cd /usr/bin`, 保险起见,建议分两步\n- `rm python`\n- `ln -s python3.6m python`\n- 如需更新,`pip3 install --upgrade pip`, 8.1.1->19.0.3\n- `python -V`\n![](./something-hard-install-docker/1.png)\n\n","slug":"something-hard-install-docker","published":1,"updated":"2019-05-13T11:51:19.996Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma24002cekvevi8klydb","content":"<p>本文记录了一些在Docker中比较难以安装、频繁报错的库，如cuda、cudnn。</p>\n<a id=\"more\"></a>\n<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>在配置镜像时, <strong>强烈建议将源更改为国内镜像站</strong>, 因为国外有些镜像站链接速度很慢, 更新也很慢, 很多库无法正确安装</p>\n<p>我所使用的镜像站为<code>sources.list</code>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class=\"line\">deb http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb-src http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</span><br></pre></td></tr></table></figure>\n<p>在<code>Dockerfile</code>或者在容器内使用命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class=\"line\">COPY sources.list /etc/apt/sources.list</span><br></pre></td></tr></table></figure></p>\n<p>将源替换.</p>\n<h1 id=\"CUDA-9-0-开发者版\"><a href=\"#CUDA-9-0-开发者版\" class=\"headerlink\" title=\"CUDA 9.0 开发者版\"></a>CUDA 9.0 开发者版</h1><p><code>dockerfile</code>如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM nvidia/cuda:9.0-runtime-ubuntu16.04</span><br><span class=\"line\">LABEL maintainer &quot;Keavnn &lt;https://stepneverstop.github.io&gt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y --allow-unauthenticated --no-install-recommends \\</span><br><span class=\"line\">        cuda-libraries-dev-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-nvml-dev-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-minimal-build-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-command-line-tools-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-core-9-0=9.0.176.3-1 \\</span><br><span class=\"line\">        cuda-cublas-dev-9-0=9.0.176.4-1 \\</span><br><span class=\"line\">        libnccl-dev=$NCCL_VERSION-1+cuda9.0 &amp;&amp; \\</span><br><span class=\"line\">    rm -rf /var/lib/apt/lists/*</span><br><span class=\"line\"></span><br><span class=\"line\">ENV LIBRARY_PATH /usr/local/cuda/lib64/stubs</span><br></pre></td></tr></table></figure></p>\n<p><strong>—allow-unauthenticated</strong> 这句命令很重要, 不使用的话很有可能安装失败</p>\n<h1 id=\"cudnn-7-0-5\"><a href=\"#cudnn-7-0-5\" class=\"headerlink\" title=\"cudnn 7.0.5\"></a>cudnn 7.0.5</h1><ul>\n<li><a href=\"https://developer.nvidia.com/rdp/cudnn-archive\" rel=\"external nofollow\" target=\"_blank\">https://developer.nvidia.com/rdp/cudnn-archive</a> 下载cuDNN Libraries for Linux,不要下载 Power 8</li>\n<li>把下载好的包上传到FTP服务器, 或者传输到容器内, 或者直接在容器中下载好</li>\n<li><code>cd</code>到包位置</li>\n<li><code>cp cudnn-9.0-linux-x64-v7.solitairetheme8 cudnn-9.0-linux-x64-v7.tgz</code></li>\n<li><code>tar -xvf cudnn-9.0-linux-x64-v7.tgz</code></li>\n<li><code>cp include/* /usr/local/cuda-9.0/include</code></li>\n<li><code>cp lib64/* /usr/local/cuda-9.0/lib64</code></li>\n<li><code>chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn*</code> <strong>这一步如果cuda是base版本,则没有include文件夹,需要手动创建后再执行</strong></li>\n<li><code>export PATH=/usr/local/cuda-9.0/bin:$PATH</code></li>\n<li><code>cd</code>到<code>/usr/local/cuda-9.0/lib64</code></li>\n<li><code>nano ~/.bashrc</code>,关联环境变量</li>\n<li>在最后一行加入<code>export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH</code></li>\n<li><code>source ~/.bashrc</code></li>\n<li><code>ldconfig -v</code></li>\n<li>使用<code>cat /usr/local/cuda-9.0/include/cudnn.h | grep CUDNN_MAJOR -A 2</code> 查看cudnn版本<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png\" alt=\"\"></li>\n</ul>\n<h1 id=\"jemalloc\"><a href=\"#jemalloc\" class=\"headerlink\" title=\"jemalloc\"></a>jemalloc</h1><p>选择安装<code>jemalloc</code>,这个工具可以加速编译,碎片整理,具体请自行谷歌</p>\n<ul>\n<li><code>apt-get install autoconf</code></li>\n<li><code>apt-get install automake</code></li>\n<li><code>apt-get install libtool</code></li>\n<li><code>git clone https://github.com/jemalloc/jemalloc.git</code></li>\n<li><code>cd jemalloc</code></li>\n<li><code>git checkout 4.5.0</code>安装4.5.0版本的jemalloc,5.x版本的有坑,深坑</li>\n<li><code>./autogen.sh</code></li>\n<li><code>make</code></li>\n<li><code>make install_bin install_include install_lib</code>,之所以不使用<code>make install</code>是因为会报错,如下: <img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png\" alt=\"\"></li>\n</ul>\n<h1 id=\"Python3-6\"><a href=\"#Python3-6\" class=\"headerlink\" title=\"Python3.6\"></a>Python3.6</h1><p>记得<code>sudo</code></p>\n<ul>\n<li><code>apt-get install software-properties-common</code></li>\n<li><code>add-apt-repository ppa:jonathonf/python-3.6</code>, 按<code>ENTER</code></li>\n<li><code>apt-get update &amp;&amp; apt-get install python3.6 -y</code></li>\n<li>修改系统默认的<code>python</code>版本为3.6</li>\n<li><code>cd /usr/bin</code>, 保险起见,建议分两步</li>\n<li><code>rm python</code></li>\n<li><code>ln -s python3.6m python</code></li>\n<li>如需更新,<code>pip3 install --upgrade pip</code>, 8.1.1-&gt;19.0.3</li>\n<li><code>python -V</code><br><img src=\"./something-hard-install-docker/1.png\" alt=\"\"></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>本文记录了一些在Docker中比较难以安装、频繁报错的库，如cuda、cudnn。</p>","more":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>在配置镜像时, <strong>强烈建议将源更改为国内镜像站</strong>, 因为国外有些镜像站链接速度很慢, 更新也很慢, 很多库无法正确安装</p>\n<p>我所使用的镜像站为<code>sources.list</code>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class=\"line\">deb http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb-src http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</span><br></pre></td></tr></table></figure>\n<p>在<code>Dockerfile</code>或者在容器内使用命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class=\"line\">COPY sources.list /etc/apt/sources.list</span><br></pre></td></tr></table></figure></p>\n<p>将源替换.</p>\n<h1 id=\"CUDA-9-0-开发者版\"><a href=\"#CUDA-9-0-开发者版\" class=\"headerlink\" title=\"CUDA 9.0 开发者版\"></a>CUDA 9.0 开发者版</h1><p><code>dockerfile</code>如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM nvidia/cuda:9.0-runtime-ubuntu16.04</span><br><span class=\"line\">LABEL maintainer &quot;Keavnn &lt;https://stepneverstop.github.io&gt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y --allow-unauthenticated --no-install-recommends \\</span><br><span class=\"line\">        cuda-libraries-dev-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-nvml-dev-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-minimal-build-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-command-line-tools-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-core-9-0=9.0.176.3-1 \\</span><br><span class=\"line\">        cuda-cublas-dev-9-0=9.0.176.4-1 \\</span><br><span class=\"line\">        libnccl-dev=$NCCL_VERSION-1+cuda9.0 &amp;&amp; \\</span><br><span class=\"line\">    rm -rf /var/lib/apt/lists/*</span><br><span class=\"line\"></span><br><span class=\"line\">ENV LIBRARY_PATH /usr/local/cuda/lib64/stubs</span><br></pre></td></tr></table></figure></p>\n<p><strong>—allow-unauthenticated</strong> 这句命令很重要, 不使用的话很有可能安装失败</p>\n<h1 id=\"cudnn-7-0-5\"><a href=\"#cudnn-7-0-5\" class=\"headerlink\" title=\"cudnn 7.0.5\"></a>cudnn 7.0.5</h1><ul>\n<li><a href=\"https://developer.nvidia.com/rdp/cudnn-archive\" rel=\"external nofollow\" target=\"_blank\">https://developer.nvidia.com/rdp/cudnn-archive</a> 下载cuDNN Libraries for Linux,不要下载 Power 8</li>\n<li>把下载好的包上传到FTP服务器, 或者传输到容器内, 或者直接在容器中下载好</li>\n<li><code>cd</code>到包位置</li>\n<li><code>cp cudnn-9.0-linux-x64-v7.solitairetheme8 cudnn-9.0-linux-x64-v7.tgz</code></li>\n<li><code>tar -xvf cudnn-9.0-linux-x64-v7.tgz</code></li>\n<li><code>cp include/* /usr/local/cuda-9.0/include</code></li>\n<li><code>cp lib64/* /usr/local/cuda-9.0/lib64</code></li>\n<li><code>chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn*</code> <strong>这一步如果cuda是base版本,则没有include文件夹,需要手动创建后再执行</strong></li>\n<li><code>export PATH=/usr/local/cuda-9.0/bin:$PATH</code></li>\n<li><code>cd</code>到<code>/usr/local/cuda-9.0/lib64</code></li>\n<li><code>nano ~/.bashrc</code>,关联环境变量</li>\n<li>在最后一行加入<code>export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH</code></li>\n<li><code>source ~/.bashrc</code></li>\n<li><code>ldconfig -v</code></li>\n<li>使用<code>cat /usr/local/cuda-9.0/include/cudnn.h | grep CUDNN_MAJOR -A 2</code> 查看cudnn版本<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png\" alt=\"\"></li>\n</ul>\n<h1 id=\"jemalloc\"><a href=\"#jemalloc\" class=\"headerlink\" title=\"jemalloc\"></a>jemalloc</h1><p>选择安装<code>jemalloc</code>,这个工具可以加速编译,碎片整理,具体请自行谷歌</p>\n<ul>\n<li><code>apt-get install autoconf</code></li>\n<li><code>apt-get install automake</code></li>\n<li><code>apt-get install libtool</code></li>\n<li><code>git clone https://github.com/jemalloc/jemalloc.git</code></li>\n<li><code>cd jemalloc</code></li>\n<li><code>git checkout 4.5.0</code>安装4.5.0版本的jemalloc,5.x版本的有坑,深坑</li>\n<li><code>./autogen.sh</code></li>\n<li><code>make</code></li>\n<li><code>make install_bin install_include install_lib</code>,之所以不使用<code>make install</code>是因为会报错,如下: <img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png\" alt=\"\"></li>\n</ul>\n<h1 id=\"Python3-6\"><a href=\"#Python3-6\" class=\"headerlink\" title=\"Python3.6\"></a>Python3.6</h1><p>记得<code>sudo</code></p>\n<ul>\n<li><code>apt-get install software-properties-common</code></li>\n<li><code>add-apt-repository ppa:jonathonf/python-3.6</code>, 按<code>ENTER</code></li>\n<li><code>apt-get update &amp;&amp; apt-get install python3.6 -y</code></li>\n<li>修改系统默认的<code>python</code>版本为3.6</li>\n<li><code>cd /usr/bin</code>, 保险起见,建议分两步</li>\n<li><code>rm python</code></li>\n<li><code>ln -s python3.6m python</code></li>\n<li>如需更新,<code>pip3 install --upgrade pip</code>, 8.1.1-&gt;19.0.3</li>\n<li><code>python -V</code><br><img src=\"./something-hard-install-docker/1.png\" alt=\"\"></li>\n</ul>"},{"title":"Hindsight Experience Replay","copyright":true,"mathjax":true,"top":1,"date":"2019-05-28T10:38:56.000Z","keywords":null,"description":null,"_content":"\n本文介绍了一个“事后诸葛亮”的经验池机制，简称为**HER**，它可以很好地应用于**稀疏奖励**和**二分奖励**的问题中，不需要复杂的奖励函数工程设计。\n\n推荐：\n\n- 稀疏奖励问题的一种解决方案\n- 通俗易懂\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf](https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf)\n\n>Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL).  \n\n强化学习问题中最棘手的问题之一就是稀疏奖励。\n\n本文提出了一个新颖的技术：Hindsight Experience Replay（HER），可以从稀疏、二分的奖励问题中高效采样并进行学习，而且可以应用于**所有的Off-Policy**算法中。\n\n![](./Hindsight-Experience-Replay/hindsight.png)\n\nHindsight意为事后，结合强化学习中序贯决策问题的特性，我们很容易就可以猜想到，“事后”要不然指的是在状态s下执行动作a之后，要不然指的就是当一个episode结束之后。其实，文中对常规经验池的改进也正是运用了这样的含义。\n\n> HER lets an agent learn from undesired outcomes and tackles the problem of sparse rewards in Reinforcement Learning (RL).——Zhao, R., & Tresp, V. (2018). Energy-Based Hindsight Experience Prioritization. *CoRL*.\n\nHER使智能体从没达到的结果中去学习，解决了强化学习中稀疏奖励的问题。\n\n## 二分奖励 binary reward\n\n简言之，完成目标为一个值，没完成目标为另一个值。如：\n\n- $S_{T}=Goal，r=0$\n- $S\\neq Goal, r=-1. for \\ S \\in \\mathbb{S}$\n\n## 稀疏奖励 sparse reward\n\n简言之，完成目标的episode太少或者完成目标的步数太长，导致负奖励的样本数过多\n\n# 文中精要\n\n在机器人领域，要想使强化学习训练它完美执行某任务，往往需要设计合理的奖励函数，但是设计这样的奖励函数工程师不仅需要懂得强化学习的领域知识，也需要懂得机器人、运动学等领域的知识。而且，有这些知识也未必能设计出很好的奖励函数供智能体进行学习。因此，如果可以从简单的奖励函数（如二分奖励）学习到可完成任务的模型，那就不需要费心设计复杂的奖励函数了。\n\n文中介绍了一个例子来引入HER：\n\n- 名称：bit-flipping environment\n- 状态空间$\\mathcal{S}=\\left \\{ 0,1 \\right \\}^{n}$\n- 动作空间$\\mathcal{A}=\\left \\{ 0,1,\\cdots,n-1 \\right \\}$\n- 规则：对于每个episode，均匀采样长度为$n$的初始状态$s_{0}$（如$n=5，s_{0}=10101$）和目标状态$s_{g}$，每一步从动作空间中选取一个动作$a$，翻转$s_{0}$第$a$个位置的值，如$a=1\\Rightarrow s_{1}=11101$，直到回合结束或者翻转后的状态与$s_{g}$相同\n- 奖励函数：$r_{g}(s,a)=-\\left [ s \\neq g \\right ]$，即达到目标状态则为0，未达到目标状态则为-1。这个很容易理解，$s \\neq g \\Rightarrow true \\doteq 1，s = g \\Rightarrow false \\doteq 0$\n\n*注：下文如无特殊说明，$g$即表示目标状态$s_{g}$*\n\n> Standard RL algorithms are bound to fail in this environment for n > 40 because they will never experience any reward other than -1. Notice that using techniques for improving exploration (e.g. VIME (Houthooft et al., 2016), count-based exploration (Ostrovski et al., 2017) or bootstrapped DQN (Osband et al., 2016)) does not help here because the real problem is not in lack of diversity of states being visited, rather it is simply impractical to explore such a large state space.  \n\n当序列长度$n$大于40时，传统的强化学习算法就算有各种探索机制的加持，也不能学会解决这个问题，因为这个问题完全不是缺乏探索，而是**状态太多，探索不完**，导致奖励极其稀疏，算法根本不知道需要优化的目标在哪里。\n\n为了解决这个问题，作者指出了两个思路：\n\n1. 使用shaped reward（简言之，将reward设计成某些变量的函数，如$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$，即奖励函数为当前状态与目标状态的欧氏距离的负数），将训练的算法逐步引导至奖励函数增大的决策空间。但是这种方法可能很难应用于复杂的问题中。\n2. 使用HER——事后经验池机制\n\n## HER\n\n> The pivotal idea behind our approach is to re-examine this trajectory with a different goal — while this trajectory may not help us learn how to achieve the state g, it definitely tells us something about how to achieve the state $s_{T}$ .\n\nHER的主要思想就是：**为什么一定要考虑我们设定的目标呢？假设我们想让一个智能体学会移动到某个位置，它在一个episode中没有学到移动到目标位置就算失败吗？假定序列为$s_{0},s_{1},s_{2}, \\cdots ,s_{T}$，目标为$g$，我们何不换一种思路考虑：如果我们在episode开始前就将目标状态$g$设置为$s_{T}$，即$g=s_{T}$，那么这样看来智能体不就算是完成目标了吗？**\n\n![](./Hindsight-Experience-Replay/Her.png)\n\nHER就是运用了这个思想对经验池进行了扩充，将稀疏奖励问题给转化成非稀疏奖励，大大的扩展了经验池中完成任务的经验数量。\n\nHER主要特点：\n\n- 传统经验池存入的是状态$s$，而HER存入的是$s||g$，也就是`tf.concat(s,g)`\n- 训练算法的输入也是$s||g$，也就是需要在当前状态后边连结上**每个episode的**目标状态，每个episode的目标状态可能不同\n- HER对经验池进行了扩充，不仅存入实际采样得到的transition/experience，$\\left ( s_{t}||g,a_{t},r_{t},s_{t+1}||g \\right )$，也要在回合结束时**重新设置目标状态**，得到相应的奖励值（在二分奖励问题中，只有在$s=g$时奖励才需要更改），存入“事后”（当初如果这样就好啦！）的经验$\\left ( s_{t}||g',a_{t},r_{t}',s_{t+1}||g' \\right )$，详见伪代码，这个事后经验究竟存入多少份、多少种，由超参数$k$控制，下文讲解。\n- HER更适合解决多目标问题，多目标的意思为，目标点非固定，每个episode的目标状态可以不相同。详见实验部分\n\nHER的几种扩展方式：\n\n> future — replay with k random states which come from the same episode as the transition being replayed and were observed after it,\n> episode — replay with k random states coming from the same episode as the transition being replayed,\n> random — replay with k random states encountered so far in the whole training procedure.\n\n- 未来模式——future：在一个序列$s_{0},s_{1},s_{2},\\cdots,s_{T}$中，如果遍历到状态$s_{2}$，则在$s_{3},\\cdots,s_{T}$之间随机抽取$k$个状态作为目标状态$g'$，并依此向经验池中存入$\\left ( s_{2}||g',a_{2},r_{2}',s_{3}||g' \\right )$，**特点：一个episode的后续部分**\n- 回合模式——episode：在一个序列$s_{0},s_{1},s_{2},...,s_{T}$中，如果遍历到状态$s_{2}$，则在整个序列中随机抽取$k$个状态作为目标状态$g'$，并依此向经验池中存入$\\left ( s_{2}||g',a_{2},r_{2}',s_{3}||g' \\right )$，**特点：一个episode**\n\n- 随机模式——random：在一个序列$s_{0},s_{1},s_{2},...,s_{T}$中，如果遍历到状态$s_{2}$，则在多个序列$\\tau_{0},\\tau_{1},\\tau_{2},\\cdots$中随机抽取$k$个状态作为目标状态$g'$，并依此向经验池中存入$\\left ( s_{2}||g',a_{2},r_{2}',s_{3}||g' \\right )$，**特点：多个episode**\n\n- 最终模式——final：在一个序列$s_{0},s_{1},s_{2},\\cdots,s_{T}$中，如果遍历到状态$s_{2}$，则之间令$g'=s_{T}$，并向经验池中存入$\\left ( s_{2}||g',a_{2},r_{2}',s_{3}||g' \\right )$，**特点：一个episode的最后一个状态，如果设置k，则存入k个相同的经验**\n\n## 伪代码\n\n![](./Hindsight-Experience-Replay/pseudo.png)\n\n解析：\n\n1. 伪代码中没有提到超参数$k$，其实在循环条件$\\textbf{for} \\ g' \\in G \\ \\textbf{do}$中循环执行了$k$次\n2. $||$操作为连结操作，简言之，将两个长度为5的向量合并成一个长度为10的向量\n3. $G:=\\mathbb{S}(\\textbf{current episode})$即为上文提到的四种扩展模式：future、episode、random、final。\n4. 奖励函数$r(s,a,g)=-\\left [ f_{g}(s)=0 \\right ]$即为前文提到的$r_{g}(s,a)=-\\left [ s \\neq g \\right ]$，即完成为0，未完成为-1，具体奖励函数可以根据我们的使用环境设计\n5. $a_{t} \\leftarrow \\pi_{b}(s_{t}||g)$表示神经网络的输入为当前状态与目标状态的连结\n\n## HER的优点\n\n1. 可解决稀疏奖励、二分奖励问题\n2. 可适用于所有的Off-Policy算法\n3. 提升了数据采样效率\n\n# 实验部分\n\n文中实验结果：[https://goo.gl/SMrQnI](https://goo.gl/SMrQnI)\n\n实验部分的完整细节请参考论文原文。\n\n## 环境\n\n- 7自由度机械臂\n- 模拟环境：MuJoCo\n- 任务分为3种\n  - Pushing，推：锁定机械臂的钳子，移动机械臂将物体推到目标点\n  - Sliding，滑动：类似于冰球运动，锁定机械臂的钳子，移动机械臂给与物体一个力，使物体可以在较光滑的桌面上滑动并且达到目标位置\n  - Pick-and-place，摆放：解锁钳子，使用机械臂夹起物体并移动至空中目标点\n\n![](./Hindsight-Experience-Replay/tasks.png)\n\n## 算法\n\n- DDPG\n- Adam优化器\n- 多层感知机MLPs\n- ReLU激活函数\n- 8核并行，更新参数后取平均\n- A-C网络都是3个隐藏层，每层64个隐节点，Actor输出层用tanh激活函数\n- 经验池大小为$10^{6}$，折扣因子$\\gamma=0.98$，学习率$\\alpha=0.001$，探索因子$\\epsilon = 0.2$\n\n> With probability 20% we sample (uniformly) a random action from the hypercube of valid actions. \n\nDDPG使用了随机探索机制\n\n## 训练结果\n\n### final模式与future模式对比\n\n![](./Hindsight-Experience-Replay/finalvsfuture.png)\n\n- 红色曲线为future模式，蓝色曲线为final模式，绿色曲线为使用了[count-based](https://arxiv.org/pdf/1703.01310.pdf)的DDPG，褐红色虚线为原始DDPG\n- 从左至右依次是Pushing，Sliding，Pick-and-place任务\n- 超参数$k=4$\n- 这个实验中，目标状态会变，即为多个目标状态\n\n结果分析：\n\n- future模式比final效果更好\n- 使用了count-based的DDPG智能稍微解决一下Sliding任务\n- 使用HER的DDPG可以完全胜任三个任务\n- 证明了HER是使从稀疏、二分奖励问题中学习成为可能的关键因素\n\n### 单个目标状态的实验\n\n![](./Hindsight-Experience-Replay/singlegoal.png)\n\n- 蓝色曲线为使用了HER的DDPG，文中并未说明HER是哪种模式，**猜测**是final模式，因为文中实验部分之前都是以final模式进行举例\n- 绿色曲线代表应用了count-based的DDPG，褐红色虚线为原始DDPG\n- 实验中，目标状态都为同一状态$g$\n\n结果分析：\n\n- DDPG+HER比原始DDPG的性能要好很多\n- **相比于多个目标的实验，可以发现，在多目标的任务中DDPG训练更快**，所以在实际中，即使我们只关心一个目标，我们最好也使用多个目标来训练\n\n### HER应用于reward shaping问题中\n\n前文已经说过，reward shaping可以简单理解为将奖励函数设置为某些变量的函数，如$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$，即奖励函数为当前状态与目标状态的欧氏距离的负数\n\n![](./Hindsight-Experience-Replay/rewardshape.png)\n\n- 奖励函数为$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$\n\n结果分析：\n\n- 无论使用怎样的reward shaping函数，DDPG、DDPG+HER都不能解决这个问题\n\n- 作者认为原因有二：\n\n  - > There is a huge discrepancy between what we optimize (i.e. a shaped reward function) and the success condition (i.e.: is the object within some radius from the goal at the end of the episode);  \n  \n    判定完成目标的条件和要优化的问题有巨大的矛盾（虽然我也不理解这到底是什么意思，索性就直接抄了过来）\n  \n  - > Shaped rewards penalize for inappropriate behaviour (e.g. moving the box in a wrong direction) which may hinder exploration. It can cause the agent to learn not to touch the box at all if it can not manipulate it precisely and we noticed such behaviour in some of our experiments. \n  \n    reward shaping阻碍了探索\n  \n- > Our results suggest that domain-agnostic reward shaping does not work well (at least in the simple forms we have tried). Of course for every problem there exists a reward which makes it easy (Ng et al., 1999) but designing such shaped rewards requires a lot of domain knowledge and may in some cases not be much easier than directly scripting the policy. This strengthens our belief that learning from sparse, binary rewards is an important problem. \n\n  研究结果表明，与领域无关的reward shaping效果并不好\n\n### 四种模式比较\n\n![](./Hindsight-Experience-Replay/fourmodel.png)\n\n- 红色代表future模式，蓝色代表final模式，绿色代表episode模式，紫色代表episode模式，褐红色虚线代表原始DDPG\n- 横坐标代表超参数$k$，第一行三个图的纵坐标代表最高得分，第二行三个图的纵坐标代表平均得分\n\n结果分析：\n\n- 效果：future>final>episode>random>no HER\n\n- 稳定性：final(好)=no-HER(差)>future>episode>random\n\n- future模式是唯一一个可以解决Sliding任务的，在$k=4$或者$k=8$时效果最好\n\n- 增大$k$超过8会使性能有所下降，主要是因为$k$过大导致经验池中原始真实数据所占的比例太小\n\n- > It confirms that the most valuable goals for replay are the ones which are going to be achieved in the near future \n\n  它证实了回放经验中最有价值的目标是那些在不久的将来能实现的目标\n\n*注：作者根据 future 模式提出了最近邻的 future 模式，即把$g'$设置为$s_{t+1}$，并且进行了实验，实验结果不如 future 模式。*","source":"_posts/Hindsight-Experience-Replay.md","raw":"---\ntitle: Hindsight Experience Replay\ncopyright: true\nmathjax: true\ntop: 1\ndate: 2019-05-28 18:38:56\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n本文介绍了一个“事后诸葛亮”的经验池机制，简称为**HER**，它可以很好地应用于**稀疏奖励**和**二分奖励**的问题中，不需要复杂的奖励函数工程设计。\n\n推荐：\n\n- 稀疏奖励问题的一种解决方案\n- 通俗易懂\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf](https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf)\n\n>Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL).  \n\n强化学习问题中最棘手的问题之一就是稀疏奖励。\n\n本文提出了一个新颖的技术：Hindsight Experience Replay（HER），可以从稀疏、二分的奖励问题中高效采样并进行学习，而且可以应用于**所有的Off-Policy**算法中。\n\n![](./Hindsight-Experience-Replay/hindsight.png)\n\nHindsight意为事后，结合强化学习中序贯决策问题的特性，我们很容易就可以猜想到，“事后”要不然指的是在状态s下执行动作a之后，要不然指的就是当一个episode结束之后。其实，文中对常规经验池的改进也正是运用了这样的含义。\n\n> HER lets an agent learn from undesired outcomes and tackles the problem of sparse rewards in Reinforcement Learning (RL).——Zhao, R., & Tresp, V. (2018). Energy-Based Hindsight Experience Prioritization. *CoRL*.\n\nHER使智能体从没达到的结果中去学习，解决了强化学习中稀疏奖励的问题。\n\n## 二分奖励 binary reward\n\n简言之，完成目标为一个值，没完成目标为另一个值。如：\n\n- $S_{T}=Goal，r=0$\n- $S\\neq Goal, r=-1. for \\ S \\in \\mathbb{S}$\n\n## 稀疏奖励 sparse reward\n\n简言之，完成目标的episode太少或者完成目标的步数太长，导致负奖励的样本数过多\n\n# 文中精要\n\n在机器人领域，要想使强化学习训练它完美执行某任务，往往需要设计合理的奖励函数，但是设计这样的奖励函数工程师不仅需要懂得强化学习的领域知识，也需要懂得机器人、运动学等领域的知识。而且，有这些知识也未必能设计出很好的奖励函数供智能体进行学习。因此，如果可以从简单的奖励函数（如二分奖励）学习到可完成任务的模型，那就不需要费心设计复杂的奖励函数了。\n\n文中介绍了一个例子来引入HER：\n\n- 名称：bit-flipping environment\n- 状态空间$\\mathcal{S}=\\left \\{ 0,1 \\right \\}^{n}$\n- 动作空间$\\mathcal{A}=\\left \\{ 0,1,\\cdots,n-1 \\right \\}$\n- 规则：对于每个episode，均匀采样长度为$n$的初始状态$s_{0}$（如$n=5，s_{0}=10101$）和目标状态$s_{g}$，每一步从动作空间中选取一个动作$a$，翻转$s_{0}$第$a$个位置的值，如$a=1\\Rightarrow s_{1}=11101$，直到回合结束或者翻转后的状态与$s_{g}$相同\n- 奖励函数：$r_{g}(s,a)=-\\left [ s \\neq g \\right ]$，即达到目标状态则为0，未达到目标状态则为-1。这个很容易理解，$s \\neq g \\Rightarrow true \\doteq 1，s = g \\Rightarrow false \\doteq 0$\n\n*注：下文如无特殊说明，$g$即表示目标状态$s_{g}$*\n\n> Standard RL algorithms are bound to fail in this environment for n > 40 because they will never experience any reward other than -1. Notice that using techniques for improving exploration (e.g. VIME (Houthooft et al., 2016), count-based exploration (Ostrovski et al., 2017) or bootstrapped DQN (Osband et al., 2016)) does not help here because the real problem is not in lack of diversity of states being visited, rather it is simply impractical to explore such a large state space.  \n\n当序列长度$n$大于40时，传统的强化学习算法就算有各种探索机制的加持，也不能学会解决这个问题，因为这个问题完全不是缺乏探索，而是**状态太多，探索不完**，导致奖励极其稀疏，算法根本不知道需要优化的目标在哪里。\n\n为了解决这个问题，作者指出了两个思路：\n\n1. 使用shaped reward（简言之，将reward设计成某些变量的函数，如$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$，即奖励函数为当前状态与目标状态的欧氏距离的负数），将训练的算法逐步引导至奖励函数增大的决策空间。但是这种方法可能很难应用于复杂的问题中。\n2. 使用HER——事后经验池机制\n\n## HER\n\n> The pivotal idea behind our approach is to re-examine this trajectory with a different goal — while this trajectory may not help us learn how to achieve the state g, it definitely tells us something about how to achieve the state $s_{T}$ .\n\nHER的主要思想就是：**为什么一定要考虑我们设定的目标呢？假设我们想让一个智能体学会移动到某个位置，它在一个episode中没有学到移动到目标位置就算失败吗？假定序列为$s_{0},s_{1},s_{2}, \\cdots ,s_{T}$，目标为$g$，我们何不换一种思路考虑：如果我们在episode开始前就将目标状态$g$设置为$s_{T}$，即$g=s_{T}$，那么这样看来智能体不就算是完成目标了吗？**\n\n![](./Hindsight-Experience-Replay/Her.png)\n\nHER就是运用了这个思想对经验池进行了扩充，将稀疏奖励问题给转化成非稀疏奖励，大大的扩展了经验池中完成任务的经验数量。\n\nHER主要特点：\n\n- 传统经验池存入的是状态$s$，而HER存入的是$s||g$，也就是`tf.concat(s,g)`\n- 训练算法的输入也是$s||g$，也就是需要在当前状态后边连结上**每个episode的**目标状态，每个episode的目标状态可能不同\n- HER对经验池进行了扩充，不仅存入实际采样得到的transition/experience，$\\left ( s_{t}||g,a_{t},r_{t},s_{t+1}||g \\right )$，也要在回合结束时**重新设置目标状态**，得到相应的奖励值（在二分奖励问题中，只有在$s=g$时奖励才需要更改），存入“事后”（当初如果这样就好啦！）的经验$\\left ( s_{t}||g',a_{t},r_{t}',s_{t+1}||g' \\right )$，详见伪代码，这个事后经验究竟存入多少份、多少种，由超参数$k$控制，下文讲解。\n- HER更适合解决多目标问题，多目标的意思为，目标点非固定，每个episode的目标状态可以不相同。详见实验部分\n\nHER的几种扩展方式：\n\n> future — replay with k random states which come from the same episode as the transition being replayed and were observed after it,\n> episode — replay with k random states coming from the same episode as the transition being replayed,\n> random — replay with k random states encountered so far in the whole training procedure.\n\n- 未来模式——future：在一个序列$s_{0},s_{1},s_{2},\\cdots,s_{T}$中，如果遍历到状态$s_{2}$，则在$s_{3},\\cdots,s_{T}$之间随机抽取$k$个状态作为目标状态$g'$，并依此向经验池中存入$\\left ( s_{2}||g',a_{2},r_{2}',s_{3}||g' \\right )$，**特点：一个episode的后续部分**\n- 回合模式——episode：在一个序列$s_{0},s_{1},s_{2},...,s_{T}$中，如果遍历到状态$s_{2}$，则在整个序列中随机抽取$k$个状态作为目标状态$g'$，并依此向经验池中存入$\\left ( s_{2}||g',a_{2},r_{2}',s_{3}||g' \\right )$，**特点：一个episode**\n\n- 随机模式——random：在一个序列$s_{0},s_{1},s_{2},...,s_{T}$中，如果遍历到状态$s_{2}$，则在多个序列$\\tau_{0},\\tau_{1},\\tau_{2},\\cdots$中随机抽取$k$个状态作为目标状态$g'$，并依此向经验池中存入$\\left ( s_{2}||g',a_{2},r_{2}',s_{3}||g' \\right )$，**特点：多个episode**\n\n- 最终模式——final：在一个序列$s_{0},s_{1},s_{2},\\cdots,s_{T}$中，如果遍历到状态$s_{2}$，则之间令$g'=s_{T}$，并向经验池中存入$\\left ( s_{2}||g',a_{2},r_{2}',s_{3}||g' \\right )$，**特点：一个episode的最后一个状态，如果设置k，则存入k个相同的经验**\n\n## 伪代码\n\n![](./Hindsight-Experience-Replay/pseudo.png)\n\n解析：\n\n1. 伪代码中没有提到超参数$k$，其实在循环条件$\\textbf{for} \\ g' \\in G \\ \\textbf{do}$中循环执行了$k$次\n2. $||$操作为连结操作，简言之，将两个长度为5的向量合并成一个长度为10的向量\n3. $G:=\\mathbb{S}(\\textbf{current episode})$即为上文提到的四种扩展模式：future、episode、random、final。\n4. 奖励函数$r(s,a,g)=-\\left [ f_{g}(s)=0 \\right ]$即为前文提到的$r_{g}(s,a)=-\\left [ s \\neq g \\right ]$，即完成为0，未完成为-1，具体奖励函数可以根据我们的使用环境设计\n5. $a_{t} \\leftarrow \\pi_{b}(s_{t}||g)$表示神经网络的输入为当前状态与目标状态的连结\n\n## HER的优点\n\n1. 可解决稀疏奖励、二分奖励问题\n2. 可适用于所有的Off-Policy算法\n3. 提升了数据采样效率\n\n# 实验部分\n\n文中实验结果：[https://goo.gl/SMrQnI](https://goo.gl/SMrQnI)\n\n实验部分的完整细节请参考论文原文。\n\n## 环境\n\n- 7自由度机械臂\n- 模拟环境：MuJoCo\n- 任务分为3种\n  - Pushing，推：锁定机械臂的钳子，移动机械臂将物体推到目标点\n  - Sliding，滑动：类似于冰球运动，锁定机械臂的钳子，移动机械臂给与物体一个力，使物体可以在较光滑的桌面上滑动并且达到目标位置\n  - Pick-and-place，摆放：解锁钳子，使用机械臂夹起物体并移动至空中目标点\n\n![](./Hindsight-Experience-Replay/tasks.png)\n\n## 算法\n\n- DDPG\n- Adam优化器\n- 多层感知机MLPs\n- ReLU激活函数\n- 8核并行，更新参数后取平均\n- A-C网络都是3个隐藏层，每层64个隐节点，Actor输出层用tanh激活函数\n- 经验池大小为$10^{6}$，折扣因子$\\gamma=0.98$，学习率$\\alpha=0.001$，探索因子$\\epsilon = 0.2$\n\n> With probability 20% we sample (uniformly) a random action from the hypercube of valid actions. \n\nDDPG使用了随机探索机制\n\n## 训练结果\n\n### final模式与future模式对比\n\n![](./Hindsight-Experience-Replay/finalvsfuture.png)\n\n- 红色曲线为future模式，蓝色曲线为final模式，绿色曲线为使用了[count-based](https://arxiv.org/pdf/1703.01310.pdf)的DDPG，褐红色虚线为原始DDPG\n- 从左至右依次是Pushing，Sliding，Pick-and-place任务\n- 超参数$k=4$\n- 这个实验中，目标状态会变，即为多个目标状态\n\n结果分析：\n\n- future模式比final效果更好\n- 使用了count-based的DDPG智能稍微解决一下Sliding任务\n- 使用HER的DDPG可以完全胜任三个任务\n- 证明了HER是使从稀疏、二分奖励问题中学习成为可能的关键因素\n\n### 单个目标状态的实验\n\n![](./Hindsight-Experience-Replay/singlegoal.png)\n\n- 蓝色曲线为使用了HER的DDPG，文中并未说明HER是哪种模式，**猜测**是final模式，因为文中实验部分之前都是以final模式进行举例\n- 绿色曲线代表应用了count-based的DDPG，褐红色虚线为原始DDPG\n- 实验中，目标状态都为同一状态$g$\n\n结果分析：\n\n- DDPG+HER比原始DDPG的性能要好很多\n- **相比于多个目标的实验，可以发现，在多目标的任务中DDPG训练更快**，所以在实际中，即使我们只关心一个目标，我们最好也使用多个目标来训练\n\n### HER应用于reward shaping问题中\n\n前文已经说过，reward shaping可以简单理解为将奖励函数设置为某些变量的函数，如$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$，即奖励函数为当前状态与目标状态的欧氏距离的负数\n\n![](./Hindsight-Experience-Replay/rewardshape.png)\n\n- 奖励函数为$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$\n\n结果分析：\n\n- 无论使用怎样的reward shaping函数，DDPG、DDPG+HER都不能解决这个问题\n\n- 作者认为原因有二：\n\n  - > There is a huge discrepancy between what we optimize (i.e. a shaped reward function) and the success condition (i.e.: is the object within some radius from the goal at the end of the episode);  \n  \n    判定完成目标的条件和要优化的问题有巨大的矛盾（虽然我也不理解这到底是什么意思，索性就直接抄了过来）\n  \n  - > Shaped rewards penalize for inappropriate behaviour (e.g. moving the box in a wrong direction) which may hinder exploration. It can cause the agent to learn not to touch the box at all if it can not manipulate it precisely and we noticed such behaviour in some of our experiments. \n  \n    reward shaping阻碍了探索\n  \n- > Our results suggest that domain-agnostic reward shaping does not work well (at least in the simple forms we have tried). Of course for every problem there exists a reward which makes it easy (Ng et al., 1999) but designing such shaped rewards requires a lot of domain knowledge and may in some cases not be much easier than directly scripting the policy. This strengthens our belief that learning from sparse, binary rewards is an important problem. \n\n  研究结果表明，与领域无关的reward shaping效果并不好\n\n### 四种模式比较\n\n![](./Hindsight-Experience-Replay/fourmodel.png)\n\n- 红色代表future模式，蓝色代表final模式，绿色代表episode模式，紫色代表episode模式，褐红色虚线代表原始DDPG\n- 横坐标代表超参数$k$，第一行三个图的纵坐标代表最高得分，第二行三个图的纵坐标代表平均得分\n\n结果分析：\n\n- 效果：future>final>episode>random>no HER\n\n- 稳定性：final(好)=no-HER(差)>future>episode>random\n\n- future模式是唯一一个可以解决Sliding任务的，在$k=4$或者$k=8$时效果最好\n\n- 增大$k$超过8会使性能有所下降，主要是因为$k$过大导致经验池中原始真实数据所占的比例太小\n\n- > It confirms that the most valuable goals for replay are the ones which are going to be achieved in the near future \n\n  它证实了回放经验中最有价值的目标是那些在不久的将来能实现的目标\n\n*注：作者根据 future 模式提出了最近邻的 future 模式，即把$g'$设置为$s_{t+1}$，并且进行了实验，实验结果不如 future 模式。*","slug":"Hindsight-Experience-Replay","published":1,"updated":"2019-05-30T07:57:28.345Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma27002dekvez4bso9fs","content":"<p>本文介绍了一个“事后诸葛亮”的经验池机制，简称为<strong>HER</strong>，它可以很好地应用于<strong>稀疏奖励</strong>和<strong>二分奖励</strong>的问题中，不需要复杂的奖励函数工程设计。</p>\n<p>推荐：</p>\n<ul>\n<li>稀疏奖励问题的一种解决方案</li>\n<li>通俗易懂</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf\" rel=\"external nofollow\" target=\"_blank\">https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf</a></p>\n<blockquote>\n<p>Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL).  </p>\n</blockquote>\n<p>强化学习问题中最棘手的问题之一就是稀疏奖励。</p>\n<p>本文提出了一个新颖的技术：Hindsight Experience Replay（HER），可以从稀疏、二分的奖励问题中高效采样并进行学习，而且可以应用于<strong>所有的Off-Policy</strong>算法中。</p>\n<p><img src=\"./Hindsight-Experience-Replay/hindsight.png\" alt=\"\"></p>\n<p>Hindsight意为事后，结合强化学习中序贯决策问题的特性，我们很容易就可以猜想到，“事后”要不然指的是在状态s下执行动作a之后，要不然指的就是当一个episode结束之后。其实，文中对常规经验池的改进也正是运用了这样的含义。</p>\n<blockquote>\n<p>HER lets an agent learn from undesired outcomes and tackles the problem of sparse rewards in Reinforcement Learning (RL).——Zhao, R., &amp; Tresp, V. (2018). Energy-Based Hindsight Experience Prioritization. <em>CoRL</em>.</p>\n</blockquote>\n<p>HER使智能体从没达到的结果中去学习，解决了强化学习中稀疏奖励的问题。</p>\n<h2 id=\"二分奖励-binary-reward\"><a href=\"#二分奖励-binary-reward\" class=\"headerlink\" title=\"二分奖励 binary reward\"></a>二分奖励 binary reward</h2><p>简言之，完成目标为一个值，没完成目标为另一个值。如：</p>\n<ul>\n<li>$S_{T}=Goal，r=0$</li>\n<li>$S\\neq Goal, r=-1. for \\ S \\in \\mathbb{S}$</li>\n</ul>\n<h2 id=\"稀疏奖励-sparse-reward\"><a href=\"#稀疏奖励-sparse-reward\" class=\"headerlink\" title=\"稀疏奖励 sparse reward\"></a>稀疏奖励 sparse reward</h2><p>简言之，完成目标的episode太少或者完成目标的步数太长，导致负奖励的样本数过多</p>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><p>在机器人领域，要想使强化学习训练它完美执行某任务，往往需要设计合理的奖励函数，但是设计这样的奖励函数工程师不仅需要懂得强化学习的领域知识，也需要懂得机器人、运动学等领域的知识。而且，有这些知识也未必能设计出很好的奖励函数供智能体进行学习。因此，如果可以从简单的奖励函数（如二分奖励）学习到可完成任务的模型，那就不需要费心设计复杂的奖励函数了。</p>\n<p>文中介绍了一个例子来引入HER：</p>\n<ul>\n<li>名称：bit-flipping environment</li>\n<li>状态空间$\\mathcal{S}=\\left \\{ 0,1 \\right \\}^{n}$</li>\n<li>动作空间$\\mathcal{A}=\\left \\{ 0,1,\\cdots,n-1 \\right \\}$</li>\n<li>规则：对于每个episode，均匀采样长度为$n$的初始状态$s_{0}$（如$n=5，s_{0}=10101$）和目标状态$s_{g}$，每一步从动作空间中选取一个动作$a$，翻转$s_{0}$第$a$个位置的值，如$a=1\\Rightarrow s_{1}=11101$，直到回合结束或者翻转后的状态与$s_{g}$相同</li>\n<li>奖励函数：$r_{g}(s,a)=-\\left [ s \\neq g \\right ]$，即达到目标状态则为0，未达到目标状态则为-1。这个很容易理解，$s \\neq g \\Rightarrow true \\doteq 1，s = g \\Rightarrow false \\doteq 0$</li>\n</ul>\n<p><em>注：下文如无特殊说明，$g$即表示目标状态$s_{g}$</em></p>\n<blockquote>\n<p>Standard RL algorithms are bound to fail in this environment for n &gt; 40 because they will never experience any reward other than -1. Notice that using techniques for improving exploration (e.g. VIME (Houthooft et al., 2016), count-based exploration (Ostrovski et al., 2017) or bootstrapped DQN (Osband et al., 2016)) does not help here because the real problem is not in lack of diversity of states being visited, rather it is simply impractical to explore such a large state space.  </p>\n</blockquote>\n<p>当序列长度$n$大于40时，传统的强化学习算法就算有各种探索机制的加持，也不能学会解决这个问题，因为这个问题完全不是缺乏探索，而是<strong>状态太多，探索不完</strong>，导致奖励极其稀疏，算法根本不知道需要优化的目标在哪里。</p>\n<p>为了解决这个问题，作者指出了两个思路：</p>\n<ol>\n<li>使用shaped reward（简言之，将reward设计成某些变量的函数，如$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$，即奖励函数为当前状态与目标状态的欧氏距离的负数），将训练的算法逐步引导至奖励函数增大的决策空间。但是这种方法可能很难应用于复杂的问题中。</li>\n<li>使用HER——事后经验池机制</li>\n</ol>\n<h2 id=\"HER\"><a href=\"#HER\" class=\"headerlink\" title=\"HER\"></a>HER</h2><blockquote>\n<p>The pivotal idea behind our approach is to re-examine this trajectory with a different goal — while this trajectory may not help us learn how to achieve the state g, it definitely tells us something about how to achieve the state $s_{T}$ .</p>\n</blockquote>\n<p>HER的主要思想就是：<strong>为什么一定要考虑我们设定的目标呢？假设我们想让一个智能体学会移动到某个位置，它在一个episode中没有学到移动到目标位置就算失败吗？假定序列为$s_{0},s_{1},s_{2}, \\cdots ,s_{T}$，目标为$g$，我们何不换一种思路考虑：如果我们在episode开始前就将目标状态$g$设置为$s_{T}$，即$g=s_{T}$，那么这样看来智能体不就算是完成目标了吗？</strong></p>\n<p><img src=\"./Hindsight-Experience-Replay/Her.png\" alt=\"\"></p>\n<p>HER就是运用了这个思想对经验池进行了扩充，将稀疏奖励问题给转化成非稀疏奖励，大大的扩展了经验池中完成任务的经验数量。</p>\n<p>HER主要特点：</p>\n<ul>\n<li>传统经验池存入的是状态$s$，而HER存入的是$s||g$，也就是<code>tf.concat(s,g)</code></li>\n<li>训练算法的输入也是$s||g$，也就是需要在当前状态后边连结上<strong>每个episode的</strong>目标状态，每个episode的目标状态可能不同</li>\n<li>HER对经验池进行了扩充，不仅存入实际采样得到的transition/experience，$\\left ( s_{t}||g,a_{t},r_{t},s_{t+1}||g \\right )$，也要在回合结束时<strong>重新设置目标状态</strong>，得到相应的奖励值（在二分奖励问题中，只有在$s=g$时奖励才需要更改），存入“事后”（当初如果这样就好啦！）的经验$\\left ( s_{t}||g’,a_{t},r_{t}’,s_{t+1}||g’ \\right )$，详见伪代码，这个事后经验究竟存入多少份、多少种，由超参数$k$控制，下文讲解。</li>\n<li>HER更适合解决多目标问题，多目标的意思为，目标点非固定，每个episode的目标状态可以不相同。详见实验部分</li>\n</ul>\n<p>HER的几种扩展方式：</p>\n<blockquote>\n<p>future — replay with k random states which come from the same episode as the transition being replayed and were observed after it,<br>episode — replay with k random states coming from the same episode as the transition being replayed,<br>random — replay with k random states encountered so far in the whole training procedure.</p>\n</blockquote>\n<ul>\n<li>未来模式——future：在一个序列$s_{0},s_{1},s_{2},\\cdots,s_{T}$中，如果遍历到状态$s_{2}$，则在$s_{3},\\cdots,s_{T}$之间随机抽取$k$个状态作为目标状态$g’$，并依此向经验池中存入$\\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \\right )$，<strong>特点：一个episode的后续部分</strong></li>\n<li><p>回合模式——episode：在一个序列$s_{0},s_{1},s_{2},…,s_{T}$中，如果遍历到状态$s_{2}$，则在整个序列中随机抽取$k$个状态作为目标状态$g’$，并依此向经验池中存入$\\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \\right )$，<strong>特点：一个episode</strong></p>\n</li>\n<li><p>随机模式——random：在一个序列$s_{0},s_{1},s_{2},…,s_{T}$中，如果遍历到状态$s_{2}$，则在多个序列$\\tau_{0},\\tau_{1},\\tau_{2},\\cdots$中随机抽取$k$个状态作为目标状态$g’$，并依此向经验池中存入$\\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \\right )$，<strong>特点：多个episode</strong></p>\n</li>\n<li><p>最终模式——final：在一个序列$s_{0},s_{1},s_{2},\\cdots,s_{T}$中，如果遍历到状态$s_{2}$，则之间令$g’=s_{T}$，并向经验池中存入$\\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \\right )$，<strong>特点：一个episode的最后一个状态，如果设置k，则存入k个相同的经验</strong></p>\n</li>\n</ul>\n<h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./Hindsight-Experience-Replay/pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ol>\n<li>伪代码中没有提到超参数$k$，其实在循环条件$\\textbf{for} \\ g’ \\in G \\ \\textbf{do}$中循环执行了$k$次</li>\n<li>$||$操作为连结操作，简言之，将两个长度为5的向量合并成一个长度为10的向量</li>\n<li>$G:=\\mathbb{S}(\\textbf{current episode})$即为上文提到的四种扩展模式：future、episode、random、final。</li>\n<li>奖励函数$r(s,a,g)=-\\left [ f_{g}(s)=0 \\right ]$即为前文提到的$r_{g}(s,a)=-\\left [ s \\neq g \\right ]$，即完成为0，未完成为-1，具体奖励函数可以根据我们的使用环境设计</li>\n<li>$a_{t} \\leftarrow \\pi_{b}(s_{t}||g)$表示神经网络的输入为当前状态与目标状态的连结</li>\n</ol>\n<h2 id=\"HER的优点\"><a href=\"#HER的优点\" class=\"headerlink\" title=\"HER的优点\"></a>HER的优点</h2><ol>\n<li>可解决稀疏奖励、二分奖励问题</li>\n<li>可适用于所有的Off-Policy算法</li>\n<li>提升了数据采样效率</li>\n</ol>\n<h1 id=\"实验部分\"><a href=\"#实验部分\" class=\"headerlink\" title=\"实验部分\"></a>实验部分</h1><p>文中实验结果：<a href=\"https://goo.gl/SMrQnI\" rel=\"external nofollow\" target=\"_blank\">https://goo.gl/SMrQnI</a></p>\n<p>实验部分的完整细节请参考论文原文。</p>\n<h2 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h2><ul>\n<li>7自由度机械臂</li>\n<li>模拟环境：MuJoCo</li>\n<li>任务分为3种<ul>\n<li>Pushing，推：锁定机械臂的钳子，移动机械臂将物体推到目标点</li>\n<li>Sliding，滑动：类似于冰球运动，锁定机械臂的钳子，移动机械臂给与物体一个力，使物体可以在较光滑的桌面上滑动并且达到目标位置</li>\n<li>Pick-and-place，摆放：解锁钳子，使用机械臂夹起物体并移动至空中目标点</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"./Hindsight-Experience-Replay/tasks.png\" alt=\"\"></p>\n<h2 id=\"算法\"><a href=\"#算法\" class=\"headerlink\" title=\"算法\"></a>算法</h2><ul>\n<li>DDPG</li>\n<li>Adam优化器</li>\n<li>多层感知机MLPs</li>\n<li>ReLU激活函数</li>\n<li>8核并行，更新参数后取平均</li>\n<li>A-C网络都是3个隐藏层，每层64个隐节点，Actor输出层用tanh激活函数</li>\n<li>经验池大小为$10^{6}$，折扣因子$\\gamma=0.98$，学习率$\\alpha=0.001$，探索因子$\\epsilon = 0.2$</li>\n</ul>\n<blockquote>\n<p>With probability 20% we sample (uniformly) a random action from the hypercube of valid actions. </p>\n</blockquote>\n<p>DDPG使用了随机探索机制</p>\n<h2 id=\"训练结果\"><a href=\"#训练结果\" class=\"headerlink\" title=\"训练结果\"></a>训练结果</h2><h3 id=\"final模式与future模式对比\"><a href=\"#final模式与future模式对比\" class=\"headerlink\" title=\"final模式与future模式对比\"></a>final模式与future模式对比</h3><p><img src=\"./Hindsight-Experience-Replay/finalvsfuture.png\" alt=\"\"></p>\n<ul>\n<li>红色曲线为future模式，蓝色曲线为final模式，绿色曲线为使用了<a href=\"https://arxiv.org/pdf/1703.01310.pdf\" rel=\"external nofollow\" target=\"_blank\">count-based</a>的DDPG，褐红色虚线为原始DDPG</li>\n<li>从左至右依次是Pushing，Sliding，Pick-and-place任务</li>\n<li>超参数$k=4$</li>\n<li>这个实验中，目标状态会变，即为多个目标状态</li>\n</ul>\n<p>结果分析：</p>\n<ul>\n<li>future模式比final效果更好</li>\n<li>使用了count-based的DDPG智能稍微解决一下Sliding任务</li>\n<li>使用HER的DDPG可以完全胜任三个任务</li>\n<li>证明了HER是使从稀疏、二分奖励问题中学习成为可能的关键因素</li>\n</ul>\n<h3 id=\"单个目标状态的实验\"><a href=\"#单个目标状态的实验\" class=\"headerlink\" title=\"单个目标状态的实验\"></a>单个目标状态的实验</h3><p><img src=\"./Hindsight-Experience-Replay/singlegoal.png\" alt=\"\"></p>\n<ul>\n<li>蓝色曲线为使用了HER的DDPG，文中并未说明HER是哪种模式，<strong>猜测</strong>是final模式，因为文中实验部分之前都是以final模式进行举例</li>\n<li>绿色曲线代表应用了count-based的DDPG，褐红色虚线为原始DDPG</li>\n<li>实验中，目标状态都为同一状态$g$</li>\n</ul>\n<p>结果分析：</p>\n<ul>\n<li>DDPG+HER比原始DDPG的性能要好很多</li>\n<li><strong>相比于多个目标的实验，可以发现，在多目标的任务中DDPG训练更快</strong>，所以在实际中，即使我们只关心一个目标，我们最好也使用多个目标来训练</li>\n</ul>\n<h3 id=\"HER应用于reward-shaping问题中\"><a href=\"#HER应用于reward-shaping问题中\" class=\"headerlink\" title=\"HER应用于reward shaping问题中\"></a>HER应用于reward shaping问题中</h3><p>前文已经说过，reward shaping可以简单理解为将奖励函数设置为某些变量的函数，如$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$，即奖励函数为当前状态与目标状态的欧氏距离的负数</p>\n<p><img src=\"./Hindsight-Experience-Replay/rewardshape.png\" alt=\"\"></p>\n<ul>\n<li>奖励函数为$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$</li>\n</ul>\n<p>结果分析：</p>\n<ul>\n<li><p>无论使用怎样的reward shaping函数，DDPG、DDPG+HER都不能解决这个问题</p>\n</li>\n<li><p>作者认为原因有二：</p>\n<ul>\n<li><blockquote>\n<p>There is a huge discrepancy between what we optimize (i.e. a shaped reward function) and the success condition (i.e.: is the object within some radius from the goal at the end of the episode);  </p>\n</blockquote>\n<p>判定完成目标的条件和要优化的问题有巨大的矛盾（虽然我也不理解这到底是什么意思，索性就直接抄了过来）</p>\n</li>\n<li><blockquote>\n<p>Shaped rewards penalize for inappropriate behaviour (e.g. moving the box in a wrong direction) which may hinder exploration. It can cause the agent to learn not to touch the box at all if it can not manipulate it precisely and we noticed such behaviour in some of our experiments. </p>\n</blockquote>\n<p>reward shaping阻碍了探索</p>\n</li>\n</ul>\n</li>\n<li><blockquote>\n<p>Our results suggest that domain-agnostic reward shaping does not work well (at least in the simple forms we have tried). Of course for every problem there exists a reward which makes it easy (Ng et al., 1999) but designing such shaped rewards requires a lot of domain knowledge and may in some cases not be much easier than directly scripting the policy. This strengthens our belief that learning from sparse, binary rewards is an important problem. </p>\n</blockquote>\n<p>研究结果表明，与领域无关的reward shaping效果并不好</p>\n</li>\n</ul>\n<h3 id=\"四种模式比较\"><a href=\"#四种模式比较\" class=\"headerlink\" title=\"四种模式比较\"></a>四种模式比较</h3><p><img src=\"./Hindsight-Experience-Replay/fourmodel.png\" alt=\"\"></p>\n<ul>\n<li>红色代表future模式，蓝色代表final模式，绿色代表episode模式，紫色代表episode模式，褐红色虚线代表原始DDPG</li>\n<li>横坐标代表超参数$k$，第一行三个图的纵坐标代表最高得分，第二行三个图的纵坐标代表平均得分</li>\n</ul>\n<p>结果分析：</p>\n<ul>\n<li><p>效果：future&gt;final&gt;episode&gt;random&gt;no HER</p>\n</li>\n<li><p>稳定性：final(好)=no-HER(差)&gt;future&gt;episode&gt;random</p>\n</li>\n<li><p>future模式是唯一一个可以解决Sliding任务的，在$k=4$或者$k=8$时效果最好</p>\n</li>\n<li><p>增大$k$超过8会使性能有所下降，主要是因为$k$过大导致经验池中原始真实数据所占的比例太小</p>\n</li>\n<li><blockquote>\n<p>It confirms that the most valuable goals for replay are the ones which are going to be achieved in the near future </p>\n</blockquote>\n<p>它证实了回放经验中最有价值的目标是那些在不久的将来能实现的目标</p>\n</li>\n</ul>\n<p><em>注：作者根据 future 模式提出了最近邻的 future 模式，即把$g’$设置为$s_{t+1}$，并且进行了实验，实验结果不如 future 模式。</em></p>\n","site":{"data":{}},"excerpt":"<p>本文介绍了一个“事后诸葛亮”的经验池机制，简称为<strong>HER</strong>，它可以很好地应用于<strong>稀疏奖励</strong>和<strong>二分奖励</strong>的问题中，不需要复杂的奖励函数工程设计。</p>\n<p>推荐：</p>\n<ul>\n<li>稀疏奖励问题的一种解决方案</li>\n<li>通俗易懂</li>\n</ul>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf\" rel=\"external nofollow\" target=\"_blank\">https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf</a></p>\n<blockquote>\n<p>Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL).  </p>\n</blockquote>\n<p>强化学习问题中最棘手的问题之一就是稀疏奖励。</p>\n<p>本文提出了一个新颖的技术：Hindsight Experience Replay（HER），可以从稀疏、二分的奖励问题中高效采样并进行学习，而且可以应用于<strong>所有的Off-Policy</strong>算法中。</p>\n<p><img src=\"./Hindsight-Experience-Replay/hindsight.png\" alt=\"\"></p>\n<p>Hindsight意为事后，结合强化学习中序贯决策问题的特性，我们很容易就可以猜想到，“事后”要不然指的是在状态s下执行动作a之后，要不然指的就是当一个episode结束之后。其实，文中对常规经验池的改进也正是运用了这样的含义。</p>\n<blockquote>\n<p>HER lets an agent learn from undesired outcomes and tackles the problem of sparse rewards in Reinforcement Learning (RL).——Zhao, R., &amp; Tresp, V. (2018). Energy-Based Hindsight Experience Prioritization. <em>CoRL</em>.</p>\n</blockquote>\n<p>HER使智能体从没达到的结果中去学习，解决了强化学习中稀疏奖励的问题。</p>\n<h2 id=\"二分奖励-binary-reward\"><a href=\"#二分奖励-binary-reward\" class=\"headerlink\" title=\"二分奖励 binary reward\"></a>二分奖励 binary reward</h2><p>简言之，完成目标为一个值，没完成目标为另一个值。如：</p>\n<ul>\n<li>$S_{T}=Goal，r=0$</li>\n<li>$S\\neq Goal, r=-1. for \\ S \\in \\mathbb{S}$</li>\n</ul>\n<h2 id=\"稀疏奖励-sparse-reward\"><a href=\"#稀疏奖励-sparse-reward\" class=\"headerlink\" title=\"稀疏奖励 sparse reward\"></a>稀疏奖励 sparse reward</h2><p>简言之，完成目标的episode太少或者完成目标的步数太长，导致负奖励的样本数过多</p>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><p>在机器人领域，要想使强化学习训练它完美执行某任务，往往需要设计合理的奖励函数，但是设计这样的奖励函数工程师不仅需要懂得强化学习的领域知识，也需要懂得机器人、运动学等领域的知识。而且，有这些知识也未必能设计出很好的奖励函数供智能体进行学习。因此，如果可以从简单的奖励函数（如二分奖励）学习到可完成任务的模型，那就不需要费心设计复杂的奖励函数了。</p>\n<p>文中介绍了一个例子来引入HER：</p>\n<ul>\n<li>名称：bit-flipping environment</li>\n<li>状态空间$\\mathcal{S}=\\left \\{ 0,1 \\right \\}^{n}$</li>\n<li>动作空间$\\mathcal{A}=\\left \\{ 0,1,\\cdots,n-1 \\right \\}$</li>\n<li>规则：对于每个episode，均匀采样长度为$n$的初始状态$s_{0}$（如$n=5，s_{0}=10101$）和目标状态$s_{g}$，每一步从动作空间中选取一个动作$a$，翻转$s_{0}$第$a$个位置的值，如$a=1\\Rightarrow s_{1}=11101$，直到回合结束或者翻转后的状态与$s_{g}$相同</li>\n<li>奖励函数：$r_{g}(s,a)=-\\left [ s \\neq g \\right ]$，即达到目标状态则为0，未达到目标状态则为-1。这个很容易理解，$s \\neq g \\Rightarrow true \\doteq 1，s = g \\Rightarrow false \\doteq 0$</li>\n</ul>\n<p><em>注：下文如无特殊说明，$g$即表示目标状态$s_{g}$</em></p>\n<blockquote>\n<p>Standard RL algorithms are bound to fail in this environment for n &gt; 40 because they will never experience any reward other than -1. Notice that using techniques for improving exploration (e.g. VIME (Houthooft et al., 2016), count-based exploration (Ostrovski et al., 2017) or bootstrapped DQN (Osband et al., 2016)) does not help here because the real problem is not in lack of diversity of states being visited, rather it is simply impractical to explore such a large state space.  </p>\n</blockquote>\n<p>当序列长度$n$大于40时，传统的强化学习算法就算有各种探索机制的加持，也不能学会解决这个问题，因为这个问题完全不是缺乏探索，而是<strong>状态太多，探索不完</strong>，导致奖励极其稀疏，算法根本不知道需要优化的目标在哪里。</p>\n<p>为了解决这个问题，作者指出了两个思路：</p>\n<ol>\n<li>使用shaped reward（简言之，将reward设计成某些变量的函数，如$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$，即奖励函数为当前状态与目标状态的欧氏距离的负数），将训练的算法逐步引导至奖励函数增大的决策空间。但是这种方法可能很难应用于复杂的问题中。</li>\n<li>使用HER——事后经验池机制</li>\n</ol>\n<h2 id=\"HER\"><a href=\"#HER\" class=\"headerlink\" title=\"HER\"></a>HER</h2><blockquote>\n<p>The pivotal idea behind our approach is to re-examine this trajectory with a different goal — while this trajectory may not help us learn how to achieve the state g, it definitely tells us something about how to achieve the state $s_{T}$ .</p>\n</blockquote>\n<p>HER的主要思想就是：<strong>为什么一定要考虑我们设定的目标呢？假设我们想让一个智能体学会移动到某个位置，它在一个episode中没有学到移动到目标位置就算失败吗？假定序列为$s_{0},s_{1},s_{2}, \\cdots ,s_{T}$，目标为$g$，我们何不换一种思路考虑：如果我们在episode开始前就将目标状态$g$设置为$s_{T}$，即$g=s_{T}$，那么这样看来智能体不就算是完成目标了吗？</strong></p>\n<p><img src=\"./Hindsight-Experience-Replay/Her.png\" alt=\"\"></p>\n<p>HER就是运用了这个思想对经验池进行了扩充，将稀疏奖励问题给转化成非稀疏奖励，大大的扩展了经验池中完成任务的经验数量。</p>\n<p>HER主要特点：</p>\n<ul>\n<li>传统经验池存入的是状态$s$，而HER存入的是$s||g$，也就是<code>tf.concat(s,g)</code></li>\n<li>训练算法的输入也是$s||g$，也就是需要在当前状态后边连结上<strong>每个episode的</strong>目标状态，每个episode的目标状态可能不同</li>\n<li>HER对经验池进行了扩充，不仅存入实际采样得到的transition/experience，$\\left ( s_{t}||g,a_{t},r_{t},s_{t+1}||g \\right )$，也要在回合结束时<strong>重新设置目标状态</strong>，得到相应的奖励值（在二分奖励问题中，只有在$s=g$时奖励才需要更改），存入“事后”（当初如果这样就好啦！）的经验$\\left ( s_{t}||g’,a_{t},r_{t}’,s_{t+1}||g’ \\right )$，详见伪代码，这个事后经验究竟存入多少份、多少种，由超参数$k$控制，下文讲解。</li>\n<li>HER更适合解决多目标问题，多目标的意思为，目标点非固定，每个episode的目标状态可以不相同。详见实验部分</li>\n</ul>\n<p>HER的几种扩展方式：</p>\n<blockquote>\n<p>future — replay with k random states which come from the same episode as the transition being replayed and were observed after it,<br>episode — replay with k random states coming from the same episode as the transition being replayed,<br>random — replay with k random states encountered so far in the whole training procedure.</p>\n</blockquote>\n<ul>\n<li>未来模式——future：在一个序列$s_{0},s_{1},s_{2},\\cdots,s_{T}$中，如果遍历到状态$s_{2}$，则在$s_{3},\\cdots,s_{T}$之间随机抽取$k$个状态作为目标状态$g’$，并依此向经验池中存入$\\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \\right )$，<strong>特点：一个episode的后续部分</strong></li>\n<li><p>回合模式——episode：在一个序列$s_{0},s_{1},s_{2},…,s_{T}$中，如果遍历到状态$s_{2}$，则在整个序列中随机抽取$k$个状态作为目标状态$g’$，并依此向经验池中存入$\\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \\right )$，<strong>特点：一个episode</strong></p>\n</li>\n<li><p>随机模式——random：在一个序列$s_{0},s_{1},s_{2},…,s_{T}$中，如果遍历到状态$s_{2}$，则在多个序列$\\tau_{0},\\tau_{1},\\tau_{2},\\cdots$中随机抽取$k$个状态作为目标状态$g’$，并依此向经验池中存入$\\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \\right )$，<strong>特点：多个episode</strong></p>\n</li>\n<li><p>最终模式——final：在一个序列$s_{0},s_{1},s_{2},\\cdots,s_{T}$中，如果遍历到状态$s_{2}$，则之间令$g’=s_{T}$，并向经验池中存入$\\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \\right )$，<strong>特点：一个episode的最后一个状态，如果设置k，则存入k个相同的经验</strong></p>\n</li>\n</ul>\n<h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./Hindsight-Experience-Replay/pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ol>\n<li>伪代码中没有提到超参数$k$，其实在循环条件$\\textbf{for} \\ g’ \\in G \\ \\textbf{do}$中循环执行了$k$次</li>\n<li>$||$操作为连结操作，简言之，将两个长度为5的向量合并成一个长度为10的向量</li>\n<li>$G:=\\mathbb{S}(\\textbf{current episode})$即为上文提到的四种扩展模式：future、episode、random、final。</li>\n<li>奖励函数$r(s,a,g)=-\\left [ f_{g}(s)=0 \\right ]$即为前文提到的$r_{g}(s,a)=-\\left [ s \\neq g \\right ]$，即完成为0，未完成为-1，具体奖励函数可以根据我们的使用环境设计</li>\n<li>$a_{t} \\leftarrow \\pi_{b}(s_{t}||g)$表示神经网络的输入为当前状态与目标状态的连结</li>\n</ol>\n<h2 id=\"HER的优点\"><a href=\"#HER的优点\" class=\"headerlink\" title=\"HER的优点\"></a>HER的优点</h2><ol>\n<li>可解决稀疏奖励、二分奖励问题</li>\n<li>可适用于所有的Off-Policy算法</li>\n<li>提升了数据采样效率</li>\n</ol>\n<h1 id=\"实验部分\"><a href=\"#实验部分\" class=\"headerlink\" title=\"实验部分\"></a>实验部分</h1><p>文中实验结果：<a href=\"https://goo.gl/SMrQnI\" rel=\"external nofollow\" target=\"_blank\">https://goo.gl/SMrQnI</a></p>\n<p>实验部分的完整细节请参考论文原文。</p>\n<h2 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h2><ul>\n<li>7自由度机械臂</li>\n<li>模拟环境：MuJoCo</li>\n<li>任务分为3种<ul>\n<li>Pushing，推：锁定机械臂的钳子，移动机械臂将物体推到目标点</li>\n<li>Sliding，滑动：类似于冰球运动，锁定机械臂的钳子，移动机械臂给与物体一个力，使物体可以在较光滑的桌面上滑动并且达到目标位置</li>\n<li>Pick-and-place，摆放：解锁钳子，使用机械臂夹起物体并移动至空中目标点</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"./Hindsight-Experience-Replay/tasks.png\" alt=\"\"></p>\n<h2 id=\"算法\"><a href=\"#算法\" class=\"headerlink\" title=\"算法\"></a>算法</h2><ul>\n<li>DDPG</li>\n<li>Adam优化器</li>\n<li>多层感知机MLPs</li>\n<li>ReLU激活函数</li>\n<li>8核并行，更新参数后取平均</li>\n<li>A-C网络都是3个隐藏层，每层64个隐节点，Actor输出层用tanh激活函数</li>\n<li>经验池大小为$10^{6}$，折扣因子$\\gamma=0.98$，学习率$\\alpha=0.001$，探索因子$\\epsilon = 0.2$</li>\n</ul>\n<blockquote>\n<p>With probability 20% we sample (uniformly) a random action from the hypercube of valid actions. </p>\n</blockquote>\n<p>DDPG使用了随机探索机制</p>\n<h2 id=\"训练结果\"><a href=\"#训练结果\" class=\"headerlink\" title=\"训练结果\"></a>训练结果</h2><h3 id=\"final模式与future模式对比\"><a href=\"#final模式与future模式对比\" class=\"headerlink\" title=\"final模式与future模式对比\"></a>final模式与future模式对比</h3><p><img src=\"./Hindsight-Experience-Replay/finalvsfuture.png\" alt=\"\"></p>\n<ul>\n<li>红色曲线为future模式，蓝色曲线为final模式，绿色曲线为使用了<a href=\"https://arxiv.org/pdf/1703.01310.pdf\" rel=\"external nofollow\" target=\"_blank\">count-based</a>的DDPG，褐红色虚线为原始DDPG</li>\n<li>从左至右依次是Pushing，Sliding，Pick-and-place任务</li>\n<li>超参数$k=4$</li>\n<li>这个实验中，目标状态会变，即为多个目标状态</li>\n</ul>\n<p>结果分析：</p>\n<ul>\n<li>future模式比final效果更好</li>\n<li>使用了count-based的DDPG智能稍微解决一下Sliding任务</li>\n<li>使用HER的DDPG可以完全胜任三个任务</li>\n<li>证明了HER是使从稀疏、二分奖励问题中学习成为可能的关键因素</li>\n</ul>\n<h3 id=\"单个目标状态的实验\"><a href=\"#单个目标状态的实验\" class=\"headerlink\" title=\"单个目标状态的实验\"></a>单个目标状态的实验</h3><p><img src=\"./Hindsight-Experience-Replay/singlegoal.png\" alt=\"\"></p>\n<ul>\n<li>蓝色曲线为使用了HER的DDPG，文中并未说明HER是哪种模式，<strong>猜测</strong>是final模式，因为文中实验部分之前都是以final模式进行举例</li>\n<li>绿色曲线代表应用了count-based的DDPG，褐红色虚线为原始DDPG</li>\n<li>实验中，目标状态都为同一状态$g$</li>\n</ul>\n<p>结果分析：</p>\n<ul>\n<li>DDPG+HER比原始DDPG的性能要好很多</li>\n<li><strong>相比于多个目标的实验，可以发现，在多目标的任务中DDPG训练更快</strong>，所以在实际中，即使我们只关心一个目标，我们最好也使用多个目标来训练</li>\n</ul>\n<h3 id=\"HER应用于reward-shaping问题中\"><a href=\"#HER应用于reward-shaping问题中\" class=\"headerlink\" title=\"HER应用于reward shaping问题中\"></a>HER应用于reward shaping问题中</h3><p>前文已经说过，reward shaping可以简单理解为将奖励函数设置为某些变量的函数，如$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$，即奖励函数为当前状态与目标状态的欧氏距离的负数</p>\n<p><img src=\"./Hindsight-Experience-Replay/rewardshape.png\" alt=\"\"></p>\n<ul>\n<li>奖励函数为$r_{g}(s,a)=-\\left || s-g \\right ||^{2}$</li>\n</ul>\n<p>结果分析：</p>\n<ul>\n<li><p>无论使用怎样的reward shaping函数，DDPG、DDPG+HER都不能解决这个问题</p>\n</li>\n<li><p>作者认为原因有二：</p>\n<ul>\n<li><blockquote>\n<p>There is a huge discrepancy between what we optimize (i.e. a shaped reward function) and the success condition (i.e.: is the object within some radius from the goal at the end of the episode);  </p>\n</blockquote>\n<p>判定完成目标的条件和要优化的问题有巨大的矛盾（虽然我也不理解这到底是什么意思，索性就直接抄了过来）</p>\n</li>\n<li><blockquote>\n<p>Shaped rewards penalize for inappropriate behaviour (e.g. moving the box in a wrong direction) which may hinder exploration. It can cause the agent to learn not to touch the box at all if it can not manipulate it precisely and we noticed such behaviour in some of our experiments. </p>\n</blockquote>\n<p>reward shaping阻碍了探索</p>\n</li>\n</ul>\n</li>\n<li><blockquote>\n<p>Our results suggest that domain-agnostic reward shaping does not work well (at least in the simple forms we have tried). Of course for every problem there exists a reward which makes it easy (Ng et al., 1999) but designing such shaped rewards requires a lot of domain knowledge and may in some cases not be much easier than directly scripting the policy. This strengthens our belief that learning from sparse, binary rewards is an important problem. </p>\n</blockquote>\n<p>研究结果表明，与领域无关的reward shaping效果并不好</p>\n</li>\n</ul>\n<h3 id=\"四种模式比较\"><a href=\"#四种模式比较\" class=\"headerlink\" title=\"四种模式比较\"></a>四种模式比较</h3><p><img src=\"./Hindsight-Experience-Replay/fourmodel.png\" alt=\"\"></p>\n<ul>\n<li>红色代表future模式，蓝色代表final模式，绿色代表episode模式，紫色代表episode模式，褐红色虚线代表原始DDPG</li>\n<li>横坐标代表超参数$k$，第一行三个图的纵坐标代表最高得分，第二行三个图的纵坐标代表平均得分</li>\n</ul>\n<p>结果分析：</p>\n<ul>\n<li><p>效果：future&gt;final&gt;episode&gt;random&gt;no HER</p>\n</li>\n<li><p>稳定性：final(好)=no-HER(差)&gt;future&gt;episode&gt;random</p>\n</li>\n<li><p>future模式是唯一一个可以解决Sliding任务的，在$k=4$或者$k=8$时效果最好</p>\n</li>\n<li><p>增大$k$超过8会使性能有所下降，主要是因为$k$过大导致经验池中原始真实数据所占的比例太小</p>\n</li>\n<li><blockquote>\n<p>It confirms that the most valuable goals for replay are the ones which are going to be achieved in the near future </p>\n</blockquote>\n<p>它证实了回放经验中最有价值的目标是那些在不久的将来能实现的目标</p>\n</li>\n</ul>\n<p><em>注：作者根据 future 模式提出了最近邻的 future 模式，即把$g’$设置为$s_{t+1}$，并且进行了实验，实验结果不如 future 模式。</em></p>"},{"title":"Asynchronous Methods for Deep Reinforcement Learning","copyright":true,"mathjax":true,"top":1,"date":"2019-05-30T07:22:13.000Z","keywords":null,"description":null,"_content":"\n本文提出了A3C模型，即Asynchronous Advantage Actor-Critic，是A2C的异步版本，使用CPU多核而不用GPU进行训练，文中说效果比使用GPU反而更好。\n\n推荐：\n\n- 并行梯度优化的佳作\n- 通俗易懂\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)\n\n本文提出了一个概念简单、异步梯度优化的轻量级深度强化学习训练框架。提出该框架的初衷是占用少的资源稳定深度神经网络的学习。\n\n该框架可适用于：\n\n- 基于值与基于策略的方法\n- on-policy与off-policy的方法\n- 离散与连续问题\n\n效果：\n\n- 稳定训练\n- 在Atari游戏上使用DQN算法，一个16核CPU比Nvidia K40 GPU快，使用A3C算法可以快一倍\n- 成功适用于很多连续运动学控制问题，例如图像输入的3D迷宫\n\n> the sequence of observed data encountered by an online RL agent is non-stationary, and on-line RL updates are strongly correlated. By storing the agent’s data in an experience replay memory, the data can be batched or randomly sampled from different time-steps. Aggregating over memory in this way reduces non-stationarity and decorrelates updates, but at the same time limits the methods to off-policy reinforcement learning algorithms \n\n文中指出，on-policy方法训练不稳定，数据相关性很强，off-policy机制结合经验池机制减轻了训练的不稳定性和数据相关性。\n\n经验池的缺点：\n\n1. 占用内存，增加计算量\n2. 需要off-policy算法\n\n本文为深度强化学习提供了一个非常不同的范例，不使用经验池机制，而使用异步并行的方法在多个**相同环境**中执行多个智能体。（同时训练多个不同环境没有进行描述和实验）\n\n这种异步并行方式的优点是：\n\n1. 实现减轻数据相关性的效果，使训练稳定\n2. 可用于大范围on-policy算法，如Sarsa，n-step方法，A-C方法，也可用于off-policy方法，如Q-learning\n3. 利用深度神经网络设计算法，保持鲁棒性与有效性（不予置评）\n4. 不使用GPU，只使用多核CPU，反而训练时间短\n5. 比大规模分布式需要更少的资源占用（内存、算力等）\n6. **相比于[Gorila](https://arxiv.org/pdf/1507.04296.pdf)异步训练方式，本文中的训练方式不需要中心服务器，其使用的是共享内存模式**\n7. 单机运行减少了通信（梯度和超参数）的开销\n\n# 文中精要\n\n由于本文思想非常简单，所以精简描述论文精华\n\n本文使用**[Hogwild!](https://arxiv.org/pdf/1106.5730.pdf)**方式进行异步梯度下降\n\nHogwild! 这个方法的提出也很偶然，容我步步道来：\n\n我们使用函数对样本集进行拟合如下图所示\n\n![](./asynchronous-methods-for-drl/regression.png)\n\n当我们想要判断函数是否拟合的不错，我们往往使用损失函数来衡量，损失越小，则代表函数拟合得越好（但，过拟合不是我们想要的）。\n\n![](./asynchronous-methods-for-drl/lossfunction.png)\n\n为了减小损失，我们常用梯度下降算法来优化。标准的梯度下降原理如下图所示，如果函数为凸函数，且更新步长很小，那么在有限步长内总可以下降至函数最小点，即，获得使损失函数最小的参数$\\theta^{\\ast}$\n\n![](./asynchronous-methods-for-drl/gd.png)\n\n之后出现了SGD，也就是随机梯度下降，这种方法差不多在60年代提出，由于思想过于简单，迭代次数很长，一直不被主流优化算法接受。但是，当大数据时代到来时，SGD变成了很普遍的优化方法。\n\nSGD的算法流程如下：\n\n- 选一个初始参数向量$\\theta$和正步长$\\alpha$\n- 循环直到满足结束条件：\n  - 从训练集中随机选择一个样本$x_{i}$\n  - 更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$\n\nSGD的优点：\n\n1. 少内存占用。SGD不需要所有的样本集进行计算梯度，只需要从样本集中抽取一个样本进行训练。频繁的采样操作可以使用高速缓存来加速训练。\n2. 收敛至**可接受的解**速度很快。其实我们不希望看到过拟合，当然也不希望看到欠拟合，SGD正好是这两个极端的trade-off。SGD可以很快的收敛到一个较好的解，相对于样本集较好的解比相对于样本集最好的解的泛化能力可要强得多。下图展示了SGD与标准梯度下降的损失函数曲线比较。\n\n![](./asynchronous-methods-for-drl/sgdvsgd.png)\n\n看图像可能会觉得SGD并没有严格下降，有时会有损失上升的倾向，但是，总体来看，这种方法最终也是可以收敛到最小值的。总体上，它使损失进行了下降。\n\n对于熟悉并行编程的人来说，如果让他们设计并行随机梯度下降，他们一定会像这样设计：\n\n- 每个线程从训练集随机抽取一个样本$x_{i}$\n  - 锁参数$\\theta$\n  - 线程读参数$\\theta$\n  - 线程更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$\n  - 解锁\n\n更新前锁定参数，更新后解锁参数。对于许多问题，更新这一步骤耗时在微秒级，而锁参数耗时在毫秒级，这意味着锁参数要比更新多占用1000多倍的时间。虽然有一些其他方法可以对该过程进行优化，但差距还是很明显。\n\n如果，**注释掉关于锁的代码呢？**这真是一个大胆的想法，但是Hogwild!就是这么做的。（这都是一个叫Feng Niu的人搞出来的，不知是出于好奇还是在Debug，他在研究加速SGD的时候注释掉了锁的代码，算法不仅有效，还提升了一百多倍。所以说，多试试总是好的。。。）流程如下：\n\n- 每个线程从训练集随机抽取一个样本$x_{i}$\n  - ~~锁参数$\\theta $~~​\n  - 线程读参数$\\theta$\n  - 线程更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$\n  - ~~解锁~~\n\n> In a sentence, the main idea of Hogwild! is — “**Remove all thread locks from parallel SGD code**.” In Hogwild!, threads can overwrite each other by writing at the same time and compute gradients using a stale version of the “current solution.” \n\n我们不禁都有一个疑问，这么做，真的可以吗？还真别说，经过实验表明，该方法取得了多线程的益处却没有数学效率上的负面影响。\n\n本文中的异步更新就是用这种方法，唯一有问题的可能就是两个线程同写，但是即便是同写，写入还是有先后的，最多也就是把前一个线程写入的给覆盖掉，丢弃一个线程的数据更新而已，无伤大雅，至于多个线程写后读、读后写倒都没有大的影响，不影响参数更新与收敛。\n\n## 伪代码\n\n**本文中所有伪代码都使用Hogwild!方式进行梯度更新。**\n\n以下伪代码都是单个线程中actor-learning的操作。每个线程中的actor可以使用不同的探索机制，实验证明不同的探索机制可以提高算法的鲁棒性和性能效果。\n\n首先介绍异步one-step Q-Learning的训练模式\n\n![](./asynchronous-methods-for-drl/a1stepq.png)\n\n解析：\n\n- 相比后两个伪代码，该代码中各线程是不需要复制用于选择动作的训练网络（因为各方对target network的定义不同，有些人认为等待赋值的是目标网络，有些人认为需要训练的是目标网络，因此，此处不使用目标网络的术语）的，即每次选择动作，都使用其他线程可能更新过的Q网络进行决策。这是因为不需要使用同一个决策模型向后看多步\n- $\\theta$是动态变化的，即在该线程的训练过程中，其他线程也可能对参数$\\theta$进行了更新\n- $I_{target}$代表双Q学习赋值的间隔\n- $I_{AsyncUpdate}$代表单线程对共享参数$\\theta$更新的间隔\n\n---\n\n接下来是异步n-step Q-Learning的训练模式：\n\n相比于n-step，one-step方法中获得的立即奖励$r$只影响导致其产生的$Q(s,a)$，从而通过$Q(s,a)$间接影响其他的动作值，这会使训练过程很慢，因为需要多次更新才能将奖励传播到前面的相关状态和动作。使奖励传播更快的一个方法就是使用n-step回报。\n$$\nG_{t}=r_{t}+\\gamma r_{t+1}+\\cdots+\\gamma^{n-1} r_{t+n-1}+\\max _{a} \\gamma^{n} Q\\left(s_{t+n}, a\\right)\n$$\n![](./asynchronous-methods-for-drl/anstepq.png)\n\n解析：\n\n- 相比于one-step，该算法为每个线程配置了一个备份网络$\\color{red}{\\theta'}$\n- $t_{max}$为n-step中的$n$\n\n---\n\n最后是异步A2C，即A3C的训练模式：\n\n![](./asynchronous-methods-for-drl/a3c.png)\n\n解析：\n\n- $t_{max}$为n-step向前看的步数\n- 文中针对该模型将A-C网络架构共享了部分神经网络参数，并且对actor网络的损失函数公式进行了改造：$\\nabla_{\\theta^{\\prime}} \\log \\pi\\left(a_{t} | s_{t} ; \\theta^{\\prime}\\right)\\left(R_{t}-V\\left(s_{t} ; \\theta_{v}\\right)\\right)+\\color{red}{\\beta \\nabla_{\\theta^{\\prime}} H\\left(\\pi\\left(s_{t} ; \\theta^{\\prime}\\right)\\right)}$，即添加了熵正则化项，它的作用是增加探索，避免网络过早地收敛至局部最优，$\\beta$为超参数\n\n正态分布的熵可以表示为$-\\frac{1}{2}\\left(\\log \\left(2 \\pi \\sigma^{2}\\right)+1\\right)$\n\n# 实验部分\n\n实验结果视频：\n\n- [TORCS A3C训练驾驶汽车](https://youtu.be/0xo1Ldx3L5Q)\n- [MuJoco 一些训练效果](https://youtu.be/Ajjc08-iPx8)\n- [Labyrinth 3D迷宫](https://youtu.be/nMR5mjCFZCw)\n\nAtari 2600 实验结果：\n\n![](./asynchronous-methods-for-drl/table1.png)\n\n不同线程数的加速效果：\n\n![](./asynchronous-methods-for-drl/table2.png)\n\n文中比较了三种优化函数的性能，分别是：\n\n- 动量SGD，Momentum SGD\n- RMSProp\n- Shared RMSProp\n\nRMSProp是这样更新的：\n$$\ng=\\alpha g+(1-\\alpha) \\Delta \\theta^{2}\n$$\n\n$$\n\\theta \\leftarrow \\theta-\\eta \\frac{\\Delta \\theta}{\\sqrt{g+\\epsilon}}\n$$\n\n$\\eta$为学习率，$\\alpha$为RMSProp折扣因子\n\nRMSProp与Shared RMSProp的差别就是：\n\n- Shared RMSProp各线程共享参数$g$，且无锁异步更新\n- RMSProp各线程独立一个参数$g$\n\n实验结果如下：\n\n测试每种算法50次试验，得分从高到低排列，算法为n-step Q-Learning和A3C\n\n**综合来看，三种优化方式效果差别不大，但是Shared RMSProp>RMSProp>Momentum SGD**\n\n![](./asynchronous-methods-for-drl/threeoptimizer.png)\n\n---\n\n实验结果太多，懒得贴了，总之，这种异步框架方法的优点是：\n\n- 使用离线在线策略，基于值基于策略方法，离散连续问题\n- 稳定训练\n- 加速训练\n- 比经验池少消耗资源\n\n# 引用\n\n> [Parallel Machine Learning with Hogwild!](https://medium.com/@krishna_srd/parallel-machine-learning-with-hogwild-f945ad7e48a4)","source":"_posts/asynchronous-methods-for-drl.md","raw":"---\ntitle: Asynchronous Methods for Deep Reinforcement Learning\ncopyright: true\nmathjax: true\ntop: 1\ndate: 2019-05-30 15:22:13\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n本文提出了A3C模型，即Asynchronous Advantage Actor-Critic，是A2C的异步版本，使用CPU多核而不用GPU进行训练，文中说效果比使用GPU反而更好。\n\n推荐：\n\n- 并行梯度优化的佳作\n- 通俗易懂\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)\n\n本文提出了一个概念简单、异步梯度优化的轻量级深度强化学习训练框架。提出该框架的初衷是占用少的资源稳定深度神经网络的学习。\n\n该框架可适用于：\n\n- 基于值与基于策略的方法\n- on-policy与off-policy的方法\n- 离散与连续问题\n\n效果：\n\n- 稳定训练\n- 在Atari游戏上使用DQN算法，一个16核CPU比Nvidia K40 GPU快，使用A3C算法可以快一倍\n- 成功适用于很多连续运动学控制问题，例如图像输入的3D迷宫\n\n> the sequence of observed data encountered by an online RL agent is non-stationary, and on-line RL updates are strongly correlated. By storing the agent’s data in an experience replay memory, the data can be batched or randomly sampled from different time-steps. Aggregating over memory in this way reduces non-stationarity and decorrelates updates, but at the same time limits the methods to off-policy reinforcement learning algorithms \n\n文中指出，on-policy方法训练不稳定，数据相关性很强，off-policy机制结合经验池机制减轻了训练的不稳定性和数据相关性。\n\n经验池的缺点：\n\n1. 占用内存，增加计算量\n2. 需要off-policy算法\n\n本文为深度强化学习提供了一个非常不同的范例，不使用经验池机制，而使用异步并行的方法在多个**相同环境**中执行多个智能体。（同时训练多个不同环境没有进行描述和实验）\n\n这种异步并行方式的优点是：\n\n1. 实现减轻数据相关性的效果，使训练稳定\n2. 可用于大范围on-policy算法，如Sarsa，n-step方法，A-C方法，也可用于off-policy方法，如Q-learning\n3. 利用深度神经网络设计算法，保持鲁棒性与有效性（不予置评）\n4. 不使用GPU，只使用多核CPU，反而训练时间短\n5. 比大规模分布式需要更少的资源占用（内存、算力等）\n6. **相比于[Gorila](https://arxiv.org/pdf/1507.04296.pdf)异步训练方式，本文中的训练方式不需要中心服务器，其使用的是共享内存模式**\n7. 单机运行减少了通信（梯度和超参数）的开销\n\n# 文中精要\n\n由于本文思想非常简单，所以精简描述论文精华\n\n本文使用**[Hogwild!](https://arxiv.org/pdf/1106.5730.pdf)**方式进行异步梯度下降\n\nHogwild! 这个方法的提出也很偶然，容我步步道来：\n\n我们使用函数对样本集进行拟合如下图所示\n\n![](./asynchronous-methods-for-drl/regression.png)\n\n当我们想要判断函数是否拟合的不错，我们往往使用损失函数来衡量，损失越小，则代表函数拟合得越好（但，过拟合不是我们想要的）。\n\n![](./asynchronous-methods-for-drl/lossfunction.png)\n\n为了减小损失，我们常用梯度下降算法来优化。标准的梯度下降原理如下图所示，如果函数为凸函数，且更新步长很小，那么在有限步长内总可以下降至函数最小点，即，获得使损失函数最小的参数$\\theta^{\\ast}$\n\n![](./asynchronous-methods-for-drl/gd.png)\n\n之后出现了SGD，也就是随机梯度下降，这种方法差不多在60年代提出，由于思想过于简单，迭代次数很长，一直不被主流优化算法接受。但是，当大数据时代到来时，SGD变成了很普遍的优化方法。\n\nSGD的算法流程如下：\n\n- 选一个初始参数向量$\\theta$和正步长$\\alpha$\n- 循环直到满足结束条件：\n  - 从训练集中随机选择一个样本$x_{i}$\n  - 更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$\n\nSGD的优点：\n\n1. 少内存占用。SGD不需要所有的样本集进行计算梯度，只需要从样本集中抽取一个样本进行训练。频繁的采样操作可以使用高速缓存来加速训练。\n2. 收敛至**可接受的解**速度很快。其实我们不希望看到过拟合，当然也不希望看到欠拟合，SGD正好是这两个极端的trade-off。SGD可以很快的收敛到一个较好的解，相对于样本集较好的解比相对于样本集最好的解的泛化能力可要强得多。下图展示了SGD与标准梯度下降的损失函数曲线比较。\n\n![](./asynchronous-methods-for-drl/sgdvsgd.png)\n\n看图像可能会觉得SGD并没有严格下降，有时会有损失上升的倾向，但是，总体来看，这种方法最终也是可以收敛到最小值的。总体上，它使损失进行了下降。\n\n对于熟悉并行编程的人来说，如果让他们设计并行随机梯度下降，他们一定会像这样设计：\n\n- 每个线程从训练集随机抽取一个样本$x_{i}$\n  - 锁参数$\\theta$\n  - 线程读参数$\\theta$\n  - 线程更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$\n  - 解锁\n\n更新前锁定参数，更新后解锁参数。对于许多问题，更新这一步骤耗时在微秒级，而锁参数耗时在毫秒级，这意味着锁参数要比更新多占用1000多倍的时间。虽然有一些其他方法可以对该过程进行优化，但差距还是很明显。\n\n如果，**注释掉关于锁的代码呢？**这真是一个大胆的想法，但是Hogwild!就是这么做的。（这都是一个叫Feng Niu的人搞出来的，不知是出于好奇还是在Debug，他在研究加速SGD的时候注释掉了锁的代码，算法不仅有效，还提升了一百多倍。所以说，多试试总是好的。。。）流程如下：\n\n- 每个线程从训练集随机抽取一个样本$x_{i}$\n  - ~~锁参数$\\theta $~~​\n  - 线程读参数$\\theta$\n  - 线程更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$\n  - ~~解锁~~\n\n> In a sentence, the main idea of Hogwild! is — “**Remove all thread locks from parallel SGD code**.” In Hogwild!, threads can overwrite each other by writing at the same time and compute gradients using a stale version of the “current solution.” \n\n我们不禁都有一个疑问，这么做，真的可以吗？还真别说，经过实验表明，该方法取得了多线程的益处却没有数学效率上的负面影响。\n\n本文中的异步更新就是用这种方法，唯一有问题的可能就是两个线程同写，但是即便是同写，写入还是有先后的，最多也就是把前一个线程写入的给覆盖掉，丢弃一个线程的数据更新而已，无伤大雅，至于多个线程写后读、读后写倒都没有大的影响，不影响参数更新与收敛。\n\n## 伪代码\n\n**本文中所有伪代码都使用Hogwild!方式进行梯度更新。**\n\n以下伪代码都是单个线程中actor-learning的操作。每个线程中的actor可以使用不同的探索机制，实验证明不同的探索机制可以提高算法的鲁棒性和性能效果。\n\n首先介绍异步one-step Q-Learning的训练模式\n\n![](./asynchronous-methods-for-drl/a1stepq.png)\n\n解析：\n\n- 相比后两个伪代码，该代码中各线程是不需要复制用于选择动作的训练网络（因为各方对target network的定义不同，有些人认为等待赋值的是目标网络，有些人认为需要训练的是目标网络，因此，此处不使用目标网络的术语）的，即每次选择动作，都使用其他线程可能更新过的Q网络进行决策。这是因为不需要使用同一个决策模型向后看多步\n- $\\theta$是动态变化的，即在该线程的训练过程中，其他线程也可能对参数$\\theta$进行了更新\n- $I_{target}$代表双Q学习赋值的间隔\n- $I_{AsyncUpdate}$代表单线程对共享参数$\\theta$更新的间隔\n\n---\n\n接下来是异步n-step Q-Learning的训练模式：\n\n相比于n-step，one-step方法中获得的立即奖励$r$只影响导致其产生的$Q(s,a)$，从而通过$Q(s,a)$间接影响其他的动作值，这会使训练过程很慢，因为需要多次更新才能将奖励传播到前面的相关状态和动作。使奖励传播更快的一个方法就是使用n-step回报。\n$$\nG_{t}=r_{t}+\\gamma r_{t+1}+\\cdots+\\gamma^{n-1} r_{t+n-1}+\\max _{a} \\gamma^{n} Q\\left(s_{t+n}, a\\right)\n$$\n![](./asynchronous-methods-for-drl/anstepq.png)\n\n解析：\n\n- 相比于one-step，该算法为每个线程配置了一个备份网络$\\color{red}{\\theta'}$\n- $t_{max}$为n-step中的$n$\n\n---\n\n最后是异步A2C，即A3C的训练模式：\n\n![](./asynchronous-methods-for-drl/a3c.png)\n\n解析：\n\n- $t_{max}$为n-step向前看的步数\n- 文中针对该模型将A-C网络架构共享了部分神经网络参数，并且对actor网络的损失函数公式进行了改造：$\\nabla_{\\theta^{\\prime}} \\log \\pi\\left(a_{t} | s_{t} ; \\theta^{\\prime}\\right)\\left(R_{t}-V\\left(s_{t} ; \\theta_{v}\\right)\\right)+\\color{red}{\\beta \\nabla_{\\theta^{\\prime}} H\\left(\\pi\\left(s_{t} ; \\theta^{\\prime}\\right)\\right)}$，即添加了熵正则化项，它的作用是增加探索，避免网络过早地收敛至局部最优，$\\beta$为超参数\n\n正态分布的熵可以表示为$-\\frac{1}{2}\\left(\\log \\left(2 \\pi \\sigma^{2}\\right)+1\\right)$\n\n# 实验部分\n\n实验结果视频：\n\n- [TORCS A3C训练驾驶汽车](https://youtu.be/0xo1Ldx3L5Q)\n- [MuJoco 一些训练效果](https://youtu.be/Ajjc08-iPx8)\n- [Labyrinth 3D迷宫](https://youtu.be/nMR5mjCFZCw)\n\nAtari 2600 实验结果：\n\n![](./asynchronous-methods-for-drl/table1.png)\n\n不同线程数的加速效果：\n\n![](./asynchronous-methods-for-drl/table2.png)\n\n文中比较了三种优化函数的性能，分别是：\n\n- 动量SGD，Momentum SGD\n- RMSProp\n- Shared RMSProp\n\nRMSProp是这样更新的：\n$$\ng=\\alpha g+(1-\\alpha) \\Delta \\theta^{2}\n$$\n\n$$\n\\theta \\leftarrow \\theta-\\eta \\frac{\\Delta \\theta}{\\sqrt{g+\\epsilon}}\n$$\n\n$\\eta$为学习率，$\\alpha$为RMSProp折扣因子\n\nRMSProp与Shared RMSProp的差别就是：\n\n- Shared RMSProp各线程共享参数$g$，且无锁异步更新\n- RMSProp各线程独立一个参数$g$\n\n实验结果如下：\n\n测试每种算法50次试验，得分从高到低排列，算法为n-step Q-Learning和A3C\n\n**综合来看，三种优化方式效果差别不大，但是Shared RMSProp>RMSProp>Momentum SGD**\n\n![](./asynchronous-methods-for-drl/threeoptimizer.png)\n\n---\n\n实验结果太多，懒得贴了，总之，这种异步框架方法的优点是：\n\n- 使用离线在线策略，基于值基于策略方法，离散连续问题\n- 稳定训练\n- 加速训练\n- 比经验池少消耗资源\n\n# 引用\n\n> [Parallel Machine Learning with Hogwild!](https://medium.com/@krishna_srd/parallel-machine-learning-with-hogwild-f945ad7e48a4)","slug":"asynchronous-methods-for-drl","published":1,"updated":"2019-05-31T03:25:16.041Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma2e002fekveuph10svx","content":"<p>本文提出了A3C模型，即Asynchronous Advantage Actor-Critic，是A2C的异步版本，使用CPU多核而不用GPU进行训练，文中说效果比使用GPU反而更好。</p>\n<p>推荐：</p>\n<ul>\n<li>并行梯度优化的佳作</li>\n<li>通俗易懂</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1602.01783.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1602.01783.pdf</a></p>\n<p>本文提出了一个概念简单、异步梯度优化的轻量级深度强化学习训练框架。提出该框架的初衷是占用少的资源稳定深度神经网络的学习。</p>\n<p>该框架可适用于：</p>\n<ul>\n<li>基于值与基于策略的方法</li>\n<li>on-policy与off-policy的方法</li>\n<li>离散与连续问题</li>\n</ul>\n<p>效果：</p>\n<ul>\n<li>稳定训练</li>\n<li>在Atari游戏上使用DQN算法，一个16核CPU比Nvidia K40 GPU快，使用A3C算法可以快一倍</li>\n<li>成功适用于很多连续运动学控制问题，例如图像输入的3D迷宫</li>\n</ul>\n<blockquote>\n<p>the sequence of observed data encountered by an online RL agent is non-stationary, and on-line RL updates are strongly correlated. By storing the agent’s data in an experience replay memory, the data can be batched or randomly sampled from different time-steps. Aggregating over memory in this way reduces non-stationarity and decorrelates updates, but at the same time limits the methods to off-policy reinforcement learning algorithms </p>\n</blockquote>\n<p>文中指出，on-policy方法训练不稳定，数据相关性很强，off-policy机制结合经验池机制减轻了训练的不稳定性和数据相关性。</p>\n<p>经验池的缺点：</p>\n<ol>\n<li>占用内存，增加计算量</li>\n<li>需要off-policy算法</li>\n</ol>\n<p>本文为深度强化学习提供了一个非常不同的范例，不使用经验池机制，而使用异步并行的方法在多个<strong>相同环境</strong>中执行多个智能体。（同时训练多个不同环境没有进行描述和实验）</p>\n<p>这种异步并行方式的优点是：</p>\n<ol>\n<li>实现减轻数据相关性的效果，使训练稳定</li>\n<li>可用于大范围on-policy算法，如Sarsa，n-step方法，A-C方法，也可用于off-policy方法，如Q-learning</li>\n<li>利用深度神经网络设计算法，保持鲁棒性与有效性（不予置评）</li>\n<li>不使用GPU，只使用多核CPU，反而训练时间短</li>\n<li>比大规模分布式需要更少的资源占用（内存、算力等）</li>\n<li><strong>相比于<a href=\"https://arxiv.org/pdf/1507.04296.pdf\" rel=\"external nofollow\" target=\"_blank\">Gorila</a>异步训练方式，本文中的训练方式不需要中心服务器，其使用的是共享内存模式</strong></li>\n<li>单机运行减少了通信（梯度和超参数）的开销</li>\n</ol>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><p>由于本文思想非常简单，所以精简描述论文精华</p>\n<p>本文使用<strong><a href=\"https://arxiv.org/pdf/1106.5730.pdf\" rel=\"external nofollow\" target=\"_blank\">Hogwild!</a></strong>方式进行异步梯度下降</p>\n<p>Hogwild! 这个方法的提出也很偶然，容我步步道来：</p>\n<p>我们使用函数对样本集进行拟合如下图所示</p>\n<p><img src=\"./asynchronous-methods-for-drl/regression.png\" alt=\"\"></p>\n<p>当我们想要判断函数是否拟合的不错，我们往往使用损失函数来衡量，损失越小，则代表函数拟合得越好（但，过拟合不是我们想要的）。</p>\n<p><img src=\"./asynchronous-methods-for-drl/lossfunction.png\" alt=\"\"></p>\n<p>为了减小损失，我们常用梯度下降算法来优化。标准的梯度下降原理如下图所示，如果函数为凸函数，且更新步长很小，那么在有限步长内总可以下降至函数最小点，即，获得使损失函数最小的参数$\\theta^{\\ast}$</p>\n<p><img src=\"./asynchronous-methods-for-drl/gd.png\" alt=\"\"></p>\n<p>之后出现了SGD，也就是随机梯度下降，这种方法差不多在60年代提出，由于思想过于简单，迭代次数很长，一直不被主流优化算法接受。但是，当大数据时代到来时，SGD变成了很普遍的优化方法。</p>\n<p>SGD的算法流程如下：</p>\n<ul>\n<li>选一个初始参数向量$\\theta$和正步长$\\alpha$</li>\n<li>循环直到满足结束条件：<ul>\n<li>从训练集中随机选择一个样本$x_{i}$</li>\n<li>更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$</li>\n</ul>\n</li>\n</ul>\n<p>SGD的优点：</p>\n<ol>\n<li>少内存占用。SGD不需要所有的样本集进行计算梯度，只需要从样本集中抽取一个样本进行训练。频繁的采样操作可以使用高速缓存来加速训练。</li>\n<li>收敛至<strong>可接受的解</strong>速度很快。其实我们不希望看到过拟合，当然也不希望看到欠拟合，SGD正好是这两个极端的trade-off。SGD可以很快的收敛到一个较好的解，相对于样本集较好的解比相对于样本集最好的解的泛化能力可要强得多。下图展示了SGD与标准梯度下降的损失函数曲线比较。</li>\n</ol>\n<p><img src=\"./asynchronous-methods-for-drl/sgdvsgd.png\" alt=\"\"></p>\n<p>看图像可能会觉得SGD并没有严格下降，有时会有损失上升的倾向，但是，总体来看，这种方法最终也是可以收敛到最小值的。总体上，它使损失进行了下降。</p>\n<p>对于熟悉并行编程的人来说，如果让他们设计并行随机梯度下降，他们一定会像这样设计：</p>\n<ul>\n<li>每个线程从训练集随机抽取一个样本$x_{i}$<ul>\n<li>锁参数$\\theta$</li>\n<li>线程读参数$\\theta$</li>\n<li>线程更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$</li>\n<li>解锁</li>\n</ul>\n</li>\n</ul>\n<p>更新前锁定参数，更新后解锁参数。对于许多问题，更新这一步骤耗时在微秒级，而锁参数耗时在毫秒级，这意味着锁参数要比更新多占用1000多倍的时间。虽然有一些其他方法可以对该过程进行优化，但差距还是很明显。</p>\n<p>如果，<strong>注释掉关于锁的代码呢？</strong>这真是一个大胆的想法，但是Hogwild!就是这么做的。（这都是一个叫Feng Niu的人搞出来的，不知是出于好奇还是在Debug，他在研究加速SGD的时候注释掉了锁的代码，算法不仅有效，还提升了一百多倍。所以说，多试试总是好的。。。）流程如下：</p>\n<ul>\n<li>每个线程从训练集随机抽取一个样本$x_{i}$<ul>\n<li><del>锁参数$\\theta $</del>​</li>\n<li>线程读参数$\\theta$</li>\n<li>线程更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$</li>\n<li><del>解锁</del></li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>In a sentence, the main idea of Hogwild! is — “<strong>Remove all thread locks from parallel SGD code</strong>.” In Hogwild!, threads can overwrite each other by writing at the same time and compute gradients using a stale version of the “current solution.” </p>\n</blockquote>\n<p>我们不禁都有一个疑问，这么做，真的可以吗？还真别说，经过实验表明，该方法取得了多线程的益处却没有数学效率上的负面影响。</p>\n<p>本文中的异步更新就是用这种方法，唯一有问题的可能就是两个线程同写，但是即便是同写，写入还是有先后的，最多也就是把前一个线程写入的给覆盖掉，丢弃一个线程的数据更新而已，无伤大雅，至于多个线程写后读、读后写倒都没有大的影响，不影响参数更新与收敛。</p>\n<h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><strong>本文中所有伪代码都使用Hogwild!方式进行梯度更新。</strong></p>\n<p>以下伪代码都是单个线程中actor-learning的操作。每个线程中的actor可以使用不同的探索机制，实验证明不同的探索机制可以提高算法的鲁棒性和性能效果。</p>\n<p>首先介绍异步one-step Q-Learning的训练模式</p>\n<p><img src=\"./asynchronous-methods-for-drl/a1stepq.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>相比后两个伪代码，该代码中各线程是不需要复制用于选择动作的训练网络（因为各方对target network的定义不同，有些人认为等待赋值的是目标网络，有些人认为需要训练的是目标网络，因此，此处不使用目标网络的术语）的，即每次选择动作，都使用其他线程可能更新过的Q网络进行决策。这是因为不需要使用同一个决策模型向后看多步</li>\n<li>$\\theta$是动态变化的，即在该线程的训练过程中，其他线程也可能对参数$\\theta$进行了更新</li>\n<li>$I_{target}$代表双Q学习赋值的间隔</li>\n<li>$I_{AsyncUpdate}$代表单线程对共享参数$\\theta$更新的间隔</li>\n</ul>\n<hr>\n<p>接下来是异步n-step Q-Learning的训练模式：</p>\n<p>相比于n-step，one-step方法中获得的立即奖励$r$只影响导致其产生的$Q(s,a)$，从而通过$Q(s,a)$间接影响其他的动作值，这会使训练过程很慢，因为需要多次更新才能将奖励传播到前面的相关状态和动作。使奖励传播更快的一个方法就是使用n-step回报。</p>\n<script type=\"math/tex; mode=display\">\nG_{t}=r_{t}+\\gamma r_{t+1}+\\cdots+\\gamma^{n-1} r_{t+n-1}+\\max _{a} \\gamma^{n} Q\\left(s_{t+n}, a\\right)</script><p><img src=\"./asynchronous-methods-for-drl/anstepq.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>相比于one-step，该算法为每个线程配置了一个备份网络$\\color{red}{\\theta’}$</li>\n<li>$t_{max}$为n-step中的$n$</li>\n</ul>\n<hr>\n<p>最后是异步A2C，即A3C的训练模式：</p>\n<p><img src=\"./asynchronous-methods-for-drl/a3c.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>$t_{max}$为n-step向前看的步数</li>\n<li>文中针对该模型将A-C网络架构共享了部分神经网络参数，并且对actor网络的损失函数公式进行了改造：$\\nabla_{\\theta^{\\prime}} \\log \\pi\\left(a_{t} | s_{t} ; \\theta^{\\prime}\\right)\\left(R_{t}-V\\left(s_{t} ; \\theta_{v}\\right)\\right)+\\color{red}{\\beta \\nabla_{\\theta^{\\prime}} H\\left(\\pi\\left(s_{t} ; \\theta^{\\prime}\\right)\\right)}$，即添加了熵正则化项，它的作用是增加探索，避免网络过早地收敛至局部最优，$\\beta$为超参数</li>\n</ul>\n<p>正态分布的熵可以表示为$-\\frac{1}{2}\\left(\\log \\left(2 \\pi \\sigma^{2}\\right)+1\\right)$</p>\n<h1 id=\"实验部分\"><a href=\"#实验部分\" class=\"headerlink\" title=\"实验部分\"></a>实验部分</h1><p>实验结果视频：</p>\n<ul>\n<li><a href=\"https://youtu.be/0xo1Ldx3L5Q\" rel=\"external nofollow\" target=\"_blank\">TORCS A3C训练驾驶汽车</a></li>\n<li><a href=\"https://youtu.be/Ajjc08-iPx8\" rel=\"external nofollow\" target=\"_blank\">MuJoco 一些训练效果</a></li>\n<li><a href=\"https://youtu.be/nMR5mjCFZCw\" rel=\"external nofollow\" target=\"_blank\">Labyrinth 3D迷宫</a></li>\n</ul>\n<p>Atari 2600 实验结果：</p>\n<p><img src=\"./asynchronous-methods-for-drl/table1.png\" alt=\"\"></p>\n<p>不同线程数的加速效果：</p>\n<p><img src=\"./asynchronous-methods-for-drl/table2.png\" alt=\"\"></p>\n<p>文中比较了三种优化函数的性能，分别是：</p>\n<ul>\n<li>动量SGD，Momentum SGD</li>\n<li>RMSProp</li>\n<li>Shared RMSProp</li>\n</ul>\n<p>RMSProp是这样更新的：</p>\n<script type=\"math/tex; mode=display\">\ng=\\alpha g+(1-\\alpha) \\Delta \\theta^{2}</script><script type=\"math/tex; mode=display\">\n\\theta \\leftarrow \\theta-\\eta \\frac{\\Delta \\theta}{\\sqrt{g+\\epsilon}}</script><p>$\\eta$为学习率，$\\alpha$为RMSProp折扣因子</p>\n<p>RMSProp与Shared RMSProp的差别就是：</p>\n<ul>\n<li>Shared RMSProp各线程共享参数$g$，且无锁异步更新</li>\n<li>RMSProp各线程独立一个参数$g$</li>\n</ul>\n<p>实验结果如下：</p>\n<p>测试每种算法50次试验，得分从高到低排列，算法为n-step Q-Learning和A3C</p>\n<p><strong>综合来看，三种优化方式效果差别不大，但是Shared RMSProp&gt;RMSProp&gt;Momentum SGD</strong></p>\n<p><img src=\"./asynchronous-methods-for-drl/threeoptimizer.png\" alt=\"\"></p>\n<hr>\n<p>实验结果太多，懒得贴了，总之，这种异步框架方法的优点是：</p>\n<ul>\n<li>使用离线在线策略，基于值基于策略方法，离散连续问题</li>\n<li>稳定训练</li>\n<li>加速训练</li>\n<li>比经验池少消耗资源</li>\n</ul>\n<h1 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h1><blockquote>\n<p><a href=\"https://medium.com/@krishna_srd/parallel-machine-learning-with-hogwild-f945ad7e48a4\" rel=\"external nofollow\" target=\"_blank\">Parallel Machine Learning with Hogwild!</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p>本文提出了A3C模型，即Asynchronous Advantage Actor-Critic，是A2C的异步版本，使用CPU多核而不用GPU进行训练，文中说效果比使用GPU反而更好。</p>\n<p>推荐：</p>\n<ul>\n<li>并行梯度优化的佳作</li>\n<li>通俗易懂</li>\n</ul>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1602.01783.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1602.01783.pdf</a></p>\n<p>本文提出了一个概念简单、异步梯度优化的轻量级深度强化学习训练框架。提出该框架的初衷是占用少的资源稳定深度神经网络的学习。</p>\n<p>该框架可适用于：</p>\n<ul>\n<li>基于值与基于策略的方法</li>\n<li>on-policy与off-policy的方法</li>\n<li>离散与连续问题</li>\n</ul>\n<p>效果：</p>\n<ul>\n<li>稳定训练</li>\n<li>在Atari游戏上使用DQN算法，一个16核CPU比Nvidia K40 GPU快，使用A3C算法可以快一倍</li>\n<li>成功适用于很多连续运动学控制问题，例如图像输入的3D迷宫</li>\n</ul>\n<blockquote>\n<p>the sequence of observed data encountered by an online RL agent is non-stationary, and on-line RL updates are strongly correlated. By storing the agent’s data in an experience replay memory, the data can be batched or randomly sampled from different time-steps. Aggregating over memory in this way reduces non-stationarity and decorrelates updates, but at the same time limits the methods to off-policy reinforcement learning algorithms </p>\n</blockquote>\n<p>文中指出，on-policy方法训练不稳定，数据相关性很强，off-policy机制结合经验池机制减轻了训练的不稳定性和数据相关性。</p>\n<p>经验池的缺点：</p>\n<ol>\n<li>占用内存，增加计算量</li>\n<li>需要off-policy算法</li>\n</ol>\n<p>本文为深度强化学习提供了一个非常不同的范例，不使用经验池机制，而使用异步并行的方法在多个<strong>相同环境</strong>中执行多个智能体。（同时训练多个不同环境没有进行描述和实验）</p>\n<p>这种异步并行方式的优点是：</p>\n<ol>\n<li>实现减轻数据相关性的效果，使训练稳定</li>\n<li>可用于大范围on-policy算法，如Sarsa，n-step方法，A-C方法，也可用于off-policy方法，如Q-learning</li>\n<li>利用深度神经网络设计算法，保持鲁棒性与有效性（不予置评）</li>\n<li>不使用GPU，只使用多核CPU，反而训练时间短</li>\n<li>比大规模分布式需要更少的资源占用（内存、算力等）</li>\n<li><strong>相比于<a href=\"https://arxiv.org/pdf/1507.04296.pdf\" rel=\"external nofollow\" target=\"_blank\">Gorila</a>异步训练方式，本文中的训练方式不需要中心服务器，其使用的是共享内存模式</strong></li>\n<li>单机运行减少了通信（梯度和超参数）的开销</li>\n</ol>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><p>由于本文思想非常简单，所以精简描述论文精华</p>\n<p>本文使用<strong><a href=\"https://arxiv.org/pdf/1106.5730.pdf\" rel=\"external nofollow\" target=\"_blank\">Hogwild!</a></strong>方式进行异步梯度下降</p>\n<p>Hogwild! 这个方法的提出也很偶然，容我步步道来：</p>\n<p>我们使用函数对样本集进行拟合如下图所示</p>\n<p><img src=\"./asynchronous-methods-for-drl/regression.png\" alt=\"\"></p>\n<p>当我们想要判断函数是否拟合的不错，我们往往使用损失函数来衡量，损失越小，则代表函数拟合得越好（但，过拟合不是我们想要的）。</p>\n<p><img src=\"./asynchronous-methods-for-drl/lossfunction.png\" alt=\"\"></p>\n<p>为了减小损失，我们常用梯度下降算法来优化。标准的梯度下降原理如下图所示，如果函数为凸函数，且更新步长很小，那么在有限步长内总可以下降至函数最小点，即，获得使损失函数最小的参数$\\theta^{\\ast}$</p>\n<p><img src=\"./asynchronous-methods-for-drl/gd.png\" alt=\"\"></p>\n<p>之后出现了SGD，也就是随机梯度下降，这种方法差不多在60年代提出，由于思想过于简单，迭代次数很长，一直不被主流优化算法接受。但是，当大数据时代到来时，SGD变成了很普遍的优化方法。</p>\n<p>SGD的算法流程如下：</p>\n<ul>\n<li>选一个初始参数向量$\\theta$和正步长$\\alpha$</li>\n<li>循环直到满足结束条件：<ul>\n<li>从训练集中随机选择一个样本$x_{i}$</li>\n<li>更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$</li>\n</ul>\n</li>\n</ul>\n<p>SGD的优点：</p>\n<ol>\n<li>少内存占用。SGD不需要所有的样本集进行计算梯度，只需要从样本集中抽取一个样本进行训练。频繁的采样操作可以使用高速缓存来加速训练。</li>\n<li>收敛至<strong>可接受的解</strong>速度很快。其实我们不希望看到过拟合，当然也不希望看到欠拟合，SGD正好是这两个极端的trade-off。SGD可以很快的收敛到一个较好的解，相对于样本集较好的解比相对于样本集最好的解的泛化能力可要强得多。下图展示了SGD与标准梯度下降的损失函数曲线比较。</li>\n</ol>\n<p><img src=\"./asynchronous-methods-for-drl/sgdvsgd.png\" alt=\"\"></p>\n<p>看图像可能会觉得SGD并没有严格下降，有时会有损失上升的倾向，但是，总体来看，这种方法最终也是可以收敛到最小值的。总体上，它使损失进行了下降。</p>\n<p>对于熟悉并行编程的人来说，如果让他们设计并行随机梯度下降，他们一定会像这样设计：</p>\n<ul>\n<li>每个线程从训练集随机抽取一个样本$x_{i}$<ul>\n<li>锁参数$\\theta$</li>\n<li>线程读参数$\\theta$</li>\n<li>线程更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$</li>\n<li>解锁</li>\n</ul>\n</li>\n</ul>\n<p>更新前锁定参数，更新后解锁参数。对于许多问题，更新这一步骤耗时在微秒级，而锁参数耗时在毫秒级，这意味着锁参数要比更新多占用1000多倍的时间。虽然有一些其他方法可以对该过程进行优化，但差距还是很明显。</p>\n<p>如果，<strong>注释掉关于锁的代码呢？</strong>这真是一个大胆的想法，但是Hogwild!就是这么做的。（这都是一个叫Feng Niu的人搞出来的，不知是出于好奇还是在Debug，他在研究加速SGD的时候注释掉了锁的代码，算法不仅有效，还提升了一百多倍。所以说，多试试总是好的。。。）流程如下：</p>\n<ul>\n<li>每个线程从训练集随机抽取一个样本$x_{i}$<ul>\n<li><del>锁参数$\\theta $</del>​</li>\n<li>线程读参数$\\theta$</li>\n<li>线程更新参数$\\theta \\leftarrow\\left(\\theta-\\alpha \\nabla L\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\\right)$</li>\n<li><del>解锁</del></li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>In a sentence, the main idea of Hogwild! is — “<strong>Remove all thread locks from parallel SGD code</strong>.” In Hogwild!, threads can overwrite each other by writing at the same time and compute gradients using a stale version of the “current solution.” </p>\n</blockquote>\n<p>我们不禁都有一个疑问，这么做，真的可以吗？还真别说，经过实验表明，该方法取得了多线程的益处却没有数学效率上的负面影响。</p>\n<p>本文中的异步更新就是用这种方法，唯一有问题的可能就是两个线程同写，但是即便是同写，写入还是有先后的，最多也就是把前一个线程写入的给覆盖掉，丢弃一个线程的数据更新而已，无伤大雅，至于多个线程写后读、读后写倒都没有大的影响，不影响参数更新与收敛。</p>\n<h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><strong>本文中所有伪代码都使用Hogwild!方式进行梯度更新。</strong></p>\n<p>以下伪代码都是单个线程中actor-learning的操作。每个线程中的actor可以使用不同的探索机制，实验证明不同的探索机制可以提高算法的鲁棒性和性能效果。</p>\n<p>首先介绍异步one-step Q-Learning的训练模式</p>\n<p><img src=\"./asynchronous-methods-for-drl/a1stepq.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>相比后两个伪代码，该代码中各线程是不需要复制用于选择动作的训练网络（因为各方对target network的定义不同，有些人认为等待赋值的是目标网络，有些人认为需要训练的是目标网络，因此，此处不使用目标网络的术语）的，即每次选择动作，都使用其他线程可能更新过的Q网络进行决策。这是因为不需要使用同一个决策模型向后看多步</li>\n<li>$\\theta$是动态变化的，即在该线程的训练过程中，其他线程也可能对参数$\\theta$进行了更新</li>\n<li>$I_{target}$代表双Q学习赋值的间隔</li>\n<li>$I_{AsyncUpdate}$代表单线程对共享参数$\\theta$更新的间隔</li>\n</ul>\n<hr>\n<p>接下来是异步n-step Q-Learning的训练模式：</p>\n<p>相比于n-step，one-step方法中获得的立即奖励$r$只影响导致其产生的$Q(s,a)$，从而通过$Q(s,a)$间接影响其他的动作值，这会使训练过程很慢，因为需要多次更新才能将奖励传播到前面的相关状态和动作。使奖励传播更快的一个方法就是使用n-step回报。</p>\n<script type=\"math/tex; mode=display\">\nG_{t}=r_{t}+\\gamma r_{t+1}+\\cdots+\\gamma^{n-1} r_{t+n-1}+\\max _{a} \\gamma^{n} Q\\left(s_{t+n}, a\\right)</script><p><img src=\"./asynchronous-methods-for-drl/anstepq.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>相比于one-step，该算法为每个线程配置了一个备份网络$\\color{red}{\\theta’}$</li>\n<li>$t_{max}$为n-step中的$n$</li>\n</ul>\n<hr>\n<p>最后是异步A2C，即A3C的训练模式：</p>\n<p><img src=\"./asynchronous-methods-for-drl/a3c.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>$t_{max}$为n-step向前看的步数</li>\n<li>文中针对该模型将A-C网络架构共享了部分神经网络参数，并且对actor网络的损失函数公式进行了改造：$\\nabla_{\\theta^{\\prime}} \\log \\pi\\left(a_{t} | s_{t} ; \\theta^{\\prime}\\right)\\left(R_{t}-V\\left(s_{t} ; \\theta_{v}\\right)\\right)+\\color{red}{\\beta \\nabla_{\\theta^{\\prime}} H\\left(\\pi\\left(s_{t} ; \\theta^{\\prime}\\right)\\right)}$，即添加了熵正则化项，它的作用是增加探索，避免网络过早地收敛至局部最优，$\\beta$为超参数</li>\n</ul>\n<p>正态分布的熵可以表示为$-\\frac{1}{2}\\left(\\log \\left(2 \\pi \\sigma^{2}\\right)+1\\right)$</p>\n<h1 id=\"实验部分\"><a href=\"#实验部分\" class=\"headerlink\" title=\"实验部分\"></a>实验部分</h1><p>实验结果视频：</p>\n<ul>\n<li><a href=\"https://youtu.be/0xo1Ldx3L5Q\" rel=\"external nofollow\" target=\"_blank\">TORCS A3C训练驾驶汽车</a></li>\n<li><a href=\"https://youtu.be/Ajjc08-iPx8\" rel=\"external nofollow\" target=\"_blank\">MuJoco 一些训练效果</a></li>\n<li><a href=\"https://youtu.be/nMR5mjCFZCw\" rel=\"external nofollow\" target=\"_blank\">Labyrinth 3D迷宫</a></li>\n</ul>\n<p>Atari 2600 实验结果：</p>\n<p><img src=\"./asynchronous-methods-for-drl/table1.png\" alt=\"\"></p>\n<p>不同线程数的加速效果：</p>\n<p><img src=\"./asynchronous-methods-for-drl/table2.png\" alt=\"\"></p>\n<p>文中比较了三种优化函数的性能，分别是：</p>\n<ul>\n<li>动量SGD，Momentum SGD</li>\n<li>RMSProp</li>\n<li>Shared RMSProp</li>\n</ul>\n<p>RMSProp是这样更新的：</p>\n<script type=\"math/tex; mode=display\">\ng=\\alpha g+(1-\\alpha) \\Delta \\theta^{2}</script><script type=\"math/tex; mode=display\">\n\\theta \\leftarrow \\theta-\\eta \\frac{\\Delta \\theta}{\\sqrt{g+\\epsilon}}</script><p>$\\eta$为学习率，$\\alpha$为RMSProp折扣因子</p>\n<p>RMSProp与Shared RMSProp的差别就是：</p>\n<ul>\n<li>Shared RMSProp各线程共享参数$g$，且无锁异步更新</li>\n<li>RMSProp各线程独立一个参数$g$</li>\n</ul>\n<p>实验结果如下：</p>\n<p>测试每种算法50次试验，得分从高到低排列，算法为n-step Q-Learning和A3C</p>\n<p><strong>综合来看，三种优化方式效果差别不大，但是Shared RMSProp&gt;RMSProp&gt;Momentum SGD</strong></p>\n<p><img src=\"./asynchronous-methods-for-drl/threeoptimizer.png\" alt=\"\"></p>\n<hr>\n<p>实验结果太多，懒得贴了，总之，这种异步框架方法的优点是：</p>\n<ul>\n<li>使用离线在线策略，基于值基于策略方法，离散连续问题</li>\n<li>稳定训练</li>\n<li>加速训练</li>\n<li>比经验池少消耗资源</li>\n</ul>\n<h1 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h1><blockquote>\n<p><a href=\"https://medium.com/@krishna_srd/parallel-machine-learning-with-hogwild-f945ad7e48a4\" rel=\"external nofollow\" target=\"_blank\">Parallel Machine Learning with Hogwild!</a></p>\n</blockquote>"},{"title":"动态规划 Dynamic Programming","copyright":true,"top":1,"date":"2019-05-12T12:16:22.000Z","mathjax":true,"keywords":null,"description":null,"_content":"\n本文介绍了强化学习问题中最简单基本的算法——动态规划（Dynamic Programming），介绍了贝尔曼方程在该算法中的应用。\n\n<!--more-->\n\n# DP的基本概念\n\n> 动态规划(dynamic programming)是运筹学的一个分支，是求解决策过程(decision process)最优化的数学方法。20世纪50年代初美国数学家R.E.Bellman等人在研究多阶段决策过程(multistep decision process)的优化问题时，提出了著名的最优化原理(principle of optimality)，把多阶段过程转化为一系列单阶段问题，利用各阶段之间的关系，逐个求解，创立了解决这类过程优化问题的新方法——动态规划。1957年出版了他的名著《Dynamic Programming》，这是该领域的第一本著作。——[百度百科]([https://baike.baidu.com/item/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/529408?fr=aladdin](https://baike.baidu.com/item/动态规划/529408?fr=aladdin))\n\n动态规划-DP算法指的不是单一一个算法，而是**一系列可以在给定满足MDP的完全可知环境模型中计算出最优策略的算法**。\n\nDP的特点：\n\n- Model-Based\n- Value-Based\n- Off-Policy(这个比较牵强，因为DP不涉及采样、预测，完全靠planning)\n\nDP具有很重要的理论基础作用，但是在现在的强化学习问题中，DP并不常使用，主要原因有二：\n\n- 需要完全可知的模型，状态空间、动作空间离散，状态转移、奖励函数可知且确定\n- 计算量很大(每次更新都需要完全规划所有可能性)\n\n在一些表格型的问题中，如完全可知的迷宫，可以使用DP，但是要解决人类现实世界极其复杂的问题、任务，DP可能就有些力不从心啦。\n\n其实，所有的强化学习算法都可以被认为是在**不完全可知的环境**中使用**少量计算**得到如DP效果一样的策略（最优策略）。\n\n# 算法\n\n先回顾一下之前提到的贝尔曼方程。\n\n贝尔曼期望方程：\n$$\nv_{\\pi}(s) =\\sum_{a}\\pi(a\\mid s)\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\n$$\n\n$$\nq_{\\pi}(s,a) =\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\n$$\n\n贝尔曼最优方程：\n$$\nv_{*}(s) =\\max_{a}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\right]\n$$\n\n$$\nq_{*}(s,a) =\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\n$$\n\n为什么要再次提到贝尔曼方程呢？因为动态规划算法中的策略迭代、值迭代就是将贝尔曼方程给结合了起来。\n\n回顾一下在[价值与贝尔曼方差](./价值与贝尔曼方程.html)最后的例子中迭代计算$V(S)$和$Q(S,A)$，遍历所有$Q(S,A)$以计算$V(S)$，再遍历所有$V(S)$以计算$Q(S,A)$。在一个简单的场景下循环迭代至收敛就需要很大的计算量，如果在复杂场景中(还是完全可知的)迭代计算可想有多费时费力，动态规划DP下的Policy Iteration和Value Iteration减轻了计算的负担，同时又不影响收敛性。\n\n引用《Reinfocement Learning : An Introduction》中的一个网格世界的例子：\n\n![](./dynamic-programming/gridworld.png)\n\n在这个例子中，有1-14个非终态以及两个终态(左上角、右下角)，动作空间为上下左右四种，在边缘位置的状态，例如$S=1$，可选的动作只有左右下三种，且等概率选择每种动作，每进行一次移动，就给予-1的奖励值。智能体需要尽快的到达网格世界的出口-终态，以获得尽量少的负奖励(即累计奖励最大)。\n\n如果按照**先遍历所有$Q(S,A)$以计算$V(S)$，再遍历所有$V(S)$以计算$Q(S,A)$**的方式计算，值函数的表格将会如下图所示：\n\n![](./dynamic-programming/iteration.png)\n\n左边展示的是进行$k$次迭代，使值函数表格可以收敛，右边表示在每次迭代中，取$a=argmax_{a}q(s,a)$的策略。\n\n可以发现，这样迭代$V(S)$至收敛有两个浪费算力的地方：\n\n1. 选择动作的概率完全按照环境设置，导致计算状态$s$的值函数时，最差的动作$a$所带来的影响也被计算在其中，但其实真正执行的时候，永远不会执行该动作。\n2. $k=3$与$k=10$时的策略表示一样，也就意味着，不必等到$V(S)$迭代至完全收敛就有可能可以获得最优策略，那么后续迭代完全没有用处，造成了资源浪费。\n\n策略迭代、值迭代的思想都是贪心策略，但策略迭代针对问题1通过**剪裁可选动作**的方式进行了优化，值迭代针对问题2通过**取最大动作值函数**的方式进行了优化。\n\n## 策略迭代 Policy Iteration\n\n动作是通过策略产生的，因此势必需要对初始策略（GridWorld中的完全随机策略）进行替换，以达到更改动作选取概率的目的。\n\n既然要在不同的阶段更改动作选择的概率，那么要进行多次策略的更改，随之而来的问题就是：\n\n1. 值函数迭代更新到什么情况下时，开始更新策略\n2. 如何更新策略？\n\n针对第一个问题，我们使用**策略评估Policy Evaluation**的方式来解决，针对第二个问题，我们使用**策略提升Policy Improvation**的方式来解决，最终当新策略与旧策略相同时，终止迭代，得到最优策略。\n$$\n\\pi_{0} \\xrightarrow{E} v_{\\pi_{0}} \\xrightarrow{I} \\pi_{1} \\xrightarrow{E} v_{\\pi_{1}} \\xrightarrow{I} \\pi_{2} \\xrightarrow{E} ... \\xrightarrow{I} \\pi_{\\ast} \\xrightarrow{E} v_{\\pi_{\\ast}}\n$$\n\n### 伪代码\n\n![](./dynamic-programming/pi.png)\n\n### 策略评估 Policy Evaluation\n\n问题：值函数迭代更新到什么情况下时，开始更新策略\n\n答：设置更新幅度阈值$\\theta$，当sweep(横扫)一遍状态空间计算$V(S)$时，与上次更新时的$V(S)$相比较，如果最大的更新幅度小于阈值$\\theta$，即$\\Delta \\lt \\theta$，则认为策略评估已经完成，开始进行策略更新\n\n特点：使用了贝尔曼期望方程-$v_{\\pi}(s)$\n\n*注：策略评估指的不是评估一个策略的好坏，而是在当前策略下评估所有的状态值，使状态值表格近似收敛。*\n\n### 策略提升 Policy Improvement\n\n问题：如何更新策略？\n\n答：根据策略评估步骤得到的值函数$V(S)$，计算$Q(S,A)$表格，选取每个状态下使动作值函数最大的动作作为新的动作集，每个动作的选择概率相同，接着进行策略评估。\n\n特点：使用了贝尔曼期望方程-$q_{\\pi}(s,a)$\n\n## 值迭代 Value Iteration\n\n策略迭代的一个缺点是在得到最优策略$\\pi_{\\ast}$之前，需要多次更新策略，每次更新策略都会引起可选取动作的改变，这会引起在更新完策略后的前几次策略评估中值函数偏差比较大，导致在策略评估过程中需要花费大量的迭代来减小更新幅度$\\Delta$，因此需要多次sweep(横扫)$V(S)$来迭代计算。\n\n值迭代的思想是：**能不能通过早停的方式，在不更改策略的情况下，直接一次得到最优策略？**\n\n值迭代运用了策略评估、策略迭代的思想，并将它们融合在一起，即不更新策略，直接选择动作值函数$q(s,a)$最大的动作作为状态值$v(s)$，直接迭代出**近似最优**（早停，$\\Delta \\lt \\theta$即可）状态价值函数$V_{\\ast}(S)$，使用贪心策略进而得到最优策略$\\pi_{\\ast}$。\n\n特点：使用了贝尔曼最优方程-$v_{\\ast}(s)，q_{\\ast}(s,a)$。\n\n### 伪代码\n\n![](./dynamic-programming/vi.png)\n\n## PI与VI的比较\n\n![](./dynamic-programming/pivsvi.png)\n\n相同点：\n\n- 在$0 \\leq \\gamma \\lt 1$，有限MDPs环境中，两种方式都可以收敛到最优策略$\\pi_{\\ast}$\n- 都使用了贝尔曼方程进行状态值函数的迭代\n\n不同点：\n\n1. 收敛方式\n   - 策略迭代PI包括策略评估Policy Evaluation和策略提升Policy Improvement，这两部循环迭代至策略收敛\n   - 值迭代VI包括找到最优状态值函数和一步提取策略，这两步不需要循环迭代，而是根据最优值函数直接得到最优策略\n2. 动作改变方式\n   - 策略迭代PI获得新策略$\\pi_{new}$后，**更改每个状态的可选动作集**，多次横扫（遍历）$V(S)$\n   - 值迭代VI过程中不产生策略，不更改每个状态的可选动作集，但是**只取每个状态下动作值函数最大的动作作为状态值**，一次横扫（遍历）$V(S)$。（这里需要解释一下，虽然循环是多次遍历，但是因为max操作，每次遍历每个状态所选取的动作不一定一样，虽然策略一直是随机策略，没有产生新策略，但是计算过程没有遍历到所有动作，可以隐含的看作是一个新策略，因此每次遍历时这个“隐策略”都会改变，所以称为一次遍历。）\n3. 计算方式\n   - 策略迭代PI使用贝尔曼期望方程\n   - 值迭代VI使用贝尔曼最优方程\n4. Policy方式\n   - 策略迭代PI是On-Policy\n   - 值迭代VI是Off-Policy\n5. 稳定性检查\n   - 策略迭代PI中更新策略时进行了策略稳定性检查，判断是否收敛\n   - 值迭代VI获得新策略$\\pi$时没有进行策略稳定性检查\n\n至于策略迭代PI与值迭代VI的收敛速度，**通常情况**下，PI的迭代次数更少，VI的运行时间更少。\n\n> [What is the difference between value iteration and policy iteration?](https://stackoverflow.com/a/42493295/11483803)\n>\n> [《Reinforcement Learning : An Introduction 2nd Edition》p77](http://incompleteideas.net/book/RLbook2018.pdf)\n\n","source":"_posts/dynamic-programming.md","raw":"---\ntitle: 动态规划 Dynamic Programming\ncopyright: true\ntop: 1\ndate: 2019-05-12 20:16:22\nmathjax: true\nkeywords: \ndescription: \ncategories: ReinforcementLearning\ntags:\n- rl\n---\n\n本文介绍了强化学习问题中最简单基本的算法——动态规划（Dynamic Programming），介绍了贝尔曼方程在该算法中的应用。\n\n<!--more-->\n\n# DP的基本概念\n\n> 动态规划(dynamic programming)是运筹学的一个分支，是求解决策过程(decision process)最优化的数学方法。20世纪50年代初美国数学家R.E.Bellman等人在研究多阶段决策过程(multistep decision process)的优化问题时，提出了著名的最优化原理(principle of optimality)，把多阶段过程转化为一系列单阶段问题，利用各阶段之间的关系，逐个求解，创立了解决这类过程优化问题的新方法——动态规划。1957年出版了他的名著《Dynamic Programming》，这是该领域的第一本著作。——[百度百科]([https://baike.baidu.com/item/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/529408?fr=aladdin](https://baike.baidu.com/item/动态规划/529408?fr=aladdin))\n\n动态规划-DP算法指的不是单一一个算法，而是**一系列可以在给定满足MDP的完全可知环境模型中计算出最优策略的算法**。\n\nDP的特点：\n\n- Model-Based\n- Value-Based\n- Off-Policy(这个比较牵强，因为DP不涉及采样、预测，完全靠planning)\n\nDP具有很重要的理论基础作用，但是在现在的强化学习问题中，DP并不常使用，主要原因有二：\n\n- 需要完全可知的模型，状态空间、动作空间离散，状态转移、奖励函数可知且确定\n- 计算量很大(每次更新都需要完全规划所有可能性)\n\n在一些表格型的问题中，如完全可知的迷宫，可以使用DP，但是要解决人类现实世界极其复杂的问题、任务，DP可能就有些力不从心啦。\n\n其实，所有的强化学习算法都可以被认为是在**不完全可知的环境**中使用**少量计算**得到如DP效果一样的策略（最优策略）。\n\n# 算法\n\n先回顾一下之前提到的贝尔曼方程。\n\n贝尔曼期望方程：\n$$\nv_{\\pi}(s) =\\sum_{a}\\pi(a\\mid s)\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\n$$\n\n$$\nq_{\\pi}(s,a) =\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\n$$\n\n贝尔曼最优方程：\n$$\nv_{*}(s) =\\max_{a}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\right]\n$$\n\n$$\nq_{*}(s,a) =\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\n$$\n\n为什么要再次提到贝尔曼方程呢？因为动态规划算法中的策略迭代、值迭代就是将贝尔曼方程给结合了起来。\n\n回顾一下在[价值与贝尔曼方差](./价值与贝尔曼方程.html)最后的例子中迭代计算$V(S)$和$Q(S,A)$，遍历所有$Q(S,A)$以计算$V(S)$，再遍历所有$V(S)$以计算$Q(S,A)$。在一个简单的场景下循环迭代至收敛就需要很大的计算量，如果在复杂场景中(还是完全可知的)迭代计算可想有多费时费力，动态规划DP下的Policy Iteration和Value Iteration减轻了计算的负担，同时又不影响收敛性。\n\n引用《Reinfocement Learning : An Introduction》中的一个网格世界的例子：\n\n![](./dynamic-programming/gridworld.png)\n\n在这个例子中，有1-14个非终态以及两个终态(左上角、右下角)，动作空间为上下左右四种，在边缘位置的状态，例如$S=1$，可选的动作只有左右下三种，且等概率选择每种动作，每进行一次移动，就给予-1的奖励值。智能体需要尽快的到达网格世界的出口-终态，以获得尽量少的负奖励(即累计奖励最大)。\n\n如果按照**先遍历所有$Q(S,A)$以计算$V(S)$，再遍历所有$V(S)$以计算$Q(S,A)$**的方式计算，值函数的表格将会如下图所示：\n\n![](./dynamic-programming/iteration.png)\n\n左边展示的是进行$k$次迭代，使值函数表格可以收敛，右边表示在每次迭代中，取$a=argmax_{a}q(s,a)$的策略。\n\n可以发现，这样迭代$V(S)$至收敛有两个浪费算力的地方：\n\n1. 选择动作的概率完全按照环境设置，导致计算状态$s$的值函数时，最差的动作$a$所带来的影响也被计算在其中，但其实真正执行的时候，永远不会执行该动作。\n2. $k=3$与$k=10$时的策略表示一样，也就意味着，不必等到$V(S)$迭代至完全收敛就有可能可以获得最优策略，那么后续迭代完全没有用处，造成了资源浪费。\n\n策略迭代、值迭代的思想都是贪心策略，但策略迭代针对问题1通过**剪裁可选动作**的方式进行了优化，值迭代针对问题2通过**取最大动作值函数**的方式进行了优化。\n\n## 策略迭代 Policy Iteration\n\n动作是通过策略产生的，因此势必需要对初始策略（GridWorld中的完全随机策略）进行替换，以达到更改动作选取概率的目的。\n\n既然要在不同的阶段更改动作选择的概率，那么要进行多次策略的更改，随之而来的问题就是：\n\n1. 值函数迭代更新到什么情况下时，开始更新策略\n2. 如何更新策略？\n\n针对第一个问题，我们使用**策略评估Policy Evaluation**的方式来解决，针对第二个问题，我们使用**策略提升Policy Improvation**的方式来解决，最终当新策略与旧策略相同时，终止迭代，得到最优策略。\n$$\n\\pi_{0} \\xrightarrow{E} v_{\\pi_{0}} \\xrightarrow{I} \\pi_{1} \\xrightarrow{E} v_{\\pi_{1}} \\xrightarrow{I} \\pi_{2} \\xrightarrow{E} ... \\xrightarrow{I} \\pi_{\\ast} \\xrightarrow{E} v_{\\pi_{\\ast}}\n$$\n\n### 伪代码\n\n![](./dynamic-programming/pi.png)\n\n### 策略评估 Policy Evaluation\n\n问题：值函数迭代更新到什么情况下时，开始更新策略\n\n答：设置更新幅度阈值$\\theta$，当sweep(横扫)一遍状态空间计算$V(S)$时，与上次更新时的$V(S)$相比较，如果最大的更新幅度小于阈值$\\theta$，即$\\Delta \\lt \\theta$，则认为策略评估已经完成，开始进行策略更新\n\n特点：使用了贝尔曼期望方程-$v_{\\pi}(s)$\n\n*注：策略评估指的不是评估一个策略的好坏，而是在当前策略下评估所有的状态值，使状态值表格近似收敛。*\n\n### 策略提升 Policy Improvement\n\n问题：如何更新策略？\n\n答：根据策略评估步骤得到的值函数$V(S)$，计算$Q(S,A)$表格，选取每个状态下使动作值函数最大的动作作为新的动作集，每个动作的选择概率相同，接着进行策略评估。\n\n特点：使用了贝尔曼期望方程-$q_{\\pi}(s,a)$\n\n## 值迭代 Value Iteration\n\n策略迭代的一个缺点是在得到最优策略$\\pi_{\\ast}$之前，需要多次更新策略，每次更新策略都会引起可选取动作的改变，这会引起在更新完策略后的前几次策略评估中值函数偏差比较大，导致在策略评估过程中需要花费大量的迭代来减小更新幅度$\\Delta$，因此需要多次sweep(横扫)$V(S)$来迭代计算。\n\n值迭代的思想是：**能不能通过早停的方式，在不更改策略的情况下，直接一次得到最优策略？**\n\n值迭代运用了策略评估、策略迭代的思想，并将它们融合在一起，即不更新策略，直接选择动作值函数$q(s,a)$最大的动作作为状态值$v(s)$，直接迭代出**近似最优**（早停，$\\Delta \\lt \\theta$即可）状态价值函数$V_{\\ast}(S)$，使用贪心策略进而得到最优策略$\\pi_{\\ast}$。\n\n特点：使用了贝尔曼最优方程-$v_{\\ast}(s)，q_{\\ast}(s,a)$。\n\n### 伪代码\n\n![](./dynamic-programming/vi.png)\n\n## PI与VI的比较\n\n![](./dynamic-programming/pivsvi.png)\n\n相同点：\n\n- 在$0 \\leq \\gamma \\lt 1$，有限MDPs环境中，两种方式都可以收敛到最优策略$\\pi_{\\ast}$\n- 都使用了贝尔曼方程进行状态值函数的迭代\n\n不同点：\n\n1. 收敛方式\n   - 策略迭代PI包括策略评估Policy Evaluation和策略提升Policy Improvement，这两部循环迭代至策略收敛\n   - 值迭代VI包括找到最优状态值函数和一步提取策略，这两步不需要循环迭代，而是根据最优值函数直接得到最优策略\n2. 动作改变方式\n   - 策略迭代PI获得新策略$\\pi_{new}$后，**更改每个状态的可选动作集**，多次横扫（遍历）$V(S)$\n   - 值迭代VI过程中不产生策略，不更改每个状态的可选动作集，但是**只取每个状态下动作值函数最大的动作作为状态值**，一次横扫（遍历）$V(S)$。（这里需要解释一下，虽然循环是多次遍历，但是因为max操作，每次遍历每个状态所选取的动作不一定一样，虽然策略一直是随机策略，没有产生新策略，但是计算过程没有遍历到所有动作，可以隐含的看作是一个新策略，因此每次遍历时这个“隐策略”都会改变，所以称为一次遍历。）\n3. 计算方式\n   - 策略迭代PI使用贝尔曼期望方程\n   - 值迭代VI使用贝尔曼最优方程\n4. Policy方式\n   - 策略迭代PI是On-Policy\n   - 值迭代VI是Off-Policy\n5. 稳定性检查\n   - 策略迭代PI中更新策略时进行了策略稳定性检查，判断是否收敛\n   - 值迭代VI获得新策略$\\pi$时没有进行策略稳定性检查\n\n至于策略迭代PI与值迭代VI的收敛速度，**通常情况**下，PI的迭代次数更少，VI的运行时间更少。\n\n> [What is the difference between value iteration and policy iteration?](https://stackoverflow.com/a/42493295/11483803)\n>\n> [《Reinforcement Learning : An Introduction 2nd Edition》p77](http://incompleteideas.net/book/RLbook2018.pdf)\n\n","slug":"dynamic-programming","published":1,"updated":"2019-05-13T12:42:28.856Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma2r002hekve0ekqsbs4","content":"<p>本文介绍了强化学习问题中最简单基本的算法——动态规划（Dynamic Programming），介绍了贝尔曼方程在该算法中的应用。</p>\n<a id=\"more\"></a>\n<h1 id=\"DP的基本概念\"><a href=\"#DP的基本概念\" class=\"headerlink\" title=\"DP的基本概念\"></a>DP的基本概念</h1><blockquote>\n<p>动态规划(dynamic programming)是运筹学的一个分支，是求解决策过程(decision process)最优化的数学方法。20世纪50年代初美国数学家R.E.Bellman等人在研究多阶段决策过程(multistep decision process)的优化问题时，提出了著名的最优化原理(principle of optimality)，把多阶段过程转化为一系列单阶段问题，利用各阶段之间的关系，逐个求解，创立了解决这类过程优化问题的新方法——动态规划。1957年出版了他的名著《Dynamic Programming》，这是该领域的第一本著作。——<a href=\"[https://baike.baidu.com/item/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/529408?fr=aladdin](https://baike.baidu.com/item/动态规划/529408?fr=aladdin\">百度百科</a>)</p>\n</blockquote>\n<p>动态规划-DP算法指的不是单一一个算法，而是<strong>一系列可以在给定满足MDP的完全可知环境模型中计算出最优策略的算法</strong>。</p>\n<p>DP的特点：</p>\n<ul>\n<li>Model-Based</li>\n<li>Value-Based</li>\n<li>Off-Policy(这个比较牵强，因为DP不涉及采样、预测，完全靠planning)</li>\n</ul>\n<p>DP具有很重要的理论基础作用，但是在现在的强化学习问题中，DP并不常使用，主要原因有二：</p>\n<ul>\n<li>需要完全可知的模型，状态空间、动作空间离散，状态转移、奖励函数可知且确定</li>\n<li>计算量很大(每次更新都需要完全规划所有可能性)</li>\n</ul>\n<p>在一些表格型的问题中，如完全可知的迷宫，可以使用DP，但是要解决人类现实世界极其复杂的问题、任务，DP可能就有些力不从心啦。</p>\n<p>其实，所有的强化学习算法都可以被认为是在<strong>不完全可知的环境</strong>中使用<strong>少量计算</strong>得到如DP效果一样的策略（最优策略）。</p>\n<h1 id=\"算法\"><a href=\"#算法\" class=\"headerlink\" title=\"算法\"></a>算法</h1><p>先回顾一下之前提到的贝尔曼方程。</p>\n<p>贝尔曼期望方程：</p>\n<script type=\"math/tex; mode=display\">\nv_{\\pi}(s) =\\sum_{a}\\pi(a\\mid s)\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]</script><script type=\"math/tex; mode=display\">\nq_{\\pi}(s,a) =\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]</script><p>贝尔曼最优方程：</p>\n<script type=\"math/tex; mode=display\">\nv_{*}(s) =\\max_{a}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\right]</script><script type=\"math/tex; mode=display\">\nq_{*}(s,a) =\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]</script><p>为什么要再次提到贝尔曼方程呢？因为动态规划算法中的策略迭代、值迭代就是将贝尔曼方程给结合了起来。</p>\n<p>回顾一下在<a href=\"./价值与贝尔曼方程.html\">价值与贝尔曼方差</a>最后的例子中迭代计算$V(S)$和$Q(S,A)$，遍历所有$Q(S,A)$以计算$V(S)$，再遍历所有$V(S)$以计算$Q(S,A)$。在一个简单的场景下循环迭代至收敛就需要很大的计算量，如果在复杂场景中(还是完全可知的)迭代计算可想有多费时费力，动态规划DP下的Policy Iteration和Value Iteration减轻了计算的负担，同时又不影响收敛性。</p>\n<p>引用《Reinfocement Learning : An Introduction》中的一个网格世界的例子：</p>\n<p><img src=\"./dynamic-programming/gridworld.png\" alt=\"\"></p>\n<p>在这个例子中，有1-14个非终态以及两个终态(左上角、右下角)，动作空间为上下左右四种，在边缘位置的状态，例如$S=1$，可选的动作只有左右下三种，且等概率选择每种动作，每进行一次移动，就给予-1的奖励值。智能体需要尽快的到达网格世界的出口-终态，以获得尽量少的负奖励(即累计奖励最大)。</p>\n<p>如果按照<strong>先遍历所有$Q(S,A)$以计算$V(S)$，再遍历所有$V(S)$以计算$Q(S,A)$</strong>的方式计算，值函数的表格将会如下图所示：</p>\n<p><img src=\"./dynamic-programming/iteration.png\" alt=\"\"></p>\n<p>左边展示的是进行$k$次迭代，使值函数表格可以收敛，右边表示在每次迭代中，取$a=argmax_{a}q(s,a)$的策略。</p>\n<p>可以发现，这样迭代$V(S)$至收敛有两个浪费算力的地方：</p>\n<ol>\n<li>选择动作的概率完全按照环境设置，导致计算状态$s$的值函数时，最差的动作$a$所带来的影响也被计算在其中，但其实真正执行的时候，永远不会执行该动作。</li>\n<li>$k=3$与$k=10$时的策略表示一样，也就意味着，不必等到$V(S)$迭代至完全收敛就有可能可以获得最优策略，那么后续迭代完全没有用处，造成了资源浪费。</li>\n</ol>\n<p>策略迭代、值迭代的思想都是贪心策略，但策略迭代针对问题1通过<strong>剪裁可选动作</strong>的方式进行了优化，值迭代针对问题2通过<strong>取最大动作值函数</strong>的方式进行了优化。</p>\n<h2 id=\"策略迭代-Policy-Iteration\"><a href=\"#策略迭代-Policy-Iteration\" class=\"headerlink\" title=\"策略迭代 Policy Iteration\"></a>策略迭代 Policy Iteration</h2><p>动作是通过策略产生的，因此势必需要对初始策略（GridWorld中的完全随机策略）进行替换，以达到更改动作选取概率的目的。</p>\n<p>既然要在不同的阶段更改动作选择的概率，那么要进行多次策略的更改，随之而来的问题就是：</p>\n<ol>\n<li>值函数迭代更新到什么情况下时，开始更新策略</li>\n<li>如何更新策略？</li>\n</ol>\n<p>针对第一个问题，我们使用<strong>策略评估Policy Evaluation</strong>的方式来解决，针对第二个问题，我们使用<strong>策略提升Policy Improvation</strong>的方式来解决，最终当新策略与旧策略相同时，终止迭代，得到最优策略。</p>\n<script type=\"math/tex; mode=display\">\n\\pi_{0} \\xrightarrow{E} v_{\\pi_{0}} \\xrightarrow{I} \\pi_{1} \\xrightarrow{E} v_{\\pi_{1}} \\xrightarrow{I} \\pi_{2} \\xrightarrow{E} ... \\xrightarrow{I} \\pi_{\\ast} \\xrightarrow{E} v_{\\pi_{\\ast}}</script><h3 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h3><p><img src=\"./dynamic-programming/pi.png\" alt=\"\"></p>\n<h3 id=\"策略评估-Policy-Evaluation\"><a href=\"#策略评估-Policy-Evaluation\" class=\"headerlink\" title=\"策略评估 Policy Evaluation\"></a>策略评估 Policy Evaluation</h3><p>问题：值函数迭代更新到什么情况下时，开始更新策略</p>\n<p>答：设置更新幅度阈值$\\theta$，当sweep(横扫)一遍状态空间计算$V(S)$时，与上次更新时的$V(S)$相比较，如果最大的更新幅度小于阈值$\\theta$，即$\\Delta \\lt \\theta$，则认为策略评估已经完成，开始进行策略更新</p>\n<p>特点：使用了贝尔曼期望方程-$v_{\\pi}(s)$</p>\n<p><em>注：策略评估指的不是评估一个策略的好坏，而是在当前策略下评估所有的状态值，使状态值表格近似收敛。</em></p>\n<h3 id=\"策略提升-Policy-Improvement\"><a href=\"#策略提升-Policy-Improvement\" class=\"headerlink\" title=\"策略提升 Policy Improvement\"></a>策略提升 Policy Improvement</h3><p>问题：如何更新策略？</p>\n<p>答：根据策略评估步骤得到的值函数$V(S)$，计算$Q(S,A)$表格，选取每个状态下使动作值函数最大的动作作为新的动作集，每个动作的选择概率相同，接着进行策略评估。</p>\n<p>特点：使用了贝尔曼期望方程-$q_{\\pi}(s,a)$</p>\n<h2 id=\"值迭代-Value-Iteration\"><a href=\"#值迭代-Value-Iteration\" class=\"headerlink\" title=\"值迭代 Value Iteration\"></a>值迭代 Value Iteration</h2><p>策略迭代的一个缺点是在得到最优策略$\\pi_{\\ast}$之前，需要多次更新策略，每次更新策略都会引起可选取动作的改变，这会引起在更新完策略后的前几次策略评估中值函数偏差比较大，导致在策略评估过程中需要花费大量的迭代来减小更新幅度$\\Delta$，因此需要多次sweep(横扫)$V(S)$来迭代计算。</p>\n<p>值迭代的思想是：<strong>能不能通过早停的方式，在不更改策略的情况下，直接一次得到最优策略？</strong></p>\n<p>值迭代运用了策略评估、策略迭代的思想，并将它们融合在一起，即不更新策略，直接选择动作值函数$q(s,a)$最大的动作作为状态值$v(s)$，直接迭代出<strong>近似最优</strong>（早停，$\\Delta \\lt \\theta$即可）状态价值函数$V_{\\ast}(S)$，使用贪心策略进而得到最优策略$\\pi_{\\ast}$。</p>\n<p>特点：使用了贝尔曼最优方程-$v_{\\ast}(s)，q_{\\ast}(s,a)$。</p>\n<h3 id=\"伪代码-1\"><a href=\"#伪代码-1\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h3><p><img src=\"./dynamic-programming/vi.png\" alt=\"\"></p>\n<h2 id=\"PI与VI的比较\"><a href=\"#PI与VI的比较\" class=\"headerlink\" title=\"PI与VI的比较\"></a>PI与VI的比较</h2><p><img src=\"./dynamic-programming/pivsvi.png\" alt=\"\"></p>\n<p>相同点：</p>\n<ul>\n<li>在$0 \\leq \\gamma \\lt 1$，有限MDPs环境中，两种方式都可以收敛到最优策略$\\pi_{\\ast}$</li>\n<li>都使用了贝尔曼方程进行状态值函数的迭代</li>\n</ul>\n<p>不同点：</p>\n<ol>\n<li>收敛方式<ul>\n<li>策略迭代PI包括策略评估Policy Evaluation和策略提升Policy Improvement，这两部循环迭代至策略收敛</li>\n<li>值迭代VI包括找到最优状态值函数和一步提取策略，这两步不需要循环迭代，而是根据最优值函数直接得到最优策略</li>\n</ul>\n</li>\n<li>动作改变方式<ul>\n<li>策略迭代PI获得新策略$\\pi_{new}$后，<strong>更改每个状态的可选动作集</strong>，多次横扫（遍历）$V(S)$</li>\n<li>值迭代VI过程中不产生策略，不更改每个状态的可选动作集，但是<strong>只取每个状态下动作值函数最大的动作作为状态值</strong>，一次横扫（遍历）$V(S)$。（这里需要解释一下，虽然循环是多次遍历，但是因为max操作，每次遍历每个状态所选取的动作不一定一样，虽然策略一直是随机策略，没有产生新策略，但是计算过程没有遍历到所有动作，可以隐含的看作是一个新策略，因此每次遍历时这个“隐策略”都会改变，所以称为一次遍历。）</li>\n</ul>\n</li>\n<li>计算方式<ul>\n<li>策略迭代PI使用贝尔曼期望方程</li>\n<li>值迭代VI使用贝尔曼最优方程</li>\n</ul>\n</li>\n<li>Policy方式<ul>\n<li>策略迭代PI是On-Policy</li>\n<li>值迭代VI是Off-Policy</li>\n</ul>\n</li>\n<li>稳定性检查<ul>\n<li>策略迭代PI中更新策略时进行了策略稳定性检查，判断是否收敛</li>\n<li>值迭代VI获得新策略$\\pi$时没有进行策略稳定性检查</li>\n</ul>\n</li>\n</ol>\n<p>至于策略迭代PI与值迭代VI的收敛速度，<strong>通常情况</strong>下，PI的迭代次数更少，VI的运行时间更少。</p>\n<blockquote>\n<p><a href=\"https://stackoverflow.com/a/42493295/11483803\" rel=\"external nofollow\" target=\"_blank\">What is the difference between value iteration and policy iteration?</a></p>\n<p><a href=\"http://incompleteideas.net/book/RLbook2018.pdf\" rel=\"external nofollow\" target=\"_blank\">《Reinforcement Learning : An Introduction 2nd Edition》p77</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p>本文介绍了强化学习问题中最简单基本的算法——动态规划（Dynamic Programming），介绍了贝尔曼方程在该算法中的应用。</p>","more":"<h1 id=\"DP的基本概念\"><a href=\"#DP的基本概念\" class=\"headerlink\" title=\"DP的基本概念\"></a>DP的基本概念</h1><blockquote>\n<p>动态规划(dynamic programming)是运筹学的一个分支，是求解决策过程(decision process)最优化的数学方法。20世纪50年代初美国数学家R.E.Bellman等人在研究多阶段决策过程(multistep decision process)的优化问题时，提出了著名的最优化原理(principle of optimality)，把多阶段过程转化为一系列单阶段问题，利用各阶段之间的关系，逐个求解，创立了解决这类过程优化问题的新方法——动态规划。1957年出版了他的名著《Dynamic Programming》，这是该领域的第一本著作。——<a href=\"[https://baike.baidu.com/item/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/529408?fr=aladdin](https://baike.baidu.com/item/动态规划/529408?fr=aladdin\">百度百科</a>)</p>\n</blockquote>\n<p>动态规划-DP算法指的不是单一一个算法，而是<strong>一系列可以在给定满足MDP的完全可知环境模型中计算出最优策略的算法</strong>。</p>\n<p>DP的特点：</p>\n<ul>\n<li>Model-Based</li>\n<li>Value-Based</li>\n<li>Off-Policy(这个比较牵强，因为DP不涉及采样、预测，完全靠planning)</li>\n</ul>\n<p>DP具有很重要的理论基础作用，但是在现在的强化学习问题中，DP并不常使用，主要原因有二：</p>\n<ul>\n<li>需要完全可知的模型，状态空间、动作空间离散，状态转移、奖励函数可知且确定</li>\n<li>计算量很大(每次更新都需要完全规划所有可能性)</li>\n</ul>\n<p>在一些表格型的问题中，如完全可知的迷宫，可以使用DP，但是要解决人类现实世界极其复杂的问题、任务，DP可能就有些力不从心啦。</p>\n<p>其实，所有的强化学习算法都可以被认为是在<strong>不完全可知的环境</strong>中使用<strong>少量计算</strong>得到如DP效果一样的策略（最优策略）。</p>\n<h1 id=\"算法\"><a href=\"#算法\" class=\"headerlink\" title=\"算法\"></a>算法</h1><p>先回顾一下之前提到的贝尔曼方程。</p>\n<p>贝尔曼期望方程：</p>\n<script type=\"math/tex; mode=display\">\nv_{\\pi}(s) =\\sum_{a}\\pi(a\\mid s)\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]</script><script type=\"math/tex; mode=display\">\nq_{\\pi}(s,a) =\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]</script><p>贝尔曼最优方程：</p>\n<script type=\"math/tex; mode=display\">\nv_{*}(s) =\\max_{a}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\right]</script><script type=\"math/tex; mode=display\">\nq_{*}(s,a) =\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]</script><p>为什么要再次提到贝尔曼方程呢？因为动态规划算法中的策略迭代、值迭代就是将贝尔曼方程给结合了起来。</p>\n<p>回顾一下在<a href=\"./价值与贝尔曼方程.html\">价值与贝尔曼方差</a>最后的例子中迭代计算$V(S)$和$Q(S,A)$，遍历所有$Q(S,A)$以计算$V(S)$，再遍历所有$V(S)$以计算$Q(S,A)$。在一个简单的场景下循环迭代至收敛就需要很大的计算量，如果在复杂场景中(还是完全可知的)迭代计算可想有多费时费力，动态规划DP下的Policy Iteration和Value Iteration减轻了计算的负担，同时又不影响收敛性。</p>\n<p>引用《Reinfocement Learning : An Introduction》中的一个网格世界的例子：</p>\n<p><img src=\"./dynamic-programming/gridworld.png\" alt=\"\"></p>\n<p>在这个例子中，有1-14个非终态以及两个终态(左上角、右下角)，动作空间为上下左右四种，在边缘位置的状态，例如$S=1$，可选的动作只有左右下三种，且等概率选择每种动作，每进行一次移动，就给予-1的奖励值。智能体需要尽快的到达网格世界的出口-终态，以获得尽量少的负奖励(即累计奖励最大)。</p>\n<p>如果按照<strong>先遍历所有$Q(S,A)$以计算$V(S)$，再遍历所有$V(S)$以计算$Q(S,A)$</strong>的方式计算，值函数的表格将会如下图所示：</p>\n<p><img src=\"./dynamic-programming/iteration.png\" alt=\"\"></p>\n<p>左边展示的是进行$k$次迭代，使值函数表格可以收敛，右边表示在每次迭代中，取$a=argmax_{a}q(s,a)$的策略。</p>\n<p>可以发现，这样迭代$V(S)$至收敛有两个浪费算力的地方：</p>\n<ol>\n<li>选择动作的概率完全按照环境设置，导致计算状态$s$的值函数时，最差的动作$a$所带来的影响也被计算在其中，但其实真正执行的时候，永远不会执行该动作。</li>\n<li>$k=3$与$k=10$时的策略表示一样，也就意味着，不必等到$V(S)$迭代至完全收敛就有可能可以获得最优策略，那么后续迭代完全没有用处，造成了资源浪费。</li>\n</ol>\n<p>策略迭代、值迭代的思想都是贪心策略，但策略迭代针对问题1通过<strong>剪裁可选动作</strong>的方式进行了优化，值迭代针对问题2通过<strong>取最大动作值函数</strong>的方式进行了优化。</p>\n<h2 id=\"策略迭代-Policy-Iteration\"><a href=\"#策略迭代-Policy-Iteration\" class=\"headerlink\" title=\"策略迭代 Policy Iteration\"></a>策略迭代 Policy Iteration</h2><p>动作是通过策略产生的，因此势必需要对初始策略（GridWorld中的完全随机策略）进行替换，以达到更改动作选取概率的目的。</p>\n<p>既然要在不同的阶段更改动作选择的概率，那么要进行多次策略的更改，随之而来的问题就是：</p>\n<ol>\n<li>值函数迭代更新到什么情况下时，开始更新策略</li>\n<li>如何更新策略？</li>\n</ol>\n<p>针对第一个问题，我们使用<strong>策略评估Policy Evaluation</strong>的方式来解决，针对第二个问题，我们使用<strong>策略提升Policy Improvation</strong>的方式来解决，最终当新策略与旧策略相同时，终止迭代，得到最优策略。</p>\n<script type=\"math/tex; mode=display\">\n\\pi_{0} \\xrightarrow{E} v_{\\pi_{0}} \\xrightarrow{I} \\pi_{1} \\xrightarrow{E} v_{\\pi_{1}} \\xrightarrow{I} \\pi_{2} \\xrightarrow{E} ... \\xrightarrow{I} \\pi_{\\ast} \\xrightarrow{E} v_{\\pi_{\\ast}}</script><h3 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h3><p><img src=\"./dynamic-programming/pi.png\" alt=\"\"></p>\n<h3 id=\"策略评估-Policy-Evaluation\"><a href=\"#策略评估-Policy-Evaluation\" class=\"headerlink\" title=\"策略评估 Policy Evaluation\"></a>策略评估 Policy Evaluation</h3><p>问题：值函数迭代更新到什么情况下时，开始更新策略</p>\n<p>答：设置更新幅度阈值$\\theta$，当sweep(横扫)一遍状态空间计算$V(S)$时，与上次更新时的$V(S)$相比较，如果最大的更新幅度小于阈值$\\theta$，即$\\Delta \\lt \\theta$，则认为策略评估已经完成，开始进行策略更新</p>\n<p>特点：使用了贝尔曼期望方程-$v_{\\pi}(s)$</p>\n<p><em>注：策略评估指的不是评估一个策略的好坏，而是在当前策略下评估所有的状态值，使状态值表格近似收敛。</em></p>\n<h3 id=\"策略提升-Policy-Improvement\"><a href=\"#策略提升-Policy-Improvement\" class=\"headerlink\" title=\"策略提升 Policy Improvement\"></a>策略提升 Policy Improvement</h3><p>问题：如何更新策略？</p>\n<p>答：根据策略评估步骤得到的值函数$V(S)$，计算$Q(S,A)$表格，选取每个状态下使动作值函数最大的动作作为新的动作集，每个动作的选择概率相同，接着进行策略评估。</p>\n<p>特点：使用了贝尔曼期望方程-$q_{\\pi}(s,a)$</p>\n<h2 id=\"值迭代-Value-Iteration\"><a href=\"#值迭代-Value-Iteration\" class=\"headerlink\" title=\"值迭代 Value Iteration\"></a>值迭代 Value Iteration</h2><p>策略迭代的一个缺点是在得到最优策略$\\pi_{\\ast}$之前，需要多次更新策略，每次更新策略都会引起可选取动作的改变，这会引起在更新完策略后的前几次策略评估中值函数偏差比较大，导致在策略评估过程中需要花费大量的迭代来减小更新幅度$\\Delta$，因此需要多次sweep(横扫)$V(S)$来迭代计算。</p>\n<p>值迭代的思想是：<strong>能不能通过早停的方式，在不更改策略的情况下，直接一次得到最优策略？</strong></p>\n<p>值迭代运用了策略评估、策略迭代的思想，并将它们融合在一起，即不更新策略，直接选择动作值函数$q(s,a)$最大的动作作为状态值$v(s)$，直接迭代出<strong>近似最优</strong>（早停，$\\Delta \\lt \\theta$即可）状态价值函数$V_{\\ast}(S)$，使用贪心策略进而得到最优策略$\\pi_{\\ast}$。</p>\n<p>特点：使用了贝尔曼最优方程-$v_{\\ast}(s)，q_{\\ast}(s,a)$。</p>\n<h3 id=\"伪代码-1\"><a href=\"#伪代码-1\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h3><p><img src=\"./dynamic-programming/vi.png\" alt=\"\"></p>\n<h2 id=\"PI与VI的比较\"><a href=\"#PI与VI的比较\" class=\"headerlink\" title=\"PI与VI的比较\"></a>PI与VI的比较</h2><p><img src=\"./dynamic-programming/pivsvi.png\" alt=\"\"></p>\n<p>相同点：</p>\n<ul>\n<li>在$0 \\leq \\gamma \\lt 1$，有限MDPs环境中，两种方式都可以收敛到最优策略$\\pi_{\\ast}$</li>\n<li>都使用了贝尔曼方程进行状态值函数的迭代</li>\n</ul>\n<p>不同点：</p>\n<ol>\n<li>收敛方式<ul>\n<li>策略迭代PI包括策略评估Policy Evaluation和策略提升Policy Improvement，这两部循环迭代至策略收敛</li>\n<li>值迭代VI包括找到最优状态值函数和一步提取策略，这两步不需要循环迭代，而是根据最优值函数直接得到最优策略</li>\n</ul>\n</li>\n<li>动作改变方式<ul>\n<li>策略迭代PI获得新策略$\\pi_{new}$后，<strong>更改每个状态的可选动作集</strong>，多次横扫（遍历）$V(S)$</li>\n<li>值迭代VI过程中不产生策略，不更改每个状态的可选动作集，但是<strong>只取每个状态下动作值函数最大的动作作为状态值</strong>，一次横扫（遍历）$V(S)$。（这里需要解释一下，虽然循环是多次遍历，但是因为max操作，每次遍历每个状态所选取的动作不一定一样，虽然策略一直是随机策略，没有产生新策略，但是计算过程没有遍历到所有动作，可以隐含的看作是一个新策略，因此每次遍历时这个“隐策略”都会改变，所以称为一次遍历。）</li>\n</ul>\n</li>\n<li>计算方式<ul>\n<li>策略迭代PI使用贝尔曼期望方程</li>\n<li>值迭代VI使用贝尔曼最优方程</li>\n</ul>\n</li>\n<li>Policy方式<ul>\n<li>策略迭代PI是On-Policy</li>\n<li>值迭代VI是Off-Policy</li>\n</ul>\n</li>\n<li>稳定性检查<ul>\n<li>策略迭代PI中更新策略时进行了策略稳定性检查，判断是否收敛</li>\n<li>值迭代VI获得新策略$\\pi$时没有进行策略稳定性检查</li>\n</ul>\n</li>\n</ol>\n<p>至于策略迭代PI与值迭代VI的收敛速度，<strong>通常情况</strong>下，PI的迭代次数更少，VI的运行时间更少。</p>\n<blockquote>\n<p><a href=\"https://stackoverflow.com/a/42493295/11483803\" rel=\"external nofollow\" target=\"_blank\">What is the difference between value iteration and policy iteration?</a></p>\n<p><a href=\"http://incompleteideas.net/book/RLbook2018.pdf\" rel=\"external nofollow\" target=\"_blank\">《Reinforcement Learning : An Introduction 2nd Edition》p77</a></p>\n</blockquote>"},{"title":"Build一个基于Mxnet的Sniper镜像","copyright":true,"top":1,"date":"2019-01-02T13:58:44.000Z","_content":"\n本文记录了如何在学校机器学习平台上创建一个基于Mxnet的Sniper镜像。\n\n<!--more-->\n\n# 说明\n\n由于此镜像是用于学校机器学习平台,所以文中会出现FTP服务器等字眼,其实是在平台上使用镜像创建一个容器时,平台会**自动**将服务器上我所申请的文件存储区`mount`到创建的容器,我通过`FileZilla`FTP工具与在平台申请的文件存储区进行连接\n​\t\n本文教程虽然有了一个FTP过程,但是如果是生成本地镜像,不考虑FTP,无视文中相关部分即可\n\n**虽然本文中写了关于压缩的相关内容,但是最终并没有使用压缩,原因是由于压缩后出现未知问题,导致在平台上创建的容器不能使用宿主机的NVIDIA驱动,并不能成功运行Demo**\n\n# 环境\n\n本机环境\n- windows 10 专业版\n- docker client version 18.09.0\n- docker server version 18.09.0\n- FTP工具 FileZilla\n\n平台环境\n- docker version 17.06.2-ce\n\n镜像环境\n- python 2.7.12\n- CUDA version 9.0.176\n- pip 9.0.3\n\n[SNIPER](https://github.com/mahyarnajibi/SNIPER)\n[机器学习平台](http://10.0.4.228),这是学校资源\n\n# 一 配置基础镜像\n\n从学校机器学习平台上拉取原始镜像,因为这个镜像配好了一些基本的环境,如python2.x,CUDA9.0等等,所以直接使用它们的镜像作为基础镜像比较省心省力\n`docker pull hub.hoc.ccshu.net/ces/deepo:all-py27-jupyter-ssh`\n\n拉取到镜像之后,可以选择使用`Dockerfile`来生成我们需要的镜像,但是往往我们需要在镜像中添加许多库/包/插件,而且使用`Dockerfile`来生成镜像很容易出BUG.当然,最好的方式是使用`Dockerfile`,前提是你能确保`Dockerfile`文件中的每一行命令都不会出错.\n在当前情况下,我选择使用从容器生成镜像的方法,这种方式会使得最终生成的镜像占内存巨大,但是可以在容器内部调试每一步配置过程.\n使用`docker run -itd --name [name] hub.hoc.ccshu.net/ces/deepo:all-py27-jupyter-ssh`开启一个容器\n\n使用`docker ps -a`查看正在运行的容器`ID`\n\n使用`docker exec -it [name] /bin/bash`进入容器\n\n在容器中使用`cat /etc/issue`命令查看容器的操作系统版本\n\n结果输出: `Ubuntu 16.04.4 LTS \\n \\l`\n\n## 安装 apt-file\n\n安装`apt-file`\n\n`apt-get install apt-file -y`\n\n出现错误:\n\n![](./create-sniper-docker-image/Snipaste_2019-01-03_08-30-41.png)\n\n使用`apt-get install apt-file -y --fix-missing`同样不能解决问题\n\n考虑**换源**\n\n`cp /etc/apt/sources.list /etc/apt/sources.list.bak`备份系统原有的源\n\n安装Linux下的文本编辑器`nano`,执行命令`apt-get install nano -y`\n安装`nano`成功后,执行`nano /etc/apt/sources.list`修改源文件\n在打开的文件中,将内容替换为\n```\n# deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted\ndeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties\ndeb http://archive.canonical.com/ubuntu xenial partner\ndeb-src http://archive.canonical.com/ubuntu xenial partner\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse\n```\n\n这里使用的源是阿里的镜像站,也可以使用网易163的,源如下:\n```\ndeb http://mirrors.163.com/ubuntu/ xenial main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ xenial main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse\n```\n\n更改好源文件后,执行`sudo apt-get update`更新源\n\n再次执行`apt-get install apt-file -y`,可以成功安装`apt-file`包\n\n之后执行`apt-file update`更新apt-file cache\n使用`apt-file find [name]`可以查找计算机上文件的位置,很方便\n使用`apt-file search [name]`可以搜索缺少的库,解决文件缺失依赖\n选择好自己需要的包,然后使用`apt-get install [name]`即可\n\n- 如果需要把镜像上传到云上使用,有可能需要网络服务,\n- 执行`apt-get install net-tools`安装ifconfig\n- 执行`apt-get install iputils-ping`安装ping\n\n此时为了避免诸如使用`ping [IP]`有效,但是`ping [HOST]`无效的情况,需要使用`nano /etc/resolv.conf`修改配置文件\n将`namespace`后的IP地址更改为`8.8.8.8`或者`4.4.4.4`\n*或者使用`echo \"nameserver 114.114.114.114 > /etc/resolv.conf\"`也可以*\n退出保存即可\n\n*有可能上述修改DNS的方式并不成功,原因是在云上运行容器时,配置文件自动修改,如果发生这种情况,请每次在新开一个容器时,手动修改配置文件的DNS服务器,使其可以使用网络服务*\n\n# 二 安装编译依赖各种包\n\n在电脑上空闲的地方,从Github拉取Sniper项目\n\n`git clone --recursive https://github.com/mahyarnajibi/SNIPER.git`\n\n- 因为我是在学校机器学习平台上运行docker容器,所以选择直接将clone下的文件上传至容器`mount`的ftp服务器,使用的软件是`FileZilla`\n\n- 上传成功后可以在容器内通过`cd /data/[file or folder name]`进行访问\n\n如果要在本地镜像内操作的话,也可以直接把本机文件或文件夹拷贝过去\n`docker cp 本地文件路径 ID全称:容器路径`\n\n---\n\n`cd /data/SNIPER/SNIPER-mxnet`\n`make USE_CUDA_PATH=/usr/local/cuda-9.0`\n输出信息:\n![](./create-sniper-docker-image/Snipaste_2019-01-03_09-42-07.png)\n\n## 安装 jemalloc\n\n选择安装`jemalloc`,这个工具可以加速编译,碎片整理,具体请自行谷歌\n- `apt-get install autoconf`\n- `apt-get install automake`\n- `apt-get install libtool`\n- `git clone https://github.com/jemalloc/jemalloc.git`\n- `cd jemalloc`\n- `git checkout 4.5.0`安装4.5.0版本的jemalloc,5.x版本的有坑,深坑\n- `./autogen.sh`\n- `make`\n- `make install_bin install_include install_lib`,之所以不使用`make install`是因为会报错,如下: ![](./create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png)\n\n切换至`SNIPER-mxnet`文件夹,再次`make USE_CUDA_PATH=/usr/local/cuda-9.0`\n虽然可以编译,但是有以下信息: \n![](./create-sniper-docker-image/Snipaste_2019-01-03_10-03-30.png)\n强迫症必须搞定它,果断`ctrl+c`终止编译\n\n## 安装 pkg-config\n\n- 打开[https://pkg-config.freedesktop.org/releases/](https://pkg-config.freedesktop.org/releases/)\n- 下载最新的,现在看到的是`pkg-config-0.29.2.tar.gz`\n- 下载好之后,通过`FileZilla`等工具传输到FTP服务器\n- 在容器内`cd`到压缩包位置\n- `tar -xf pkg-config-0.29.2.tar.gz`\n- `cd pkg-config-0.29.2`\n- `./configure --with-internal-glib`,注意,中间是一个空格,非常关键\n- `make && make install`\n![](./create-sniper-docker-image/Snipaste_2019-01-03_10-11-01.png)\n\n再次`make USE_CUDA_PATH=/usr/local/cuda-9.0`\n算了，还是安装一下cudnn吧\n\n## 安装 cudnn7.0\n- [https://developer.nvidia.com/rdp/cudnn-archive](https://developer.nvidia.com/rdp/cudnn-archive) 下载cuDNN Libraries for Linux,不要下载 Power 8\n- 把下载好的包上传到FTP服务器\n- `cd`到包位置\n- `cp cudnn-9.0-linux-x64-v7.solitairetheme8 cudnn-9.0-linux-x64-v7.tgz`\n- `tar -xvf cudnn-9.0-linux-x64-v7.tgz`\n- `cp include/* /usr/local/cuda-9.0/include`\n- `cp lib64/* /usr/local/cuda-9.0/lib64`\n- `chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn*`\n- `export PATH=/usr/local/cuda-9.0/bin:$PATH`\n- `cd`到`/usr/local/cuda-9.0/lib64`\n- `nano ~/.bashrc`,关联环境变量\n- 在最后一行加入`export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH`\n- `source ~/.bashrc`\n- `ldconfig -v`\n- 使用`cat /usr/local/cuda-9.0/include/cudnn.h | grep CUDNN_MAJOR -A 2` 查看cudnn版本\n![](./create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png)\n\n## 安装 OpenCV\n- 使用`pkg-config opencv --modversion`查看\n- 发现已经有OpenCV\n![](./create-sniper-docker-image/Snipaste_2019-01-03_10-57-56.png)\n\n## 安装 OpenBLAS\n- `apt-get install libopenblas-dev`\n\n## 编译 Mxnet\n\n`make USE_CUDA_PATH=/usr/local/cuda-9.0`\n![](./create-sniper-docker-image/Snipaste_2019-01-03_13-49-28.png)\n\n**心好累,总共make了将近两个半小时**\n\n编译`c++`文件`bash scripts/compile.sh`\n这一步一定要在`/SNIPER/`文件夹下,不然贼坑,绝对不要`cd`到`/SNIPER/scripts`文件夹下再`bash compile.sh`,因为代码内有`cd lib/nms`等,如果不在`/SNIPER`文件夹下,会找不到文件\n\n如果出现`syntax error near unexpected token `$'\\r''`错误,可以使用`sed`命令将`\\r`去掉,或者是在[Github](https://github.com/mahyarnajibi/SNIPER/blob/master/scripts/compile.sh)上将代码复制,使用`nano`编辑然后粘贴\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-32-08.png)\n可以使用`cat -v [filename]`查看\n![]./create-sniper-docker-image/Snipaste_2019-01-03_16-33-27.png)\n以`^M`结尾的代表你所处理的文件换行符是dos格式的`\"\\r\\n\"`\n\n我选择第二种笨方法,因为涉及的代码并不多\n\n执行结果:\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-24-49.png)\n\n## 安装 dos2unix\n\n由于发现这种简单的复制粘贴方式并不能很好的解决,所以查了一些[相关资料](https://blog.csdn.net/lovelovelovelovelo/article/details/79239068)\n选择使用`dos2unix`来转换\n\n- `apt-get install dos2unix`\n- `dos2unix [filename]`\n\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-40-53.png)\n问题解决啦\n\n## 安装依赖\n\n在`/SNIPER/`文件夹下`pip install -r requirements.txt`\n一定要确保镜像内可以联网\n\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-29-58.png)\n\n## 测试Demo\n\n- `bash download_sniper_detector.sh`,download_sniper_detector.sh\n文件在`/SNIPER/scripts`文件夹下\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-44-22.png)\n- `cd .. && python demo.py`\n![](./create-sniper-docker-image/Snipaste_2019-01-03_17-05-30.png)\n\n**运行成功!!!**\n\n# 三 生成镜像\n- 使用`exit`退出容器\n- 使用`docker ps -a`查看容器ID\n- 使用`docker stop [ID]`停止容器\n- 使用`docker commit -a \"作者信息\" -m \"附带信息\" [ID] [name]:[tag]`生成镜像,会返回一个`sha256`开头的长ID,这个就是生成的镜像ID\n- 使用`docker images`查看生成的镜像\n- 如果需要的话,使用`docker push [name]:[tag]`将刚刚生成的镜像推送到云上\n\n# 四 压缩镜像\n\n**压缩镜像非常麻烦,但是也是有方法的,目前大概三种方法**\n\n1. 使用`Dockerfile`生成镜像\n2. 这种方法需要让容器在运行状态,使用`docker export [ID] | docker import - [name]:[tag]`导出容器快照,并从快照生成镜像,这种方式可以大大压缩镜像,但是缺点是有可能会使得镜像中的环境变量、开放端口、默认进入命令改变或消失.使用这种方式时,最好在生成镜像之后,创建一个`Dockerfile`文件,`From`这个镜像,并添加端口和命令入口\n3. 使用`docker-squash`压缩镜像,这个方法适用于Linux和Mac系统\n\n目前可以运行的镜像是13.6G\n![](./create-sniper-docker-image/Snipaste_2019-01-03_17-12-36.png)\n`hub.hoc.ccshu.net/wjs/sniper:v1.1`\n现在要对它进行压缩\n\n## 第一步,移除镜像内的SNIPER文件夹,把其放到FTP服务器上去\n\n- 开启一个容器`docker run -itd --name [name] [id]`\n- 复制容器内文件到本地`docker cp [长ID]:[容器内路径] [本地路径]`,将放置在本地的文件夹上传至FTP服务器\n- 进入容器`docker exec -it [name] /bin/bash`\n- 删除容器内文件夹`/SNIPER/`,使用`rm -rf SNIPER`,**一定要小心使用**\n- 退出容器`exit`\n\n## 第二步,压缩镜像\n\n压缩容器\n`docker export [ID] | docker import - [name]:[tag]`\n\n可以看到,镜像体积少了大约2个G\n![](./create-sniper-docker-image/Snipaste_2019-01-03_17-39-07.png)\n\n由于使用这种方法会使得镜像丢失部分信息,所以,创建一个新的`Dockerfile`,在其中添加缺失的信息\n\n## 第三步,完善镜像\n\n在任意位置新建`Dockerfile`\n输入\n```\nFROM [name]:[tag]\nEXPOSE 22\nENTRYPOINT [\"/usr/sbin/sshd\",\"-D\"]\n```\n![](./create-sniper-docker-image/Snipaste_2019-01-03_18-08-49.png)\n\n然后`docker build -t [name]:[tag] .`,不要忘了最后的`.`\n\n## 第四步 Push\n\n`docker push [name]:[tag]`\n\n至此,所有配置以及完成\n镜像在`hoc.hoc.ccshu.net`的私有仓库里\nSNIPER文件夹放置在机器学习平台服务器`mount`的目录里\n\n# 五 测试\n\n- 在平台上创建容器\n![](./create-sniper-docker-image/Snipaste_2019-01-03_18-11-49.png)\n\n- 耐心等待创建完成\n![](./create-sniper-docker-image/Snipaste_2019-01-03_18-14-21.png)\n\n- 创建成功\n![](./create-sniper-docker-image/Snipaste_2019-01-03_18-20-07.png)\n\n- 测试结果\n![](./create-sniper-docker-image/Snipaste_2019-01-03_18-31-16.png)\n**测试失败**\n\n**但是,使用未压缩的镜像测试成功**\n","source":"_posts/create-sniper-docker-image.md","raw":"---\ntitle: Build一个基于Mxnet的Sniper镜像\ncopyright: true\ntop: 1\ndate: 2019-01-02 21:58:44\ncategories:  Docker\ntags:\n- docker\n- mxnet\n- sniper\n\n---\n\n本文记录了如何在学校机器学习平台上创建一个基于Mxnet的Sniper镜像。\n\n<!--more-->\n\n# 说明\n\n由于此镜像是用于学校机器学习平台,所以文中会出现FTP服务器等字眼,其实是在平台上使用镜像创建一个容器时,平台会**自动**将服务器上我所申请的文件存储区`mount`到创建的容器,我通过`FileZilla`FTP工具与在平台申请的文件存储区进行连接\n​\t\n本文教程虽然有了一个FTP过程,但是如果是生成本地镜像,不考虑FTP,无视文中相关部分即可\n\n**虽然本文中写了关于压缩的相关内容,但是最终并没有使用压缩,原因是由于压缩后出现未知问题,导致在平台上创建的容器不能使用宿主机的NVIDIA驱动,并不能成功运行Demo**\n\n# 环境\n\n本机环境\n- windows 10 专业版\n- docker client version 18.09.0\n- docker server version 18.09.0\n- FTP工具 FileZilla\n\n平台环境\n- docker version 17.06.2-ce\n\n镜像环境\n- python 2.7.12\n- CUDA version 9.0.176\n- pip 9.0.3\n\n[SNIPER](https://github.com/mahyarnajibi/SNIPER)\n[机器学习平台](http://10.0.4.228),这是学校资源\n\n# 一 配置基础镜像\n\n从学校机器学习平台上拉取原始镜像,因为这个镜像配好了一些基本的环境,如python2.x,CUDA9.0等等,所以直接使用它们的镜像作为基础镜像比较省心省力\n`docker pull hub.hoc.ccshu.net/ces/deepo:all-py27-jupyter-ssh`\n\n拉取到镜像之后,可以选择使用`Dockerfile`来生成我们需要的镜像,但是往往我们需要在镜像中添加许多库/包/插件,而且使用`Dockerfile`来生成镜像很容易出BUG.当然,最好的方式是使用`Dockerfile`,前提是你能确保`Dockerfile`文件中的每一行命令都不会出错.\n在当前情况下,我选择使用从容器生成镜像的方法,这种方式会使得最终生成的镜像占内存巨大,但是可以在容器内部调试每一步配置过程.\n使用`docker run -itd --name [name] hub.hoc.ccshu.net/ces/deepo:all-py27-jupyter-ssh`开启一个容器\n\n使用`docker ps -a`查看正在运行的容器`ID`\n\n使用`docker exec -it [name] /bin/bash`进入容器\n\n在容器中使用`cat /etc/issue`命令查看容器的操作系统版本\n\n结果输出: `Ubuntu 16.04.4 LTS \\n \\l`\n\n## 安装 apt-file\n\n安装`apt-file`\n\n`apt-get install apt-file -y`\n\n出现错误:\n\n![](./create-sniper-docker-image/Snipaste_2019-01-03_08-30-41.png)\n\n使用`apt-get install apt-file -y --fix-missing`同样不能解决问题\n\n考虑**换源**\n\n`cp /etc/apt/sources.list /etc/apt/sources.list.bak`备份系统原有的源\n\n安装Linux下的文本编辑器`nano`,执行命令`apt-get install nano -y`\n安装`nano`成功后,执行`nano /etc/apt/sources.list`修改源文件\n在打开的文件中,将内容替换为\n```\n# deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted\ndeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties\ndeb http://archive.canonical.com/ubuntu xenial partner\ndeb-src http://archive.canonical.com/ubuntu xenial partner\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse\n```\n\n这里使用的源是阿里的镜像站,也可以使用网易163的,源如下:\n```\ndeb http://mirrors.163.com/ubuntu/ xenial main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ xenial main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse\n```\n\n更改好源文件后,执行`sudo apt-get update`更新源\n\n再次执行`apt-get install apt-file -y`,可以成功安装`apt-file`包\n\n之后执行`apt-file update`更新apt-file cache\n使用`apt-file find [name]`可以查找计算机上文件的位置,很方便\n使用`apt-file search [name]`可以搜索缺少的库,解决文件缺失依赖\n选择好自己需要的包,然后使用`apt-get install [name]`即可\n\n- 如果需要把镜像上传到云上使用,有可能需要网络服务,\n- 执行`apt-get install net-tools`安装ifconfig\n- 执行`apt-get install iputils-ping`安装ping\n\n此时为了避免诸如使用`ping [IP]`有效,但是`ping [HOST]`无效的情况,需要使用`nano /etc/resolv.conf`修改配置文件\n将`namespace`后的IP地址更改为`8.8.8.8`或者`4.4.4.4`\n*或者使用`echo \"nameserver 114.114.114.114 > /etc/resolv.conf\"`也可以*\n退出保存即可\n\n*有可能上述修改DNS的方式并不成功,原因是在云上运行容器时,配置文件自动修改,如果发生这种情况,请每次在新开一个容器时,手动修改配置文件的DNS服务器,使其可以使用网络服务*\n\n# 二 安装编译依赖各种包\n\n在电脑上空闲的地方,从Github拉取Sniper项目\n\n`git clone --recursive https://github.com/mahyarnajibi/SNIPER.git`\n\n- 因为我是在学校机器学习平台上运行docker容器,所以选择直接将clone下的文件上传至容器`mount`的ftp服务器,使用的软件是`FileZilla`\n\n- 上传成功后可以在容器内通过`cd /data/[file or folder name]`进行访问\n\n如果要在本地镜像内操作的话,也可以直接把本机文件或文件夹拷贝过去\n`docker cp 本地文件路径 ID全称:容器路径`\n\n---\n\n`cd /data/SNIPER/SNIPER-mxnet`\n`make USE_CUDA_PATH=/usr/local/cuda-9.0`\n输出信息:\n![](./create-sniper-docker-image/Snipaste_2019-01-03_09-42-07.png)\n\n## 安装 jemalloc\n\n选择安装`jemalloc`,这个工具可以加速编译,碎片整理,具体请自行谷歌\n- `apt-get install autoconf`\n- `apt-get install automake`\n- `apt-get install libtool`\n- `git clone https://github.com/jemalloc/jemalloc.git`\n- `cd jemalloc`\n- `git checkout 4.5.0`安装4.5.0版本的jemalloc,5.x版本的有坑,深坑\n- `./autogen.sh`\n- `make`\n- `make install_bin install_include install_lib`,之所以不使用`make install`是因为会报错,如下: ![](./create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png)\n\n切换至`SNIPER-mxnet`文件夹,再次`make USE_CUDA_PATH=/usr/local/cuda-9.0`\n虽然可以编译,但是有以下信息: \n![](./create-sniper-docker-image/Snipaste_2019-01-03_10-03-30.png)\n强迫症必须搞定它,果断`ctrl+c`终止编译\n\n## 安装 pkg-config\n\n- 打开[https://pkg-config.freedesktop.org/releases/](https://pkg-config.freedesktop.org/releases/)\n- 下载最新的,现在看到的是`pkg-config-0.29.2.tar.gz`\n- 下载好之后,通过`FileZilla`等工具传输到FTP服务器\n- 在容器内`cd`到压缩包位置\n- `tar -xf pkg-config-0.29.2.tar.gz`\n- `cd pkg-config-0.29.2`\n- `./configure --with-internal-glib`,注意,中间是一个空格,非常关键\n- `make && make install`\n![](./create-sniper-docker-image/Snipaste_2019-01-03_10-11-01.png)\n\n再次`make USE_CUDA_PATH=/usr/local/cuda-9.0`\n算了，还是安装一下cudnn吧\n\n## 安装 cudnn7.0\n- [https://developer.nvidia.com/rdp/cudnn-archive](https://developer.nvidia.com/rdp/cudnn-archive) 下载cuDNN Libraries for Linux,不要下载 Power 8\n- 把下载好的包上传到FTP服务器\n- `cd`到包位置\n- `cp cudnn-9.0-linux-x64-v7.solitairetheme8 cudnn-9.0-linux-x64-v7.tgz`\n- `tar -xvf cudnn-9.0-linux-x64-v7.tgz`\n- `cp include/* /usr/local/cuda-9.0/include`\n- `cp lib64/* /usr/local/cuda-9.0/lib64`\n- `chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn*`\n- `export PATH=/usr/local/cuda-9.0/bin:$PATH`\n- `cd`到`/usr/local/cuda-9.0/lib64`\n- `nano ~/.bashrc`,关联环境变量\n- 在最后一行加入`export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH`\n- `source ~/.bashrc`\n- `ldconfig -v`\n- 使用`cat /usr/local/cuda-9.0/include/cudnn.h | grep CUDNN_MAJOR -A 2` 查看cudnn版本\n![](./create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png)\n\n## 安装 OpenCV\n- 使用`pkg-config opencv --modversion`查看\n- 发现已经有OpenCV\n![](./create-sniper-docker-image/Snipaste_2019-01-03_10-57-56.png)\n\n## 安装 OpenBLAS\n- `apt-get install libopenblas-dev`\n\n## 编译 Mxnet\n\n`make USE_CUDA_PATH=/usr/local/cuda-9.0`\n![](./create-sniper-docker-image/Snipaste_2019-01-03_13-49-28.png)\n\n**心好累,总共make了将近两个半小时**\n\n编译`c++`文件`bash scripts/compile.sh`\n这一步一定要在`/SNIPER/`文件夹下,不然贼坑,绝对不要`cd`到`/SNIPER/scripts`文件夹下再`bash compile.sh`,因为代码内有`cd lib/nms`等,如果不在`/SNIPER`文件夹下,会找不到文件\n\n如果出现`syntax error near unexpected token `$'\\r''`错误,可以使用`sed`命令将`\\r`去掉,或者是在[Github](https://github.com/mahyarnajibi/SNIPER/blob/master/scripts/compile.sh)上将代码复制,使用`nano`编辑然后粘贴\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-32-08.png)\n可以使用`cat -v [filename]`查看\n![]./create-sniper-docker-image/Snipaste_2019-01-03_16-33-27.png)\n以`^M`结尾的代表你所处理的文件换行符是dos格式的`\"\\r\\n\"`\n\n我选择第二种笨方法,因为涉及的代码并不多\n\n执行结果:\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-24-49.png)\n\n## 安装 dos2unix\n\n由于发现这种简单的复制粘贴方式并不能很好的解决,所以查了一些[相关资料](https://blog.csdn.net/lovelovelovelovelo/article/details/79239068)\n选择使用`dos2unix`来转换\n\n- `apt-get install dos2unix`\n- `dos2unix [filename]`\n\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-40-53.png)\n问题解决啦\n\n## 安装依赖\n\n在`/SNIPER/`文件夹下`pip install -r requirements.txt`\n一定要确保镜像内可以联网\n\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-29-58.png)\n\n## 测试Demo\n\n- `bash download_sniper_detector.sh`,download_sniper_detector.sh\n文件在`/SNIPER/scripts`文件夹下\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-44-22.png)\n- `cd .. && python demo.py`\n![](./create-sniper-docker-image/Snipaste_2019-01-03_17-05-30.png)\n\n**运行成功!!!**\n\n# 三 生成镜像\n- 使用`exit`退出容器\n- 使用`docker ps -a`查看容器ID\n- 使用`docker stop [ID]`停止容器\n- 使用`docker commit -a \"作者信息\" -m \"附带信息\" [ID] [name]:[tag]`生成镜像,会返回一个`sha256`开头的长ID,这个就是生成的镜像ID\n- 使用`docker images`查看生成的镜像\n- 如果需要的话,使用`docker push [name]:[tag]`将刚刚生成的镜像推送到云上\n\n# 四 压缩镜像\n\n**压缩镜像非常麻烦,但是也是有方法的,目前大概三种方法**\n\n1. 使用`Dockerfile`生成镜像\n2. 这种方法需要让容器在运行状态,使用`docker export [ID] | docker import - [name]:[tag]`导出容器快照,并从快照生成镜像,这种方式可以大大压缩镜像,但是缺点是有可能会使得镜像中的环境变量、开放端口、默认进入命令改变或消失.使用这种方式时,最好在生成镜像之后,创建一个`Dockerfile`文件,`From`这个镜像,并添加端口和命令入口\n3. 使用`docker-squash`压缩镜像,这个方法适用于Linux和Mac系统\n\n目前可以运行的镜像是13.6G\n![](./create-sniper-docker-image/Snipaste_2019-01-03_17-12-36.png)\n`hub.hoc.ccshu.net/wjs/sniper:v1.1`\n现在要对它进行压缩\n\n## 第一步,移除镜像内的SNIPER文件夹,把其放到FTP服务器上去\n\n- 开启一个容器`docker run -itd --name [name] [id]`\n- 复制容器内文件到本地`docker cp [长ID]:[容器内路径] [本地路径]`,将放置在本地的文件夹上传至FTP服务器\n- 进入容器`docker exec -it [name] /bin/bash`\n- 删除容器内文件夹`/SNIPER/`,使用`rm -rf SNIPER`,**一定要小心使用**\n- 退出容器`exit`\n\n## 第二步,压缩镜像\n\n压缩容器\n`docker export [ID] | docker import - [name]:[tag]`\n\n可以看到,镜像体积少了大约2个G\n![](./create-sniper-docker-image/Snipaste_2019-01-03_17-39-07.png)\n\n由于使用这种方法会使得镜像丢失部分信息,所以,创建一个新的`Dockerfile`,在其中添加缺失的信息\n\n## 第三步,完善镜像\n\n在任意位置新建`Dockerfile`\n输入\n```\nFROM [name]:[tag]\nEXPOSE 22\nENTRYPOINT [\"/usr/sbin/sshd\",\"-D\"]\n```\n![](./create-sniper-docker-image/Snipaste_2019-01-03_18-08-49.png)\n\n然后`docker build -t [name]:[tag] .`,不要忘了最后的`.`\n\n## 第四步 Push\n\n`docker push [name]:[tag]`\n\n至此,所有配置以及完成\n镜像在`hoc.hoc.ccshu.net`的私有仓库里\nSNIPER文件夹放置在机器学习平台服务器`mount`的目录里\n\n# 五 测试\n\n- 在平台上创建容器\n![](./create-sniper-docker-image/Snipaste_2019-01-03_18-11-49.png)\n\n- 耐心等待创建完成\n![](./create-sniper-docker-image/Snipaste_2019-01-03_18-14-21.png)\n\n- 创建成功\n![](./create-sniper-docker-image/Snipaste_2019-01-03_18-20-07.png)\n\n- 测试结果\n![](./create-sniper-docker-image/Snipaste_2019-01-03_18-31-16.png)\n**测试失败**\n\n**但是,使用未压缩的镜像测试成功**\n","slug":"create-sniper-docker-image","published":1,"updated":"2019-05-13T11:44:34.919Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma2v002kekvef8cmnf3z","content":"<p>本文记录了如何在学校机器学习平台上创建一个基于Mxnet的Sniper镜像。</p>\n<a id=\"more\"></a>\n<h1 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h1><p>由于此镜像是用于学校机器学习平台,所以文中会出现FTP服务器等字眼,其实是在平台上使用镜像创建一个容器时,平台会<strong>自动</strong>将服务器上我所申请的文件存储区<code>mount</code>到创建的容器,我通过<code>FileZilla</code>FTP工具与在平台申请的文件存储区进行连接<br>​<br>本文教程虽然有了一个FTP过程,但是如果是生成本地镜像,不考虑FTP,无视文中相关部分即可</p>\n<p><strong>虽然本文中写了关于压缩的相关内容,但是最终并没有使用压缩,原因是由于压缩后出现未知问题,导致在平台上创建的容器不能使用宿主机的NVIDIA驱动,并不能成功运行Demo</strong></p>\n<h1 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h1><p>本机环境</p>\n<ul>\n<li>windows 10 专业版</li>\n<li>docker client version 18.09.0</li>\n<li>docker server version 18.09.0</li>\n<li>FTP工具 FileZilla</li>\n</ul>\n<p>平台环境</p>\n<ul>\n<li>docker version 17.06.2-ce</li>\n</ul>\n<p>镜像环境</p>\n<ul>\n<li>python 2.7.12</li>\n<li>CUDA version 9.0.176</li>\n<li>pip 9.0.3</li>\n</ul>\n<p><a href=\"https://github.com/mahyarnajibi/SNIPER\" rel=\"external nofollow\" target=\"_blank\">SNIPER</a><br><a href=\"http://10.0.4.228\" rel=\"external nofollow\" target=\"_blank\">机器学习平台</a>,这是学校资源</p>\n<h1 id=\"一-配置基础镜像\"><a href=\"#一-配置基础镜像\" class=\"headerlink\" title=\"一 配置基础镜像\"></a>一 配置基础镜像</h1><p>从学校机器学习平台上拉取原始镜像,因为这个镜像配好了一些基本的环境,如python2.x,CUDA9.0等等,所以直接使用它们的镜像作为基础镜像比较省心省力<br><code>docker pull hub.hoc.ccshu.net/ces/deepo:all-py27-jupyter-ssh</code></p>\n<p>拉取到镜像之后,可以选择使用<code>Dockerfile</code>来生成我们需要的镜像,但是往往我们需要在镜像中添加许多库/包/插件,而且使用<code>Dockerfile</code>来生成镜像很容易出BUG.当然,最好的方式是使用<code>Dockerfile</code>,前提是你能确保<code>Dockerfile</code>文件中的每一行命令都不会出错.<br>在当前情况下,我选择使用从容器生成镜像的方法,这种方式会使得最终生成的镜像占内存巨大,但是可以在容器内部调试每一步配置过程.<br>使用<code>docker run -itd --name [name] hub.hoc.ccshu.net/ces/deepo:all-py27-jupyter-ssh</code>开启一个容器</p>\n<p>使用<code>docker ps -a</code>查看正在运行的容器<code>ID</code></p>\n<p>使用<code>docker exec -it [name] /bin/bash</code>进入容器</p>\n<p>在容器中使用<code>cat /etc/issue</code>命令查看容器的操作系统版本</p>\n<p>结果输出: <code>Ubuntu 16.04.4 LTS \\n \\l</code></p>\n<h2 id=\"安装-apt-file\"><a href=\"#安装-apt-file\" class=\"headerlink\" title=\"安装 apt-file\"></a>安装 apt-file</h2><p>安装<code>apt-file</code></p>\n<p><code>apt-get install apt-file -y</code></p>\n<p>出现错误:</p>\n<p><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_08-30-41.png\" alt=\"\"></p>\n<p>使用<code>apt-get install apt-file -y --fix-missing</code>同样不能解决问题</p>\n<p>考虑<strong>换源</strong></p>\n<p><code>cp /etc/apt/sources.list /etc/apt/sources.list.bak</code>备份系统原有的源</p>\n<p>安装Linux下的文本编辑器<code>nano</code>,执行命令<code>apt-get install nano -y</code><br>安装<code>nano</code>成功后,执行<code>nano /etc/apt/sources.list</code>修改源文件<br>在打开的文件中,将内容替换为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted</span><br><span class=\"line\">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class=\"line\">deb http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb-src http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</span><br></pre></td></tr></table></figure></p>\n<p>这里使用的源是阿里的镜像站,也可以使用网易163的,源如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deb http://mirrors.163.com/ubuntu/ xenial main restricted universe multiverse</span><br><span class=\"line\">deb http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class=\"line\">deb http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class=\"line\">deb http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiverse</span><br><span class=\"line\">deb http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.163.com/ubuntu/ xenial main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure></p>\n<p>更改好源文件后,执行<code>sudo apt-get update</code>更新源</p>\n<p>再次执行<code>apt-get install apt-file -y</code>,可以成功安装<code>apt-file</code>包</p>\n<p>之后执行<code>apt-file update</code>更新apt-file cache<br>使用<code>apt-file find [name]</code>可以查找计算机上文件的位置,很方便<br>使用<code>apt-file search [name]</code>可以搜索缺少的库,解决文件缺失依赖<br>选择好自己需要的包,然后使用<code>apt-get install [name]</code>即可</p>\n<ul>\n<li>如果需要把镜像上传到云上使用,有可能需要网络服务,</li>\n<li>执行<code>apt-get install net-tools</code>安装ifconfig</li>\n<li>执行<code>apt-get install iputils-ping</code>安装ping</li>\n</ul>\n<p>此时为了避免诸如使用<code>ping [IP]</code>有效,但是<code>ping [HOST]</code>无效的情况,需要使用<code>nano /etc/resolv.conf</code>修改配置文件<br>将<code>namespace</code>后的IP地址更改为<code>8.8.8.8</code>或者<code>4.4.4.4</code><br><em>或者使用<code>echo &quot;nameserver 114.114.114.114 &gt; /etc/resolv.conf&quot;</code>也可以</em><br>退出保存即可</p>\n<p><em>有可能上述修改DNS的方式并不成功,原因是在云上运行容器时,配置文件自动修改,如果发生这种情况,请每次在新开一个容器时,手动修改配置文件的DNS服务器,使其可以使用网络服务</em></p>\n<h1 id=\"二-安装编译依赖各种包\"><a href=\"#二-安装编译依赖各种包\" class=\"headerlink\" title=\"二 安装编译依赖各种包\"></a>二 安装编译依赖各种包</h1><p>在电脑上空闲的地方,从Github拉取Sniper项目</p>\n<p><code>git clone --recursive https://github.com/mahyarnajibi/SNIPER.git</code></p>\n<ul>\n<li><p>因为我是在学校机器学习平台上运行docker容器,所以选择直接将clone下的文件上传至容器<code>mount</code>的ftp服务器,使用的软件是<code>FileZilla</code></p>\n</li>\n<li><p>上传成功后可以在容器内通过<code>cd /data/[file or folder name]</code>进行访问</p>\n</li>\n</ul>\n<p>如果要在本地镜像内操作的话,也可以直接把本机文件或文件夹拷贝过去<br><code>docker cp 本地文件路径 ID全称:容器路径</code></p>\n<hr>\n<p><code>cd /data/SNIPER/SNIPER-mxnet</code><br><code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br>输出信息:<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_09-42-07.png\" alt=\"\"></p>\n<h2 id=\"安装-jemalloc\"><a href=\"#安装-jemalloc\" class=\"headerlink\" title=\"安装 jemalloc\"></a>安装 jemalloc</h2><p>选择安装<code>jemalloc</code>,这个工具可以加速编译,碎片整理,具体请自行谷歌</p>\n<ul>\n<li><code>apt-get install autoconf</code></li>\n<li><code>apt-get install automake</code></li>\n<li><code>apt-get install libtool</code></li>\n<li><code>git clone https://github.com/jemalloc/jemalloc.git</code></li>\n<li><code>cd jemalloc</code></li>\n<li><code>git checkout 4.5.0</code>安装4.5.0版本的jemalloc,5.x版本的有坑,深坑</li>\n<li><code>./autogen.sh</code></li>\n<li><code>make</code></li>\n<li><code>make install_bin install_include install_lib</code>,之所以不使用<code>make install</code>是因为会报错,如下: <img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png\" alt=\"\"></li>\n</ul>\n<p>切换至<code>SNIPER-mxnet</code>文件夹,再次<code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br>虽然可以编译,但是有以下信息:<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_10-03-30.png\" alt=\"\"><br>强迫症必须搞定它,果断<code>ctrl+c</code>终止编译</p>\n<h2 id=\"安装-pkg-config\"><a href=\"#安装-pkg-config\" class=\"headerlink\" title=\"安装 pkg-config\"></a>安装 pkg-config</h2><ul>\n<li>打开<a href=\"https://pkg-config.freedesktop.org/releases/\" rel=\"external nofollow\" target=\"_blank\">https://pkg-config.freedesktop.org/releases/</a></li>\n<li>下载最新的,现在看到的是<code>pkg-config-0.29.2.tar.gz</code></li>\n<li>下载好之后,通过<code>FileZilla</code>等工具传输到FTP服务器</li>\n<li>在容器内<code>cd</code>到压缩包位置</li>\n<li><code>tar -xf pkg-config-0.29.2.tar.gz</code></li>\n<li><code>cd pkg-config-0.29.2</code></li>\n<li><code>./configure --with-internal-glib</code>,注意,中间是一个空格,非常关键</li>\n<li><code>make &amp;&amp; make install</code><br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_10-11-01.png\" alt=\"\"></li>\n</ul>\n<p>再次<code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br>算了，还是安装一下cudnn吧</p>\n<h2 id=\"安装-cudnn7-0\"><a href=\"#安装-cudnn7-0\" class=\"headerlink\" title=\"安装 cudnn7.0\"></a>安装 cudnn7.0</h2><ul>\n<li><a href=\"https://developer.nvidia.com/rdp/cudnn-archive\" rel=\"external nofollow\" target=\"_blank\">https://developer.nvidia.com/rdp/cudnn-archive</a> 下载cuDNN Libraries for Linux,不要下载 Power 8</li>\n<li>把下载好的包上传到FTP服务器</li>\n<li><code>cd</code>到包位置</li>\n<li><code>cp cudnn-9.0-linux-x64-v7.solitairetheme8 cudnn-9.0-linux-x64-v7.tgz</code></li>\n<li><code>tar -xvf cudnn-9.0-linux-x64-v7.tgz</code></li>\n<li><code>cp include/* /usr/local/cuda-9.0/include</code></li>\n<li><code>cp lib64/* /usr/local/cuda-9.0/lib64</code></li>\n<li><code>chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn*</code></li>\n<li><code>export PATH=/usr/local/cuda-9.0/bin:$PATH</code></li>\n<li><code>cd</code>到<code>/usr/local/cuda-9.0/lib64</code></li>\n<li><code>nano ~/.bashrc</code>,关联环境变量</li>\n<li>在最后一行加入<code>export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH</code></li>\n<li><code>source ~/.bashrc</code></li>\n<li><code>ldconfig -v</code></li>\n<li>使用<code>cat /usr/local/cuda-9.0/include/cudnn.h | grep CUDNN_MAJOR -A 2</code> 查看cudnn版本<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"安装-OpenCV\"><a href=\"#安装-OpenCV\" class=\"headerlink\" title=\"安装 OpenCV\"></a>安装 OpenCV</h2><ul>\n<li>使用<code>pkg-config opencv --modversion</code>查看</li>\n<li>发现已经有OpenCV<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_10-57-56.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"安装-OpenBLAS\"><a href=\"#安装-OpenBLAS\" class=\"headerlink\" title=\"安装 OpenBLAS\"></a>安装 OpenBLAS</h2><ul>\n<li><code>apt-get install libopenblas-dev</code></li>\n</ul>\n<h2 id=\"编译-Mxnet\"><a href=\"#编译-Mxnet\" class=\"headerlink\" title=\"编译 Mxnet\"></a>编译 Mxnet</h2><p><code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_13-49-28.png\" alt=\"\"></p>\n<p><strong>心好累,总共make了将近两个半小时</strong></p>\n<p>编译<code>c++</code>文件<code>bash scripts/compile.sh</code><br>这一步一定要在<code>/SNIPER/</code>文件夹下,不然贼坑,绝对不要<code>cd</code>到<code>/SNIPER/scripts</code>文件夹下再<code>bash compile.sh</code>,因为代码内有<code>cd lib/nms</code>等,如果不在<code>/SNIPER</code>文件夹下,会找不到文件</p>\n<p>如果出现<code>syntax error near unexpected token</code>$’\\r’’<code>错误,可以使用</code>sed<code>命令将</code>\\r<code>去掉,或者是在[Github](https://github.com/mahyarnajibi/SNIPER/blob/master/scripts/compile.sh)上将代码复制,使用</code>nano<code>编辑然后粘贴\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-32-08.png)\n可以使用</code>cat -v [filename]<code>查看\n![]./create-sniper-docker-image/Snipaste_2019-01-03_16-33-27.png)\n以</code>^M<code>结尾的代表你所处理的文件换行符是dos格式的</code>“\\r\\n”`</p>\n<p>我选择第二种笨方法,因为涉及的代码并不多</p>\n<p>执行结果:<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_16-24-49.png\" alt=\"\"></p>\n<h2 id=\"安装-dos2unix\"><a href=\"#安装-dos2unix\" class=\"headerlink\" title=\"安装 dos2unix\"></a>安装 dos2unix</h2><p>由于发现这种简单的复制粘贴方式并不能很好的解决,所以查了一些<a href=\"https://blog.csdn.net/lovelovelovelovelo/article/details/79239068\" rel=\"external nofollow\" target=\"_blank\">相关资料</a><br>选择使用<code>dos2unix</code>来转换</p>\n<ul>\n<li><code>apt-get install dos2unix</code></li>\n<li><code>dos2unix [filename]</code></li>\n</ul>\n<p><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_16-40-53.png\" alt=\"\"><br>问题解决啦</p>\n<h2 id=\"安装依赖\"><a href=\"#安装依赖\" class=\"headerlink\" title=\"安装依赖\"></a>安装依赖</h2><p>在<code>/SNIPER/</code>文件夹下<code>pip install -r requirements.txt</code><br>一定要确保镜像内可以联网</p>\n<p><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_16-29-58.png\" alt=\"\"></p>\n<h2 id=\"测试Demo\"><a href=\"#测试Demo\" class=\"headerlink\" title=\"测试Demo\"></a>测试Demo</h2><ul>\n<li><code>bash download_sniper_detector.sh</code>,download_sniper_detector.sh<br>文件在<code>/SNIPER/scripts</code>文件夹下<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_16-44-22.png\" alt=\"\"></li>\n<li><code>cd .. &amp;&amp; python demo.py</code><br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_17-05-30.png\" alt=\"\"></li>\n</ul>\n<p><strong>运行成功!!!</strong></p>\n<h1 id=\"三-生成镜像\"><a href=\"#三-生成镜像\" class=\"headerlink\" title=\"三 生成镜像\"></a>三 生成镜像</h1><ul>\n<li>使用<code>exit</code>退出容器</li>\n<li>使用<code>docker ps -a</code>查看容器ID</li>\n<li>使用<code>docker stop [ID]</code>停止容器</li>\n<li>使用<code>docker commit -a &quot;作者信息&quot; -m &quot;附带信息&quot; [ID] [name]:[tag]</code>生成镜像,会返回一个<code>sha256</code>开头的长ID,这个就是生成的镜像ID</li>\n<li>使用<code>docker images</code>查看生成的镜像</li>\n<li>如果需要的话,使用<code>docker push [name]:[tag]</code>将刚刚生成的镜像推送到云上</li>\n</ul>\n<h1 id=\"四-压缩镜像\"><a href=\"#四-压缩镜像\" class=\"headerlink\" title=\"四 压缩镜像\"></a>四 压缩镜像</h1><p><strong>压缩镜像非常麻烦,但是也是有方法的,目前大概三种方法</strong></p>\n<ol>\n<li>使用<code>Dockerfile</code>生成镜像</li>\n<li>这种方法需要让容器在运行状态,使用<code>docker export [ID] | docker import - [name]:[tag]</code>导出容器快照,并从快照生成镜像,这种方式可以大大压缩镜像,但是缺点是有可能会使得镜像中的环境变量、开放端口、默认进入命令改变或消失.使用这种方式时,最好在生成镜像之后,创建一个<code>Dockerfile</code>文件,<code>From</code>这个镜像,并添加端口和命令入口</li>\n<li>使用<code>docker-squash</code>压缩镜像,这个方法适用于Linux和Mac系统</li>\n</ol>\n<p>目前可以运行的镜像是13.6G<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_17-12-36.png\" alt=\"\"><br><code>hub.hoc.ccshu.net/wjs/sniper:v1.1</code><br>现在要对它进行压缩</p>\n<h2 id=\"第一步-移除镜像内的SNIPER文件夹-把其放到FTP服务器上去\"><a href=\"#第一步-移除镜像内的SNIPER文件夹-把其放到FTP服务器上去\" class=\"headerlink\" title=\"第一步,移除镜像内的SNIPER文件夹,把其放到FTP服务器上去\"></a>第一步,移除镜像内的SNIPER文件夹,把其放到FTP服务器上去</h2><ul>\n<li>开启一个容器<code>docker run -itd --name [name] [id]</code></li>\n<li>复制容器内文件到本地<code>docker cp [长ID]:[容器内路径] [本地路径]</code>,将放置在本地的文件夹上传至FTP服务器</li>\n<li>进入容器<code>docker exec -it [name] /bin/bash</code></li>\n<li>删除容器内文件夹<code>/SNIPER/</code>,使用<code>rm -rf SNIPER</code>,<strong>一定要小心使用</strong></li>\n<li>退出容器<code>exit</code></li>\n</ul>\n<h2 id=\"第二步-压缩镜像\"><a href=\"#第二步-压缩镜像\" class=\"headerlink\" title=\"第二步,压缩镜像\"></a>第二步,压缩镜像</h2><p>压缩容器<br><code>docker export [ID] | docker import - [name]:[tag]</code></p>\n<p>可以看到,镜像体积少了大约2个G<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_17-39-07.png\" alt=\"\"></p>\n<p>由于使用这种方法会使得镜像丢失部分信息,所以,创建一个新的<code>Dockerfile</code>,在其中添加缺失的信息</p>\n<h2 id=\"第三步-完善镜像\"><a href=\"#第三步-完善镜像\" class=\"headerlink\" title=\"第三步,完善镜像\"></a>第三步,完善镜像</h2><p>在任意位置新建<code>Dockerfile</code><br>输入<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM [name]:[tag]</span><br><span class=\"line\">EXPOSE 22</span><br><span class=\"line\">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_18-08-49.png\" alt=\"\"></p>\n<p>然后<code>docker build -t [name]:[tag] .</code>,不要忘了最后的<code>.</code></p>\n<h2 id=\"第四步-Push\"><a href=\"#第四步-Push\" class=\"headerlink\" title=\"第四步 Push\"></a>第四步 Push</h2><p><code>docker push [name]:[tag]</code></p>\n<p>至此,所有配置以及完成<br>镜像在<code>hoc.hoc.ccshu.net</code>的私有仓库里<br>SNIPER文件夹放置在机器学习平台服务器<code>mount</code>的目录里</p>\n<h1 id=\"五-测试\"><a href=\"#五-测试\" class=\"headerlink\" title=\"五 测试\"></a>五 测试</h1><ul>\n<li><p>在平台上创建容器<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_18-11-49.png\" alt=\"\"></p>\n</li>\n<li><p>耐心等待创建完成<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_18-14-21.png\" alt=\"\"></p>\n</li>\n<li><p>创建成功<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_18-20-07.png\" alt=\"\"></p>\n</li>\n<li><p>测试结果<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_18-31-16.png\" alt=\"\"><br><strong>测试失败</strong></p>\n</li>\n</ul>\n<p><strong>但是,使用未压缩的镜像测试成功</strong></p>\n","site":{"data":{}},"excerpt":"<p>本文记录了如何在学校机器学习平台上创建一个基于Mxnet的Sniper镜像。</p>","more":"<h1 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h1><p>由于此镜像是用于学校机器学习平台,所以文中会出现FTP服务器等字眼,其实是在平台上使用镜像创建一个容器时,平台会<strong>自动</strong>将服务器上我所申请的文件存储区<code>mount</code>到创建的容器,我通过<code>FileZilla</code>FTP工具与在平台申请的文件存储区进行连接<br>​<br>本文教程虽然有了一个FTP过程,但是如果是生成本地镜像,不考虑FTP,无视文中相关部分即可</p>\n<p><strong>虽然本文中写了关于压缩的相关内容,但是最终并没有使用压缩,原因是由于压缩后出现未知问题,导致在平台上创建的容器不能使用宿主机的NVIDIA驱动,并不能成功运行Demo</strong></p>\n<h1 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h1><p>本机环境</p>\n<ul>\n<li>windows 10 专业版</li>\n<li>docker client version 18.09.0</li>\n<li>docker server version 18.09.0</li>\n<li>FTP工具 FileZilla</li>\n</ul>\n<p>平台环境</p>\n<ul>\n<li>docker version 17.06.2-ce</li>\n</ul>\n<p>镜像环境</p>\n<ul>\n<li>python 2.7.12</li>\n<li>CUDA version 9.0.176</li>\n<li>pip 9.0.3</li>\n</ul>\n<p><a href=\"https://github.com/mahyarnajibi/SNIPER\" rel=\"external nofollow\" target=\"_blank\">SNIPER</a><br><a href=\"http://10.0.4.228\" rel=\"external nofollow\" target=\"_blank\">机器学习平台</a>,这是学校资源</p>\n<h1 id=\"一-配置基础镜像\"><a href=\"#一-配置基础镜像\" class=\"headerlink\" title=\"一 配置基础镜像\"></a>一 配置基础镜像</h1><p>从学校机器学习平台上拉取原始镜像,因为这个镜像配好了一些基本的环境,如python2.x,CUDA9.0等等,所以直接使用它们的镜像作为基础镜像比较省心省力<br><code>docker pull hub.hoc.ccshu.net/ces/deepo:all-py27-jupyter-ssh</code></p>\n<p>拉取到镜像之后,可以选择使用<code>Dockerfile</code>来生成我们需要的镜像,但是往往我们需要在镜像中添加许多库/包/插件,而且使用<code>Dockerfile</code>来生成镜像很容易出BUG.当然,最好的方式是使用<code>Dockerfile</code>,前提是你能确保<code>Dockerfile</code>文件中的每一行命令都不会出错.<br>在当前情况下,我选择使用从容器生成镜像的方法,这种方式会使得最终生成的镜像占内存巨大,但是可以在容器内部调试每一步配置过程.<br>使用<code>docker run -itd --name [name] hub.hoc.ccshu.net/ces/deepo:all-py27-jupyter-ssh</code>开启一个容器</p>\n<p>使用<code>docker ps -a</code>查看正在运行的容器<code>ID</code></p>\n<p>使用<code>docker exec -it [name] /bin/bash</code>进入容器</p>\n<p>在容器中使用<code>cat /etc/issue</code>命令查看容器的操作系统版本</p>\n<p>结果输出: <code>Ubuntu 16.04.4 LTS \\n \\l</code></p>\n<h2 id=\"安装-apt-file\"><a href=\"#安装-apt-file\" class=\"headerlink\" title=\"安装 apt-file\"></a>安装 apt-file</h2><p>安装<code>apt-file</code></p>\n<p><code>apt-get install apt-file -y</code></p>\n<p>出现错误:</p>\n<p><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_08-30-41.png\" alt=\"\"></p>\n<p>使用<code>apt-get install apt-file -y --fix-missing</code>同样不能解决问题</p>\n<p>考虑<strong>换源</strong></p>\n<p><code>cp /etc/apt/sources.list /etc/apt/sources.list.bak</code>备份系统原有的源</p>\n<p>安装Linux下的文本编辑器<code>nano</code>,执行命令<code>apt-get install nano -y</code><br>安装<code>nano</code>成功后,执行<code>nano /etc/apt/sources.list</code>修改源文件<br>在打开的文件中,将内容替换为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted</span><br><span class=\"line\">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class=\"line\">deb http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb-src http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</span><br></pre></td></tr></table></figure></p>\n<p>这里使用的源是阿里的镜像站,也可以使用网易163的,源如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deb http://mirrors.163.com/ubuntu/ xenial main restricted universe multiverse</span><br><span class=\"line\">deb http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class=\"line\">deb http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class=\"line\">deb http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiverse</span><br><span class=\"line\">deb http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.163.com/ubuntu/ xenial main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure></p>\n<p>更改好源文件后,执行<code>sudo apt-get update</code>更新源</p>\n<p>再次执行<code>apt-get install apt-file -y</code>,可以成功安装<code>apt-file</code>包</p>\n<p>之后执行<code>apt-file update</code>更新apt-file cache<br>使用<code>apt-file find [name]</code>可以查找计算机上文件的位置,很方便<br>使用<code>apt-file search [name]</code>可以搜索缺少的库,解决文件缺失依赖<br>选择好自己需要的包,然后使用<code>apt-get install [name]</code>即可</p>\n<ul>\n<li>如果需要把镜像上传到云上使用,有可能需要网络服务,</li>\n<li>执行<code>apt-get install net-tools</code>安装ifconfig</li>\n<li>执行<code>apt-get install iputils-ping</code>安装ping</li>\n</ul>\n<p>此时为了避免诸如使用<code>ping [IP]</code>有效,但是<code>ping [HOST]</code>无效的情况,需要使用<code>nano /etc/resolv.conf</code>修改配置文件<br>将<code>namespace</code>后的IP地址更改为<code>8.8.8.8</code>或者<code>4.4.4.4</code><br><em>或者使用<code>echo &quot;nameserver 114.114.114.114 &gt; /etc/resolv.conf&quot;</code>也可以</em><br>退出保存即可</p>\n<p><em>有可能上述修改DNS的方式并不成功,原因是在云上运行容器时,配置文件自动修改,如果发生这种情况,请每次在新开一个容器时,手动修改配置文件的DNS服务器,使其可以使用网络服务</em></p>\n<h1 id=\"二-安装编译依赖各种包\"><a href=\"#二-安装编译依赖各种包\" class=\"headerlink\" title=\"二 安装编译依赖各种包\"></a>二 安装编译依赖各种包</h1><p>在电脑上空闲的地方,从Github拉取Sniper项目</p>\n<p><code>git clone --recursive https://github.com/mahyarnajibi/SNIPER.git</code></p>\n<ul>\n<li><p>因为我是在学校机器学习平台上运行docker容器,所以选择直接将clone下的文件上传至容器<code>mount</code>的ftp服务器,使用的软件是<code>FileZilla</code></p>\n</li>\n<li><p>上传成功后可以在容器内通过<code>cd /data/[file or folder name]</code>进行访问</p>\n</li>\n</ul>\n<p>如果要在本地镜像内操作的话,也可以直接把本机文件或文件夹拷贝过去<br><code>docker cp 本地文件路径 ID全称:容器路径</code></p>\n<hr>\n<p><code>cd /data/SNIPER/SNIPER-mxnet</code><br><code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br>输出信息:<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_09-42-07.png\" alt=\"\"></p>\n<h2 id=\"安装-jemalloc\"><a href=\"#安装-jemalloc\" class=\"headerlink\" title=\"安装 jemalloc\"></a>安装 jemalloc</h2><p>选择安装<code>jemalloc</code>,这个工具可以加速编译,碎片整理,具体请自行谷歌</p>\n<ul>\n<li><code>apt-get install autoconf</code></li>\n<li><code>apt-get install automake</code></li>\n<li><code>apt-get install libtool</code></li>\n<li><code>git clone https://github.com/jemalloc/jemalloc.git</code></li>\n<li><code>cd jemalloc</code></li>\n<li><code>git checkout 4.5.0</code>安装4.5.0版本的jemalloc,5.x版本的有坑,深坑</li>\n<li><code>./autogen.sh</code></li>\n<li><code>make</code></li>\n<li><code>make install_bin install_include install_lib</code>,之所以不使用<code>make install</code>是因为会报错,如下: <img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png\" alt=\"\"></li>\n</ul>\n<p>切换至<code>SNIPER-mxnet</code>文件夹,再次<code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br>虽然可以编译,但是有以下信息:<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_10-03-30.png\" alt=\"\"><br>强迫症必须搞定它,果断<code>ctrl+c</code>终止编译</p>\n<h2 id=\"安装-pkg-config\"><a href=\"#安装-pkg-config\" class=\"headerlink\" title=\"安装 pkg-config\"></a>安装 pkg-config</h2><ul>\n<li>打开<a href=\"https://pkg-config.freedesktop.org/releases/\" rel=\"external nofollow\" target=\"_blank\">https://pkg-config.freedesktop.org/releases/</a></li>\n<li>下载最新的,现在看到的是<code>pkg-config-0.29.2.tar.gz</code></li>\n<li>下载好之后,通过<code>FileZilla</code>等工具传输到FTP服务器</li>\n<li>在容器内<code>cd</code>到压缩包位置</li>\n<li><code>tar -xf pkg-config-0.29.2.tar.gz</code></li>\n<li><code>cd pkg-config-0.29.2</code></li>\n<li><code>./configure --with-internal-glib</code>,注意,中间是一个空格,非常关键</li>\n<li><code>make &amp;&amp; make install</code><br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_10-11-01.png\" alt=\"\"></li>\n</ul>\n<p>再次<code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br>算了，还是安装一下cudnn吧</p>\n<h2 id=\"安装-cudnn7-0\"><a href=\"#安装-cudnn7-0\" class=\"headerlink\" title=\"安装 cudnn7.0\"></a>安装 cudnn7.0</h2><ul>\n<li><a href=\"https://developer.nvidia.com/rdp/cudnn-archive\" rel=\"external nofollow\" target=\"_blank\">https://developer.nvidia.com/rdp/cudnn-archive</a> 下载cuDNN Libraries for Linux,不要下载 Power 8</li>\n<li>把下载好的包上传到FTP服务器</li>\n<li><code>cd</code>到包位置</li>\n<li><code>cp cudnn-9.0-linux-x64-v7.solitairetheme8 cudnn-9.0-linux-x64-v7.tgz</code></li>\n<li><code>tar -xvf cudnn-9.0-linux-x64-v7.tgz</code></li>\n<li><code>cp include/* /usr/local/cuda-9.0/include</code></li>\n<li><code>cp lib64/* /usr/local/cuda-9.0/lib64</code></li>\n<li><code>chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn*</code></li>\n<li><code>export PATH=/usr/local/cuda-9.0/bin:$PATH</code></li>\n<li><code>cd</code>到<code>/usr/local/cuda-9.0/lib64</code></li>\n<li><code>nano ~/.bashrc</code>,关联环境变量</li>\n<li>在最后一行加入<code>export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH</code></li>\n<li><code>source ~/.bashrc</code></li>\n<li><code>ldconfig -v</code></li>\n<li>使用<code>cat /usr/local/cuda-9.0/include/cudnn.h | grep CUDNN_MAJOR -A 2</code> 查看cudnn版本<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"安装-OpenCV\"><a href=\"#安装-OpenCV\" class=\"headerlink\" title=\"安装 OpenCV\"></a>安装 OpenCV</h2><ul>\n<li>使用<code>pkg-config opencv --modversion</code>查看</li>\n<li>发现已经有OpenCV<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_10-57-56.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"安装-OpenBLAS\"><a href=\"#安装-OpenBLAS\" class=\"headerlink\" title=\"安装 OpenBLAS\"></a>安装 OpenBLAS</h2><ul>\n<li><code>apt-get install libopenblas-dev</code></li>\n</ul>\n<h2 id=\"编译-Mxnet\"><a href=\"#编译-Mxnet\" class=\"headerlink\" title=\"编译 Mxnet\"></a>编译 Mxnet</h2><p><code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_13-49-28.png\" alt=\"\"></p>\n<p><strong>心好累,总共make了将近两个半小时</strong></p>\n<p>编译<code>c++</code>文件<code>bash scripts/compile.sh</code><br>这一步一定要在<code>/SNIPER/</code>文件夹下,不然贼坑,绝对不要<code>cd</code>到<code>/SNIPER/scripts</code>文件夹下再<code>bash compile.sh</code>,因为代码内有<code>cd lib/nms</code>等,如果不在<code>/SNIPER</code>文件夹下,会找不到文件</p>\n<p>如果出现<code>syntax error near unexpected token</code>$’\\r’’<code>错误,可以使用</code>sed<code>命令将</code>\\r<code>去掉,或者是在[Github](https://github.com/mahyarnajibi/SNIPER/blob/master/scripts/compile.sh)上将代码复制,使用</code>nano<code>编辑然后粘贴\n![](./create-sniper-docker-image/Snipaste_2019-01-03_16-32-08.png)\n可以使用</code>cat -v [filename]<code>查看\n![]./create-sniper-docker-image/Snipaste_2019-01-03_16-33-27.png)\n以</code>^M<code>结尾的代表你所处理的文件换行符是dos格式的</code>“\\r\\n”`</p>\n<p>我选择第二种笨方法,因为涉及的代码并不多</p>\n<p>执行结果:<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_16-24-49.png\" alt=\"\"></p>\n<h2 id=\"安装-dos2unix\"><a href=\"#安装-dos2unix\" class=\"headerlink\" title=\"安装 dos2unix\"></a>安装 dos2unix</h2><p>由于发现这种简单的复制粘贴方式并不能很好的解决,所以查了一些<a href=\"https://blog.csdn.net/lovelovelovelovelo/article/details/79239068\" rel=\"external nofollow\" target=\"_blank\">相关资料</a><br>选择使用<code>dos2unix</code>来转换</p>\n<ul>\n<li><code>apt-get install dos2unix</code></li>\n<li><code>dos2unix [filename]</code></li>\n</ul>\n<p><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_16-40-53.png\" alt=\"\"><br>问题解决啦</p>\n<h2 id=\"安装依赖\"><a href=\"#安装依赖\" class=\"headerlink\" title=\"安装依赖\"></a>安装依赖</h2><p>在<code>/SNIPER/</code>文件夹下<code>pip install -r requirements.txt</code><br>一定要确保镜像内可以联网</p>\n<p><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_16-29-58.png\" alt=\"\"></p>\n<h2 id=\"测试Demo\"><a href=\"#测试Demo\" class=\"headerlink\" title=\"测试Demo\"></a>测试Demo</h2><ul>\n<li><code>bash download_sniper_detector.sh</code>,download_sniper_detector.sh<br>文件在<code>/SNIPER/scripts</code>文件夹下<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_16-44-22.png\" alt=\"\"></li>\n<li><code>cd .. &amp;&amp; python demo.py</code><br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_17-05-30.png\" alt=\"\"></li>\n</ul>\n<p><strong>运行成功!!!</strong></p>\n<h1 id=\"三-生成镜像\"><a href=\"#三-生成镜像\" class=\"headerlink\" title=\"三 生成镜像\"></a>三 生成镜像</h1><ul>\n<li>使用<code>exit</code>退出容器</li>\n<li>使用<code>docker ps -a</code>查看容器ID</li>\n<li>使用<code>docker stop [ID]</code>停止容器</li>\n<li>使用<code>docker commit -a &quot;作者信息&quot; -m &quot;附带信息&quot; [ID] [name]:[tag]</code>生成镜像,会返回一个<code>sha256</code>开头的长ID,这个就是生成的镜像ID</li>\n<li>使用<code>docker images</code>查看生成的镜像</li>\n<li>如果需要的话,使用<code>docker push [name]:[tag]</code>将刚刚生成的镜像推送到云上</li>\n</ul>\n<h1 id=\"四-压缩镜像\"><a href=\"#四-压缩镜像\" class=\"headerlink\" title=\"四 压缩镜像\"></a>四 压缩镜像</h1><p><strong>压缩镜像非常麻烦,但是也是有方法的,目前大概三种方法</strong></p>\n<ol>\n<li>使用<code>Dockerfile</code>生成镜像</li>\n<li>这种方法需要让容器在运行状态,使用<code>docker export [ID] | docker import - [name]:[tag]</code>导出容器快照,并从快照生成镜像,这种方式可以大大压缩镜像,但是缺点是有可能会使得镜像中的环境变量、开放端口、默认进入命令改变或消失.使用这种方式时,最好在生成镜像之后,创建一个<code>Dockerfile</code>文件,<code>From</code>这个镜像,并添加端口和命令入口</li>\n<li>使用<code>docker-squash</code>压缩镜像,这个方法适用于Linux和Mac系统</li>\n</ol>\n<p>目前可以运行的镜像是13.6G<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_17-12-36.png\" alt=\"\"><br><code>hub.hoc.ccshu.net/wjs/sniper:v1.1</code><br>现在要对它进行压缩</p>\n<h2 id=\"第一步-移除镜像内的SNIPER文件夹-把其放到FTP服务器上去\"><a href=\"#第一步-移除镜像内的SNIPER文件夹-把其放到FTP服务器上去\" class=\"headerlink\" title=\"第一步,移除镜像内的SNIPER文件夹,把其放到FTP服务器上去\"></a>第一步,移除镜像内的SNIPER文件夹,把其放到FTP服务器上去</h2><ul>\n<li>开启一个容器<code>docker run -itd --name [name] [id]</code></li>\n<li>复制容器内文件到本地<code>docker cp [长ID]:[容器内路径] [本地路径]</code>,将放置在本地的文件夹上传至FTP服务器</li>\n<li>进入容器<code>docker exec -it [name] /bin/bash</code></li>\n<li>删除容器内文件夹<code>/SNIPER/</code>,使用<code>rm -rf SNIPER</code>,<strong>一定要小心使用</strong></li>\n<li>退出容器<code>exit</code></li>\n</ul>\n<h2 id=\"第二步-压缩镜像\"><a href=\"#第二步-压缩镜像\" class=\"headerlink\" title=\"第二步,压缩镜像\"></a>第二步,压缩镜像</h2><p>压缩容器<br><code>docker export [ID] | docker import - [name]:[tag]</code></p>\n<p>可以看到,镜像体积少了大约2个G<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_17-39-07.png\" alt=\"\"></p>\n<p>由于使用这种方法会使得镜像丢失部分信息,所以,创建一个新的<code>Dockerfile</code>,在其中添加缺失的信息</p>\n<h2 id=\"第三步-完善镜像\"><a href=\"#第三步-完善镜像\" class=\"headerlink\" title=\"第三步,完善镜像\"></a>第三步,完善镜像</h2><p>在任意位置新建<code>Dockerfile</code><br>输入<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM [name]:[tag]</span><br><span class=\"line\">EXPOSE 22</span><br><span class=\"line\">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_18-08-49.png\" alt=\"\"></p>\n<p>然后<code>docker build -t [name]:[tag] .</code>,不要忘了最后的<code>.</code></p>\n<h2 id=\"第四步-Push\"><a href=\"#第四步-Push\" class=\"headerlink\" title=\"第四步 Push\"></a>第四步 Push</h2><p><code>docker push [name]:[tag]</code></p>\n<p>至此,所有配置以及完成<br>镜像在<code>hoc.hoc.ccshu.net</code>的私有仓库里<br>SNIPER文件夹放置在机器学习平台服务器<code>mount</code>的目录里</p>\n<h1 id=\"五-测试\"><a href=\"#五-测试\" class=\"headerlink\" title=\"五 测试\"></a>五 测试</h1><ul>\n<li><p>在平台上创建容器<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_18-11-49.png\" alt=\"\"></p>\n</li>\n<li><p>耐心等待创建完成<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_18-14-21.png\" alt=\"\"></p>\n</li>\n<li><p>创建成功<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_18-20-07.png\" alt=\"\"></p>\n</li>\n<li><p>测试结果<br><img src=\"./create-sniper-docker-image/Snipaste_2019-01-03_18-31-16.png\" alt=\"\"><br><strong>测试失败</strong></p>\n</li>\n</ul>\n<p><strong>但是,使用未压缩的镜像测试成功</strong></p>"},{"title":"Maximum Entropy-Regularized Multi-Goal Reinforcement-Learning","copyright":true,"mathjax":true,"top":1,"date":"2019-06-12T12:49:32.000Z","keywords":null,"description":null,"_content":"\n这篇论文将强化学习的目标与最大熵结合了起来，提出了简称为MEP的经验池机制。许多将熵与强化学习结合的方法都是考虑可选动作分布的熵，该篇论文很新颖的使用的是“迹”的熵。\n\n推荐程度中等偏下：\n\n- 有些地方解释的不是很清楚\n- 熵的结合方式特殊，可以一看\n- 有些公式推导过于复杂，难懂\n- 有些参考文献标注不准，如A3C算法的论文并没有使用熵的概念，却在熵相关的语句进行了标注\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1905.08786.pdf](https://arxiv.org/pdf/1905.08786.pdf)\n\n原作者代码地址：[https://github.com/ruizhaogit/mep.git](https://github.com/ruizhaogit/mep.git)\n\n该文章发于2019年的ICML，与之前写过的《Energy-Based Hindsight Experience Prioritization》为同一作者。\n\n本文主要做了三个贡献：\n\n1. 修改了目标函数，提出了最大熵正则化多目标强化学习(Maximum Entropy-Regularized Multi-Goal RL)的想法\n2. 推导出替代(surrogate)目标函数，是第1步目标函数的一个下界，可以使算法稳定优化\n3. 提出了Maximum Entropy-based Prioritization(MEP)的经验池框架\n\n# 文中精要\n\n文中目标函数的构造主要受Guiasu于1971年提出的加权熵(Weighted Entropy)启发。加权熵表示为：\n$$\n\\mathcal{H}_{p}^{w}=-\\sum_{k=1}^{K} w_{k} p_{k} \\log p_{k}\n$$\n$w_{k}$表示权重。\n\n## 多目标强化学习的符号表示\n\n用$p\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right)$表示一个迹出现的概率：\n\n- $\\boldsymbol{\\tau}=s_{1}, a_{1}, s_{2}, a_{2}, \\ldots, s_{T-1}, a_{T-1}, s_{T}$代表迹\n- $g^{e}$代表一个真实的目标，即不是“事后诸葛亮”假定的目标，$g^{e} \\in \\operatorname{Val}\\left(G^{e}\\right)$，后一项为目标空间，一般情况下，可以视为状态空间$\\mathcal{S}$的子集\n- $\\theta$代表策略参数\n\n展开来写，一个迹在策略$\\theta$被采样到的概率为\n$$\np\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right)=p\\left(s_{1}\\right) \\prod_{t=1}^{T-1} p\\left(a_{t} | s_{t}, g^{e}, \\boldsymbol{\\theta}\\right) p\\left(s_{t+1} | s_{t}, a_{t}\\right)\n$$\n由此定义策略$\\theta$下的期望奖励回报，也就是目标函数，表示为\n$$\n\\begin{aligned} \\eta(\\boldsymbol{\\theta}) &=\\mathbb{E}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\ &=\\sum_{g^{e}} p\\left(g^{e}\\right) \\sum_{\\boldsymbol{\\tau}} p\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\end{aligned}\n$$\n很容易理解，也就是在传统的目标函数前边加了一项关于每个目标求积分的步骤。\n\n如果使用off-policy算法且用经验池机制来提升采样效率，那么目标函数为\n$$\n\\eta^{\\mathcal{R}}(\\boldsymbol{\\theta})=\\sum_{\\boldsymbol{\\tau}, g^{e}} p_{\\mathcal{R}}\\left(\\boldsymbol{\\tau}, g^{e} | \\boldsymbol{\\theta}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right)\n$$\n其中$\\mathcal{R}$表示经验池。注意，$\\eta^{\\mathcal{R}}(\\boldsymbol{\\theta})$将$\\eta(\\boldsymbol{\\theta})$的前两个积分项合在一起写了，所以是联合概率而不是条件概率。\n\n## 最大熵正则化目标函数\n\n将前文提到的$\\eta(\\boldsymbol{\\theta})$与加权熵结合起来，就构造出了本文中新的目标函数，即\n$$\n\\begin{aligned} \\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) &=\\mathcal{H}_{p}^{w}\\left(\\mathcal{T}^{g}\\right) \\\\ &=\\mathbb{E}_{p}\\left[\\color{red} {\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)}} \\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\end{aligned}\n$$\n$\\color{red} {p\\left(\\boldsymbol{\\tau}^{g}\\right)}$代表$\\sum_{g^{e}} p_{\\mathcal{R}}\\left(\\tau^{g}, g^{e} | \\boldsymbol{\\theta} \\right )$，期望右下角的$p$也是$p\\left(\\boldsymbol{\\tau}^{g}\\right)$的意思。\n\n**与传统的结合方式不同的是，这种结合方式并没有将熵作为一个加和的项，而是相乘。**\n\n仔细想一下，如果将这个式子视为加权熵，那么权重系数是累计奖励$\\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right)$，这样做的直观解释是：**对于各种各样的迹，给与累计回报大的以更多权重，使算法直到要朝哪个迹的方向优化。**\n\n反而，如果将前一个对数项视为传统强化学习目标函数的权重系数，那么这样的直观解释是：**迹出现的概率越低，就越新颖，反而要使其权重增加，从而驱使算法向探索的方向优化。**\n\n## 替代目标函数\n\n文中指出，$\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) $中的$\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)}$这一项是无界的，即取值范围为$[0,+\\infty]$，这会导致通用值函数近似的训练不稳定，因此提出了可靠地替代目标函数$\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})$，这个目标函数是原目标函数的一个下界。\n\n> the weight, $\\log \\left(1 / p\\left(\\boldsymbol{\\tau}^{g}\\right)\\right)$, is unbounded, which makes the training of the universal function approximator unstable. \n\n表示为\n$$\n\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})=Z \\cdot \\mathbb{E}_{\\color{red}{q}}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right]\n$$\n注意，期望积分的是迹分布函数$q$，而不是经验池中真实的迹分布函数$p$。\n\n那么，新的迹分布函数$q\\left(\\boldsymbol{\\tau}^{g}\\right)$是怎么得来的？\n\n首先，使用Latent Varibale Model(LVM)对$p\\left(\\boldsymbol{\\tau}^{g}\\right)$的潜在分布进行建模，因为LVM适合于对复杂的分布进行建模。\n\n> We use a Latent Variable Model(LVM) (Murphy, 2012) to model the underlying distribution of $p\\left(\\boldsymbol{\\tau}^{g}\\right)$, since LVM is suitable for modeling complex distributions.\n\n将分布用混合高斯模型MoG表示，\n$$\np\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)=\\frac{1}{Z} \\sum_{i=k}^{K} c_{k} \\mathcal{N}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)\n$$\n其中，\n\n- $K$为隐变量的个数\n- $\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}$分布为均值与协方差矩阵\n- $c_{k}$为混合系数\n- $Z$为配分函数（归一化系数）\n- $\\phi$为模型参数，包含所有的均值、协方差矩阵和混合系数\n\n文中接着使用$p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)$的补作为经验重放的优先级，即\n$$\n\\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right) \\propto 1-p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)\n$$\n补越大，就代表迹出现的概率越低，那么就对该迹赋予更大的优先级。作者想通过过采样这些迹来增大训练时迹分布的熵。直观的解释就是，概率分布不均匀，那么让概率大的出现次数少，概率小的出现次数多，这样就会使采样的分布朝着均匀分布的方向移动，从而使熵值增加。\n\n文中由此引出了新的迹分布$q\\left(\\boldsymbol{\\tau}^{g}\\right)$，它表示为原始分布与其补的联合分布，\n$$\n\\begin{aligned} q\\left(\\boldsymbol{\\tau}^{g}\\right) & \\propto \\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right) p\\left(\\boldsymbol{\\tau}^{g}\\right) \\\\ & \\propto\\left(1-p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)\\right) p\\left(\\boldsymbol{\\tau}^{g}\\right) \\\\ & \\approx p\\left(\\boldsymbol{\\tau}^{g}\\right)-p\\left(\\boldsymbol{\\tau}^{g}\\right)^{2} \\end{aligned}\n$$\n文中将正比的比例设置为$\\color{blue}{\\frac{1}{Z}}$，即$q\\left(\\boldsymbol{\\tau}^{g}\\right)=\\frac{1}{Z} p\\left(\\boldsymbol{\\tau}^{g}\\right)\\left(1-p\\left(\\boldsymbol{\\tau}^{g}\\right)\\right)$，接下来就可以证明$\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})$为$\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) $的下界：\n$$\n\\begin{aligned} \n\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta}) &=Z \\cdot \\mathbb{E}_{q}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\ &=\\sum_{\\boldsymbol{\\tau}^{g}} Z \\cdot q\\left(\\boldsymbol{\\tau}^{g}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&=\\sum_{\\tau^{g}} \\frac{Z}{Z} p\\left(\\tau^{g}\\right)\\left(1-p\\left(\\tau^{g}\\right)\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&<\\sum_{\\tau^{g}}-p\\left(\\boldsymbol{\\tau}^{g}\\right) \\log p\\left(\\boldsymbol{\\tau}^{g}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&=\\mathbb{E}_{p}\\left[\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)} \\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\\n&=\\mathcal{H}_{p}^{w}\\left(\\mathcal{T}^{g}\\right) \\\\\n&=\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta})\n\\end{aligned}\n$$\n在小于号不等式那一步，使用了函数的性质：$\\log x<x-1$。很容易可以画出$f(x)=\\ln{x}-x+1$在区间[0,1]上的图像：\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/inequality.png)\n\n文中进一步证明了$q$分布的熵比$p$分布的熵更大，因此，由$q$分布来进行采样可以使采样更均匀（= =！那么直接使用均匀分布采样不更好吗？），使学习的目标更多样化。证明过程篇幅过长，详见原论文附录，\n$$\np\\left(\\boldsymbol{\\tau}^{g}\\right), \\text { where } p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right) \\in(0,1) \\text { and } \\sum_{i=1}^{N} p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=1\n$$\n\n$$\nq\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=\\frac{1}{Z} p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\left(1-p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\right), \\text { where } \\sum_{i=1}^{N} q\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=1\n$$\n\n$$\n\\mathcal{H}_{q}\\left(\\mathcal{T}^{g}\\right)-\\mathcal{H}_{p}\\left(\\mathcal{T}^{g}\\right) \\geq 0\n$$\n\n## 基于最大熵的优先级\n\n文中说是基于最大熵的优先经验回放，但是除了使用$q\\left(\\boldsymbol{\\tau}^{g}\\right)$分布之外，在这一部分没有体现出熵的影子，优先级的设置为\n$$\nq\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=\\frac{\\operatorname{rank}\\left(q\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\right)}{\\sum_{n=1}^{N} \\operatorname{rank}\\left(q\\left(\\boldsymbol{\\tau}_{n}^{g}\\right)\\right)}\n$$\n使用排序作为衡量优先级的标准是因为这种方式更具鲁棒性，对异常值不敏感。\n\n之前文中说想要使出现概率低的迹以更高的概率被从经验池中采样到，如果是按照新的$q$分布来定义迹出现的概率，那么到这里是有些说不通的，因为，前文提到$q\\left(\\boldsymbol{\\tau}^{g}\\right) \\approx p\\left(\\boldsymbol{\\tau}^{g}\\right)-p\\left(\\boldsymbol{\\tau}^{g}\\right)^{2} $，那么画出$f(x)=x-x^2$在区间[0，1]上的图像为：\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/x-x^2.png)\n\n由此可见，q值小的地方为p值小与p值大的地方，这就会导致出现概率最大、概率最小的迹被经验池重复的次数多，与前文所讲不同。\n\n而如果考虑以真实的p分布来定义迹出现的概率，那么此处不应该使用$\\operatorname{rank}\\left(q\\left(\\tau_{i}^{g}\\right)\\right)$作为排序的标准，而应该是$\\operatorname{rank}\\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)$，或者$\\operatorname{rank}p\\left(\\boldsymbol{\\tau}^{g}\\right)$。\n\n总之，这里关于优先级的解释不是很清楚。\n\n## 流程示意图\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/MEP.png)\n\n## 伪代码\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/pseudo.png)\n\n解析：\n\n- 每一次迭代，都重新构造优先采样分布$q\\left(\\tau^{g}\\right)$\n\n# 实验部分\n\n## 环境\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/env.png)\n\n- 算法：DDPG\n- 5个随机种子进行实验，取最好的结果\n- 19个CPU\n- 训练200个epoch\n\n## 实验结果\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/mean-success.png)\n\n- **以训练的epoch为标准**，使用了MEP的收敛速度更快\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/training-time.png)\n\n可以看到，结合MEP的效果最好，但是我觉得这种对比一点都不严谨，从数据上看，不使用MEP比使用MEP的训练时间更短，且效果也差不多，那么如果训练相同的时间，说不定使用MEP的效果并没有不使用的好。\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/sample-efficiency.png)\n\n固定成功率，在机器人实验中，提升了1.95倍采样效率，即使用更少的样本训练相同的效果。\n\n# 疑问\n\n前文提到原目标函数中的对数项会使训练值函数不稳定，为什么？\n\n为什么构造新的迹分布$q\\left(\\boldsymbol{\\tau}^{g}\\right)$为那样的形式？\n\n","source":"_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning.md","raw":"---\ntitle: Maximum Entropy-Regularized Multi-Goal Reinforcement-Learning\ncopyright: true\nmathjax: true\ntop: 1\ndate: 2019-06-12 20:49:32\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n这篇论文将强化学习的目标与最大熵结合了起来，提出了简称为MEP的经验池机制。许多将熵与强化学习结合的方法都是考虑可选动作分布的熵，该篇论文很新颖的使用的是“迹”的熵。\n\n推荐程度中等偏下：\n\n- 有些地方解释的不是很清楚\n- 熵的结合方式特殊，可以一看\n- 有些公式推导过于复杂，难懂\n- 有些参考文献标注不准，如A3C算法的论文并没有使用熵的概念，却在熵相关的语句进行了标注\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1905.08786.pdf](https://arxiv.org/pdf/1905.08786.pdf)\n\n原作者代码地址：[https://github.com/ruizhaogit/mep.git](https://github.com/ruizhaogit/mep.git)\n\n该文章发于2019年的ICML，与之前写过的《Energy-Based Hindsight Experience Prioritization》为同一作者。\n\n本文主要做了三个贡献：\n\n1. 修改了目标函数，提出了最大熵正则化多目标强化学习(Maximum Entropy-Regularized Multi-Goal RL)的想法\n2. 推导出替代(surrogate)目标函数，是第1步目标函数的一个下界，可以使算法稳定优化\n3. 提出了Maximum Entropy-based Prioritization(MEP)的经验池框架\n\n# 文中精要\n\n文中目标函数的构造主要受Guiasu于1971年提出的加权熵(Weighted Entropy)启发。加权熵表示为：\n$$\n\\mathcal{H}_{p}^{w}=-\\sum_{k=1}^{K} w_{k} p_{k} \\log p_{k}\n$$\n$w_{k}$表示权重。\n\n## 多目标强化学习的符号表示\n\n用$p\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right)$表示一个迹出现的概率：\n\n- $\\boldsymbol{\\tau}=s_{1}, a_{1}, s_{2}, a_{2}, \\ldots, s_{T-1}, a_{T-1}, s_{T}$代表迹\n- $g^{e}$代表一个真实的目标，即不是“事后诸葛亮”假定的目标，$g^{e} \\in \\operatorname{Val}\\left(G^{e}\\right)$，后一项为目标空间，一般情况下，可以视为状态空间$\\mathcal{S}$的子集\n- $\\theta$代表策略参数\n\n展开来写，一个迹在策略$\\theta$被采样到的概率为\n$$\np\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right)=p\\left(s_{1}\\right) \\prod_{t=1}^{T-1} p\\left(a_{t} | s_{t}, g^{e}, \\boldsymbol{\\theta}\\right) p\\left(s_{t+1} | s_{t}, a_{t}\\right)\n$$\n由此定义策略$\\theta$下的期望奖励回报，也就是目标函数，表示为\n$$\n\\begin{aligned} \\eta(\\boldsymbol{\\theta}) &=\\mathbb{E}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\ &=\\sum_{g^{e}} p\\left(g^{e}\\right) \\sum_{\\boldsymbol{\\tau}} p\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\end{aligned}\n$$\n很容易理解，也就是在传统的目标函数前边加了一项关于每个目标求积分的步骤。\n\n如果使用off-policy算法且用经验池机制来提升采样效率，那么目标函数为\n$$\n\\eta^{\\mathcal{R}}(\\boldsymbol{\\theta})=\\sum_{\\boldsymbol{\\tau}, g^{e}} p_{\\mathcal{R}}\\left(\\boldsymbol{\\tau}, g^{e} | \\boldsymbol{\\theta}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right)\n$$\n其中$\\mathcal{R}$表示经验池。注意，$\\eta^{\\mathcal{R}}(\\boldsymbol{\\theta})$将$\\eta(\\boldsymbol{\\theta})$的前两个积分项合在一起写了，所以是联合概率而不是条件概率。\n\n## 最大熵正则化目标函数\n\n将前文提到的$\\eta(\\boldsymbol{\\theta})$与加权熵结合起来，就构造出了本文中新的目标函数，即\n$$\n\\begin{aligned} \\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) &=\\mathcal{H}_{p}^{w}\\left(\\mathcal{T}^{g}\\right) \\\\ &=\\mathbb{E}_{p}\\left[\\color{red} {\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)}} \\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\end{aligned}\n$$\n$\\color{red} {p\\left(\\boldsymbol{\\tau}^{g}\\right)}$代表$\\sum_{g^{e}} p_{\\mathcal{R}}\\left(\\tau^{g}, g^{e} | \\boldsymbol{\\theta} \\right )$，期望右下角的$p$也是$p\\left(\\boldsymbol{\\tau}^{g}\\right)$的意思。\n\n**与传统的结合方式不同的是，这种结合方式并没有将熵作为一个加和的项，而是相乘。**\n\n仔细想一下，如果将这个式子视为加权熵，那么权重系数是累计奖励$\\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right)$，这样做的直观解释是：**对于各种各样的迹，给与累计回报大的以更多权重，使算法直到要朝哪个迹的方向优化。**\n\n反而，如果将前一个对数项视为传统强化学习目标函数的权重系数，那么这样的直观解释是：**迹出现的概率越低，就越新颖，反而要使其权重增加，从而驱使算法向探索的方向优化。**\n\n## 替代目标函数\n\n文中指出，$\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) $中的$\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)}$这一项是无界的，即取值范围为$[0,+\\infty]$，这会导致通用值函数近似的训练不稳定，因此提出了可靠地替代目标函数$\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})$，这个目标函数是原目标函数的一个下界。\n\n> the weight, $\\log \\left(1 / p\\left(\\boldsymbol{\\tau}^{g}\\right)\\right)$, is unbounded, which makes the training of the universal function approximator unstable. \n\n表示为\n$$\n\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})=Z \\cdot \\mathbb{E}_{\\color{red}{q}}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right]\n$$\n注意，期望积分的是迹分布函数$q$，而不是经验池中真实的迹分布函数$p$。\n\n那么，新的迹分布函数$q\\left(\\boldsymbol{\\tau}^{g}\\right)$是怎么得来的？\n\n首先，使用Latent Varibale Model(LVM)对$p\\left(\\boldsymbol{\\tau}^{g}\\right)$的潜在分布进行建模，因为LVM适合于对复杂的分布进行建模。\n\n> We use a Latent Variable Model(LVM) (Murphy, 2012) to model the underlying distribution of $p\\left(\\boldsymbol{\\tau}^{g}\\right)$, since LVM is suitable for modeling complex distributions.\n\n将分布用混合高斯模型MoG表示，\n$$\np\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)=\\frac{1}{Z} \\sum_{i=k}^{K} c_{k} \\mathcal{N}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)\n$$\n其中，\n\n- $K$为隐变量的个数\n- $\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}$分布为均值与协方差矩阵\n- $c_{k}$为混合系数\n- $Z$为配分函数（归一化系数）\n- $\\phi$为模型参数，包含所有的均值、协方差矩阵和混合系数\n\n文中接着使用$p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)$的补作为经验重放的优先级，即\n$$\n\\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right) \\propto 1-p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)\n$$\n补越大，就代表迹出现的概率越低，那么就对该迹赋予更大的优先级。作者想通过过采样这些迹来增大训练时迹分布的熵。直观的解释就是，概率分布不均匀，那么让概率大的出现次数少，概率小的出现次数多，这样就会使采样的分布朝着均匀分布的方向移动，从而使熵值增加。\n\n文中由此引出了新的迹分布$q\\left(\\boldsymbol{\\tau}^{g}\\right)$，它表示为原始分布与其补的联合分布，\n$$\n\\begin{aligned} q\\left(\\boldsymbol{\\tau}^{g}\\right) & \\propto \\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right) p\\left(\\boldsymbol{\\tau}^{g}\\right) \\\\ & \\propto\\left(1-p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)\\right) p\\left(\\boldsymbol{\\tau}^{g}\\right) \\\\ & \\approx p\\left(\\boldsymbol{\\tau}^{g}\\right)-p\\left(\\boldsymbol{\\tau}^{g}\\right)^{2} \\end{aligned}\n$$\n文中将正比的比例设置为$\\color{blue}{\\frac{1}{Z}}$，即$q\\left(\\boldsymbol{\\tau}^{g}\\right)=\\frac{1}{Z} p\\left(\\boldsymbol{\\tau}^{g}\\right)\\left(1-p\\left(\\boldsymbol{\\tau}^{g}\\right)\\right)$，接下来就可以证明$\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})$为$\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) $的下界：\n$$\n\\begin{aligned} \n\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta}) &=Z \\cdot \\mathbb{E}_{q}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\ &=\\sum_{\\boldsymbol{\\tau}^{g}} Z \\cdot q\\left(\\boldsymbol{\\tau}^{g}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&=\\sum_{\\tau^{g}} \\frac{Z}{Z} p\\left(\\tau^{g}\\right)\\left(1-p\\left(\\tau^{g}\\right)\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&<\\sum_{\\tau^{g}}-p\\left(\\boldsymbol{\\tau}^{g}\\right) \\log p\\left(\\boldsymbol{\\tau}^{g}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&=\\mathbb{E}_{p}\\left[\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)} \\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\\n&=\\mathcal{H}_{p}^{w}\\left(\\mathcal{T}^{g}\\right) \\\\\n&=\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta})\n\\end{aligned}\n$$\n在小于号不等式那一步，使用了函数的性质：$\\log x<x-1$。很容易可以画出$f(x)=\\ln{x}-x+1$在区间[0,1]上的图像：\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/inequality.png)\n\n文中进一步证明了$q$分布的熵比$p$分布的熵更大，因此，由$q$分布来进行采样可以使采样更均匀（= =！那么直接使用均匀分布采样不更好吗？），使学习的目标更多样化。证明过程篇幅过长，详见原论文附录，\n$$\np\\left(\\boldsymbol{\\tau}^{g}\\right), \\text { where } p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right) \\in(0,1) \\text { and } \\sum_{i=1}^{N} p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=1\n$$\n\n$$\nq\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=\\frac{1}{Z} p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\left(1-p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\right), \\text { where } \\sum_{i=1}^{N} q\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=1\n$$\n\n$$\n\\mathcal{H}_{q}\\left(\\mathcal{T}^{g}\\right)-\\mathcal{H}_{p}\\left(\\mathcal{T}^{g}\\right) \\geq 0\n$$\n\n## 基于最大熵的优先级\n\n文中说是基于最大熵的优先经验回放，但是除了使用$q\\left(\\boldsymbol{\\tau}^{g}\\right)$分布之外，在这一部分没有体现出熵的影子，优先级的设置为\n$$\nq\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=\\frac{\\operatorname{rank}\\left(q\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\right)}{\\sum_{n=1}^{N} \\operatorname{rank}\\left(q\\left(\\boldsymbol{\\tau}_{n}^{g}\\right)\\right)}\n$$\n使用排序作为衡量优先级的标准是因为这种方式更具鲁棒性，对异常值不敏感。\n\n之前文中说想要使出现概率低的迹以更高的概率被从经验池中采样到，如果是按照新的$q$分布来定义迹出现的概率，那么到这里是有些说不通的，因为，前文提到$q\\left(\\boldsymbol{\\tau}^{g}\\right) \\approx p\\left(\\boldsymbol{\\tau}^{g}\\right)-p\\left(\\boldsymbol{\\tau}^{g}\\right)^{2} $，那么画出$f(x)=x-x^2$在区间[0，1]上的图像为：\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/x-x^2.png)\n\n由此可见，q值小的地方为p值小与p值大的地方，这就会导致出现概率最大、概率最小的迹被经验池重复的次数多，与前文所讲不同。\n\n而如果考虑以真实的p分布来定义迹出现的概率，那么此处不应该使用$\\operatorname{rank}\\left(q\\left(\\tau_{i}^{g}\\right)\\right)$作为排序的标准，而应该是$\\operatorname{rank}\\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)$，或者$\\operatorname{rank}p\\left(\\boldsymbol{\\tau}^{g}\\right)$。\n\n总之，这里关于优先级的解释不是很清楚。\n\n## 流程示意图\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/MEP.png)\n\n## 伪代码\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/pseudo.png)\n\n解析：\n\n- 每一次迭代，都重新构造优先采样分布$q\\left(\\tau^{g}\\right)$\n\n# 实验部分\n\n## 环境\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/env.png)\n\n- 算法：DDPG\n- 5个随机种子进行实验，取最好的结果\n- 19个CPU\n- 训练200个epoch\n\n## 实验结果\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/mean-success.png)\n\n- **以训练的epoch为标准**，使用了MEP的收敛速度更快\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/training-time.png)\n\n可以看到，结合MEP的效果最好，但是我觉得这种对比一点都不严谨，从数据上看，不使用MEP比使用MEP的训练时间更短，且效果也差不多，那么如果训练相同的时间，说不定使用MEP的效果并没有不使用的好。\n\n![](./maximum-entropy-regularized-multi-goal-reinforcement-learning/sample-efficiency.png)\n\n固定成功率，在机器人实验中，提升了1.95倍采样效率，即使用更少的样本训练相同的效果。\n\n# 疑问\n\n前文提到原目标函数中的对数项会使训练值函数不稳定，为什么？\n\n为什么构造新的迹分布$q\\left(\\boldsymbol{\\tau}^{g}\\right)$为那样的形式？\n\n","slug":"maximum-entropy-regularized-multi-goal-reinforcement-learning","published":1,"updated":"2019-06-13T02:08:05.463Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma2y002nekveqvkcldly","content":"<p>这篇论文将强化学习的目标与最大熵结合了起来，提出了简称为MEP的经验池机制。许多将熵与强化学习结合的方法都是考虑可选动作分布的熵，该篇论文很新颖的使用的是“迹”的熵。</p>\n<p>推荐程度中等偏下：</p>\n<ul>\n<li>有些地方解释的不是很清楚</li>\n<li>熵的结合方式特殊，可以一看</li>\n<li>有些公式推导过于复杂，难懂</li>\n<li>有些参考文献标注不准，如A3C算法的论文并没有使用熵的概念，却在熵相关的语句进行了标注</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1905.08786.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1905.08786.pdf</a></p>\n<p>原作者代码地址：<a href=\"https://github.com/ruizhaogit/mep.git\" rel=\"external nofollow\" target=\"_blank\">https://github.com/ruizhaogit/mep.git</a></p>\n<p>该文章发于2019年的ICML，与之前写过的《Energy-Based Hindsight Experience Prioritization》为同一作者。</p>\n<p>本文主要做了三个贡献：</p>\n<ol>\n<li>修改了目标函数，提出了最大熵正则化多目标强化学习(Maximum Entropy-Regularized Multi-Goal RL)的想法</li>\n<li>推导出替代(surrogate)目标函数，是第1步目标函数的一个下界，可以使算法稳定优化</li>\n<li>提出了Maximum Entropy-based Prioritization(MEP)的经验池框架</li>\n</ol>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><p>文中目标函数的构造主要受Guiasu于1971年提出的加权熵(Weighted Entropy)启发。加权熵表示为：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{H}_{p}^{w}=-\\sum_{k=1}^{K} w_{k} p_{k} \\log p_{k}</script><p>$w_{k}$表示权重。</p>\n<h2 id=\"多目标强化学习的符号表示\"><a href=\"#多目标强化学习的符号表示\" class=\"headerlink\" title=\"多目标强化学习的符号表示\"></a>多目标强化学习的符号表示</h2><p>用$p\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right)$表示一个迹出现的概率：</p>\n<ul>\n<li>$\\boldsymbol{\\tau}=s_{1}, a_{1}, s_{2}, a_{2}, \\ldots, s_{T-1}, a_{T-1}, s_{T}$代表迹</li>\n<li>$g^{e}$代表一个真实的目标，即不是“事后诸葛亮”假定的目标，$g^{e} \\in \\operatorname{Val}\\left(G^{e}\\right)$，后一项为目标空间，一般情况下，可以视为状态空间$\\mathcal{S}$的子集</li>\n<li>$\\theta$代表策略参数</li>\n</ul>\n<p>展开来写，一个迹在策略$\\theta$被采样到的概率为</p>\n<script type=\"math/tex; mode=display\">\np\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right)=p\\left(s_{1}\\right) \\prod_{t=1}^{T-1} p\\left(a_{t} | s_{t}, g^{e}, \\boldsymbol{\\theta}\\right) p\\left(s_{t+1} | s_{t}, a_{t}\\right)</script><p>由此定义策略$\\theta$下的期望奖励回报，也就是目标函数，表示为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} \\eta(\\boldsymbol{\\theta}) &=\\mathbb{E}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\ &=\\sum_{g^{e}} p\\left(g^{e}\\right) \\sum_{\\boldsymbol{\\tau}} p\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\end{aligned}</script><p>很容易理解，也就是在传统的目标函数前边加了一项关于每个目标求积分的步骤。</p>\n<p>如果使用off-policy算法且用经验池机制来提升采样效率，那么目标函数为</p>\n<script type=\"math/tex; mode=display\">\n\\eta^{\\mathcal{R}}(\\boldsymbol{\\theta})=\\sum_{\\boldsymbol{\\tau}, g^{e}} p_{\\mathcal{R}}\\left(\\boldsymbol{\\tau}, g^{e} | \\boldsymbol{\\theta}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right)</script><p>其中$\\mathcal{R}$表示经验池。注意，$\\eta^{\\mathcal{R}}(\\boldsymbol{\\theta})$将$\\eta(\\boldsymbol{\\theta})$的前两个积分项合在一起写了，所以是联合概率而不是条件概率。</p>\n<h2 id=\"最大熵正则化目标函数\"><a href=\"#最大熵正则化目标函数\" class=\"headerlink\" title=\"最大熵正则化目标函数\"></a>最大熵正则化目标函数</h2><p>将前文提到的$\\eta(\\boldsymbol{\\theta})$与加权熵结合起来，就构造出了本文中新的目标函数，即</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} \\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) &=\\mathcal{H}_{p}^{w}\\left(\\mathcal{T}^{g}\\right) \\\\ &=\\mathbb{E}_{p}\\left[\\color{red} {\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)}} \\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\end{aligned}</script><p>$\\color{red} {p\\left(\\boldsymbol{\\tau}^{g}\\right)}$代表$\\sum_{g^{e}} p_{\\mathcal{R}}\\left(\\tau^{g}, g^{e} | \\boldsymbol{\\theta} \\right )$，期望右下角的$p$也是$p\\left(\\boldsymbol{\\tau}^{g}\\right)$的意思。</p>\n<p><strong>与传统的结合方式不同的是，这种结合方式并没有将熵作为一个加和的项，而是相乘。</strong></p>\n<p>仔细想一下，如果将这个式子视为加权熵，那么权重系数是累计奖励$\\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right)$，这样做的直观解释是：<strong>对于各种各样的迹，给与累计回报大的以更多权重，使算法直到要朝哪个迹的方向优化。</strong></p>\n<p>反而，如果将前一个对数项视为传统强化学习目标函数的权重系数，那么这样的直观解释是：<strong>迹出现的概率越低，就越新颖，反而要使其权重增加，从而驱使算法向探索的方向优化。</strong></p>\n<h2 id=\"替代目标函数\"><a href=\"#替代目标函数\" class=\"headerlink\" title=\"替代目标函数\"></a>替代目标函数</h2><p>文中指出，$\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) $中的$\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)}$这一项是无界的，即取值范围为$[0,+\\infty]$，这会导致通用值函数近似的训练不稳定，因此提出了可靠地替代目标函数$\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})$，这个目标函数是原目标函数的一个下界。</p>\n<blockquote>\n<p>the weight, $\\log \\left(1 / p\\left(\\boldsymbol{\\tau}^{g}\\right)\\right)$, is unbounded, which makes the training of the universal function approximator unstable. </p>\n</blockquote>\n<p>表示为</p>\n<script type=\"math/tex; mode=display\">\n\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})=Z \\cdot \\mathbb{E}_{\\color{red}{q}}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right]</script><p>注意，期望积分的是迹分布函数$q$，而不是经验池中真实的迹分布函数$p$。</p>\n<p>那么，新的迹分布函数$q\\left(\\boldsymbol{\\tau}^{g}\\right)$是怎么得来的？</p>\n<p>首先，使用Latent Varibale Model(LVM)对$p\\left(\\boldsymbol{\\tau}^{g}\\right)$的潜在分布进行建模，因为LVM适合于对复杂的分布进行建模。</p>\n<blockquote>\n<p>We use a Latent Variable Model(LVM) (Murphy, 2012) to model the underlying distribution of $p\\left(\\boldsymbol{\\tau}^{g}\\right)$, since LVM is suitable for modeling complex distributions.</p>\n</blockquote>\n<p>将分布用混合高斯模型MoG表示，</p>\n<script type=\"math/tex; mode=display\">\np\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)=\\frac{1}{Z} \\sum_{i=k}^{K} c_{k} \\mathcal{N}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)</script><p>其中，</p>\n<ul>\n<li>$K$为隐变量的个数</li>\n<li>$\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}$分布为均值与协方差矩阵</li>\n<li>$c_{k}$为混合系数</li>\n<li>$Z$为配分函数（归一化系数）</li>\n<li>$\\phi$为模型参数，包含所有的均值、协方差矩阵和混合系数</li>\n</ul>\n<p>文中接着使用$p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)$的补作为经验重放的优先级，即</p>\n<script type=\"math/tex; mode=display\">\n\\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right) \\propto 1-p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)</script><p>补越大，就代表迹出现的概率越低，那么就对该迹赋予更大的优先级。作者想通过过采样这些迹来增大训练时迹分布的熵。直观的解释就是，概率分布不均匀，那么让概率大的出现次数少，概率小的出现次数多，这样就会使采样的分布朝着均匀分布的方向移动，从而使熵值增加。</p>\n<p>文中由此引出了新的迹分布$q\\left(\\boldsymbol{\\tau}^{g}\\right)$，它表示为原始分布与其补的联合分布，</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} q\\left(\\boldsymbol{\\tau}^{g}\\right) & \\propto \\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right) p\\left(\\boldsymbol{\\tau}^{g}\\right) \\\\ & \\propto\\left(1-p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)\\right) p\\left(\\boldsymbol{\\tau}^{g}\\right) \\\\ & \\approx p\\left(\\boldsymbol{\\tau}^{g}\\right)-p\\left(\\boldsymbol{\\tau}^{g}\\right)^{2} \\end{aligned}</script><p>文中将正比的比例设置为$\\color{blue}{\\frac{1}{Z}}$，即$q\\left(\\boldsymbol{\\tau}^{g}\\right)=\\frac{1}{Z} p\\left(\\boldsymbol{\\tau}^{g}\\right)\\left(1-p\\left(\\boldsymbol{\\tau}^{g}\\right)\\right)$，接下来就可以证明$\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})$为$\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) $的下界：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} \n\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta}) &=Z \\cdot \\mathbb{E}_{q}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\ &=\\sum_{\\boldsymbol{\\tau}^{g}} Z \\cdot q\\left(\\boldsymbol{\\tau}^{g}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&=\\sum_{\\tau^{g}} \\frac{Z}{Z} p\\left(\\tau^{g}\\right)\\left(1-p\\left(\\tau^{g}\\right)\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&<\\sum_{\\tau^{g}}-p\\left(\\boldsymbol{\\tau}^{g}\\right) \\log p\\left(\\boldsymbol{\\tau}^{g}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&=\\mathbb{E}_{p}\\left[\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)} \\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\\n&=\\mathcal{H}_{p}^{w}\\left(\\mathcal{T}^{g}\\right) \\\\\n&=\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta})\n\\end{aligned}</script><p>在小于号不等式那一步，使用了函数的性质：$\\log x&lt;x-1$。很容易可以画出$f(x)=\\ln{x}-x+1$在区间[0,1]上的图像：</p>\n<p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/inequality.png\" alt=\"\"></p>\n<p>文中进一步证明了$q$分布的熵比$p$分布的熵更大，因此，由$q$分布来进行采样可以使采样更均匀（= =！那么直接使用均匀分布采样不更好吗？），使学习的目标更多样化。证明过程篇幅过长，详见原论文附录，</p>\n<script type=\"math/tex; mode=display\">\np\\left(\\boldsymbol{\\tau}^{g}\\right), \\text { where } p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right) \\in(0,1) \\text { and } \\sum_{i=1}^{N} p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=1</script><script type=\"math/tex; mode=display\">\nq\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=\\frac{1}{Z} p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\left(1-p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\right), \\text { where } \\sum_{i=1}^{N} q\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=1</script><script type=\"math/tex; mode=display\">\n\\mathcal{H}_{q}\\left(\\mathcal{T}^{g}\\right)-\\mathcal{H}_{p}\\left(\\mathcal{T}^{g}\\right) \\geq 0</script><h2 id=\"基于最大熵的优先级\"><a href=\"#基于最大熵的优先级\" class=\"headerlink\" title=\"基于最大熵的优先级\"></a>基于最大熵的优先级</h2><p>文中说是基于最大熵的优先经验回放，但是除了使用$q\\left(\\boldsymbol{\\tau}^{g}\\right)$分布之外，在这一部分没有体现出熵的影子，优先级的设置为</p>\n<script type=\"math/tex; mode=display\">\nq\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=\\frac{\\operatorname{rank}\\left(q\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\right)}{\\sum_{n=1}^{N} \\operatorname{rank}\\left(q\\left(\\boldsymbol{\\tau}_{n}^{g}\\right)\\right)}</script><p>使用排序作为衡量优先级的标准是因为这种方式更具鲁棒性，对异常值不敏感。</p>\n<p>之前文中说想要使出现概率低的迹以更高的概率被从经验池中采样到，如果是按照新的$q$分布来定义迹出现的概率，那么到这里是有些说不通的，因为，前文提到$q\\left(\\boldsymbol{\\tau}^{g}\\right) \\approx p\\left(\\boldsymbol{\\tau}^{g}\\right)-p\\left(\\boldsymbol{\\tau}^{g}\\right)^{2} $，那么画出$f(x)=x-x^2$在区间[0，1]上的图像为：</p>\n<p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/x-x^2.png\" alt=\"\"></p>\n<p>由此可见，q值小的地方为p值小与p值大的地方，这就会导致出现概率最大、概率最小的迹被经验池重复的次数多，与前文所讲不同。</p>\n<p>而如果考虑以真实的p分布来定义迹出现的概率，那么此处不应该使用$\\operatorname{rank}\\left(q\\left(\\tau_{i}^{g}\\right)\\right)$作为排序的标准，而应该是$\\operatorname{rank}\\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)$，或者$\\operatorname{rank}p\\left(\\boldsymbol{\\tau}^{g}\\right)$。</p>\n<p>总之，这里关于优先级的解释不是很清楚。</p>\n<h2 id=\"流程示意图\"><a href=\"#流程示意图\" class=\"headerlink\" title=\"流程示意图\"></a>流程示意图</h2><p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/MEP.png\" alt=\"\"></p>\n<h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>每一次迭代，都重新构造优先采样分布$q\\left(\\tau^{g}\\right)$</li>\n</ul>\n<h1 id=\"实验部分\"><a href=\"#实验部分\" class=\"headerlink\" title=\"实验部分\"></a>实验部分</h1><h2 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h2><p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/env.png\" alt=\"\"></p>\n<ul>\n<li>算法：DDPG</li>\n<li>5个随机种子进行实验，取最好的结果</li>\n<li>19个CPU</li>\n<li>训练200个epoch</li>\n</ul>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/mean-success.png\" alt=\"\"></p>\n<ul>\n<li><strong>以训练的epoch为标准</strong>，使用了MEP的收敛速度更快</li>\n</ul>\n<p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/training-time.png\" alt=\"\"></p>\n<p>可以看到，结合MEP的效果最好，但是我觉得这种对比一点都不严谨，从数据上看，不使用MEP比使用MEP的训练时间更短，且效果也差不多，那么如果训练相同的时间，说不定使用MEP的效果并没有不使用的好。</p>\n<p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/sample-efficiency.png\" alt=\"\"></p>\n<p>固定成功率，在机器人实验中，提升了1.95倍采样效率，即使用更少的样本训练相同的效果。</p>\n<h1 id=\"疑问\"><a href=\"#疑问\" class=\"headerlink\" title=\"疑问\"></a>疑问</h1><p>前文提到原目标函数中的对数项会使训练值函数不稳定，为什么？</p>\n<p>为什么构造新的迹分布$q\\left(\\boldsymbol{\\tau}^{g}\\right)$为那样的形式？</p>\n","site":{"data":{}},"excerpt":"<p>这篇论文将强化学习的目标与最大熵结合了起来，提出了简称为MEP的经验池机制。许多将熵与强化学习结合的方法都是考虑可选动作分布的熵，该篇论文很新颖的使用的是“迹”的熵。</p>\n<p>推荐程度中等偏下：</p>\n<ul>\n<li>有些地方解释的不是很清楚</li>\n<li>熵的结合方式特殊，可以一看</li>\n<li>有些公式推导过于复杂，难懂</li>\n<li>有些参考文献标注不准，如A3C算法的论文并没有使用熵的概念，却在熵相关的语句进行了标注</li>\n</ul>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1905.08786.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1905.08786.pdf</a></p>\n<p>原作者代码地址：<a href=\"https://github.com/ruizhaogit/mep.git\" rel=\"external nofollow\" target=\"_blank\">https://github.com/ruizhaogit/mep.git</a></p>\n<p>该文章发于2019年的ICML，与之前写过的《Energy-Based Hindsight Experience Prioritization》为同一作者。</p>\n<p>本文主要做了三个贡献：</p>\n<ol>\n<li>修改了目标函数，提出了最大熵正则化多目标强化学习(Maximum Entropy-Regularized Multi-Goal RL)的想法</li>\n<li>推导出替代(surrogate)目标函数，是第1步目标函数的一个下界，可以使算法稳定优化</li>\n<li>提出了Maximum Entropy-based Prioritization(MEP)的经验池框架</li>\n</ol>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><p>文中目标函数的构造主要受Guiasu于1971年提出的加权熵(Weighted Entropy)启发。加权熵表示为：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{H}_{p}^{w}=-\\sum_{k=1}^{K} w_{k} p_{k} \\log p_{k}</script><p>$w_{k}$表示权重。</p>\n<h2 id=\"多目标强化学习的符号表示\"><a href=\"#多目标强化学习的符号表示\" class=\"headerlink\" title=\"多目标强化学习的符号表示\"></a>多目标强化学习的符号表示</h2><p>用$p\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right)$表示一个迹出现的概率：</p>\n<ul>\n<li>$\\boldsymbol{\\tau}=s_{1}, a_{1}, s_{2}, a_{2}, \\ldots, s_{T-1}, a_{T-1}, s_{T}$代表迹</li>\n<li>$g^{e}$代表一个真实的目标，即不是“事后诸葛亮”假定的目标，$g^{e} \\in \\operatorname{Val}\\left(G^{e}\\right)$，后一项为目标空间，一般情况下，可以视为状态空间$\\mathcal{S}$的子集</li>\n<li>$\\theta$代表策略参数</li>\n</ul>\n<p>展开来写，一个迹在策略$\\theta$被采样到的概率为</p>\n<script type=\"math/tex; mode=display\">\np\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right)=p\\left(s_{1}\\right) \\prod_{t=1}^{T-1} p\\left(a_{t} | s_{t}, g^{e}, \\boldsymbol{\\theta}\\right) p\\left(s_{t+1} | s_{t}, a_{t}\\right)</script><p>由此定义策略$\\theta$下的期望奖励回报，也就是目标函数，表示为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} \\eta(\\boldsymbol{\\theta}) &=\\mathbb{E}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\ &=\\sum_{g^{e}} p\\left(g^{e}\\right) \\sum_{\\boldsymbol{\\tau}} p\\left(\\boldsymbol{\\tau} | g^{e}, \\boldsymbol{\\theta}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\end{aligned}</script><p>很容易理解，也就是在传统的目标函数前边加了一项关于每个目标求积分的步骤。</p>\n<p>如果使用off-policy算法且用经验池机制来提升采样效率，那么目标函数为</p>\n<script type=\"math/tex; mode=display\">\n\\eta^{\\mathcal{R}}(\\boldsymbol{\\theta})=\\sum_{\\boldsymbol{\\tau}, g^{e}} p_{\\mathcal{R}}\\left(\\boldsymbol{\\tau}, g^{e} | \\boldsymbol{\\theta}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right)</script><p>其中$\\mathcal{R}$表示经验池。注意，$\\eta^{\\mathcal{R}}(\\boldsymbol{\\theta})$将$\\eta(\\boldsymbol{\\theta})$的前两个积分项合在一起写了，所以是联合概率而不是条件概率。</p>\n<h2 id=\"最大熵正则化目标函数\"><a href=\"#最大熵正则化目标函数\" class=\"headerlink\" title=\"最大熵正则化目标函数\"></a>最大熵正则化目标函数</h2><p>将前文提到的$\\eta(\\boldsymbol{\\theta})$与加权熵结合起来，就构造出了本文中新的目标函数，即</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} \\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) &=\\mathcal{H}_{p}^{w}\\left(\\mathcal{T}^{g}\\right) \\\\ &=\\mathbb{E}_{p}\\left[\\color{red} {\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)}} \\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\end{aligned}</script><p>$\\color{red} {p\\left(\\boldsymbol{\\tau}^{g}\\right)}$代表$\\sum_{g^{e}} p_{\\mathcal{R}}\\left(\\tau^{g}, g^{e} | \\boldsymbol{\\theta} \\right )$，期望右下角的$p$也是$p\\left(\\boldsymbol{\\tau}^{g}\\right)$的意思。</p>\n<p><strong>与传统的结合方式不同的是，这种结合方式并没有将熵作为一个加和的项，而是相乘。</strong></p>\n<p>仔细想一下，如果将这个式子视为加权熵，那么权重系数是累计奖励$\\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right)$，这样做的直观解释是：<strong>对于各种各样的迹，给与累计回报大的以更多权重，使算法直到要朝哪个迹的方向优化。</strong></p>\n<p>反而，如果将前一个对数项视为传统强化学习目标函数的权重系数，那么这样的直观解释是：<strong>迹出现的概率越低，就越新颖，反而要使其权重增加，从而驱使算法向探索的方向优化。</strong></p>\n<h2 id=\"替代目标函数\"><a href=\"#替代目标函数\" class=\"headerlink\" title=\"替代目标函数\"></a>替代目标函数</h2><p>文中指出，$\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) $中的$\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)}$这一项是无界的，即取值范围为$[0,+\\infty]$，这会导致通用值函数近似的训练不稳定，因此提出了可靠地替代目标函数$\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})$，这个目标函数是原目标函数的一个下界。</p>\n<blockquote>\n<p>the weight, $\\log \\left(1 / p\\left(\\boldsymbol{\\tau}^{g}\\right)\\right)$, is unbounded, which makes the training of the universal function approximator unstable. </p>\n</blockquote>\n<p>表示为</p>\n<script type=\"math/tex; mode=display\">\n\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})=Z \\cdot \\mathbb{E}_{\\color{red}{q}}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right]</script><p>注意，期望积分的是迹分布函数$q$，而不是经验池中真实的迹分布函数$p$。</p>\n<p>那么，新的迹分布函数$q\\left(\\boldsymbol{\\tau}^{g}\\right)$是怎么得来的？</p>\n<p>首先，使用Latent Varibale Model(LVM)对$p\\left(\\boldsymbol{\\tau}^{g}\\right)$的潜在分布进行建模，因为LVM适合于对复杂的分布进行建模。</p>\n<blockquote>\n<p>We use a Latent Variable Model(LVM) (Murphy, 2012) to model the underlying distribution of $p\\left(\\boldsymbol{\\tau}^{g}\\right)$, since LVM is suitable for modeling complex distributions.</p>\n</blockquote>\n<p>将分布用混合高斯模型MoG表示，</p>\n<script type=\"math/tex; mode=display\">\np\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)=\\frac{1}{Z} \\sum_{i=k}^{K} c_{k} \\mathcal{N}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)</script><p>其中，</p>\n<ul>\n<li>$K$为隐变量的个数</li>\n<li>$\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}$分布为均值与协方差矩阵</li>\n<li>$c_{k}$为混合系数</li>\n<li>$Z$为配分函数（归一化系数）</li>\n<li>$\\phi$为模型参数，包含所有的均值、协方差矩阵和混合系数</li>\n</ul>\n<p>文中接着使用$p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)$的补作为经验重放的优先级，即</p>\n<script type=\"math/tex; mode=display\">\n\\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right) \\propto 1-p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)</script><p>补越大，就代表迹出现的概率越低，那么就对该迹赋予更大的优先级。作者想通过过采样这些迹来增大训练时迹分布的熵。直观的解释就是，概率分布不均匀，那么让概率大的出现次数少，概率小的出现次数多，这样就会使采样的分布朝着均匀分布的方向移动，从而使熵值增加。</p>\n<p>文中由此引出了新的迹分布$q\\left(\\boldsymbol{\\tau}^{g}\\right)$，它表示为原始分布与其补的联合分布，</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} q\\left(\\boldsymbol{\\tau}^{g}\\right) & \\propto \\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right) p\\left(\\boldsymbol{\\tau}^{g}\\right) \\\\ & \\propto\\left(1-p\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)\\right) p\\left(\\boldsymbol{\\tau}^{g}\\right) \\\\ & \\approx p\\left(\\boldsymbol{\\tau}^{g}\\right)-p\\left(\\boldsymbol{\\tau}^{g}\\right)^{2} \\end{aligned}</script><p>文中将正比的比例设置为$\\color{blue}{\\frac{1}{Z}}$，即$q\\left(\\boldsymbol{\\tau}^{g}\\right)=\\frac{1}{Z} p\\left(\\boldsymbol{\\tau}^{g}\\right)\\left(1-p\\left(\\boldsymbol{\\tau}^{g}\\right)\\right)$，接下来就可以证明$\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta})$为$\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta}) $的下界：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} \n\\eta^{\\mathcal{L}}(\\boldsymbol{\\theta}) &=Z \\cdot \\mathbb{E}_{q}\\left[\\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\ &=\\sum_{\\boldsymbol{\\tau}^{g}} Z \\cdot q\\left(\\boldsymbol{\\tau}^{g}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&=\\sum_{\\tau^{g}} \\frac{Z}{Z} p\\left(\\tau^{g}\\right)\\left(1-p\\left(\\tau^{g}\\right)\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&<\\sum_{\\tau^{g}}-p\\left(\\boldsymbol{\\tau}^{g}\\right) \\log p\\left(\\boldsymbol{\\tau}^{g}\\right) \\sum_{t=1}^{T} r\\left(s_{t}, g^{e}\\right) \\\\\n&=\\mathbb{E}_{p}\\left[\\log \\frac{1}{p\\left(\\boldsymbol{\\tau}^{g}\\right)} \\sum_{t=1}^{T} r\\left(S_{t}, G^{e}\\right) | \\boldsymbol{\\theta}\\right] \\\\\n&=\\mathcal{H}_{p}^{w}\\left(\\mathcal{T}^{g}\\right) \\\\\n&=\\eta^{\\mathcal{H}}(\\boldsymbol{\\theta})\n\\end{aligned}</script><p>在小于号不等式那一步，使用了函数的性质：$\\log x&lt;x-1$。很容易可以画出$f(x)=\\ln{x}-x+1$在区间[0,1]上的图像：</p>\n<p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/inequality.png\" alt=\"\"></p>\n<p>文中进一步证明了$q$分布的熵比$p$分布的熵更大，因此，由$q$分布来进行采样可以使采样更均匀（= =！那么直接使用均匀分布采样不更好吗？），使学习的目标更多样化。证明过程篇幅过长，详见原论文附录，</p>\n<script type=\"math/tex; mode=display\">\np\\left(\\boldsymbol{\\tau}^{g}\\right), \\text { where } p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right) \\in(0,1) \\text { and } \\sum_{i=1}^{N} p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=1</script><script type=\"math/tex; mode=display\">\nq\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=\\frac{1}{Z} p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\left(1-p\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\right), \\text { where } \\sum_{i=1}^{N} q\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=1</script><script type=\"math/tex; mode=display\">\n\\mathcal{H}_{q}\\left(\\mathcal{T}^{g}\\right)-\\mathcal{H}_{p}\\left(\\mathcal{T}^{g}\\right) \\geq 0</script><h2 id=\"基于最大熵的优先级\"><a href=\"#基于最大熵的优先级\" class=\"headerlink\" title=\"基于最大熵的优先级\"></a>基于最大熵的优先级</h2><p>文中说是基于最大熵的优先经验回放，但是除了使用$q\\left(\\boldsymbol{\\tau}^{g}\\right)$分布之外，在这一部分没有体现出熵的影子，优先级的设置为</p>\n<script type=\"math/tex; mode=display\">\nq\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)=\\frac{\\operatorname{rank}\\left(q\\left(\\boldsymbol{\\tau}_{i}^{g}\\right)\\right)}{\\sum_{n=1}^{N} \\operatorname{rank}\\left(q\\left(\\boldsymbol{\\tau}_{n}^{g}\\right)\\right)}</script><p>使用排序作为衡量优先级的标准是因为这种方式更具鲁棒性，对异常值不敏感。</p>\n<p>之前文中说想要使出现概率低的迹以更高的概率被从经验池中采样到，如果是按照新的$q$分布来定义迹出现的概率，那么到这里是有些说不通的，因为，前文提到$q\\left(\\boldsymbol{\\tau}^{g}\\right) \\approx p\\left(\\boldsymbol{\\tau}^{g}\\right)-p\\left(\\boldsymbol{\\tau}^{g}\\right)^{2} $，那么画出$f(x)=x-x^2$在区间[0，1]上的图像为：</p>\n<p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/x-x^2.png\" alt=\"\"></p>\n<p>由此可见，q值小的地方为p值小与p值大的地方，这就会导致出现概率最大、概率最小的迹被经验池重复的次数多，与前文所讲不同。</p>\n<p>而如果考虑以真实的p分布来定义迹出现的概率，那么此处不应该使用$\\operatorname{rank}\\left(q\\left(\\tau_{i}^{g}\\right)\\right)$作为排序的标准，而应该是$\\operatorname{rank}\\overline{p}\\left(\\boldsymbol{\\tau}^{g} | \\boldsymbol{\\phi}\\right)$，或者$\\operatorname{rank}p\\left(\\boldsymbol{\\tau}^{g}\\right)$。</p>\n<p>总之，这里关于优先级的解释不是很清楚。</p>\n<h2 id=\"流程示意图\"><a href=\"#流程示意图\" class=\"headerlink\" title=\"流程示意图\"></a>流程示意图</h2><p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/MEP.png\" alt=\"\"></p>\n<h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>每一次迭代，都重新构造优先采样分布$q\\left(\\tau^{g}\\right)$</li>\n</ul>\n<h1 id=\"实验部分\"><a href=\"#实验部分\" class=\"headerlink\" title=\"实验部分\"></a>实验部分</h1><h2 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h2><p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/env.png\" alt=\"\"></p>\n<ul>\n<li>算法：DDPG</li>\n<li>5个随机种子进行实验，取最好的结果</li>\n<li>19个CPU</li>\n<li>训练200个epoch</li>\n</ul>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/mean-success.png\" alt=\"\"></p>\n<ul>\n<li><strong>以训练的epoch为标准</strong>，使用了MEP的收敛速度更快</li>\n</ul>\n<p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/training-time.png\" alt=\"\"></p>\n<p>可以看到，结合MEP的效果最好，但是我觉得这种对比一点都不严谨，从数据上看，不使用MEP比使用MEP的训练时间更短，且效果也差不多，那么如果训练相同的时间，说不定使用MEP的效果并没有不使用的好。</p>\n<p><img src=\"./maximum-entropy-regularized-multi-goal-reinforcement-learning/sample-efficiency.png\" alt=\"\"></p>\n<p>固定成功率，在机器人实验中，提升了1.95倍采样效率，即使用更少的样本训练相同的效果。</p>\n<h1 id=\"疑问\"><a href=\"#疑问\" class=\"headerlink\" title=\"疑问\"></a>疑问</h1><p>前文提到原目标函数中的对数项会使训练值函数不稳定，为什么？</p>\n<p>为什么构造新的迹分布$q\\left(\\boldsymbol{\\tau}^{g}\\right)$为那样的形式？</p>"},{"title":"Energy-Based Hindsight Experience Prioritization","copyright":true,"mathjax":true,"top":1,"date":"2019-05-30T00:58:58.000Z","keywords":null,"description":null,"_content":"\n本文是对HER“事后”经验池机制的一个扩展，它结合了物理学的能量知识以及优先经验回放PER对HER进行提升。简称：EBP\n\n推荐：\n\n- 创新虽不多，但是基于能量的创意可以拓宽在机器人领域训练的视野\n- 通俗易懂\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1810.01363.pdf](https://arxiv.org/pdf/1810.01363.pdf)\n\n这篇论文由慕尼黑大学博三学生[赵瑞](https://ruizhaogit.github.io)和他的导师Volker Tresp发于2018年的CoRL会议。\n\n**本文提出了一个简单高效的、基于能量的方法去优先回放“事后经验”。Energy+HER+PER**\n\n在HER中，智能体从它可完成的“虚拟”目标中进行大量学习，虚拟目标就是我们使用“事后诸葛亮”方法所调整的经验中的目标。\n\n本文针对原始HER提出了一个稍有不足的地方：经验回放是完全随机的，即没有优先级，没有考虑哪些episode哪些经验对学习更有价值，其实这个问题与PER相对于传统经验池机制也是一样的。\n\n本文中使用的功能定理（work-energy principle）来计算能量。\n\n# 文中精要\n\n相比于传统的PER优先经验回放使用TD-error作为衡量优先级的度量，本文中使用“迹能量”作为其度量。\n\n迹能量是这么定义的：\n\n- > We define a trajectory energy function as the sum of the transition energy of the target object over the trajectory. \n\n- 迹能量是一个episode中transition energy（不知道怎么翻译合适，过渡能量？经验能量？转换能量？）的总和\n\n接下来介绍一下能量在本文中是如何体现的。\n\n## 经验能量差 Transition Energy\n\n我就直接拿论文中实验场景所用到的能力来说明这个能量差。简言之，在本文的实验中主要是操作机械手臂移动物体的水平位置和垂直高度，所以物体的能量基本包含三种：\n\n1. **势能 Potential Energy** $E_{p}(s_{t})$\n2. **动能 Kinetic Energy** $E_{k}(s_{t})$\n3. **转动能，也叫角动能 Rotational Energy** $E_{r}(s_{t})$\n\n一个物体的能量由这三部分之和组成：\n$$\nE\\left(s_{t}\\right)=E_{p}\\left(s_{t}\\right)+E_{k}\\left(s_{t}\\right)+E_{r}\\left(s_{t}\\right)\n$$\n经验能量差指的就是相邻状态转移之间的能量差值，表示为：\n$$\nE_{t r a n}\\left(s_{t-1}, s_{t}\\right)=\\operatorname{clip}\\left(E\\left(s_{t}\\right)-E\\left(s_{t-1}\\right), 0, E_{t r a n}^{\\max }\\right)\n$$\n其中，\n\n- 将差值clip到0是因为我们只对由机器人做功导致物体的能量增值感兴趣\n\n- 将差值clip到$E_{t r a n}^{\\max }$是想减缓某些特别大的能量差值的影响，使**训练更稳定**\n\n*注：其实我觉得文中加这个clip操作完全是想多使用一个trick，让文章看起来更饱满一点，我个人认为不使用这个clip，或者只对下界进行clip，对算法性能是没有影响的。有待验证。*\n\n### 势能 Potential Energy\n\n物理学中学过，物体的重力势能公式为：$E=mgh$\n\n本文中这样书写：\n$$\nE_{p}(s_{t})=mgz_{t}\n$$\n\n- $m$代表物体的质量\n- $g$代表地球的重力系数，$g \\approx 9.81 \\mathrm{m} / \\mathrm{s}^{2}$\n- $z_{t}$代表物体在$t$时刻的高度$h$\n\n### 动能 Kinetic Energy\n\n物理学中学过，物体的动能公式为：\n$$\nE=\\frac{1}{2} mv^{2}=\\frac{1}{2} m\\left [ \\frac{\\sqrt{v_{x}^{2}+v_{y}^{2}+v_{z}^{2}}}{\\Delta t} \\right ]^{2}\n$$\n本文中这样书写：\n$$\nE_{k}\\left(s_{t}\\right)=\\frac{1}{2} m v_{x, t}^{2}+\\frac{1}{2} m v_{y, t}^{2}+\\frac{1}{2} m v_{z, t}^{2} \\approx \\frac{m\\left(\\left(x_{t}-x_{t-1}\\right)^{2}+\\left(y_{t}-y_{t-1}\\right)^{2}+\\left(z_{t}-z_{t-1}\\right)^{2}\\right)}{2 \\Delta t^{2}}\n$$\n\n- $v_{x, t} \\approx\\left(x_{t}-x_{t-1}\\right) / \\Delta t$\n- $v_{y, t} \\approx\\left(y_{t}-y_{t-1}\\right) / \\Delta t$\n- $v_{z, t} \\approx\\left(z_{t}-z_{t-1}\\right) / \\Delta t$\n- $\\Delta t$表示相邻两个状态之间的时间间隔，假设我们在模拟器中，1秒60帧，即每帧16.67ms，我们如果每帧执行一次动作，那么$\\Delta t=16.67ms$，如果每60帧执行一次动作，那么$\\Delta t=1s$\n\n### 转动能 Rotational Energy\n\n物理学中学过，物体的转动能公式为：$K=\\frac{1}{2} I \\cdot \\omega^{2}$，注意，中间的点代表点乘，$I$代表物体的惯性矩，$\\omega$代表物体的角速度\n\n本文中这样书写：\n$$\n\\left[ \\begin{array}{c}{\\phi} \\\\ {\\theta} \\\\ {\\psi}\\end{array}\\right]=\\left[ \\begin{array}{c}{\\arctan \\frac{2(a b+c d)}{1-2\\left(b^{2}+c^{2}\\right)}} \\\\ {\\arcsin (2(a c-d b))} \\\\ {\\arcsin \\frac{2(a d+b c)}{1-2\\left(c^{2}+d^{2}\\right)}}\\end{array}\\right]=\\left[ \\begin{array}{c}{\\operatorname{atan} 2\\left(2(a b+c d), 1-2\\left(b^{2}+c^{2}\\right)\\right)} \\\\ {\\operatorname{asin}(2(a c-d b))} \\\\ {\\operatorname{atan} 2\\left(2(a d+b c), 1-2\\left(c^{2}+d^{2}\\right)\\right)}\\end{array}\\right]\n$$\n\n$$\nE_{r}\\left(s_{t}\\right)=\\frac{1}{2} I_{x} \\omega_{x, t}^{2}+\\frac{1}{2} I_{y} \\omega_{y, t}^{2}+\\frac{1}{2} I_{z} \\omega_{z, t}^{2} \\approx \\frac{I_{x}\\left(\\phi_{t}-\\phi_{t-1}\\right)^{2}+I_{y}\\left(\\theta_{t}-\\theta_{t-1}\\right)^{2}+I_{z}\\left(\\psi_{t}-\\psi_{t-1}\\right)^{2}}{2 \\Delta t^{2}}\n$$\n\n其中$a,b,c,d$为旋转四元组，其知识可以百度或google自行了解。\n\n\n$$\nq=a+b \\imath+c \\jmath+d k\n$$\n\n$\\phi, \\theta, \\psi$代表$x,y,z$轴方向的旋转角度\n\n- $\\omega_{x, t} \\approx\\left(\\phi_{t}-\\phi_{t-1}\\right) / \\Delta_{t}$\n- $\\omega_{y, t} \\approx\\left(\\theta_{t}-\\theta_{t-1}\\right) / \\Delta_{t}$\n- $\\omega_{z, t} \\approx\\left(\\psi_{t}-\\psi_{t-1}\\right) / \\Delta_{t}$\n- $\\Delta t$与上文解释相同\n\n**$m,I_{x},I_{y},I_{z}$可以设置为常量，本文实验中设置$m=I_{x}=I_{y}=I_{z}=1$**\n\n\n\n## 迹能量 Trajectory Energy\n\n给定一个回合中所有的经验能量差，迹能量可以表示为这个回合中所有经验能量差之和：\n$$\nE_{t r a j}(\\mathcal{T})=E_{t r a j}\\left(s_{0}, s_{1}, \\ldots, s_{T}\\right)=\\sum_{t=1}^{T} E_{t r a n}\\left(s_{t-1}, s_{t}\\right)\n$$\n\n## 基于能量的优先级\n\n首先计算迹能量，然后对迹能量高的迹（episode）优先进行回放。\n\n根据迹能量计算迹的优先级为：\n$$\np\\left(\\mathcal{T}_{i}\\right)=\\frac{E_{t r a j}\\left(\\mathcal{T}_{i}\\right)}{\\sum_{n=1}^{N} E_{t r a j}\\left(\\mathcal{T}_{n}\\right)}\n$$\n$N$代表经验池中迹的总数量\n\n## 伪代码\n\n![](./energy-based-hindsight-experience-prioritization/pseudo.png)\n\n解析：\n\n- 以本文实验为例，状态$s$由七元组$\\left[x_{t}, y_{t}, z_{t}, a_{t}, b_{t}, c_{t}, d_{t}\\right]$表示，其中前三个代表物体的位置，后三个代表物体旋转的四元组。\n- 目标$g$与状态$s$的表示相同\n- $||$操作符为连结的意思，即`tf.concat(a,b)`\n- 向经验池中存入的不仅仅有$(s,a,r,s')$，还有优先级$p$与迹能量$E_{traj}$，**其实我感觉这样很多余，如果使用sum-tree结构的，存其一即可**\n- 文中所使用的HER是**future模式**\n\n**注意：**\n\n我认为伪代码中有两行很有问题，即\n\n![](./energy-based-hindsight-experience-prioritization/issue.png)\n\n我不明白为什么把原始经验$\\left(s_{t}\\left\\|g, a_{t}, r_{t}, s_{t+1}\\right\\| g, p, E_{t r a j}\\right)$存入经验池之后，需要根据优先级采样一个迹，再从采样到的迹中采样出一个经验$\\left(s_{t}, a_{t}, s_{t+1}\\right)$\n\n起初我是这么认为的，它想对经验池中迹能量高的episode进行大概率抽取，并对其中的经验进行多次扩充，由此对迹能量小的episode更加忽视，突出迹能量高的episode\n\n但是，看到下一行![](./energy-based-hindsight-experience-prioritization/issue2.png)我有一个疑问：如果根据优先级采样出的迹$\\mathcal{T}$与当前所操作的迹$\\mathcal{T}_{current}$不同，那么，为什么还要为不同迹中的经验存入相同的优先级和迹能量呢？即$\\left(s_{t}\\left\\|g^{\\prime}, a_{t}, r_{t}^{\\prime}, s_{t+1}\\right\\| g^{\\prime}, p, E_{t r a j}\\right)$\n\n这样肯定是不行的，那么只有一个答案，采样迹这一步多余的，或者说，不应该出现在这里，而应该放在最后一个循环的开始，即\n\n![](./energy-based-hindsight-experience-prioritization/issue3.png)\n\n也就是说，应该把采样迹，从迹中采样经验的步骤放在minibatch之前，这样就合情合理了。\n\n这是我自己的一个疑问，如果读者有其他见解，欢迎置评讨论。\n\n## EBP的总结\n\nEBP与PER的不同点：\n\n- EBP使用物理学中的能量\n- PER使用TD-error\n\n相比于将HER与PER结合而使用TD-error作为衡量优先级的方法，使用迹能量较少了计算量，因为PER每次回放经验都必须重新计算使用经验的新的TD-error，并存回经验池。（其实，如果使用sum-tree来构建PER，这个劣势其实很小）\n\n文中通过实验发现：比较PER与EBP的时间复杂性，显示EBP提升了算法的性能效果（performance）但是却不增加额外的计算量。PER则提升较少，计算量也增加了。\n\nEBP的优点：\n\n- 可结合任意off-policy算法\n- 结合了物理知识，使其可以应用于现实世界的问题\n- 提升采样效率进两倍\n- 相比最先进的（state-of-the-art）算法，不增加计算时间的情况下，算法效果提升了4个百分点。（此条可以忽略，因为其未必做了充分的实验来进行对比）\n- 适用于任何机器人操作任务\n- 适用于多目标算法\n\n# 实验部分\n\n文中实验结果：[https://youtu.be/jtsF2tTeUGQ](https://youtu.be/jtsF2tTeUGQ)\n\n代码地址：[https://github.com/ruizhaogit/EnergyBasedPrioritization](https://github.com/ruizhaogit/EnergyBasedPrioritization)\n\n实验部分的完整细节请参考论文原文。\n\n## 环境\n\n- OpenAI Gym与MuJoCo物理引擎\n- 一个7自由度的机械手臂，与HER中一样；一个24自由度的机器手\n- 四项任务：pick & place，机器手操作方块、蛋、笔\n\n![](./energy-based-hindsight-experience-prioritization/env.png)\n\n- 使用稀疏奖励，二分奖励，完成容忍度内目标为0，否则为-1\n\n## 算法\n\n- 文中没有说明具体使用什么算法作对比，只有伪代码中提到了DPG、DDPG\n- 文中亦没有对算法中的超参数设置、网络结构进行说明\n- 19个CPU\n- 器械臂场景$E_{t r a n}^{\\max }=0.5$，机械手场景$E_{t r a n}^{\\max }=2.5$\n- 文中主要比较了HER、HER+PER、HER+EBP\n\n## 实验结果\n\n![](./energy-based-hindsight-experience-prioritization/meansuccessrate.png)\n\n- 横坐标是训练的轮数，应该是指episode的意思\n- 纵坐标是5个随机种子实验的平均成功率\n- 蓝色代表HER+EBP，橘色代表HER，绿色代表HER+PER\n\n![](./energy-based-hindsight-experience-prioritization/trainingtime.png)\n\n结果：\n\n- 从上图可以看出，四项任务中，HER+EBP比其他两种方法收敛速度都快，效果也更好一点\n- 从上表可以看出，HER+EBP与HER的训练时间基本相同，而HER+PER要消耗10倍的时间\n\n---\n\n![](./energy-based-hindsight-experience-prioritization/finalmeanrate.png)\n\n结果：\n\n- 训练结束后，HER+EBP在四项任务中效果都最好\n- HER+EBP比HER提高了1-5个百分点，平均提升了3.75个百分点\n\n>We can see that EBP is a simple yet effective method, without increasing computational time, but still, improves current state-of-the-art methods. \n\n![](./energy-based-hindsight-experience-prioritization/sampleefficiency.png)\n\n结果：\n\n- 采样效率方面，总体来看，EBP+HER比HER提升了2倍\n\n---\n\n最后，作者比较了迹能量与TD-error的pearson相关系数\n\n- 系数为1，即正线性相关\n- 系数为-1，即负线性相关\n- 系数为0，即不线性相关\n\n![](./energy-based-hindsight-experience-prioritization/pearsoncorrelation.png)\n\n结果：\n\n- 四个实验中，迹能量与TD-error均成正相关\n- 平均下来pearson系数为0.6，说明迹能量与TD-error呈正线性相关关系，也就是说迹能量可以像TD-error一样表示经验的可学习价值","source":"_posts/energy-based-hindsight-experience-prioritization.md","raw":"---\ntitle: Energy-Based Hindsight Experience Prioritization\ncopyright: true\nmathjax: true\ntop: 1\ndate: 2019-05-30 08:58:58\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n本文是对HER“事后”经验池机制的一个扩展，它结合了物理学的能量知识以及优先经验回放PER对HER进行提升。简称：EBP\n\n推荐：\n\n- 创新虽不多，但是基于能量的创意可以拓宽在机器人领域训练的视野\n- 通俗易懂\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1810.01363.pdf](https://arxiv.org/pdf/1810.01363.pdf)\n\n这篇论文由慕尼黑大学博三学生[赵瑞](https://ruizhaogit.github.io)和他的导师Volker Tresp发于2018年的CoRL会议。\n\n**本文提出了一个简单高效的、基于能量的方法去优先回放“事后经验”。Energy+HER+PER**\n\n在HER中，智能体从它可完成的“虚拟”目标中进行大量学习，虚拟目标就是我们使用“事后诸葛亮”方法所调整的经验中的目标。\n\n本文针对原始HER提出了一个稍有不足的地方：经验回放是完全随机的，即没有优先级，没有考虑哪些episode哪些经验对学习更有价值，其实这个问题与PER相对于传统经验池机制也是一样的。\n\n本文中使用的功能定理（work-energy principle）来计算能量。\n\n# 文中精要\n\n相比于传统的PER优先经验回放使用TD-error作为衡量优先级的度量，本文中使用“迹能量”作为其度量。\n\n迹能量是这么定义的：\n\n- > We define a trajectory energy function as the sum of the transition energy of the target object over the trajectory. \n\n- 迹能量是一个episode中transition energy（不知道怎么翻译合适，过渡能量？经验能量？转换能量？）的总和\n\n接下来介绍一下能量在本文中是如何体现的。\n\n## 经验能量差 Transition Energy\n\n我就直接拿论文中实验场景所用到的能力来说明这个能量差。简言之，在本文的实验中主要是操作机械手臂移动物体的水平位置和垂直高度，所以物体的能量基本包含三种：\n\n1. **势能 Potential Energy** $E_{p}(s_{t})$\n2. **动能 Kinetic Energy** $E_{k}(s_{t})$\n3. **转动能，也叫角动能 Rotational Energy** $E_{r}(s_{t})$\n\n一个物体的能量由这三部分之和组成：\n$$\nE\\left(s_{t}\\right)=E_{p}\\left(s_{t}\\right)+E_{k}\\left(s_{t}\\right)+E_{r}\\left(s_{t}\\right)\n$$\n经验能量差指的就是相邻状态转移之间的能量差值，表示为：\n$$\nE_{t r a n}\\left(s_{t-1}, s_{t}\\right)=\\operatorname{clip}\\left(E\\left(s_{t}\\right)-E\\left(s_{t-1}\\right), 0, E_{t r a n}^{\\max }\\right)\n$$\n其中，\n\n- 将差值clip到0是因为我们只对由机器人做功导致物体的能量增值感兴趣\n\n- 将差值clip到$E_{t r a n}^{\\max }$是想减缓某些特别大的能量差值的影响，使**训练更稳定**\n\n*注：其实我觉得文中加这个clip操作完全是想多使用一个trick，让文章看起来更饱满一点，我个人认为不使用这个clip，或者只对下界进行clip，对算法性能是没有影响的。有待验证。*\n\n### 势能 Potential Energy\n\n物理学中学过，物体的重力势能公式为：$E=mgh$\n\n本文中这样书写：\n$$\nE_{p}(s_{t})=mgz_{t}\n$$\n\n- $m$代表物体的质量\n- $g$代表地球的重力系数，$g \\approx 9.81 \\mathrm{m} / \\mathrm{s}^{2}$\n- $z_{t}$代表物体在$t$时刻的高度$h$\n\n### 动能 Kinetic Energy\n\n物理学中学过，物体的动能公式为：\n$$\nE=\\frac{1}{2} mv^{2}=\\frac{1}{2} m\\left [ \\frac{\\sqrt{v_{x}^{2}+v_{y}^{2}+v_{z}^{2}}}{\\Delta t} \\right ]^{2}\n$$\n本文中这样书写：\n$$\nE_{k}\\left(s_{t}\\right)=\\frac{1}{2} m v_{x, t}^{2}+\\frac{1}{2} m v_{y, t}^{2}+\\frac{1}{2} m v_{z, t}^{2} \\approx \\frac{m\\left(\\left(x_{t}-x_{t-1}\\right)^{2}+\\left(y_{t}-y_{t-1}\\right)^{2}+\\left(z_{t}-z_{t-1}\\right)^{2}\\right)}{2 \\Delta t^{2}}\n$$\n\n- $v_{x, t} \\approx\\left(x_{t}-x_{t-1}\\right) / \\Delta t$\n- $v_{y, t} \\approx\\left(y_{t}-y_{t-1}\\right) / \\Delta t$\n- $v_{z, t} \\approx\\left(z_{t}-z_{t-1}\\right) / \\Delta t$\n- $\\Delta t$表示相邻两个状态之间的时间间隔，假设我们在模拟器中，1秒60帧，即每帧16.67ms，我们如果每帧执行一次动作，那么$\\Delta t=16.67ms$，如果每60帧执行一次动作，那么$\\Delta t=1s$\n\n### 转动能 Rotational Energy\n\n物理学中学过，物体的转动能公式为：$K=\\frac{1}{2} I \\cdot \\omega^{2}$，注意，中间的点代表点乘，$I$代表物体的惯性矩，$\\omega$代表物体的角速度\n\n本文中这样书写：\n$$\n\\left[ \\begin{array}{c}{\\phi} \\\\ {\\theta} \\\\ {\\psi}\\end{array}\\right]=\\left[ \\begin{array}{c}{\\arctan \\frac{2(a b+c d)}{1-2\\left(b^{2}+c^{2}\\right)}} \\\\ {\\arcsin (2(a c-d b))} \\\\ {\\arcsin \\frac{2(a d+b c)}{1-2\\left(c^{2}+d^{2}\\right)}}\\end{array}\\right]=\\left[ \\begin{array}{c}{\\operatorname{atan} 2\\left(2(a b+c d), 1-2\\left(b^{2}+c^{2}\\right)\\right)} \\\\ {\\operatorname{asin}(2(a c-d b))} \\\\ {\\operatorname{atan} 2\\left(2(a d+b c), 1-2\\left(c^{2}+d^{2}\\right)\\right)}\\end{array}\\right]\n$$\n\n$$\nE_{r}\\left(s_{t}\\right)=\\frac{1}{2} I_{x} \\omega_{x, t}^{2}+\\frac{1}{2} I_{y} \\omega_{y, t}^{2}+\\frac{1}{2} I_{z} \\omega_{z, t}^{2} \\approx \\frac{I_{x}\\left(\\phi_{t}-\\phi_{t-1}\\right)^{2}+I_{y}\\left(\\theta_{t}-\\theta_{t-1}\\right)^{2}+I_{z}\\left(\\psi_{t}-\\psi_{t-1}\\right)^{2}}{2 \\Delta t^{2}}\n$$\n\n其中$a,b,c,d$为旋转四元组，其知识可以百度或google自行了解。\n\n\n$$\nq=a+b \\imath+c \\jmath+d k\n$$\n\n$\\phi, \\theta, \\psi$代表$x,y,z$轴方向的旋转角度\n\n- $\\omega_{x, t} \\approx\\left(\\phi_{t}-\\phi_{t-1}\\right) / \\Delta_{t}$\n- $\\omega_{y, t} \\approx\\left(\\theta_{t}-\\theta_{t-1}\\right) / \\Delta_{t}$\n- $\\omega_{z, t} \\approx\\left(\\psi_{t}-\\psi_{t-1}\\right) / \\Delta_{t}$\n- $\\Delta t$与上文解释相同\n\n**$m,I_{x},I_{y},I_{z}$可以设置为常量，本文实验中设置$m=I_{x}=I_{y}=I_{z}=1$**\n\n\n\n## 迹能量 Trajectory Energy\n\n给定一个回合中所有的经验能量差，迹能量可以表示为这个回合中所有经验能量差之和：\n$$\nE_{t r a j}(\\mathcal{T})=E_{t r a j}\\left(s_{0}, s_{1}, \\ldots, s_{T}\\right)=\\sum_{t=1}^{T} E_{t r a n}\\left(s_{t-1}, s_{t}\\right)\n$$\n\n## 基于能量的优先级\n\n首先计算迹能量，然后对迹能量高的迹（episode）优先进行回放。\n\n根据迹能量计算迹的优先级为：\n$$\np\\left(\\mathcal{T}_{i}\\right)=\\frac{E_{t r a j}\\left(\\mathcal{T}_{i}\\right)}{\\sum_{n=1}^{N} E_{t r a j}\\left(\\mathcal{T}_{n}\\right)}\n$$\n$N$代表经验池中迹的总数量\n\n## 伪代码\n\n![](./energy-based-hindsight-experience-prioritization/pseudo.png)\n\n解析：\n\n- 以本文实验为例，状态$s$由七元组$\\left[x_{t}, y_{t}, z_{t}, a_{t}, b_{t}, c_{t}, d_{t}\\right]$表示，其中前三个代表物体的位置，后三个代表物体旋转的四元组。\n- 目标$g$与状态$s$的表示相同\n- $||$操作符为连结的意思，即`tf.concat(a,b)`\n- 向经验池中存入的不仅仅有$(s,a,r,s')$，还有优先级$p$与迹能量$E_{traj}$，**其实我感觉这样很多余，如果使用sum-tree结构的，存其一即可**\n- 文中所使用的HER是**future模式**\n\n**注意：**\n\n我认为伪代码中有两行很有问题，即\n\n![](./energy-based-hindsight-experience-prioritization/issue.png)\n\n我不明白为什么把原始经验$\\left(s_{t}\\left\\|g, a_{t}, r_{t}, s_{t+1}\\right\\| g, p, E_{t r a j}\\right)$存入经验池之后，需要根据优先级采样一个迹，再从采样到的迹中采样出一个经验$\\left(s_{t}, a_{t}, s_{t+1}\\right)$\n\n起初我是这么认为的，它想对经验池中迹能量高的episode进行大概率抽取，并对其中的经验进行多次扩充，由此对迹能量小的episode更加忽视，突出迹能量高的episode\n\n但是，看到下一行![](./energy-based-hindsight-experience-prioritization/issue2.png)我有一个疑问：如果根据优先级采样出的迹$\\mathcal{T}$与当前所操作的迹$\\mathcal{T}_{current}$不同，那么，为什么还要为不同迹中的经验存入相同的优先级和迹能量呢？即$\\left(s_{t}\\left\\|g^{\\prime}, a_{t}, r_{t}^{\\prime}, s_{t+1}\\right\\| g^{\\prime}, p, E_{t r a j}\\right)$\n\n这样肯定是不行的，那么只有一个答案，采样迹这一步多余的，或者说，不应该出现在这里，而应该放在最后一个循环的开始，即\n\n![](./energy-based-hindsight-experience-prioritization/issue3.png)\n\n也就是说，应该把采样迹，从迹中采样经验的步骤放在minibatch之前，这样就合情合理了。\n\n这是我自己的一个疑问，如果读者有其他见解，欢迎置评讨论。\n\n## EBP的总结\n\nEBP与PER的不同点：\n\n- EBP使用物理学中的能量\n- PER使用TD-error\n\n相比于将HER与PER结合而使用TD-error作为衡量优先级的方法，使用迹能量较少了计算量，因为PER每次回放经验都必须重新计算使用经验的新的TD-error，并存回经验池。（其实，如果使用sum-tree来构建PER，这个劣势其实很小）\n\n文中通过实验发现：比较PER与EBP的时间复杂性，显示EBP提升了算法的性能效果（performance）但是却不增加额外的计算量。PER则提升较少，计算量也增加了。\n\nEBP的优点：\n\n- 可结合任意off-policy算法\n- 结合了物理知识，使其可以应用于现实世界的问题\n- 提升采样效率进两倍\n- 相比最先进的（state-of-the-art）算法，不增加计算时间的情况下，算法效果提升了4个百分点。（此条可以忽略，因为其未必做了充分的实验来进行对比）\n- 适用于任何机器人操作任务\n- 适用于多目标算法\n\n# 实验部分\n\n文中实验结果：[https://youtu.be/jtsF2tTeUGQ](https://youtu.be/jtsF2tTeUGQ)\n\n代码地址：[https://github.com/ruizhaogit/EnergyBasedPrioritization](https://github.com/ruizhaogit/EnergyBasedPrioritization)\n\n实验部分的完整细节请参考论文原文。\n\n## 环境\n\n- OpenAI Gym与MuJoCo物理引擎\n- 一个7自由度的机械手臂，与HER中一样；一个24自由度的机器手\n- 四项任务：pick & place，机器手操作方块、蛋、笔\n\n![](./energy-based-hindsight-experience-prioritization/env.png)\n\n- 使用稀疏奖励，二分奖励，完成容忍度内目标为0，否则为-1\n\n## 算法\n\n- 文中没有说明具体使用什么算法作对比，只有伪代码中提到了DPG、DDPG\n- 文中亦没有对算法中的超参数设置、网络结构进行说明\n- 19个CPU\n- 器械臂场景$E_{t r a n}^{\\max }=0.5$，机械手场景$E_{t r a n}^{\\max }=2.5$\n- 文中主要比较了HER、HER+PER、HER+EBP\n\n## 实验结果\n\n![](./energy-based-hindsight-experience-prioritization/meansuccessrate.png)\n\n- 横坐标是训练的轮数，应该是指episode的意思\n- 纵坐标是5个随机种子实验的平均成功率\n- 蓝色代表HER+EBP，橘色代表HER，绿色代表HER+PER\n\n![](./energy-based-hindsight-experience-prioritization/trainingtime.png)\n\n结果：\n\n- 从上图可以看出，四项任务中，HER+EBP比其他两种方法收敛速度都快，效果也更好一点\n- 从上表可以看出，HER+EBP与HER的训练时间基本相同，而HER+PER要消耗10倍的时间\n\n---\n\n![](./energy-based-hindsight-experience-prioritization/finalmeanrate.png)\n\n结果：\n\n- 训练结束后，HER+EBP在四项任务中效果都最好\n- HER+EBP比HER提高了1-5个百分点，平均提升了3.75个百分点\n\n>We can see that EBP is a simple yet effective method, without increasing computational time, but still, improves current state-of-the-art methods. \n\n![](./energy-based-hindsight-experience-prioritization/sampleefficiency.png)\n\n结果：\n\n- 采样效率方面，总体来看，EBP+HER比HER提升了2倍\n\n---\n\n最后，作者比较了迹能量与TD-error的pearson相关系数\n\n- 系数为1，即正线性相关\n- 系数为-1，即负线性相关\n- 系数为0，即不线性相关\n\n![](./energy-based-hindsight-experience-prioritization/pearsoncorrelation.png)\n\n结果：\n\n- 四个实验中，迹能量与TD-error均成正相关\n- 平均下来pearson系数为0.6，说明迹能量与TD-error呈正线性相关关系，也就是说迹能量可以像TD-error一样表示经验的可学习价值","slug":"energy-based-hindsight-experience-prioritization","published":1,"updated":"2019-05-30T07:20:06.669Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma30002qekveia40bz6c","content":"<p>本文是对HER“事后”经验池机制的一个扩展，它结合了物理学的能量知识以及优先经验回放PER对HER进行提升。简称：EBP</p>\n<p>推荐：</p>\n<ul>\n<li>创新虽不多，但是基于能量的创意可以拓宽在机器人领域训练的视野</li>\n<li>通俗易懂</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1810.01363.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1810.01363.pdf</a></p>\n<p>这篇论文由慕尼黑大学博三学生<a href=\"https://ruizhaogit.github.io\" rel=\"external nofollow\" target=\"_blank\">赵瑞</a>和他的导师Volker Tresp发于2018年的CoRL会议。</p>\n<p><strong>本文提出了一个简单高效的、基于能量的方法去优先回放“事后经验”。Energy+HER+PER</strong></p>\n<p>在HER中，智能体从它可完成的“虚拟”目标中进行大量学习，虚拟目标就是我们使用“事后诸葛亮”方法所调整的经验中的目标。</p>\n<p>本文针对原始HER提出了一个稍有不足的地方：经验回放是完全随机的，即没有优先级，没有考虑哪些episode哪些经验对学习更有价值，其实这个问题与PER相对于传统经验池机制也是一样的。</p>\n<p>本文中使用的功能定理（work-energy principle）来计算能量。</p>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><p>相比于传统的PER优先经验回放使用TD-error作为衡量优先级的度量，本文中使用“迹能量”作为其度量。</p>\n<p>迹能量是这么定义的：</p>\n<ul>\n<li><blockquote>\n<p>We define a trajectory energy function as the sum of the transition energy of the target object over the trajectory. </p>\n</blockquote>\n</li>\n<li><p>迹能量是一个episode中transition energy（不知道怎么翻译合适，过渡能量？经验能量？转换能量？）的总和</p>\n</li>\n</ul>\n<p>接下来介绍一下能量在本文中是如何体现的。</p>\n<h2 id=\"经验能量差-Transition-Energy\"><a href=\"#经验能量差-Transition-Energy\" class=\"headerlink\" title=\"经验能量差 Transition Energy\"></a>经验能量差 Transition Energy</h2><p>我就直接拿论文中实验场景所用到的能力来说明这个能量差。简言之，在本文的实验中主要是操作机械手臂移动物体的水平位置和垂直高度，所以物体的能量基本包含三种：</p>\n<ol>\n<li><strong>势能 Potential Energy</strong> $E_{p}(s_{t})$</li>\n<li><strong>动能 Kinetic Energy</strong> $E_{k}(s_{t})$</li>\n<li><strong>转动能，也叫角动能 Rotational Energy</strong> $E_{r}(s_{t})$</li>\n</ol>\n<p>一个物体的能量由这三部分之和组成：</p>\n<script type=\"math/tex; mode=display\">\nE\\left(s_{t}\\right)=E_{p}\\left(s_{t}\\right)+E_{k}\\left(s_{t}\\right)+E_{r}\\left(s_{t}\\right)</script><p>经验能量差指的就是相邻状态转移之间的能量差值，表示为：</p>\n<script type=\"math/tex; mode=display\">\nE_{t r a n}\\left(s_{t-1}, s_{t}\\right)=\\operatorname{clip}\\left(E\\left(s_{t}\\right)-E\\left(s_{t-1}\\right), 0, E_{t r a n}^{\\max }\\right)</script><p>其中，</p>\n<ul>\n<li><p>将差值clip到0是因为我们只对由机器人做功导致物体的能量增值感兴趣</p>\n</li>\n<li><p>将差值clip到$E_{t r a n}^{\\max }$是想减缓某些特别大的能量差值的影响，使<strong>训练更稳定</strong></p>\n</li>\n</ul>\n<p><em>注：其实我觉得文中加这个clip操作完全是想多使用一个trick，让文章看起来更饱满一点，我个人认为不使用这个clip，或者只对下界进行clip，对算法性能是没有影响的。有待验证。</em></p>\n<h3 id=\"势能-Potential-Energy\"><a href=\"#势能-Potential-Energy\" class=\"headerlink\" title=\"势能 Potential Energy\"></a>势能 Potential Energy</h3><p>物理学中学过，物体的重力势能公式为：$E=mgh$</p>\n<p>本文中这样书写：</p>\n<script type=\"math/tex; mode=display\">\nE_{p}(s_{t})=mgz_{t}</script><ul>\n<li>$m$代表物体的质量</li>\n<li>$g$代表地球的重力系数，$g \\approx 9.81 \\mathrm{m} / \\mathrm{s}^{2}$</li>\n<li>$z_{t}$代表物体在$t$时刻的高度$h$</li>\n</ul>\n<h3 id=\"动能-Kinetic-Energy\"><a href=\"#动能-Kinetic-Energy\" class=\"headerlink\" title=\"动能 Kinetic Energy\"></a>动能 Kinetic Energy</h3><p>物理学中学过，物体的动能公式为：</p>\n<script type=\"math/tex; mode=display\">\nE=\\frac{1}{2} mv^{2}=\\frac{1}{2} m\\left [ \\frac{\\sqrt{v_{x}^{2}+v_{y}^{2}+v_{z}^{2}}}{\\Delta t} \\right ]^{2}</script><p>本文中这样书写：</p>\n<script type=\"math/tex; mode=display\">\nE_{k}\\left(s_{t}\\right)=\\frac{1}{2} m v_{x, t}^{2}+\\frac{1}{2} m v_{y, t}^{2}+\\frac{1}{2} m v_{z, t}^{2} \\approx \\frac{m\\left(\\left(x_{t}-x_{t-1}\\right)^{2}+\\left(y_{t}-y_{t-1}\\right)^{2}+\\left(z_{t}-z_{t-1}\\right)^{2}\\right)}{2 \\Delta t^{2}}</script><ul>\n<li>$v_{x, t} \\approx\\left(x_{t}-x_{t-1}\\right) / \\Delta t$</li>\n<li>$v_{y, t} \\approx\\left(y_{t}-y_{t-1}\\right) / \\Delta t$</li>\n<li>$v_{z, t} \\approx\\left(z_{t}-z_{t-1}\\right) / \\Delta t$</li>\n<li>$\\Delta t$表示相邻两个状态之间的时间间隔，假设我们在模拟器中，1秒60帧，即每帧16.67ms，我们如果每帧执行一次动作，那么$\\Delta t=16.67ms$，如果每60帧执行一次动作，那么$\\Delta t=1s$</li>\n</ul>\n<h3 id=\"转动能-Rotational-Energy\"><a href=\"#转动能-Rotational-Energy\" class=\"headerlink\" title=\"转动能 Rotational Energy\"></a>转动能 Rotational Energy</h3><p>物理学中学过，物体的转动能公式为：$K=\\frac{1}{2} I \\cdot \\omega^{2}$，注意，中间的点代表点乘，$I$代表物体的惯性矩，$\\omega$代表物体的角速度</p>\n<p>本文中这样书写：</p>\n<script type=\"math/tex; mode=display\">\n\\left[ \\begin{array}{c}{\\phi} \\\\ {\\theta} \\\\ {\\psi}\\end{array}\\right]=\\left[ \\begin{array}{c}{\\arctan \\frac{2(a b+c d)}{1-2\\left(b^{2}+c^{2}\\right)}} \\\\ {\\arcsin (2(a c-d b))} \\\\ {\\arcsin \\frac{2(a d+b c)}{1-2\\left(c^{2}+d^{2}\\right)}}\\end{array}\\right]=\\left[ \\begin{array}{c}{\\operatorname{atan} 2\\left(2(a b+c d), 1-2\\left(b^{2}+c^{2}\\right)\\right)} \\\\ {\\operatorname{asin}(2(a c-d b))} \\\\ {\\operatorname{atan} 2\\left(2(a d+b c), 1-2\\left(c^{2}+d^{2}\\right)\\right)}\\end{array}\\right]</script><script type=\"math/tex; mode=display\">\nE_{r}\\left(s_{t}\\right)=\\frac{1}{2} I_{x} \\omega_{x, t}^{2}+\\frac{1}{2} I_{y} \\omega_{y, t}^{2}+\\frac{1}{2} I_{z} \\omega_{z, t}^{2} \\approx \\frac{I_{x}\\left(\\phi_{t}-\\phi_{t-1}\\right)^{2}+I_{y}\\left(\\theta_{t}-\\theta_{t-1}\\right)^{2}+I_{z}\\left(\\psi_{t}-\\psi_{t-1}\\right)^{2}}{2 \\Delta t^{2}}</script><p>其中$a,b,c,d$为旋转四元组，其知识可以百度或google自行了解。</p>\n<script type=\"math/tex; mode=display\">\nq=a+b \\imath+c \\jmath+d k</script><p>$\\phi, \\theta, \\psi$代表$x,y,z$轴方向的旋转角度</p>\n<ul>\n<li>$\\omega_{x, t} \\approx\\left(\\phi_{t}-\\phi_{t-1}\\right) / \\Delta_{t}$</li>\n<li>$\\omega_{y, t} \\approx\\left(\\theta_{t}-\\theta_{t-1}\\right) / \\Delta_{t}$</li>\n<li>$\\omega_{z, t} \\approx\\left(\\psi_{t}-\\psi_{t-1}\\right) / \\Delta_{t}$</li>\n<li>$\\Delta t$与上文解释相同</li>\n</ul>\n<p><strong>$m,I_{x},I_{y},I_{z}$可以设置为常量，本文实验中设置$m=I_{x}=I_{y}=I_{z}=1$</strong></p>\n<h2 id=\"迹能量-Trajectory-Energy\"><a href=\"#迹能量-Trajectory-Energy\" class=\"headerlink\" title=\"迹能量 Trajectory Energy\"></a>迹能量 Trajectory Energy</h2><p>给定一个回合中所有的经验能量差，迹能量可以表示为这个回合中所有经验能量差之和：</p>\n<script type=\"math/tex; mode=display\">\nE_{t r a j}(\\mathcal{T})=E_{t r a j}\\left(s_{0}, s_{1}, \\ldots, s_{T}\\right)=\\sum_{t=1}^{T} E_{t r a n}\\left(s_{t-1}, s_{t}\\right)</script><h2 id=\"基于能量的优先级\"><a href=\"#基于能量的优先级\" class=\"headerlink\" title=\"基于能量的优先级\"></a>基于能量的优先级</h2><p>首先计算迹能量，然后对迹能量高的迹（episode）优先进行回放。</p>\n<p>根据迹能量计算迹的优先级为：</p>\n<script type=\"math/tex; mode=display\">\np\\left(\\mathcal{T}_{i}\\right)=\\frac{E_{t r a j}\\left(\\mathcal{T}_{i}\\right)}{\\sum_{n=1}^{N} E_{t r a j}\\left(\\mathcal{T}_{n}\\right)}</script><p>$N$代表经验池中迹的总数量</p>\n<h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./energy-based-hindsight-experience-prioritization/pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>以本文实验为例，状态$s$由七元组$\\left[x_{t}, y_{t}, z_{t}, a_{t}, b_{t}, c_{t}, d_{t}\\right]$表示，其中前三个代表物体的位置，后三个代表物体旋转的四元组。</li>\n<li>目标$g$与状态$s$的表示相同</li>\n<li>$||$操作符为连结的意思，即<code>tf.concat(a,b)</code></li>\n<li>向经验池中存入的不仅仅有$(s,a,r,s’)$，还有优先级$p$与迹能量$E_{traj}$，<strong>其实我感觉这样很多余，如果使用sum-tree结构的，存其一即可</strong></li>\n<li>文中所使用的HER是<strong>future模式</strong></li>\n</ul>\n<p><strong>注意：</strong></p>\n<p>我认为伪代码中有两行很有问题，即</p>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/issue.png\" alt=\"\"></p>\n<p>我不明白为什么把原始经验$\\left(s_{t}\\left|g, a_{t}, r_{t}, s_{t+1}\\right| g, p, E_{t r a j}\\right)$存入经验池之后，需要根据优先级采样一个迹，再从采样到的迹中采样出一个经验$\\left(s_{t}, a_{t}, s_{t+1}\\right)$</p>\n<p>起初我是这么认为的，它想对经验池中迹能量高的episode进行大概率抽取，并对其中的经验进行多次扩充，由此对迹能量小的episode更加忽视，突出迹能量高的episode</p>\n<p>但是，看到下一行<img src=\"./energy-based-hindsight-experience-prioritization/issue2.png\" alt=\"\">我有一个疑问：如果根据优先级采样出的迹$\\mathcal{T}$与当前所操作的迹$\\mathcal{T}_{current}$不同，那么，为什么还要为不同迹中的经验存入相同的优先级和迹能量呢？即$\\left(s_{t}\\left|g^{\\prime}, a_{t}, r_{t}^{\\prime}, s_{t+1}\\right| g^{\\prime}, p, E_{t r a j}\\right)$</p>\n<p>这样肯定是不行的，那么只有一个答案，采样迹这一步多余的，或者说，不应该出现在这里，而应该放在最后一个循环的开始，即</p>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/issue3.png\" alt=\"\"></p>\n<p>也就是说，应该把采样迹，从迹中采样经验的步骤放在minibatch之前，这样就合情合理了。</p>\n<p>这是我自己的一个疑问，如果读者有其他见解，欢迎置评讨论。</p>\n<h2 id=\"EBP的总结\"><a href=\"#EBP的总结\" class=\"headerlink\" title=\"EBP的总结\"></a>EBP的总结</h2><p>EBP与PER的不同点：</p>\n<ul>\n<li>EBP使用物理学中的能量</li>\n<li>PER使用TD-error</li>\n</ul>\n<p>相比于将HER与PER结合而使用TD-error作为衡量优先级的方法，使用迹能量较少了计算量，因为PER每次回放经验都必须重新计算使用经验的新的TD-error，并存回经验池。（其实，如果使用sum-tree来构建PER，这个劣势其实很小）</p>\n<p>文中通过实验发现：比较PER与EBP的时间复杂性，显示EBP提升了算法的性能效果（performance）但是却不增加额外的计算量。PER则提升较少，计算量也增加了。</p>\n<p>EBP的优点：</p>\n<ul>\n<li>可结合任意off-policy算法</li>\n<li>结合了物理知识，使其可以应用于现实世界的问题</li>\n<li>提升采样效率进两倍</li>\n<li>相比最先进的（state-of-the-art）算法，不增加计算时间的情况下，算法效果提升了4个百分点。（此条可以忽略，因为其未必做了充分的实验来进行对比）</li>\n<li>适用于任何机器人操作任务</li>\n<li>适用于多目标算法</li>\n</ul>\n<h1 id=\"实验部分\"><a href=\"#实验部分\" class=\"headerlink\" title=\"实验部分\"></a>实验部分</h1><p>文中实验结果：<a href=\"https://youtu.be/jtsF2tTeUGQ\" rel=\"external nofollow\" target=\"_blank\">https://youtu.be/jtsF2tTeUGQ</a></p>\n<p>代码地址：<a href=\"https://github.com/ruizhaogit/EnergyBasedPrioritization\" rel=\"external nofollow\" target=\"_blank\">https://github.com/ruizhaogit/EnergyBasedPrioritization</a></p>\n<p>实验部分的完整细节请参考论文原文。</p>\n<h2 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h2><ul>\n<li>OpenAI Gym与MuJoCo物理引擎</li>\n<li>一个7自由度的机械手臂，与HER中一样；一个24自由度的机器手</li>\n<li>四项任务：pick &amp; place，机器手操作方块、蛋、笔</li>\n</ul>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/env.png\" alt=\"\"></p>\n<ul>\n<li>使用稀疏奖励，二分奖励，完成容忍度内目标为0，否则为-1</li>\n</ul>\n<h2 id=\"算法\"><a href=\"#算法\" class=\"headerlink\" title=\"算法\"></a>算法</h2><ul>\n<li>文中没有说明具体使用什么算法作对比，只有伪代码中提到了DPG、DDPG</li>\n<li>文中亦没有对算法中的超参数设置、网络结构进行说明</li>\n<li>19个CPU</li>\n<li>器械臂场景$E_{t r a n}^{\\max }=0.5$，机械手场景$E_{t r a n}^{\\max }=2.5$</li>\n<li>文中主要比较了HER、HER+PER、HER+EBP</li>\n</ul>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><p><img src=\"./energy-based-hindsight-experience-prioritization/meansuccessrate.png\" alt=\"\"></p>\n<ul>\n<li>横坐标是训练的轮数，应该是指episode的意思</li>\n<li>纵坐标是5个随机种子实验的平均成功率</li>\n<li>蓝色代表HER+EBP，橘色代表HER，绿色代表HER+PER</li>\n</ul>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/trainingtime.png\" alt=\"\"></p>\n<p>结果：</p>\n<ul>\n<li>从上图可以看出，四项任务中，HER+EBP比其他两种方法收敛速度都快，效果也更好一点</li>\n<li>从上表可以看出，HER+EBP与HER的训练时间基本相同，而HER+PER要消耗10倍的时间</li>\n</ul>\n<hr>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/finalmeanrate.png\" alt=\"\"></p>\n<p>结果：</p>\n<ul>\n<li>训练结束后，HER+EBP在四项任务中效果都最好</li>\n<li>HER+EBP比HER提高了1-5个百分点，平均提升了3.75个百分点</li>\n</ul>\n<blockquote>\n<p>We can see that EBP is a simple yet effective method, without increasing computational time, but still, improves current state-of-the-art methods. </p>\n</blockquote>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/sampleefficiency.png\" alt=\"\"></p>\n<p>结果：</p>\n<ul>\n<li>采样效率方面，总体来看，EBP+HER比HER提升了2倍</li>\n</ul>\n<hr>\n<p>最后，作者比较了迹能量与TD-error的pearson相关系数</p>\n<ul>\n<li>系数为1，即正线性相关</li>\n<li>系数为-1，即负线性相关</li>\n<li>系数为0，即不线性相关</li>\n</ul>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/pearsoncorrelation.png\" alt=\"\"></p>\n<p>结果：</p>\n<ul>\n<li>四个实验中，迹能量与TD-error均成正相关</li>\n<li>平均下来pearson系数为0.6，说明迹能量与TD-error呈正线性相关关系，也就是说迹能量可以像TD-error一样表示经验的可学习价值</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>本文是对HER“事后”经验池机制的一个扩展，它结合了物理学的能量知识以及优先经验回放PER对HER进行提升。简称：EBP</p>\n<p>推荐：</p>\n<ul>\n<li>创新虽不多，但是基于能量的创意可以拓宽在机器人领域训练的视野</li>\n<li>通俗易懂</li>\n</ul>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1810.01363.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1810.01363.pdf</a></p>\n<p>这篇论文由慕尼黑大学博三学生<a href=\"https://ruizhaogit.github.io\" rel=\"external nofollow\" target=\"_blank\">赵瑞</a>和他的导师Volker Tresp发于2018年的CoRL会议。</p>\n<p><strong>本文提出了一个简单高效的、基于能量的方法去优先回放“事后经验”。Energy+HER+PER</strong></p>\n<p>在HER中，智能体从它可完成的“虚拟”目标中进行大量学习，虚拟目标就是我们使用“事后诸葛亮”方法所调整的经验中的目标。</p>\n<p>本文针对原始HER提出了一个稍有不足的地方：经验回放是完全随机的，即没有优先级，没有考虑哪些episode哪些经验对学习更有价值，其实这个问题与PER相对于传统经验池机制也是一样的。</p>\n<p>本文中使用的功能定理（work-energy principle）来计算能量。</p>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><p>相比于传统的PER优先经验回放使用TD-error作为衡量优先级的度量，本文中使用“迹能量”作为其度量。</p>\n<p>迹能量是这么定义的：</p>\n<ul>\n<li><blockquote>\n<p>We define a trajectory energy function as the sum of the transition energy of the target object over the trajectory. </p>\n</blockquote>\n</li>\n<li><p>迹能量是一个episode中transition energy（不知道怎么翻译合适，过渡能量？经验能量？转换能量？）的总和</p>\n</li>\n</ul>\n<p>接下来介绍一下能量在本文中是如何体现的。</p>\n<h2 id=\"经验能量差-Transition-Energy\"><a href=\"#经验能量差-Transition-Energy\" class=\"headerlink\" title=\"经验能量差 Transition Energy\"></a>经验能量差 Transition Energy</h2><p>我就直接拿论文中实验场景所用到的能力来说明这个能量差。简言之，在本文的实验中主要是操作机械手臂移动物体的水平位置和垂直高度，所以物体的能量基本包含三种：</p>\n<ol>\n<li><strong>势能 Potential Energy</strong> $E_{p}(s_{t})$</li>\n<li><strong>动能 Kinetic Energy</strong> $E_{k}(s_{t})$</li>\n<li><strong>转动能，也叫角动能 Rotational Energy</strong> $E_{r}(s_{t})$</li>\n</ol>\n<p>一个物体的能量由这三部分之和组成：</p>\n<script type=\"math/tex; mode=display\">\nE\\left(s_{t}\\right)=E_{p}\\left(s_{t}\\right)+E_{k}\\left(s_{t}\\right)+E_{r}\\left(s_{t}\\right)</script><p>经验能量差指的就是相邻状态转移之间的能量差值，表示为：</p>\n<script type=\"math/tex; mode=display\">\nE_{t r a n}\\left(s_{t-1}, s_{t}\\right)=\\operatorname{clip}\\left(E\\left(s_{t}\\right)-E\\left(s_{t-1}\\right), 0, E_{t r a n}^{\\max }\\right)</script><p>其中，</p>\n<ul>\n<li><p>将差值clip到0是因为我们只对由机器人做功导致物体的能量增值感兴趣</p>\n</li>\n<li><p>将差值clip到$E_{t r a n}^{\\max }$是想减缓某些特别大的能量差值的影响，使<strong>训练更稳定</strong></p>\n</li>\n</ul>\n<p><em>注：其实我觉得文中加这个clip操作完全是想多使用一个trick，让文章看起来更饱满一点，我个人认为不使用这个clip，或者只对下界进行clip，对算法性能是没有影响的。有待验证。</em></p>\n<h3 id=\"势能-Potential-Energy\"><a href=\"#势能-Potential-Energy\" class=\"headerlink\" title=\"势能 Potential Energy\"></a>势能 Potential Energy</h3><p>物理学中学过，物体的重力势能公式为：$E=mgh$</p>\n<p>本文中这样书写：</p>\n<script type=\"math/tex; mode=display\">\nE_{p}(s_{t})=mgz_{t}</script><ul>\n<li>$m$代表物体的质量</li>\n<li>$g$代表地球的重力系数，$g \\approx 9.81 \\mathrm{m} / \\mathrm{s}^{2}$</li>\n<li>$z_{t}$代表物体在$t$时刻的高度$h$</li>\n</ul>\n<h3 id=\"动能-Kinetic-Energy\"><a href=\"#动能-Kinetic-Energy\" class=\"headerlink\" title=\"动能 Kinetic Energy\"></a>动能 Kinetic Energy</h3><p>物理学中学过，物体的动能公式为：</p>\n<script type=\"math/tex; mode=display\">\nE=\\frac{1}{2} mv^{2}=\\frac{1}{2} m\\left [ \\frac{\\sqrt{v_{x}^{2}+v_{y}^{2}+v_{z}^{2}}}{\\Delta t} \\right ]^{2}</script><p>本文中这样书写：</p>\n<script type=\"math/tex; mode=display\">\nE_{k}\\left(s_{t}\\right)=\\frac{1}{2} m v_{x, t}^{2}+\\frac{1}{2} m v_{y, t}^{2}+\\frac{1}{2} m v_{z, t}^{2} \\approx \\frac{m\\left(\\left(x_{t}-x_{t-1}\\right)^{2}+\\left(y_{t}-y_{t-1}\\right)^{2}+\\left(z_{t}-z_{t-1}\\right)^{2}\\right)}{2 \\Delta t^{2}}</script><ul>\n<li>$v_{x, t} \\approx\\left(x_{t}-x_{t-1}\\right) / \\Delta t$</li>\n<li>$v_{y, t} \\approx\\left(y_{t}-y_{t-1}\\right) / \\Delta t$</li>\n<li>$v_{z, t} \\approx\\left(z_{t}-z_{t-1}\\right) / \\Delta t$</li>\n<li>$\\Delta t$表示相邻两个状态之间的时间间隔，假设我们在模拟器中，1秒60帧，即每帧16.67ms，我们如果每帧执行一次动作，那么$\\Delta t=16.67ms$，如果每60帧执行一次动作，那么$\\Delta t=1s$</li>\n</ul>\n<h3 id=\"转动能-Rotational-Energy\"><a href=\"#转动能-Rotational-Energy\" class=\"headerlink\" title=\"转动能 Rotational Energy\"></a>转动能 Rotational Energy</h3><p>物理学中学过，物体的转动能公式为：$K=\\frac{1}{2} I \\cdot \\omega^{2}$，注意，中间的点代表点乘，$I$代表物体的惯性矩，$\\omega$代表物体的角速度</p>\n<p>本文中这样书写：</p>\n<script type=\"math/tex; mode=display\">\n\\left[ \\begin{array}{c}{\\phi} \\\\ {\\theta} \\\\ {\\psi}\\end{array}\\right]=\\left[ \\begin{array}{c}{\\arctan \\frac{2(a b+c d)}{1-2\\left(b^{2}+c^{2}\\right)}} \\\\ {\\arcsin (2(a c-d b))} \\\\ {\\arcsin \\frac{2(a d+b c)}{1-2\\left(c^{2}+d^{2}\\right)}}\\end{array}\\right]=\\left[ \\begin{array}{c}{\\operatorname{atan} 2\\left(2(a b+c d), 1-2\\left(b^{2}+c^{2}\\right)\\right)} \\\\ {\\operatorname{asin}(2(a c-d b))} \\\\ {\\operatorname{atan} 2\\left(2(a d+b c), 1-2\\left(c^{2}+d^{2}\\right)\\right)}\\end{array}\\right]</script><script type=\"math/tex; mode=display\">\nE_{r}\\left(s_{t}\\right)=\\frac{1}{2} I_{x} \\omega_{x, t}^{2}+\\frac{1}{2} I_{y} \\omega_{y, t}^{2}+\\frac{1}{2} I_{z} \\omega_{z, t}^{2} \\approx \\frac{I_{x}\\left(\\phi_{t}-\\phi_{t-1}\\right)^{2}+I_{y}\\left(\\theta_{t}-\\theta_{t-1}\\right)^{2}+I_{z}\\left(\\psi_{t}-\\psi_{t-1}\\right)^{2}}{2 \\Delta t^{2}}</script><p>其中$a,b,c,d$为旋转四元组，其知识可以百度或google自行了解。</p>\n<script type=\"math/tex; mode=display\">\nq=a+b \\imath+c \\jmath+d k</script><p>$\\phi, \\theta, \\psi$代表$x,y,z$轴方向的旋转角度</p>\n<ul>\n<li>$\\omega_{x, t} \\approx\\left(\\phi_{t}-\\phi_{t-1}\\right) / \\Delta_{t}$</li>\n<li>$\\omega_{y, t} \\approx\\left(\\theta_{t}-\\theta_{t-1}\\right) / \\Delta_{t}$</li>\n<li>$\\omega_{z, t} \\approx\\left(\\psi_{t}-\\psi_{t-1}\\right) / \\Delta_{t}$</li>\n<li>$\\Delta t$与上文解释相同</li>\n</ul>\n<p><strong>$m,I_{x},I_{y},I_{z}$可以设置为常量，本文实验中设置$m=I_{x}=I_{y}=I_{z}=1$</strong></p>\n<h2 id=\"迹能量-Trajectory-Energy\"><a href=\"#迹能量-Trajectory-Energy\" class=\"headerlink\" title=\"迹能量 Trajectory Energy\"></a>迹能量 Trajectory Energy</h2><p>给定一个回合中所有的经验能量差，迹能量可以表示为这个回合中所有经验能量差之和：</p>\n<script type=\"math/tex; mode=display\">\nE_{t r a j}(\\mathcal{T})=E_{t r a j}\\left(s_{0}, s_{1}, \\ldots, s_{T}\\right)=\\sum_{t=1}^{T} E_{t r a n}\\left(s_{t-1}, s_{t}\\right)</script><h2 id=\"基于能量的优先级\"><a href=\"#基于能量的优先级\" class=\"headerlink\" title=\"基于能量的优先级\"></a>基于能量的优先级</h2><p>首先计算迹能量，然后对迹能量高的迹（episode）优先进行回放。</p>\n<p>根据迹能量计算迹的优先级为：</p>\n<script type=\"math/tex; mode=display\">\np\\left(\\mathcal{T}_{i}\\right)=\\frac{E_{t r a j}\\left(\\mathcal{T}_{i}\\right)}{\\sum_{n=1}^{N} E_{t r a j}\\left(\\mathcal{T}_{n}\\right)}</script><p>$N$代表经验池中迹的总数量</p>\n<h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./energy-based-hindsight-experience-prioritization/pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>以本文实验为例，状态$s$由七元组$\\left[x_{t}, y_{t}, z_{t}, a_{t}, b_{t}, c_{t}, d_{t}\\right]$表示，其中前三个代表物体的位置，后三个代表物体旋转的四元组。</li>\n<li>目标$g$与状态$s$的表示相同</li>\n<li>$||$操作符为连结的意思，即<code>tf.concat(a,b)</code></li>\n<li>向经验池中存入的不仅仅有$(s,a,r,s’)$，还有优先级$p$与迹能量$E_{traj}$，<strong>其实我感觉这样很多余，如果使用sum-tree结构的，存其一即可</strong></li>\n<li>文中所使用的HER是<strong>future模式</strong></li>\n</ul>\n<p><strong>注意：</strong></p>\n<p>我认为伪代码中有两行很有问题，即</p>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/issue.png\" alt=\"\"></p>\n<p>我不明白为什么把原始经验$\\left(s_{t}\\left|g, a_{t}, r_{t}, s_{t+1}\\right| g, p, E_{t r a j}\\right)$存入经验池之后，需要根据优先级采样一个迹，再从采样到的迹中采样出一个经验$\\left(s_{t}, a_{t}, s_{t+1}\\right)$</p>\n<p>起初我是这么认为的，它想对经验池中迹能量高的episode进行大概率抽取，并对其中的经验进行多次扩充，由此对迹能量小的episode更加忽视，突出迹能量高的episode</p>\n<p>但是，看到下一行<img src=\"./energy-based-hindsight-experience-prioritization/issue2.png\" alt=\"\">我有一个疑问：如果根据优先级采样出的迹$\\mathcal{T}$与当前所操作的迹$\\mathcal{T}_{current}$不同，那么，为什么还要为不同迹中的经验存入相同的优先级和迹能量呢？即$\\left(s_{t}\\left|g^{\\prime}, a_{t}, r_{t}^{\\prime}, s_{t+1}\\right| g^{\\prime}, p, E_{t r a j}\\right)$</p>\n<p>这样肯定是不行的，那么只有一个答案，采样迹这一步多余的，或者说，不应该出现在这里，而应该放在最后一个循环的开始，即</p>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/issue3.png\" alt=\"\"></p>\n<p>也就是说，应该把采样迹，从迹中采样经验的步骤放在minibatch之前，这样就合情合理了。</p>\n<p>这是我自己的一个疑问，如果读者有其他见解，欢迎置评讨论。</p>\n<h2 id=\"EBP的总结\"><a href=\"#EBP的总结\" class=\"headerlink\" title=\"EBP的总结\"></a>EBP的总结</h2><p>EBP与PER的不同点：</p>\n<ul>\n<li>EBP使用物理学中的能量</li>\n<li>PER使用TD-error</li>\n</ul>\n<p>相比于将HER与PER结合而使用TD-error作为衡量优先级的方法，使用迹能量较少了计算量，因为PER每次回放经验都必须重新计算使用经验的新的TD-error，并存回经验池。（其实，如果使用sum-tree来构建PER，这个劣势其实很小）</p>\n<p>文中通过实验发现：比较PER与EBP的时间复杂性，显示EBP提升了算法的性能效果（performance）但是却不增加额外的计算量。PER则提升较少，计算量也增加了。</p>\n<p>EBP的优点：</p>\n<ul>\n<li>可结合任意off-policy算法</li>\n<li>结合了物理知识，使其可以应用于现实世界的问题</li>\n<li>提升采样效率进两倍</li>\n<li>相比最先进的（state-of-the-art）算法，不增加计算时间的情况下，算法效果提升了4个百分点。（此条可以忽略，因为其未必做了充分的实验来进行对比）</li>\n<li>适用于任何机器人操作任务</li>\n<li>适用于多目标算法</li>\n</ul>\n<h1 id=\"实验部分\"><a href=\"#实验部分\" class=\"headerlink\" title=\"实验部分\"></a>实验部分</h1><p>文中实验结果：<a href=\"https://youtu.be/jtsF2tTeUGQ\" rel=\"external nofollow\" target=\"_blank\">https://youtu.be/jtsF2tTeUGQ</a></p>\n<p>代码地址：<a href=\"https://github.com/ruizhaogit/EnergyBasedPrioritization\" rel=\"external nofollow\" target=\"_blank\">https://github.com/ruizhaogit/EnergyBasedPrioritization</a></p>\n<p>实验部分的完整细节请参考论文原文。</p>\n<h2 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h2><ul>\n<li>OpenAI Gym与MuJoCo物理引擎</li>\n<li>一个7自由度的机械手臂，与HER中一样；一个24自由度的机器手</li>\n<li>四项任务：pick &amp; place，机器手操作方块、蛋、笔</li>\n</ul>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/env.png\" alt=\"\"></p>\n<ul>\n<li>使用稀疏奖励，二分奖励，完成容忍度内目标为0，否则为-1</li>\n</ul>\n<h2 id=\"算法\"><a href=\"#算法\" class=\"headerlink\" title=\"算法\"></a>算法</h2><ul>\n<li>文中没有说明具体使用什么算法作对比，只有伪代码中提到了DPG、DDPG</li>\n<li>文中亦没有对算法中的超参数设置、网络结构进行说明</li>\n<li>19个CPU</li>\n<li>器械臂场景$E_{t r a n}^{\\max }=0.5$，机械手场景$E_{t r a n}^{\\max }=2.5$</li>\n<li>文中主要比较了HER、HER+PER、HER+EBP</li>\n</ul>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><p><img src=\"./energy-based-hindsight-experience-prioritization/meansuccessrate.png\" alt=\"\"></p>\n<ul>\n<li>横坐标是训练的轮数，应该是指episode的意思</li>\n<li>纵坐标是5个随机种子实验的平均成功率</li>\n<li>蓝色代表HER+EBP，橘色代表HER，绿色代表HER+PER</li>\n</ul>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/trainingtime.png\" alt=\"\"></p>\n<p>结果：</p>\n<ul>\n<li>从上图可以看出，四项任务中，HER+EBP比其他两种方法收敛速度都快，效果也更好一点</li>\n<li>从上表可以看出，HER+EBP与HER的训练时间基本相同，而HER+PER要消耗10倍的时间</li>\n</ul>\n<hr>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/finalmeanrate.png\" alt=\"\"></p>\n<p>结果：</p>\n<ul>\n<li>训练结束后，HER+EBP在四项任务中效果都最好</li>\n<li>HER+EBP比HER提高了1-5个百分点，平均提升了3.75个百分点</li>\n</ul>\n<blockquote>\n<p>We can see that EBP is a simple yet effective method, without increasing computational time, but still, improves current state-of-the-art methods. </p>\n</blockquote>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/sampleefficiency.png\" alt=\"\"></p>\n<p>结果：</p>\n<ul>\n<li>采样效率方面，总体来看，EBP+HER比HER提升了2倍</li>\n</ul>\n<hr>\n<p>最后，作者比较了迹能量与TD-error的pearson相关系数</p>\n<ul>\n<li>系数为1，即正线性相关</li>\n<li>系数为-1，即负线性相关</li>\n<li>系数为0，即不线性相关</li>\n</ul>\n<p><img src=\"./energy-based-hindsight-experience-prioritization/pearsoncorrelation.png\" alt=\"\"></p>\n<p>结果：</p>\n<ul>\n<li>四个实验中，迹能量与TD-error均成正相关</li>\n<li>平均下来pearson系数为0.6，说明迹能量与TD-error呈正线性相关关系，也就是说迹能量可以像TD-error一样表示经验的可学习价值</li>\n</ul>"},{"title":"Prioritized Experience Replay","copyright":true,"mathjax":true,"top":1,"date":"2019-05-22T00:54:44.000Z","keywords":null,"description":null,"_content":"\n这篇论文介绍了优先经验回放机制，它可以使学习过程更高效。\n\n推荐：\n\n- 实用技巧\n- 通俗易懂\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf)\n\n之前重用经验（experience，transition转换五元组$\\lt s,a,r,s',done \\ or \\ \\gamma\\gt$都是从经验池中**均匀采样**，忽略了经验的重要程度，文中提到的优先经验回放框架按经验重要性增大其被采样到的概率，希望越重要的经验使用次数越多，从而增加学习效率。\n\n文中应用算法：DQN\n\n效果：相比传统经验池机制，Atari 49游戏中41胜，8负\n\n# 文中精要\n\n> Online reinforcement learning (RL) agents incrementally update their parameters (of the policy, value function or model) while they observe a stream of experience. In their simplest form, they discard incoming data immediately, after a single update. Two issues with this are (a) strongly correlated updates that break the i.i.d. assumption of many popular stochastic gradient-based algorithms, and (b) the rapid forgetting of possibly rare experiences that would be useful later on. \n>Experience replay  addresses both of these issues: with experience stored in a replay memory, it becomes possible to break the temporal correlations by mixing more and less recent experience for the updates, and rare experience will be used for more than just a single update.\n\n指出On-policy一般使用一个episode的数据进行参数更新，且数据用完即丢，这样做有两个缺点：\n\n- 数据（状态）相互关联，数据不具有独立同分布($i.i.d$)的性质，但许多流行的随机梯度算法往往有关于数据独立同分布的假设\n- 对罕见的（稀疏的）经验快速遗忘，忽略了这些罕见经验可能多次更新更有用的作用\n\n**经验池机制解决了上述两个问题**，通过混合近期经验打破它们关于时间的关联性，并从经验池中采样经验学习多次。\n\n经验池机制的优势：\n\n- 稳定了DQN值函数的训练\n- 一般情况下，经验回放可以减少训练所需的经验数量，但是需要加大计算量，消耗更多的内存，但是这往往比智能体与环境进行交互来得更方便、容易\n\n>In particular, we propose to more frequently replay transitions with high expected learning progress, s measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead o a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which e correct with importance sampling. \n\n文中根据TD-error设置优先经验回放的频率，这可能会引起两个问题：\n\n- 丢失样本多样性\n- 引入偏差\n\n分别解决方案：\n\n- 随机优先级 stochastic prioritization\n- 重要性采样\n\n>The central component of prioritized replay is the criterion by which the importance of each transition is measured. One idealised criterion would be the amount the RL agent can learn from a transition in its current state (expected learning progress). While this measure is not directly accessible,  reasonable proxy is the magnitude of a transition’s TD error $\\delta$ indicates how ‘surprising’  or unexpected the transition is: specifically, how far the value is from its next-step bootstrap estimate.\n\n优先经验回放的核心部分是如何衡量样本的重要性，并根据其重要性进行回放。\n\n最直观的衡量标准是从经验样本中可以学习的量，但是这个量不可知、不可得，于是使用TD-error $\\delta$作为这个量的替代品以衡量样本重要性。 \n\n>New transitions arrive without a known TD-error, so we put them at maximal priority in order to guarantee that all experience is seen at least once.  \n\n新的经验被存入经验池时不需计算TD-error，直接将其设置为当前经验池中最大的TD-error，保证其至少被抽中一次。\n\n---\n\n既然使用TD-error作为衡量可学习的度量，那么完全可以用贪婪的方式，选取TD-error最大的几个进行学习，但这会有几个问题：\n\n1. 由于只有在经验被重放**之后**，这个经验的TD-error才被更新，导致初始TD-error比较小的经验长时间不被使用，甚至永远不被使用。\n2. 贪婪策略聚焦于一小部分TD-error比较高的经验，当使用值函数近似时，这些经验的TD-error减小速度很慢，导致这些经验被高频重复使用，致使样本缺乏多样性而过拟合。\n\n文中提到使用**随机采样方法 stochastic sampling method**在贪婪策略与均匀采样之间“差值”来解决上述问题，其实它是一个在样本使用上的trade-off，由超参数$\\alpha$控制\n\n使用优先经验回放还有一个问题是改变了状态的分布，我们知道DQN中引入经验池是为了解决数据相关性，使数据（尽量）独立同分布的问题。但是使用优先经验回放又改变了状态的分布，这样势必会引入偏差bias，对此，文中使用**偏差退火——重要性采样结合退火因子**，来消除引入的偏差。\n\n## 随机采样方法\n\n$$\nP(i)=\\frac{p^{\\alpha}_{i}}{\\sum_{k}p^{\\alpha}_{k}}\n$$\n\n$\\alpha$超参数控制采样在uniform和greedy的偏好，是一个trade-off因子：\n\n- $\\alpha=0$，均匀采样\n- $\\alpha=1$，贪婪策略采样\n- $\\alpha \\in [0,1]$，文中没有明说$\\alpha$的取值范围\n- 引入$\\alpha$不改变优先级的单调性，只是适当调整高、低TD-error经验的优先级\n\n根据优先级$p_{i}$的设定可以将优先经验池的设计分为两种：\n\n- 直接的，基于比例的，proportional prioritization\n- 间接的，基于排名的，rank-based prioritization\n\n### Proportional Prioritization\n\n$$\np_{i}=\\left | \\delta_{i} \\right | + \\epsilon\n$$\n\n- $\\delta$表示TD-error\n- $\\epsilon$是一个小的正常数，防止TD-error为0的经验永远不被重放。\n\n### Rank-based Prioritization\n\n$$\np_{i}=\\frac{1}{rank(i)}\n$$\n\n- $rank(i)$是经验根据$\\left | \\delta_{i} \\right |$大小排序的排名\n- $P$为指数$\\alpha$的幂律分布power-law distribution\n- 这种方式更具鲁棒性，因为其对异常点不敏感，主要是因为异常点的TD-error过大或过小对rank值没有太大影响\n\n优点：\n\n- 其重尾性、厚尾性、heavy-tail property保证采样多样性\n- 分层采样使mini-batch的梯度稳定\n\n缺点：\n\n- 当在稀疏奖励场景想要使用TD-error分布结构时，会造成性能下降\n\n### 比较\n\n根据文中实验，两种方式效果基本相同，但不同场景可能一个效果很好，一个效果一般般。作者**猜想**效果相同的原因可能是因为对奖励和TD-error大量使用clip操作，消除了异常值，作者本以为Rank-based更具鲁棒性的。\n\n> Overhead is similar to rank-based prioritization.\n\n两者开销相同。\n\n## 偏差退火 Annealing The Bias\n\n我觉得应该译为消除偏差。\n\n引入重要性采样、引入退火因子$\\beta$消除偏差。将$w_{i}$除以$max_{i}w_{i}$向下缩放（减小）梯度更新幅度，稳定算法\n$$\nw_{i}=\\left ( \\frac{1}{N} \\cdot \\frac{1}{P(i)} \\right )^{\\beta}\n$$\n\n- $\\beta=0$，完全不用重要性采样\n- $\\beta=1$，常规重要性采样\n- **在训练接近尾声时，使$\\beta \\rightarrow 1$ **\n- $\\beta \\in [0,1]$，文中并没有明说$\\beta$的取值范围\n- $\\beta$的选择与$\\alpha$有关，但文中并没有说明这两个参数如何选择的关系\n\n作用：\n\n- 消除偏差\n\n> We therefore exploit the flexibility of annealing the amount of importance-sampling correction over time, by defining a schedule on the exponent β that reaches 1 only at the end of learning.\n\n应用退火重要性采样校正量的灵活性，使在学习快结束时，将$\\beta \\rightarrow 1$\n\n# 伪代码\n\n![](./Prioritized-Experience-Replay/pseudo.png)\n\n**解析**：\n\n- step-size $\\eta$可以看做是学习率，文中并没有说它具体的定义，只是说它可以调节参数更新幅度（不就是学习率嘛）\n- $K$代表采样与更新之间的步数差，也就是，先采样K次经验并存入经验池，再取mini-batch更新。\n- 采样方式：\n  - ![](./Prioritized-Experience-Replay/sum-tree.png)\n  - 采样与更新TD-error的时间复杂度为$O(log_{2}N)$\n- 学习完之后对学习使用的经验更新其TD-error\n- 重要性权重$w_{j}=\\left ( N \\cdot P(j)\\right )^{-\\beta}/max_{i}w_{i}$，由$max_{i}w_{i}=max_{i}\\left ( N \\cdot P(i)\\right )^{-\\beta}=\\left ( min_{i}N \\cdot P(i)\\right )^{-\\beta}=\\left ( N \\cdot P_{min}\\right )^{-\\beta}$,可以将其化简为$w_{j}=\\left ( \\frac{p_{min}}{p_{j}} \\right )^{\\beta}$\n- 第12行，赋值其实是$(\\left | \\delta_{i} \\right |+ \\epsilon)^{\\alpha}$，如果是rank-based，则为$rank(i)^{-\\alpha}$\n- 第6行，对于新采样到的经验，不必计算其TD-error，直接将其设置为最大即可，当使用该经验学习之后再计算其TD-error\n- $\\Delta$其实就是误差函数$\\delta^{2}$对$\\theta$的导数，只不过对于mini-batch中的各个经验使用重要性比率进行了加权求和。\n\n**注意**：\n\n> Our final  solution was to store transitions in a priority queue implemented with an array-based binary heap. The heap array was then directly used as an approximation of a sorted array, which is infrequently sorted once every $10^{6}$ steps to prevent the heap becoming too unbalanced. \n\n如果使用rank-based方法，则使用的不是sum-tree结构，而是二进制堆，由于我不了解这个结构，故目前不做阐述。\n\n# Sum Tree\n\n```python\nimport numpy as np\n\n\nclass Sum_Tree(object):\n    def __init__(self, capacity):\n        \"\"\"\n        capacity = 5，设置经验池大小\n        tree = [0,1,2,3,4,5,6,7,8,9,10,11,12] 8-12存放叶子结点p值，1-7存放父节点、根节点p值的和，0存放树节点的数量\n        data = [0,1,2,3,4,5] 1-5存放数据， 0存放capacity\n        Tree structure and array storage:\n        Tree index:\n                    1         -> storing priority sum\n              /          \\ \n             2            3\n            / \\          / \\\n          4     5       6   7\n         / \\   / \\     / \\  / \\\n        8   9 10   11 12                   -> storing priority for transitions\n        \"\"\"\n        assert capacity != 1\n        self.now = 0\n        self.parent_node_count = self.get_parent_node_count(capacity)\n        print(self.parent_node_count)\n        self.tree = np.zeros(self.parent_node_count + capacity + 1)\n        self.tree[0] = len(self.tree) - 1\n        self.data = np.zeros(capacity + 1, dtype=object)\n        self.data[0] = capacity\n\n    def add(self, p, data):\n        \"\"\"\n        p : 优先级\n        data : 数据元组\n        \"\"\"\n        tree_index = self.now + self.parent_node_count + 1\n        self.data[self.now + 1] = data\n        self._updatetree(tree_index, p)\n        self.now += 1\n        if self.now > self.data[0]:\n            self.now = 0\n\n    def _updatetree(self, tree_index, p):\n        diff = p - self.tree[tree_index]\n        self._propagate(tree_index, diff)\n        self.tree[tree_index] = p\n\n    def _propagate(self, tree_index, diff):\n        parent = tree_index // 2\n        self.tree[parent] += diff\n        if parent != 1:\n            self._propagate(parent, diff)\n    @property\n    def total(self):\n        return self.tree[1]\n\n    def get(self, seg_p_total):\n        \"\"\"\n        seg_p_total : 要采样的p的值\n        \"\"\"\n        tree_index = self._retrieve(1, seg_p_total)\n        data_index = tree_index - self.parent_node_count\n        return (tree_index, data_index, self.tree[tree_index], self.data[data_index])\n\n    def _retrieve(self, tree_index, seg_p_total):\n        left = 2 * tree_index\n        right = left + 1\n#         left = 2 * tree_index + 1\n#         right = 2 * (tree_index + 1)\n        if left >= self.tree[0]:\n            return tree_index\n        return self._retrieve(left, seg_p_total) if seg_p_total <= self.tree[left] else self._retrieve(right, seg_p_total - self.tree[left])\n\n    def pp(self):\n        print(self.tree, self.data)\n\n    def get_parent_node_count(self, capacity):\n        i = 0\n        while True:\n            if pow(2, i) < capacity <= pow(2, i + 1):\n                return pow(2, i + 1) - 1\n            i += 1\n\n\ntree = Sum_Tree(5)\ntree.add(1, 3)\ntree.add(2, 4)\ntree.add(3, 5)\ntree.add(4, 6)\ntree.add(6, 11)\ntree.pp()\nprint(tree.get(4))\n```\n\n\n\n# 优先经验回放的特点\n\n1. 新的transition被采样到时，需要将其TD-error设置为最大，以保证最近的经验更容易被采样到。\n2. 只有在从经验池中抽取到某个经验并进行学习后，才对其TD-error进行计算更新。\n\n# 实验结果\n\n算法：\n\n- DQN\n- 优化后的Double DQN\n- 为了算法稳定的原因，将reward和TD-error clip到[-1,1]\n\n优先经验池：\n\n- 经验池大小$10^{6}$\n\n- batch-size为32\n\n- K=4，即每采样4次学习一次\n\n- Rank-based：$\\alpha=0.7，\\beta_{0}=0.5$，Proportional：$\\alpha=0.6，\\beta_{0}=0.4$\n\n- > These choices are trading off aggressiveness with robustness, but it is easy to revert to a behavior closer to the baseline by reducing $\\alpha$ and/or increasing $\\beta$. \n\n## 效果\n\n### 学习速度\n\n![](./Prioritized-Experience-Replay/learning-speed.png)\n\n- 黑色代表不使用优先经验回放的DDQN\n- 蓝色代表使用Proportional Prioritization的DDQN\n- 红色代表使用Rank-based Prioritization的DDQN\n\n- 绿色的虚线为人类水平\n\n### 归一化得分\n\n这些度量不重要，重要的是使用了优先经验回放机制的确提升了2倍左右的性能。\n\n![](./Prioritized-Experience-Replay/normalized-score1.png)\n\n![](./Prioritized-Experience-Replay/normalized-score.png)\n\n# PER的代码\n\n```\nimport numpy as np\nfrom abc import ABC, abstractmethod\n\n\nclass Buffer(ABC):\n    @abstractmethod\n    def sample(self) -> list:\n        pass\n\nclass Sum_Tree(object):\n    def __init__(self, capacity):\n        \"\"\"\n        capacity = 5，设置经验池大小\n        tree = [0,1,2,3,4,5,6,7,8,9,10,11,12] 8-12存放叶子结点p值，1-7存放父节点、根节点p值的和，0存放树节点的数量\n        data = [0,1,2,3,4,5] 1-5存放数据， 0存放capacity\n        Tree structure and array storage:\n        Tree index:\n                    1         -> storing priority sum\n              /          \\ \n             2            3\n            / \\          / \\\n          4     5       6   7\n         / \\   / \\     / \\  / \\\n        8   9 10   11 12                   -> storing priority for transitions\n        \"\"\"\n        assert capacity > 0\n        self.now = 0\n        self.parent_node_count = self.get_parent_node_count(capacity)\n        print(self.parent_node_count)\n        self.tree = np.zeros(self.parent_node_count + capacity + 1)\n        self.tree[0] = len(self.tree) - 1\n        self.data = np.zeros(capacity + 1, dtype=object)\n        self.data[0] = capacity\n\n    def add(self, p, data):\n        \"\"\"\n        p : property\n        data : [s, a, r, s_, done]\n        \"\"\"\n        tree_index = self.now + self.parent_node_count + 1\n        self.data[self.now + 1] = data\n        self._updatetree(tree_index, p)\n        self.now += 1\n        if self.now > self.data[0]:\n            self.now = 0\n\n    def _updatetree(self, tree_index, p):\n        diff = p - self.tree[tree_index]\n        self._propagate(tree_index, diff)\n        self.tree[tree_index] = p\n\n    def _propagate(self, tree_index, diff):\n        parent = tree_index // 2\n        self.tree[parent] += diff\n        if parent != 1:\n            self._propagate(parent, diff)\n\n    @property\n    def total(self):\n        return self.tree[1]\n\n    def get(self, seg_p_total):\n        \"\"\"\n        seg_p_total : The value of priority to sample\n        \"\"\"\n        tree_index = self._retrieve(1, seg_p_total)\n        data_index = tree_index - self.parent_node_count\n        return (tree_index, data_index, self.tree[tree_index], self.data[data_index])\n\n    def _retrieve(self, tree_index, seg_p_total):\n        left = 2 * tree_index\n        right = left + 1\n#         left = 2 * tree_index + 1\n#         right = 2 * (tree_index + 1)\n        if left >= self.tree[0]:\n            return tree_index\n        return self._retrieve(left, seg_p_total) if seg_p_total <= self.tree[left] else self._retrieve(right, seg_p_total - self.tree[left])\n\n    def pp(self):\n        print(self.tree, self.data)\n\n    def get_parent_node_count(self, capacity):\n        i = 0\n        while True:\n            if pow(2, i) < capacity <= pow(2, i + 1):\n                return pow(2, i + 1) - 1\n            i += 1\n\n\nclass PrioritizedReplayBuffer(Buffer):\n    def __init__(self, batch_size, capacity, alpha, beta, epsilon):\n        self.batch_size = batch_size\n        self.capacity = capacity\n        self._size = 0\n        self.alpha = alpha\n        self.beta = beta\n        self.tree = Sum_Tree(capacity)\n        self.epsilon = epsilon\n        self.min_p = np.inf\n\n    def add(self, p, *args):\n        '''\n        input: priorities, [ss, as, rs, _ss, dones]\n        '''\n        p = np.power(np.abs(p) + self.epsilon, self.alpha)\n        min_p = p.min()\n        if min_p < self.min_p:\n            self.min_p = min_p\n        if hasattr(args[0], '__len__'):\n            for i in range(len(args[0])):\n                self.tree.add(p[i], tuple(arg[i] for arg in args))\n                if self._size < self.capacity:\n                    self._size += 1\n        else:\n            self.tree.add(p, args)\n            if self._size < self.capacity:\n                self._size += 1\n\n    def sample(self):\n        '''\n        output: weights, [ss, as, rs, _ss, dones]\n        '''\n        n_sample = self.batch_size if self.is_lg_batch_size else self._size\n        interval = self.tree.total / n_sample\n        segment = [self.tree.total - i * interval for i in range(n_sample + 1)]\n        t = [self.tree.get(np.random.uniform(segment[i], segment[i + 1], 1)) for i in range(n_sample)]\n        t = [np.array(e) for e in zip(*t)]\n        self.last_indexs = t[0]\n        return np.power(self.min_p / t[-2], self.beta), t[-1]\n\n    @property\n    def is_lg_batch_size(self):\n        return self._size > self.batch_size\n\n    def update_priority(self, priority):\n        '''\n        input: priorities\n        '''\n        assert hasattr(priority, '__len__')\n        assert len(priority) == len(self.last_indexs)\n        for i in range(len(priority)):\n            self.tree._updatetree(self.last_indexs[i], priority[i])\n```\n\n","source":"_posts/Prioritized-Experience-Replay.md","raw":"---\ntitle: Prioritized Experience Replay\ncopyright: true\nmathjax: true\ntop: 1\ndate: 2019-05-22 08:54:44\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n这篇论文介绍了优先经验回放机制，它可以使学习过程更高效。\n\n推荐：\n\n- 实用技巧\n- 通俗易懂\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf)\n\n之前重用经验（experience，transition转换五元组$\\lt s,a,r,s',done \\ or \\ \\gamma\\gt$都是从经验池中**均匀采样**，忽略了经验的重要程度，文中提到的优先经验回放框架按经验重要性增大其被采样到的概率，希望越重要的经验使用次数越多，从而增加学习效率。\n\n文中应用算法：DQN\n\n效果：相比传统经验池机制，Atari 49游戏中41胜，8负\n\n# 文中精要\n\n> Online reinforcement learning (RL) agents incrementally update their parameters (of the policy, value function or model) while they observe a stream of experience. In their simplest form, they discard incoming data immediately, after a single update. Two issues with this are (a) strongly correlated updates that break the i.i.d. assumption of many popular stochastic gradient-based algorithms, and (b) the rapid forgetting of possibly rare experiences that would be useful later on. \n>Experience replay  addresses both of these issues: with experience stored in a replay memory, it becomes possible to break the temporal correlations by mixing more and less recent experience for the updates, and rare experience will be used for more than just a single update.\n\n指出On-policy一般使用一个episode的数据进行参数更新，且数据用完即丢，这样做有两个缺点：\n\n- 数据（状态）相互关联，数据不具有独立同分布($i.i.d$)的性质，但许多流行的随机梯度算法往往有关于数据独立同分布的假设\n- 对罕见的（稀疏的）经验快速遗忘，忽略了这些罕见经验可能多次更新更有用的作用\n\n**经验池机制解决了上述两个问题**，通过混合近期经验打破它们关于时间的关联性，并从经验池中采样经验学习多次。\n\n经验池机制的优势：\n\n- 稳定了DQN值函数的训练\n- 一般情况下，经验回放可以减少训练所需的经验数量，但是需要加大计算量，消耗更多的内存，但是这往往比智能体与环境进行交互来得更方便、容易\n\n>In particular, we propose to more frequently replay transitions with high expected learning progress, s measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead o a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which e correct with importance sampling. \n\n文中根据TD-error设置优先经验回放的频率，这可能会引起两个问题：\n\n- 丢失样本多样性\n- 引入偏差\n\n分别解决方案：\n\n- 随机优先级 stochastic prioritization\n- 重要性采样\n\n>The central component of prioritized replay is the criterion by which the importance of each transition is measured. One idealised criterion would be the amount the RL agent can learn from a transition in its current state (expected learning progress). While this measure is not directly accessible,  reasonable proxy is the magnitude of a transition’s TD error $\\delta$ indicates how ‘surprising’  or unexpected the transition is: specifically, how far the value is from its next-step bootstrap estimate.\n\n优先经验回放的核心部分是如何衡量样本的重要性，并根据其重要性进行回放。\n\n最直观的衡量标准是从经验样本中可以学习的量，但是这个量不可知、不可得，于是使用TD-error $\\delta$作为这个量的替代品以衡量样本重要性。 \n\n>New transitions arrive without a known TD-error, so we put them at maximal priority in order to guarantee that all experience is seen at least once.  \n\n新的经验被存入经验池时不需计算TD-error，直接将其设置为当前经验池中最大的TD-error，保证其至少被抽中一次。\n\n---\n\n既然使用TD-error作为衡量可学习的度量，那么完全可以用贪婪的方式，选取TD-error最大的几个进行学习，但这会有几个问题：\n\n1. 由于只有在经验被重放**之后**，这个经验的TD-error才被更新，导致初始TD-error比较小的经验长时间不被使用，甚至永远不被使用。\n2. 贪婪策略聚焦于一小部分TD-error比较高的经验，当使用值函数近似时，这些经验的TD-error减小速度很慢，导致这些经验被高频重复使用，致使样本缺乏多样性而过拟合。\n\n文中提到使用**随机采样方法 stochastic sampling method**在贪婪策略与均匀采样之间“差值”来解决上述问题，其实它是一个在样本使用上的trade-off，由超参数$\\alpha$控制\n\n使用优先经验回放还有一个问题是改变了状态的分布，我们知道DQN中引入经验池是为了解决数据相关性，使数据（尽量）独立同分布的问题。但是使用优先经验回放又改变了状态的分布，这样势必会引入偏差bias，对此，文中使用**偏差退火——重要性采样结合退火因子**，来消除引入的偏差。\n\n## 随机采样方法\n\n$$\nP(i)=\\frac{p^{\\alpha}_{i}}{\\sum_{k}p^{\\alpha}_{k}}\n$$\n\n$\\alpha$超参数控制采样在uniform和greedy的偏好，是一个trade-off因子：\n\n- $\\alpha=0$，均匀采样\n- $\\alpha=1$，贪婪策略采样\n- $\\alpha \\in [0,1]$，文中没有明说$\\alpha$的取值范围\n- 引入$\\alpha$不改变优先级的单调性，只是适当调整高、低TD-error经验的优先级\n\n根据优先级$p_{i}$的设定可以将优先经验池的设计分为两种：\n\n- 直接的，基于比例的，proportional prioritization\n- 间接的，基于排名的，rank-based prioritization\n\n### Proportional Prioritization\n\n$$\np_{i}=\\left | \\delta_{i} \\right | + \\epsilon\n$$\n\n- $\\delta$表示TD-error\n- $\\epsilon$是一个小的正常数，防止TD-error为0的经验永远不被重放。\n\n### Rank-based Prioritization\n\n$$\np_{i}=\\frac{1}{rank(i)}\n$$\n\n- $rank(i)$是经验根据$\\left | \\delta_{i} \\right |$大小排序的排名\n- $P$为指数$\\alpha$的幂律分布power-law distribution\n- 这种方式更具鲁棒性，因为其对异常点不敏感，主要是因为异常点的TD-error过大或过小对rank值没有太大影响\n\n优点：\n\n- 其重尾性、厚尾性、heavy-tail property保证采样多样性\n- 分层采样使mini-batch的梯度稳定\n\n缺点：\n\n- 当在稀疏奖励场景想要使用TD-error分布结构时，会造成性能下降\n\n### 比较\n\n根据文中实验，两种方式效果基本相同，但不同场景可能一个效果很好，一个效果一般般。作者**猜想**效果相同的原因可能是因为对奖励和TD-error大量使用clip操作，消除了异常值，作者本以为Rank-based更具鲁棒性的。\n\n> Overhead is similar to rank-based prioritization.\n\n两者开销相同。\n\n## 偏差退火 Annealing The Bias\n\n我觉得应该译为消除偏差。\n\n引入重要性采样、引入退火因子$\\beta$消除偏差。将$w_{i}$除以$max_{i}w_{i}$向下缩放（减小）梯度更新幅度，稳定算法\n$$\nw_{i}=\\left ( \\frac{1}{N} \\cdot \\frac{1}{P(i)} \\right )^{\\beta}\n$$\n\n- $\\beta=0$，完全不用重要性采样\n- $\\beta=1$，常规重要性采样\n- **在训练接近尾声时，使$\\beta \\rightarrow 1$ **\n- $\\beta \\in [0,1]$，文中并没有明说$\\beta$的取值范围\n- $\\beta$的选择与$\\alpha$有关，但文中并没有说明这两个参数如何选择的关系\n\n作用：\n\n- 消除偏差\n\n> We therefore exploit the flexibility of annealing the amount of importance-sampling correction over time, by defining a schedule on the exponent β that reaches 1 only at the end of learning.\n\n应用退火重要性采样校正量的灵活性，使在学习快结束时，将$\\beta \\rightarrow 1$\n\n# 伪代码\n\n![](./Prioritized-Experience-Replay/pseudo.png)\n\n**解析**：\n\n- step-size $\\eta$可以看做是学习率，文中并没有说它具体的定义，只是说它可以调节参数更新幅度（不就是学习率嘛）\n- $K$代表采样与更新之间的步数差，也就是，先采样K次经验并存入经验池，再取mini-batch更新。\n- 采样方式：\n  - ![](./Prioritized-Experience-Replay/sum-tree.png)\n  - 采样与更新TD-error的时间复杂度为$O(log_{2}N)$\n- 学习完之后对学习使用的经验更新其TD-error\n- 重要性权重$w_{j}=\\left ( N \\cdot P(j)\\right )^{-\\beta}/max_{i}w_{i}$，由$max_{i}w_{i}=max_{i}\\left ( N \\cdot P(i)\\right )^{-\\beta}=\\left ( min_{i}N \\cdot P(i)\\right )^{-\\beta}=\\left ( N \\cdot P_{min}\\right )^{-\\beta}$,可以将其化简为$w_{j}=\\left ( \\frac{p_{min}}{p_{j}} \\right )^{\\beta}$\n- 第12行，赋值其实是$(\\left | \\delta_{i} \\right |+ \\epsilon)^{\\alpha}$，如果是rank-based，则为$rank(i)^{-\\alpha}$\n- 第6行，对于新采样到的经验，不必计算其TD-error，直接将其设置为最大即可，当使用该经验学习之后再计算其TD-error\n- $\\Delta$其实就是误差函数$\\delta^{2}$对$\\theta$的导数，只不过对于mini-batch中的各个经验使用重要性比率进行了加权求和。\n\n**注意**：\n\n> Our final  solution was to store transitions in a priority queue implemented with an array-based binary heap. The heap array was then directly used as an approximation of a sorted array, which is infrequently sorted once every $10^{6}$ steps to prevent the heap becoming too unbalanced. \n\n如果使用rank-based方法，则使用的不是sum-tree结构，而是二进制堆，由于我不了解这个结构，故目前不做阐述。\n\n# Sum Tree\n\n```python\nimport numpy as np\n\n\nclass Sum_Tree(object):\n    def __init__(self, capacity):\n        \"\"\"\n        capacity = 5，设置经验池大小\n        tree = [0,1,2,3,4,5,6,7,8,9,10,11,12] 8-12存放叶子结点p值，1-7存放父节点、根节点p值的和，0存放树节点的数量\n        data = [0,1,2,3,4,5] 1-5存放数据， 0存放capacity\n        Tree structure and array storage:\n        Tree index:\n                    1         -> storing priority sum\n              /          \\ \n             2            3\n            / \\          / \\\n          4     5       6   7\n         / \\   / \\     / \\  / \\\n        8   9 10   11 12                   -> storing priority for transitions\n        \"\"\"\n        assert capacity != 1\n        self.now = 0\n        self.parent_node_count = self.get_parent_node_count(capacity)\n        print(self.parent_node_count)\n        self.tree = np.zeros(self.parent_node_count + capacity + 1)\n        self.tree[0] = len(self.tree) - 1\n        self.data = np.zeros(capacity + 1, dtype=object)\n        self.data[0] = capacity\n\n    def add(self, p, data):\n        \"\"\"\n        p : 优先级\n        data : 数据元组\n        \"\"\"\n        tree_index = self.now + self.parent_node_count + 1\n        self.data[self.now + 1] = data\n        self._updatetree(tree_index, p)\n        self.now += 1\n        if self.now > self.data[0]:\n            self.now = 0\n\n    def _updatetree(self, tree_index, p):\n        diff = p - self.tree[tree_index]\n        self._propagate(tree_index, diff)\n        self.tree[tree_index] = p\n\n    def _propagate(self, tree_index, diff):\n        parent = tree_index // 2\n        self.tree[parent] += diff\n        if parent != 1:\n            self._propagate(parent, diff)\n    @property\n    def total(self):\n        return self.tree[1]\n\n    def get(self, seg_p_total):\n        \"\"\"\n        seg_p_total : 要采样的p的值\n        \"\"\"\n        tree_index = self._retrieve(1, seg_p_total)\n        data_index = tree_index - self.parent_node_count\n        return (tree_index, data_index, self.tree[tree_index], self.data[data_index])\n\n    def _retrieve(self, tree_index, seg_p_total):\n        left = 2 * tree_index\n        right = left + 1\n#         left = 2 * tree_index + 1\n#         right = 2 * (tree_index + 1)\n        if left >= self.tree[0]:\n            return tree_index\n        return self._retrieve(left, seg_p_total) if seg_p_total <= self.tree[left] else self._retrieve(right, seg_p_total - self.tree[left])\n\n    def pp(self):\n        print(self.tree, self.data)\n\n    def get_parent_node_count(self, capacity):\n        i = 0\n        while True:\n            if pow(2, i) < capacity <= pow(2, i + 1):\n                return pow(2, i + 1) - 1\n            i += 1\n\n\ntree = Sum_Tree(5)\ntree.add(1, 3)\ntree.add(2, 4)\ntree.add(3, 5)\ntree.add(4, 6)\ntree.add(6, 11)\ntree.pp()\nprint(tree.get(4))\n```\n\n\n\n# 优先经验回放的特点\n\n1. 新的transition被采样到时，需要将其TD-error设置为最大，以保证最近的经验更容易被采样到。\n2. 只有在从经验池中抽取到某个经验并进行学习后，才对其TD-error进行计算更新。\n\n# 实验结果\n\n算法：\n\n- DQN\n- 优化后的Double DQN\n- 为了算法稳定的原因，将reward和TD-error clip到[-1,1]\n\n优先经验池：\n\n- 经验池大小$10^{6}$\n\n- batch-size为32\n\n- K=4，即每采样4次学习一次\n\n- Rank-based：$\\alpha=0.7，\\beta_{0}=0.5$，Proportional：$\\alpha=0.6，\\beta_{0}=0.4$\n\n- > These choices are trading off aggressiveness with robustness, but it is easy to revert to a behavior closer to the baseline by reducing $\\alpha$ and/or increasing $\\beta$. \n\n## 效果\n\n### 学习速度\n\n![](./Prioritized-Experience-Replay/learning-speed.png)\n\n- 黑色代表不使用优先经验回放的DDQN\n- 蓝色代表使用Proportional Prioritization的DDQN\n- 红色代表使用Rank-based Prioritization的DDQN\n\n- 绿色的虚线为人类水平\n\n### 归一化得分\n\n这些度量不重要，重要的是使用了优先经验回放机制的确提升了2倍左右的性能。\n\n![](./Prioritized-Experience-Replay/normalized-score1.png)\n\n![](./Prioritized-Experience-Replay/normalized-score.png)\n\n# PER的代码\n\n```\nimport numpy as np\nfrom abc import ABC, abstractmethod\n\n\nclass Buffer(ABC):\n    @abstractmethod\n    def sample(self) -> list:\n        pass\n\nclass Sum_Tree(object):\n    def __init__(self, capacity):\n        \"\"\"\n        capacity = 5，设置经验池大小\n        tree = [0,1,2,3,4,5,6,7,8,9,10,11,12] 8-12存放叶子结点p值，1-7存放父节点、根节点p值的和，0存放树节点的数量\n        data = [0,1,2,3,4,5] 1-5存放数据， 0存放capacity\n        Tree structure and array storage:\n        Tree index:\n                    1         -> storing priority sum\n              /          \\ \n             2            3\n            / \\          / \\\n          4     5       6   7\n         / \\   / \\     / \\  / \\\n        8   9 10   11 12                   -> storing priority for transitions\n        \"\"\"\n        assert capacity > 0\n        self.now = 0\n        self.parent_node_count = self.get_parent_node_count(capacity)\n        print(self.parent_node_count)\n        self.tree = np.zeros(self.parent_node_count + capacity + 1)\n        self.tree[0] = len(self.tree) - 1\n        self.data = np.zeros(capacity + 1, dtype=object)\n        self.data[0] = capacity\n\n    def add(self, p, data):\n        \"\"\"\n        p : property\n        data : [s, a, r, s_, done]\n        \"\"\"\n        tree_index = self.now + self.parent_node_count + 1\n        self.data[self.now + 1] = data\n        self._updatetree(tree_index, p)\n        self.now += 1\n        if self.now > self.data[0]:\n            self.now = 0\n\n    def _updatetree(self, tree_index, p):\n        diff = p - self.tree[tree_index]\n        self._propagate(tree_index, diff)\n        self.tree[tree_index] = p\n\n    def _propagate(self, tree_index, diff):\n        parent = tree_index // 2\n        self.tree[parent] += diff\n        if parent != 1:\n            self._propagate(parent, diff)\n\n    @property\n    def total(self):\n        return self.tree[1]\n\n    def get(self, seg_p_total):\n        \"\"\"\n        seg_p_total : The value of priority to sample\n        \"\"\"\n        tree_index = self._retrieve(1, seg_p_total)\n        data_index = tree_index - self.parent_node_count\n        return (tree_index, data_index, self.tree[tree_index], self.data[data_index])\n\n    def _retrieve(self, tree_index, seg_p_total):\n        left = 2 * tree_index\n        right = left + 1\n#         left = 2 * tree_index + 1\n#         right = 2 * (tree_index + 1)\n        if left >= self.tree[0]:\n            return tree_index\n        return self._retrieve(left, seg_p_total) if seg_p_total <= self.tree[left] else self._retrieve(right, seg_p_total - self.tree[left])\n\n    def pp(self):\n        print(self.tree, self.data)\n\n    def get_parent_node_count(self, capacity):\n        i = 0\n        while True:\n            if pow(2, i) < capacity <= pow(2, i + 1):\n                return pow(2, i + 1) - 1\n            i += 1\n\n\nclass PrioritizedReplayBuffer(Buffer):\n    def __init__(self, batch_size, capacity, alpha, beta, epsilon):\n        self.batch_size = batch_size\n        self.capacity = capacity\n        self._size = 0\n        self.alpha = alpha\n        self.beta = beta\n        self.tree = Sum_Tree(capacity)\n        self.epsilon = epsilon\n        self.min_p = np.inf\n\n    def add(self, p, *args):\n        '''\n        input: priorities, [ss, as, rs, _ss, dones]\n        '''\n        p = np.power(np.abs(p) + self.epsilon, self.alpha)\n        min_p = p.min()\n        if min_p < self.min_p:\n            self.min_p = min_p\n        if hasattr(args[0], '__len__'):\n            for i in range(len(args[0])):\n                self.tree.add(p[i], tuple(arg[i] for arg in args))\n                if self._size < self.capacity:\n                    self._size += 1\n        else:\n            self.tree.add(p, args)\n            if self._size < self.capacity:\n                self._size += 1\n\n    def sample(self):\n        '''\n        output: weights, [ss, as, rs, _ss, dones]\n        '''\n        n_sample = self.batch_size if self.is_lg_batch_size else self._size\n        interval = self.tree.total / n_sample\n        segment = [self.tree.total - i * interval for i in range(n_sample + 1)]\n        t = [self.tree.get(np.random.uniform(segment[i], segment[i + 1], 1)) for i in range(n_sample)]\n        t = [np.array(e) for e in zip(*t)]\n        self.last_indexs = t[0]\n        return np.power(self.min_p / t[-2], self.beta), t[-1]\n\n    @property\n    def is_lg_batch_size(self):\n        return self._size > self.batch_size\n\n    def update_priority(self, priority):\n        '''\n        input: priorities\n        '''\n        assert hasattr(priority, '__len__')\n        assert len(priority) == len(self.last_indexs)\n        for i in range(len(priority)):\n            self.tree._updatetree(self.last_indexs[i], priority[i])\n```\n\n","slug":"Prioritized-Experience-Replay","published":1,"updated":"2019-10-14T12:18:20.252Z","_id":"cjxd6ma38002tekve7o57p1v8","comments":1,"layout":"post","photos":[],"link":"","content":"<p>这篇论文介绍了优先经验回放机制，它可以使学习过程更高效。</p>\n<p>推荐：</p>\n<ul>\n<li>实用技巧</li>\n<li>通俗易懂</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1511.05952.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1511.05952.pdf</a></p>\n<p>之前重用经验（experience，transition转换五元组$\\lt s,a,r,s’,done \\ or \\ \\gamma\\gt$都是从经验池中<strong>均匀采样</strong>，忽略了经验的重要程度，文中提到的优先经验回放框架按经验重要性增大其被采样到的概率，希望越重要的经验使用次数越多，从而增加学习效率。</p>\n<p>文中应用算法：DQN</p>\n<p>效果：相比传统经验池机制，Atari 49游戏中41胜，8负</p>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><blockquote>\n<p>Online reinforcement learning (RL) agents incrementally update their parameters (of the policy, value function or model) while they observe a stream of experience. In their simplest form, they discard incoming data immediately, after a single update. Two issues with this are (a) strongly correlated updates that break the i.i.d. assumption of many popular stochastic gradient-based algorithms, and (b) the rapid forgetting of possibly rare experiences that would be useful later on.<br>Experience replay  addresses both of these issues: with experience stored in a replay memory, it becomes possible to break the temporal correlations by mixing more and less recent experience for the updates, and rare experience will be used for more than just a single update.</p>\n</blockquote>\n<p>指出On-policy一般使用一个episode的数据进行参数更新，且数据用完即丢，这样做有两个缺点：</p>\n<ul>\n<li>数据（状态）相互关联，数据不具有独立同分布($i.i.d$)的性质，但许多流行的随机梯度算法往往有关于数据独立同分布的假设</li>\n<li>对罕见的（稀疏的）经验快速遗忘，忽略了这些罕见经验可能多次更新更有用的作用</li>\n</ul>\n<p><strong>经验池机制解决了上述两个问题</strong>，通过混合近期经验打破它们关于时间的关联性，并从经验池中采样经验学习多次。</p>\n<p>经验池机制的优势：</p>\n<ul>\n<li>稳定了DQN值函数的训练</li>\n<li>一般情况下，经验回放可以减少训练所需的经验数量，但是需要加大计算量，消耗更多的内存，但是这往往比智能体与环境进行交互来得更方便、容易</li>\n</ul>\n<blockquote>\n<p>In particular, we propose to more frequently replay transitions with high expected learning progress, s measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead o a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which e correct with importance sampling. </p>\n</blockquote>\n<p>文中根据TD-error设置优先经验回放的频率，这可能会引起两个问题：</p>\n<ul>\n<li>丢失样本多样性</li>\n<li>引入偏差</li>\n</ul>\n<p>分别解决方案：</p>\n<ul>\n<li>随机优先级 stochastic prioritization</li>\n<li>重要性采样</li>\n</ul>\n<blockquote>\n<p>The central component of prioritized replay is the criterion by which the importance of each transition is measured. One idealised criterion would be the amount the RL agent can learn from a transition in its current state (expected learning progress). While this measure is not directly accessible,  reasonable proxy is the magnitude of a transition’s TD error $\\delta$ indicates how ‘surprising’  or unexpected the transition is: specifically, how far the value is from its next-step bootstrap estimate.</p>\n</blockquote>\n<p>优先经验回放的核心部分是如何衡量样本的重要性，并根据其重要性进行回放。</p>\n<p>最直观的衡量标准是从经验样本中可以学习的量，但是这个量不可知、不可得，于是使用TD-error $\\delta$作为这个量的替代品以衡量样本重要性。 </p>\n<blockquote>\n<p>New transitions arrive without a known TD-error, so we put them at maximal priority in order to guarantee that all experience is seen at least once.  </p>\n</blockquote>\n<p>新的经验被存入经验池时不需计算TD-error，直接将其设置为当前经验池中最大的TD-error，保证其至少被抽中一次。</p>\n<hr>\n<p>既然使用TD-error作为衡量可学习的度量，那么完全可以用贪婪的方式，选取TD-error最大的几个进行学习，但这会有几个问题：</p>\n<ol>\n<li>由于只有在经验被重放<strong>之后</strong>，这个经验的TD-error才被更新，导致初始TD-error比较小的经验长时间不被使用，甚至永远不被使用。</li>\n<li>贪婪策略聚焦于一小部分TD-error比较高的经验，当使用值函数近似时，这些经验的TD-error减小速度很慢，导致这些经验被高频重复使用，致使样本缺乏多样性而过拟合。</li>\n</ol>\n<p>文中提到使用<strong>随机采样方法 stochastic sampling method</strong>在贪婪策略与均匀采样之间“差值”来解决上述问题，其实它是一个在样本使用上的trade-off，由超参数$\\alpha$控制</p>\n<p>使用优先经验回放还有一个问题是改变了状态的分布，我们知道DQN中引入经验池是为了解决数据相关性，使数据（尽量）独立同分布的问题。但是使用优先经验回放又改变了状态的分布，这样势必会引入偏差bias，对此，文中使用<strong>偏差退火——重要性采样结合退火因子</strong>，来消除引入的偏差。</p>\n<h2 id=\"随机采样方法\"><a href=\"#随机采样方法\" class=\"headerlink\" title=\"随机采样方法\"></a>随机采样方法</h2><script type=\"math/tex; mode=display\">\nP(i)=\\frac{p^{\\alpha}_{i}}{\\sum_{k}p^{\\alpha}_{k}}</script><p>$\\alpha$超参数控制采样在uniform和greedy的偏好，是一个trade-off因子：</p>\n<ul>\n<li>$\\alpha=0$，均匀采样</li>\n<li>$\\alpha=1$，贪婪策略采样</li>\n<li>$\\alpha \\in [0,1]$，文中没有明说$\\alpha$的取值范围</li>\n<li>引入$\\alpha$不改变优先级的单调性，只是适当调整高、低TD-error经验的优先级</li>\n</ul>\n<p>根据优先级$p_{i}$的设定可以将优先经验池的设计分为两种：</p>\n<ul>\n<li>直接的，基于比例的，proportional prioritization</li>\n<li>间接的，基于排名的，rank-based prioritization</li>\n</ul>\n<h3 id=\"Proportional-Prioritization\"><a href=\"#Proportional-Prioritization\" class=\"headerlink\" title=\"Proportional Prioritization\"></a>Proportional Prioritization</h3><script type=\"math/tex; mode=display\">\np_{i}=\\left | \\delta_{i} \\right | + \\epsilon</script><ul>\n<li>$\\delta$表示TD-error</li>\n<li>$\\epsilon$是一个小的正常数，防止TD-error为0的经验永远不被重放。</li>\n</ul>\n<h3 id=\"Rank-based-Prioritization\"><a href=\"#Rank-based-Prioritization\" class=\"headerlink\" title=\"Rank-based Prioritization\"></a>Rank-based Prioritization</h3><script type=\"math/tex; mode=display\">\np_{i}=\\frac{1}{rank(i)}</script><ul>\n<li>$rank(i)$是经验根据$\\left | \\delta_{i} \\right |$大小排序的排名</li>\n<li>$P$为指数$\\alpha$的幂律分布power-law distribution</li>\n<li>这种方式更具鲁棒性，因为其对异常点不敏感，主要是因为异常点的TD-error过大或过小对rank值没有太大影响</li>\n</ul>\n<p>优点：</p>\n<ul>\n<li>其重尾性、厚尾性、heavy-tail property保证采样多样性</li>\n<li>分层采样使mini-batch的梯度稳定</li>\n</ul>\n<p>缺点：</p>\n<ul>\n<li>当在稀疏奖励场景想要使用TD-error分布结构时，会造成性能下降</li>\n</ul>\n<h3 id=\"比较\"><a href=\"#比较\" class=\"headerlink\" title=\"比较\"></a>比较</h3><p>根据文中实验，两种方式效果基本相同，但不同场景可能一个效果很好，一个效果一般般。作者<strong>猜想</strong>效果相同的原因可能是因为对奖励和TD-error大量使用clip操作，消除了异常值，作者本以为Rank-based更具鲁棒性的。</p>\n<blockquote>\n<p>Overhead is similar to rank-based prioritization.</p>\n</blockquote>\n<p>两者开销相同。</p>\n<h2 id=\"偏差退火-Annealing-The-Bias\"><a href=\"#偏差退火-Annealing-The-Bias\" class=\"headerlink\" title=\"偏差退火 Annealing The Bias\"></a>偏差退火 Annealing The Bias</h2><p>我觉得应该译为消除偏差。</p>\n<p>引入重要性采样、引入退火因子$\\beta$消除偏差。将$w_{i}$除以$max_{i}w_{i}$向下缩放（减小）梯度更新幅度，稳定算法</p>\n<script type=\"math/tex; mode=display\">\nw_{i}=\\left ( \\frac{1}{N} \\cdot \\frac{1}{P(i)} \\right )^{\\beta}</script><ul>\n<li>$\\beta=0$，完全不用重要性采样</li>\n<li>$\\beta=1$，常规重要性采样</li>\n<li><strong>在训练接近尾声时，使$\\beta \\rightarrow 1$ </strong></li>\n<li>$\\beta \\in [0,1]$，文中并没有明说$\\beta$的取值范围</li>\n<li>$\\beta$的选择与$\\alpha$有关，但文中并没有说明这两个参数如何选择的关系</li>\n</ul>\n<p>作用：</p>\n<ul>\n<li>消除偏差</li>\n</ul>\n<blockquote>\n<p>We therefore exploit the flexibility of annealing the amount of importance-sampling correction over time, by defining a schedule on the exponent β that reaches 1 only at the end of learning.</p>\n</blockquote>\n<p>应用退火重要性采样校正量的灵活性，使在学习快结束时，将$\\beta \\rightarrow 1$</p>\n<h1 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h1><p><img src=\"./Prioritized-Experience-Replay/pseudo.png\" alt=\"\"></p>\n<p><strong>解析</strong>：</p>\n<ul>\n<li>step-size $\\eta$可以看做是学习率，文中并没有说它具体的定义，只是说它可以调节参数更新幅度（不就是学习率嘛）</li>\n<li>$K$代表采样与更新之间的步数差，也就是，先采样K次经验并存入经验池，再取mini-batch更新。</li>\n<li>采样方式：<ul>\n<li><img src=\"./Prioritized-Experience-Replay/sum-tree.png\" alt=\"\"></li>\n<li>采样与更新TD-error的时间复杂度为$O(log_{2}N)$</li>\n</ul>\n</li>\n<li>学习完之后对学习使用的经验更新其TD-error</li>\n<li>重要性权重$w_{j}=\\left ( N \\cdot P(j)\\right )^{-\\beta}/max_{i}w_{i}$，由$max_{i}w_{i}=max_{i}\\left ( N \\cdot P(i)\\right )^{-\\beta}=\\left ( min_{i}N \\cdot P(i)\\right )^{-\\beta}=\\left ( N \\cdot P_{min}\\right )^{-\\beta}$,可以将其化简为$w_{j}=\\left ( \\frac{p_{min}}{p_{j}} \\right )^{\\beta}$</li>\n<li>第12行，赋值其实是$(\\left | \\delta_{i} \\right |+ \\epsilon)^{\\alpha}$，如果是rank-based，则为$rank(i)^{-\\alpha}$</li>\n<li>第6行，对于新采样到的经验，不必计算其TD-error，直接将其设置为最大即可，当使用该经验学习之后再计算其TD-error</li>\n<li>$\\Delta$其实就是误差函数$\\delta^{2}$对$\\theta$的导数，只不过对于mini-batch中的各个经验使用重要性比率进行了加权求和。</li>\n</ul>\n<p><strong>注意</strong>：</p>\n<blockquote>\n<p>Our final  solution was to store transitions in a priority queue implemented with an array-based binary heap. The heap array was then directly used as an approximation of a sorted array, which is infrequently sorted once every $10^{6}$ steps to prevent the heap becoming too unbalanced. </p>\n</blockquote>\n<p>如果使用rank-based方法，则使用的不是sum-tree结构，而是二进制堆，由于我不了解这个结构，故目前不做阐述。</p>\n<h1 id=\"Sum-Tree\"><a href=\"#Sum-Tree\" class=\"headerlink\" title=\"Sum Tree\"></a>Sum Tree</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Sum_Tree</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, capacity)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        capacity = 5，设置经验池大小</span></span><br><span class=\"line\"><span class=\"string\">        tree = [0,1,2,3,4,5,6,7,8,9,10,11,12] 8-12存放叶子结点p值，1-7存放父节点、根节点p值的和，0存放树节点的数量</span></span><br><span class=\"line\"><span class=\"string\">        data = [0,1,2,3,4,5] 1-5存放数据， 0存放capacity</span></span><br><span class=\"line\"><span class=\"string\">        Tree structure and array storage:</span></span><br><span class=\"line\"><span class=\"string\">        Tree index:</span></span><br><span class=\"line\"><span class=\"string\">                    1         -&gt; storing priority sum</span></span><br><span class=\"line\"><span class=\"string\">              /          \\ </span></span><br><span class=\"line\"><span class=\"string\">             2            3</span></span><br><span class=\"line\"><span class=\"string\">            / \\          / \\</span></span><br><span class=\"line\"><span class=\"string\">          4     5       6   7</span></span><br><span class=\"line\"><span class=\"string\">         / \\   / \\     / \\  / \\</span></span><br><span class=\"line\"><span class=\"string\">        8   9 10   11 12                   -&gt; storing priority for transitions</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> capacity != <span class=\"number\">1</span></span><br><span class=\"line\">        self.now = <span class=\"number\">0</span></span><br><span class=\"line\">        self.parent_node_count = self.get_parent_node_count(capacity)</span><br><span class=\"line\">        print(self.parent_node_count)</span><br><span class=\"line\">        self.tree = np.zeros(self.parent_node_count + capacity + <span class=\"number\">1</span>)</span><br><span class=\"line\">        self.tree[<span class=\"number\">0</span>] = len(self.tree) - <span class=\"number\">1</span></span><br><span class=\"line\">        self.data = np.zeros(capacity + <span class=\"number\">1</span>, dtype=object)</span><br><span class=\"line\">        self.data[<span class=\"number\">0</span>] = capacity</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">add</span><span class=\"params\">(self, p, data)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        p : 优先级</span></span><br><span class=\"line\"><span class=\"string\">        data : 数据元组</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        tree_index = self.now + self.parent_node_count + <span class=\"number\">1</span></span><br><span class=\"line\">        self.data[self.now + <span class=\"number\">1</span>] = data</span><br><span class=\"line\">        self._updatetree(tree_index, p)</span><br><span class=\"line\">        self.now += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.now &gt; self.data[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            self.now = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_updatetree</span><span class=\"params\">(self, tree_index, p)</span>:</span></span><br><span class=\"line\">        diff = p - self.tree[tree_index]</span><br><span class=\"line\">        self._propagate(tree_index, diff)</span><br><span class=\"line\">        self.tree[tree_index] = p</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_propagate</span><span class=\"params\">(self, tree_index, diff)</span>:</span></span><br><span class=\"line\">        parent = tree_index // <span class=\"number\">2</span></span><br><span class=\"line\">        self.tree[parent] += diff</span><br><span class=\"line\">        <span class=\"keyword\">if</span> parent != <span class=\"number\">1</span>:</span><br><span class=\"line\">            self._propagate(parent, diff)</span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">total</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.tree[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get</span><span class=\"params\">(self, seg_p_total)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        seg_p_total : 要采样的p的值</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        tree_index = self._retrieve(<span class=\"number\">1</span>, seg_p_total)</span><br><span class=\"line\">        data_index = tree_index - self.parent_node_count</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (tree_index, data_index, self.tree[tree_index], self.data[data_index])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_retrieve</span><span class=\"params\">(self, tree_index, seg_p_total)</span>:</span></span><br><span class=\"line\">        left = <span class=\"number\">2</span> * tree_index</span><br><span class=\"line\">        right = left + <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"comment\">#         left = 2 * tree_index + 1</span></span><br><span class=\"line\"><span class=\"comment\">#         right = 2 * (tree_index + 1)</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> left &gt;= self.tree[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> tree_index</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self._retrieve(left, seg_p_total) <span class=\"keyword\">if</span> seg_p_total &lt;= self.tree[left] <span class=\"keyword\">else</span> self._retrieve(right, seg_p_total - self.tree[left])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pp</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        print(self.tree, self.data)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_parent_node_count</span><span class=\"params\">(self, capacity)</span>:</span></span><br><span class=\"line\">        i = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> pow(<span class=\"number\">2</span>, i) &lt; capacity &lt;= pow(<span class=\"number\">2</span>, i + <span class=\"number\">1</span>):</span><br><span class=\"line\">                <span class=\"keyword\">return</span> pow(<span class=\"number\">2</span>, i + <span class=\"number\">1</span>) - <span class=\"number\">1</span></span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">tree = Sum_Tree(<span class=\"number\">5</span>)</span><br><span class=\"line\">tree.add(<span class=\"number\">1</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">tree.add(<span class=\"number\">2</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">tree.add(<span class=\"number\">3</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">tree.add(<span class=\"number\">4</span>, <span class=\"number\">6</span>)</span><br><span class=\"line\">tree.add(<span class=\"number\">6</span>, <span class=\"number\">11</span>)</span><br><span class=\"line\">tree.pp()</span><br><span class=\"line\">print(tree.get(<span class=\"number\">4</span>))</span><br></pre></td></tr></table></figure>\n<h1 id=\"优先经验回放的特点\"><a href=\"#优先经验回放的特点\" class=\"headerlink\" title=\"优先经验回放的特点\"></a>优先经验回放的特点</h1><ol>\n<li>新的transition被采样到时，需要将其TD-error设置为最大，以保证最近的经验更容易被采样到。</li>\n<li>只有在从经验池中抽取到某个经验并进行学习后，才对其TD-error进行计算更新。</li>\n</ol>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>算法：</p>\n<ul>\n<li>DQN</li>\n<li>优化后的Double DQN</li>\n<li>为了算法稳定的原因，将reward和TD-error clip到[-1,1]</li>\n</ul>\n<p>优先经验池：</p>\n<ul>\n<li><p>经验池大小$10^{6}$</p>\n</li>\n<li><p>batch-size为32</p>\n</li>\n<li><p>K=4，即每采样4次学习一次</p>\n</li>\n<li><p>Rank-based：$\\alpha=0.7，\\beta_{0}=0.5$，Proportional：$\\alpha=0.6，\\beta_{0}=0.4$</p>\n</li>\n<li><blockquote>\n<p>These choices are trading off aggressiveness with robustness, but it is easy to revert to a behavior closer to the baseline by reducing $\\alpha$ and/or increasing $\\beta$. </p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"效果\"><a href=\"#效果\" class=\"headerlink\" title=\"效果\"></a>效果</h2><h3 id=\"学习速度\"><a href=\"#学习速度\" class=\"headerlink\" title=\"学习速度\"></a>学习速度</h3><p><img src=\"./Prioritized-Experience-Replay/learning-speed.png\" alt=\"\"></p>\n<ul>\n<li>黑色代表不使用优先经验回放的DDQN</li>\n<li>蓝色代表使用Proportional Prioritization的DDQN</li>\n<li><p>红色代表使用Rank-based Prioritization的DDQN</p>\n</li>\n<li><p>绿色的虚线为人类水平</p>\n</li>\n</ul>\n<h3 id=\"归一化得分\"><a href=\"#归一化得分\" class=\"headerlink\" title=\"归一化得分\"></a>归一化得分</h3><p>这些度量不重要，重要的是使用了优先经验回放机制的确提升了2倍左右的性能。</p>\n<p><img src=\"./Prioritized-Experience-Replay/normalized-score1.png\" alt=\"\"></p>\n<p><img src=\"./Prioritized-Experience-Replay/normalized-score.png\" alt=\"\"></p>\n<h1 id=\"PER的代码\"><a href=\"#PER的代码\" class=\"headerlink\" title=\"PER的代码\"></a>PER的代码</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\">from abc import ABC, abstractmethod</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">class Buffer(ABC):</span><br><span class=\"line\">    @abstractmethod</span><br><span class=\"line\">    def sample(self) -&gt; list:</span><br><span class=\"line\">        pass</span><br><span class=\"line\"></span><br><span class=\"line\">class Sum_Tree(object):</span><br><span class=\"line\">    def __init__(self, capacity):</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        capacity = 5，设置经验池大小</span><br><span class=\"line\">        tree = [0,1,2,3,4,5,6,7,8,9,10,11,12] 8-12存放叶子结点p值，1-7存放父节点、根节点p值的和，0存放树节点的数量</span><br><span class=\"line\">        data = [0,1,2,3,4,5] 1-5存放数据， 0存放capacity</span><br><span class=\"line\">        Tree structure and array storage:</span><br><span class=\"line\">        Tree index:</span><br><span class=\"line\">                    1         -&gt; storing priority sum</span><br><span class=\"line\">              /          \\ </span><br><span class=\"line\">             2            3</span><br><span class=\"line\">            / \\          / \\</span><br><span class=\"line\">          4     5       6   7</span><br><span class=\"line\">         / \\   / \\     / \\  / \\</span><br><span class=\"line\">        8   9 10   11 12                   -&gt; storing priority for transitions</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        assert capacity &gt; 0</span><br><span class=\"line\">        self.now = 0</span><br><span class=\"line\">        self.parent_node_count = self.get_parent_node_count(capacity)</span><br><span class=\"line\">        print(self.parent_node_count)</span><br><span class=\"line\">        self.tree = np.zeros(self.parent_node_count + capacity + 1)</span><br><span class=\"line\">        self.tree[0] = len(self.tree) - 1</span><br><span class=\"line\">        self.data = np.zeros(capacity + 1, dtype=object)</span><br><span class=\"line\">        self.data[0] = capacity</span><br><span class=\"line\"></span><br><span class=\"line\">    def add(self, p, data):</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        p : property</span><br><span class=\"line\">        data : [s, a, r, s_, done]</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        tree_index = self.now + self.parent_node_count + 1</span><br><span class=\"line\">        self.data[self.now + 1] = data</span><br><span class=\"line\">        self._updatetree(tree_index, p)</span><br><span class=\"line\">        self.now += 1</span><br><span class=\"line\">        if self.now &gt; self.data[0]:</span><br><span class=\"line\">            self.now = 0</span><br><span class=\"line\"></span><br><span class=\"line\">    def _updatetree(self, tree_index, p):</span><br><span class=\"line\">        diff = p - self.tree[tree_index]</span><br><span class=\"line\">        self._propagate(tree_index, diff)</span><br><span class=\"line\">        self.tree[tree_index] = p</span><br><span class=\"line\"></span><br><span class=\"line\">    def _propagate(self, tree_index, diff):</span><br><span class=\"line\">        parent = tree_index // 2</span><br><span class=\"line\">        self.tree[parent] += diff</span><br><span class=\"line\">        if parent != 1:</span><br><span class=\"line\">            self._propagate(parent, diff)</span><br><span class=\"line\"></span><br><span class=\"line\">    @property</span><br><span class=\"line\">    def total(self):</span><br><span class=\"line\">        return self.tree[1]</span><br><span class=\"line\"></span><br><span class=\"line\">    def get(self, seg_p_total):</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        seg_p_total : The value of priority to sample</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        tree_index = self._retrieve(1, seg_p_total)</span><br><span class=\"line\">        data_index = tree_index - self.parent_node_count</span><br><span class=\"line\">        return (tree_index, data_index, self.tree[tree_index], self.data[data_index])</span><br><span class=\"line\"></span><br><span class=\"line\">    def _retrieve(self, tree_index, seg_p_total):</span><br><span class=\"line\">        left = 2 * tree_index</span><br><span class=\"line\">        right = left + 1</span><br><span class=\"line\">#         left = 2 * tree_index + 1</span><br><span class=\"line\">#         right = 2 * (tree_index + 1)</span><br><span class=\"line\">        if left &gt;= self.tree[0]:</span><br><span class=\"line\">            return tree_index</span><br><span class=\"line\">        return self._retrieve(left, seg_p_total) if seg_p_total &lt;= self.tree[left] else self._retrieve(right, seg_p_total - self.tree[left])</span><br><span class=\"line\"></span><br><span class=\"line\">    def pp(self):</span><br><span class=\"line\">        print(self.tree, self.data)</span><br><span class=\"line\"></span><br><span class=\"line\">    def get_parent_node_count(self, capacity):</span><br><span class=\"line\">        i = 0</span><br><span class=\"line\">        while True:</span><br><span class=\"line\">            if pow(2, i) &lt; capacity &lt;= pow(2, i + 1):</span><br><span class=\"line\">                return pow(2, i + 1) - 1</span><br><span class=\"line\">            i += 1</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">class PrioritizedReplayBuffer(Buffer):</span><br><span class=\"line\">    def __init__(self, batch_size, capacity, alpha, beta, epsilon):</span><br><span class=\"line\">        self.batch_size = batch_size</span><br><span class=\"line\">        self.capacity = capacity</span><br><span class=\"line\">        self._size = 0</span><br><span class=\"line\">        self.alpha = alpha</span><br><span class=\"line\">        self.beta = beta</span><br><span class=\"line\">        self.tree = Sum_Tree(capacity)</span><br><span class=\"line\">        self.epsilon = epsilon</span><br><span class=\"line\">        self.min_p = np.inf</span><br><span class=\"line\"></span><br><span class=\"line\">    def add(self, p, *args):</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        input: priorities, [ss, as, rs, _ss, dones]</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        p = np.power(np.abs(p) + self.epsilon, self.alpha)</span><br><span class=\"line\">        min_p = p.min()</span><br><span class=\"line\">        if min_p &lt; self.min_p:</span><br><span class=\"line\">            self.min_p = min_p</span><br><span class=\"line\">        if hasattr(args[0], &apos;__len__&apos;):</span><br><span class=\"line\">            for i in range(len(args[0])):</span><br><span class=\"line\">                self.tree.add(p[i], tuple(arg[i] for arg in args))</span><br><span class=\"line\">                if self._size &lt; self.capacity:</span><br><span class=\"line\">                    self._size += 1</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            self.tree.add(p, args)</span><br><span class=\"line\">            if self._size &lt; self.capacity:</span><br><span class=\"line\">                self._size += 1</span><br><span class=\"line\"></span><br><span class=\"line\">    def sample(self):</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        output: weights, [ss, as, rs, _ss, dones]</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        n_sample = self.batch_size if self.is_lg_batch_size else self._size</span><br><span class=\"line\">        interval = self.tree.total / n_sample</span><br><span class=\"line\">        segment = [self.tree.total - i * interval for i in range(n_sample + 1)]</span><br><span class=\"line\">        t = [self.tree.get(np.random.uniform(segment[i], segment[i + 1], 1)) for i in range(n_sample)]</span><br><span class=\"line\">        t = [np.array(e) for e in zip(*t)]</span><br><span class=\"line\">        self.last_indexs = t[0]</span><br><span class=\"line\">        return np.power(self.min_p / t[-2], self.beta), t[-1]</span><br><span class=\"line\"></span><br><span class=\"line\">    @property</span><br><span class=\"line\">    def is_lg_batch_size(self):</span><br><span class=\"line\">        return self._size &gt; self.batch_size</span><br><span class=\"line\"></span><br><span class=\"line\">    def update_priority(self, priority):</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        input: priorities</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        assert hasattr(priority, &apos;__len__&apos;)</span><br><span class=\"line\">        assert len(priority) == len(self.last_indexs)</span><br><span class=\"line\">        for i in range(len(priority)):</span><br><span class=\"line\">            self.tree._updatetree(self.last_indexs[i], priority[i])</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>这篇论文介绍了优先经验回放机制，它可以使学习过程更高效。</p>\n<p>推荐：</p>\n<ul>\n<li>实用技巧</li>\n<li>通俗易懂</li>\n</ul>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1511.05952.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1511.05952.pdf</a></p>\n<p>之前重用经验（experience，transition转换五元组$\\lt s,a,r,s’,done \\ or \\ \\gamma\\gt$都是从经验池中<strong>均匀采样</strong>，忽略了经验的重要程度，文中提到的优先经验回放框架按经验重要性增大其被采样到的概率，希望越重要的经验使用次数越多，从而增加学习效率。</p>\n<p>文中应用算法：DQN</p>\n<p>效果：相比传统经验池机制，Atari 49游戏中41胜，8负</p>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><blockquote>\n<p>Online reinforcement learning (RL) agents incrementally update their parameters (of the policy, value function or model) while they observe a stream of experience. In their simplest form, they discard incoming data immediately, after a single update. Two issues with this are (a) strongly correlated updates that break the i.i.d. assumption of many popular stochastic gradient-based algorithms, and (b) the rapid forgetting of possibly rare experiences that would be useful later on.<br>Experience replay  addresses both of these issues: with experience stored in a replay memory, it becomes possible to break the temporal correlations by mixing more and less recent experience for the updates, and rare experience will be used for more than just a single update.</p>\n</blockquote>\n<p>指出On-policy一般使用一个episode的数据进行参数更新，且数据用完即丢，这样做有两个缺点：</p>\n<ul>\n<li>数据（状态）相互关联，数据不具有独立同分布($i.i.d$)的性质，但许多流行的随机梯度算法往往有关于数据独立同分布的假设</li>\n<li>对罕见的（稀疏的）经验快速遗忘，忽略了这些罕见经验可能多次更新更有用的作用</li>\n</ul>\n<p><strong>经验池机制解决了上述两个问题</strong>，通过混合近期经验打破它们关于时间的关联性，并从经验池中采样经验学习多次。</p>\n<p>经验池机制的优势：</p>\n<ul>\n<li>稳定了DQN值函数的训练</li>\n<li>一般情况下，经验回放可以减少训练所需的经验数量，但是需要加大计算量，消耗更多的内存，但是这往往比智能体与环境进行交互来得更方便、容易</li>\n</ul>\n<blockquote>\n<p>In particular, we propose to more frequently replay transitions with high expected learning progress, s measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead o a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which e correct with importance sampling. </p>\n</blockquote>\n<p>文中根据TD-error设置优先经验回放的频率，这可能会引起两个问题：</p>\n<ul>\n<li>丢失样本多样性</li>\n<li>引入偏差</li>\n</ul>\n<p>分别解决方案：</p>\n<ul>\n<li>随机优先级 stochastic prioritization</li>\n<li>重要性采样</li>\n</ul>\n<blockquote>\n<p>The central component of prioritized replay is the criterion by which the importance of each transition is measured. One idealised criterion would be the amount the RL agent can learn from a transition in its current state (expected learning progress). While this measure is not directly accessible,  reasonable proxy is the magnitude of a transition’s TD error $\\delta$ indicates how ‘surprising’  or unexpected the transition is: specifically, how far the value is from its next-step bootstrap estimate.</p>\n</blockquote>\n<p>优先经验回放的核心部分是如何衡量样本的重要性，并根据其重要性进行回放。</p>\n<p>最直观的衡量标准是从经验样本中可以学习的量，但是这个量不可知、不可得，于是使用TD-error $\\delta$作为这个量的替代品以衡量样本重要性。 </p>\n<blockquote>\n<p>New transitions arrive without a known TD-error, so we put them at maximal priority in order to guarantee that all experience is seen at least once.  </p>\n</blockquote>\n<p>新的经验被存入经验池时不需计算TD-error，直接将其设置为当前经验池中最大的TD-error，保证其至少被抽中一次。</p>\n<hr>\n<p>既然使用TD-error作为衡量可学习的度量，那么完全可以用贪婪的方式，选取TD-error最大的几个进行学习，但这会有几个问题：</p>\n<ol>\n<li>由于只有在经验被重放<strong>之后</strong>，这个经验的TD-error才被更新，导致初始TD-error比较小的经验长时间不被使用，甚至永远不被使用。</li>\n<li>贪婪策略聚焦于一小部分TD-error比较高的经验，当使用值函数近似时，这些经验的TD-error减小速度很慢，导致这些经验被高频重复使用，致使样本缺乏多样性而过拟合。</li>\n</ol>\n<p>文中提到使用<strong>随机采样方法 stochastic sampling method</strong>在贪婪策略与均匀采样之间“差值”来解决上述问题，其实它是一个在样本使用上的trade-off，由超参数$\\alpha$控制</p>\n<p>使用优先经验回放还有一个问题是改变了状态的分布，我们知道DQN中引入经验池是为了解决数据相关性，使数据（尽量）独立同分布的问题。但是使用优先经验回放又改变了状态的分布，这样势必会引入偏差bias，对此，文中使用<strong>偏差退火——重要性采样结合退火因子</strong>，来消除引入的偏差。</p>\n<h2 id=\"随机采样方法\"><a href=\"#随机采样方法\" class=\"headerlink\" title=\"随机采样方法\"></a>随机采样方法</h2><script type=\"math/tex; mode=display\">\nP(i)=\\frac{p^{\\alpha}_{i}}{\\sum_{k}p^{\\alpha}_{k}}</script><p>$\\alpha$超参数控制采样在uniform和greedy的偏好，是一个trade-off因子：</p>\n<ul>\n<li>$\\alpha=0$，均匀采样</li>\n<li>$\\alpha=1$，贪婪策略采样</li>\n<li>$\\alpha \\in [0,1]$，文中没有明说$\\alpha$的取值范围</li>\n<li>引入$\\alpha$不改变优先级的单调性，只是适当调整高、低TD-error经验的优先级</li>\n</ul>\n<p>根据优先级$p_{i}$的设定可以将优先经验池的设计分为两种：</p>\n<ul>\n<li>直接的，基于比例的，proportional prioritization</li>\n<li>间接的，基于排名的，rank-based prioritization</li>\n</ul>\n<h3 id=\"Proportional-Prioritization\"><a href=\"#Proportional-Prioritization\" class=\"headerlink\" title=\"Proportional Prioritization\"></a>Proportional Prioritization</h3><script type=\"math/tex; mode=display\">\np_{i}=\\left | \\delta_{i} \\right | + \\epsilon</script><ul>\n<li>$\\delta$表示TD-error</li>\n<li>$\\epsilon$是一个小的正常数，防止TD-error为0的经验永远不被重放。</li>\n</ul>\n<h3 id=\"Rank-based-Prioritization\"><a href=\"#Rank-based-Prioritization\" class=\"headerlink\" title=\"Rank-based Prioritization\"></a>Rank-based Prioritization</h3><script type=\"math/tex; mode=display\">\np_{i}=\\frac{1}{rank(i)}</script><ul>\n<li>$rank(i)$是经验根据$\\left | \\delta_{i} \\right |$大小排序的排名</li>\n<li>$P$为指数$\\alpha$的幂律分布power-law distribution</li>\n<li>这种方式更具鲁棒性，因为其对异常点不敏感，主要是因为异常点的TD-error过大或过小对rank值没有太大影响</li>\n</ul>\n<p>优点：</p>\n<ul>\n<li>其重尾性、厚尾性、heavy-tail property保证采样多样性</li>\n<li>分层采样使mini-batch的梯度稳定</li>\n</ul>\n<p>缺点：</p>\n<ul>\n<li>当在稀疏奖励场景想要使用TD-error分布结构时，会造成性能下降</li>\n</ul>\n<h3 id=\"比较\"><a href=\"#比较\" class=\"headerlink\" title=\"比较\"></a>比较</h3><p>根据文中实验，两种方式效果基本相同，但不同场景可能一个效果很好，一个效果一般般。作者<strong>猜想</strong>效果相同的原因可能是因为对奖励和TD-error大量使用clip操作，消除了异常值，作者本以为Rank-based更具鲁棒性的。</p>\n<blockquote>\n<p>Overhead is similar to rank-based prioritization.</p>\n</blockquote>\n<p>两者开销相同。</p>\n<h2 id=\"偏差退火-Annealing-The-Bias\"><a href=\"#偏差退火-Annealing-The-Bias\" class=\"headerlink\" title=\"偏差退火 Annealing The Bias\"></a>偏差退火 Annealing The Bias</h2><p>我觉得应该译为消除偏差。</p>\n<p>引入重要性采样、引入退火因子$\\beta$消除偏差。将$w_{i}$除以$max_{i}w_{i}$向下缩放（减小）梯度更新幅度，稳定算法</p>\n<script type=\"math/tex; mode=display\">\nw_{i}=\\left ( \\frac{1}{N} \\cdot \\frac{1}{P(i)} \\right )^{\\beta}</script><ul>\n<li>$\\beta=0$，完全不用重要性采样</li>\n<li>$\\beta=1$，常规重要性采样</li>\n<li><strong>在训练接近尾声时，使$\\beta \\rightarrow 1$ </strong></li>\n<li>$\\beta \\in [0,1]$，文中并没有明说$\\beta$的取值范围</li>\n<li>$\\beta$的选择与$\\alpha$有关，但文中并没有说明这两个参数如何选择的关系</li>\n</ul>\n<p>作用：</p>\n<ul>\n<li>消除偏差</li>\n</ul>\n<blockquote>\n<p>We therefore exploit the flexibility of annealing the amount of importance-sampling correction over time, by defining a schedule on the exponent β that reaches 1 only at the end of learning.</p>\n</blockquote>\n<p>应用退火重要性采样校正量的灵活性，使在学习快结束时，将$\\beta \\rightarrow 1$</p>\n<h1 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h1><p><img src=\"./Prioritized-Experience-Replay/pseudo.png\" alt=\"\"></p>\n<p><strong>解析</strong>：</p>\n<ul>\n<li>step-size $\\eta$可以看做是学习率，文中并没有说它具体的定义，只是说它可以调节参数更新幅度（不就是学习率嘛）</li>\n<li>$K$代表采样与更新之间的步数差，也就是，先采样K次经验并存入经验池，再取mini-batch更新。</li>\n<li>采样方式：<ul>\n<li><img src=\"./Prioritized-Experience-Replay/sum-tree.png\" alt=\"\"></li>\n<li>采样与更新TD-error的时间复杂度为$O(log_{2}N)$</li>\n</ul>\n</li>\n<li>学习完之后对学习使用的经验更新其TD-error</li>\n<li>重要性权重$w_{j}=\\left ( N \\cdot P(j)\\right )^{-\\beta}/max_{i}w_{i}$，由$max_{i}w_{i}=max_{i}\\left ( N \\cdot P(i)\\right )^{-\\beta}=\\left ( min_{i}N \\cdot P(i)\\right )^{-\\beta}=\\left ( N \\cdot P_{min}\\right )^{-\\beta}$,可以将其化简为$w_{j}=\\left ( \\frac{p_{min}}{p_{j}} \\right )^{\\beta}$</li>\n<li>第12行，赋值其实是$(\\left | \\delta_{i} \\right |+ \\epsilon)^{\\alpha}$，如果是rank-based，则为$rank(i)^{-\\alpha}$</li>\n<li>第6行，对于新采样到的经验，不必计算其TD-error，直接将其设置为最大即可，当使用该经验学习之后再计算其TD-error</li>\n<li>$\\Delta$其实就是误差函数$\\delta^{2}$对$\\theta$的导数，只不过对于mini-batch中的各个经验使用重要性比率进行了加权求和。</li>\n</ul>\n<p><strong>注意</strong>：</p>\n<blockquote>\n<p>Our final  solution was to store transitions in a priority queue implemented with an array-based binary heap. The heap array was then directly used as an approximation of a sorted array, which is infrequently sorted once every $10^{6}$ steps to prevent the heap becoming too unbalanced. </p>\n</blockquote>\n<p>如果使用rank-based方法，则使用的不是sum-tree结构，而是二进制堆，由于我不了解这个结构，故目前不做阐述。</p>\n<h1 id=\"Sum-Tree\"><a href=\"#Sum-Tree\" class=\"headerlink\" title=\"Sum Tree\"></a>Sum Tree</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Sum_Tree</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, capacity)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        capacity = 5，设置经验池大小</span></span><br><span class=\"line\"><span class=\"string\">        tree = [0,1,2,3,4,5,6,7,8,9,10,11,12] 8-12存放叶子结点p值，1-7存放父节点、根节点p值的和，0存放树节点的数量</span></span><br><span class=\"line\"><span class=\"string\">        data = [0,1,2,3,4,5] 1-5存放数据， 0存放capacity</span></span><br><span class=\"line\"><span class=\"string\">        Tree structure and array storage:</span></span><br><span class=\"line\"><span class=\"string\">        Tree index:</span></span><br><span class=\"line\"><span class=\"string\">                    1         -&gt; storing priority sum</span></span><br><span class=\"line\"><span class=\"string\">              /          \\ </span></span><br><span class=\"line\"><span class=\"string\">             2            3</span></span><br><span class=\"line\"><span class=\"string\">            / \\          / \\</span></span><br><span class=\"line\"><span class=\"string\">          4     5       6   7</span></span><br><span class=\"line\"><span class=\"string\">         / \\   / \\     / \\  / \\</span></span><br><span class=\"line\"><span class=\"string\">        8   9 10   11 12                   -&gt; storing priority for transitions</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> capacity != <span class=\"number\">1</span></span><br><span class=\"line\">        self.now = <span class=\"number\">0</span></span><br><span class=\"line\">        self.parent_node_count = self.get_parent_node_count(capacity)</span><br><span class=\"line\">        print(self.parent_node_count)</span><br><span class=\"line\">        self.tree = np.zeros(self.parent_node_count + capacity + <span class=\"number\">1</span>)</span><br><span class=\"line\">        self.tree[<span class=\"number\">0</span>] = len(self.tree) - <span class=\"number\">1</span></span><br><span class=\"line\">        self.data = np.zeros(capacity + <span class=\"number\">1</span>, dtype=object)</span><br><span class=\"line\">        self.data[<span class=\"number\">0</span>] = capacity</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">add</span><span class=\"params\">(self, p, data)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        p : 优先级</span></span><br><span class=\"line\"><span class=\"string\">        data : 数据元组</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        tree_index = self.now + self.parent_node_count + <span class=\"number\">1</span></span><br><span class=\"line\">        self.data[self.now + <span class=\"number\">1</span>] = data</span><br><span class=\"line\">        self._updatetree(tree_index, p)</span><br><span class=\"line\">        self.now += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.now &gt; self.data[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            self.now = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_updatetree</span><span class=\"params\">(self, tree_index, p)</span>:</span></span><br><span class=\"line\">        diff = p - self.tree[tree_index]</span><br><span class=\"line\">        self._propagate(tree_index, diff)</span><br><span class=\"line\">        self.tree[tree_index] = p</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_propagate</span><span class=\"params\">(self, tree_index, diff)</span>:</span></span><br><span class=\"line\">        parent = tree_index // <span class=\"number\">2</span></span><br><span class=\"line\">        self.tree[parent] += diff</span><br><span class=\"line\">        <span class=\"keyword\">if</span> parent != <span class=\"number\">1</span>:</span><br><span class=\"line\">            self._propagate(parent, diff)</span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">total</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.tree[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get</span><span class=\"params\">(self, seg_p_total)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        seg_p_total : 要采样的p的值</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        tree_index = self._retrieve(<span class=\"number\">1</span>, seg_p_total)</span><br><span class=\"line\">        data_index = tree_index - self.parent_node_count</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (tree_index, data_index, self.tree[tree_index], self.data[data_index])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_retrieve</span><span class=\"params\">(self, tree_index, seg_p_total)</span>:</span></span><br><span class=\"line\">        left = <span class=\"number\">2</span> * tree_index</span><br><span class=\"line\">        right = left + <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"comment\">#         left = 2 * tree_index + 1</span></span><br><span class=\"line\"><span class=\"comment\">#         right = 2 * (tree_index + 1)</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> left &gt;= self.tree[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> tree_index</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self._retrieve(left, seg_p_total) <span class=\"keyword\">if</span> seg_p_total &lt;= self.tree[left] <span class=\"keyword\">else</span> self._retrieve(right, seg_p_total - self.tree[left])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pp</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        print(self.tree, self.data)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_parent_node_count</span><span class=\"params\">(self, capacity)</span>:</span></span><br><span class=\"line\">        i = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> pow(<span class=\"number\">2</span>, i) &lt; capacity &lt;= pow(<span class=\"number\">2</span>, i + <span class=\"number\">1</span>):</span><br><span class=\"line\">                <span class=\"keyword\">return</span> pow(<span class=\"number\">2</span>, i + <span class=\"number\">1</span>) - <span class=\"number\">1</span></span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">tree = Sum_Tree(<span class=\"number\">5</span>)</span><br><span class=\"line\">tree.add(<span class=\"number\">1</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">tree.add(<span class=\"number\">2</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">tree.add(<span class=\"number\">3</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">tree.add(<span class=\"number\">4</span>, <span class=\"number\">6</span>)</span><br><span class=\"line\">tree.add(<span class=\"number\">6</span>, <span class=\"number\">11</span>)</span><br><span class=\"line\">tree.pp()</span><br><span class=\"line\">print(tree.get(<span class=\"number\">4</span>))</span><br></pre></td></tr></table></figure>\n<h1 id=\"优先经验回放的特点\"><a href=\"#优先经验回放的特点\" class=\"headerlink\" title=\"优先经验回放的特点\"></a>优先经验回放的特点</h1><ol>\n<li>新的transition被采样到时，需要将其TD-error设置为最大，以保证最近的经验更容易被采样到。</li>\n<li>只有在从经验池中抽取到某个经验并进行学习后，才对其TD-error进行计算更新。</li>\n</ol>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>算法：</p>\n<ul>\n<li>DQN</li>\n<li>优化后的Double DQN</li>\n<li>为了算法稳定的原因，将reward和TD-error clip到[-1,1]</li>\n</ul>\n<p>优先经验池：</p>\n<ul>\n<li><p>经验池大小$10^{6}$</p>\n</li>\n<li><p>batch-size为32</p>\n</li>\n<li><p>K=4，即每采样4次学习一次</p>\n</li>\n<li><p>Rank-based：$\\alpha=0.7，\\beta_{0}=0.5$，Proportional：$\\alpha=0.6，\\beta_{0}=0.4$</p>\n</li>\n<li><blockquote>\n<p>These choices are trading off aggressiveness with robustness, but it is easy to revert to a behavior closer to the baseline by reducing $\\alpha$ and/or increasing $\\beta$. </p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"效果\"><a href=\"#效果\" class=\"headerlink\" title=\"效果\"></a>效果</h2><h3 id=\"学习速度\"><a href=\"#学习速度\" class=\"headerlink\" title=\"学习速度\"></a>学习速度</h3><p><img src=\"./Prioritized-Experience-Replay/learning-speed.png\" alt=\"\"></p>\n<ul>\n<li>黑色代表不使用优先经验回放的DDQN</li>\n<li>蓝色代表使用Proportional Prioritization的DDQN</li>\n<li><p>红色代表使用Rank-based Prioritization的DDQN</p>\n</li>\n<li><p>绿色的虚线为人类水平</p>\n</li>\n</ul>\n<h3 id=\"归一化得分\"><a href=\"#归一化得分\" class=\"headerlink\" title=\"归一化得分\"></a>归一化得分</h3><p>这些度量不重要，重要的是使用了优先经验回放机制的确提升了2倍左右的性能。</p>\n<p><img src=\"./Prioritized-Experience-Replay/normalized-score1.png\" alt=\"\"></p>\n<p><img src=\"./Prioritized-Experience-Replay/normalized-score.png\" alt=\"\"></p>\n<h1 id=\"PER的代码\"><a href=\"#PER的代码\" class=\"headerlink\" title=\"PER的代码\"></a>PER的代码</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\">from abc import ABC, abstractmethod</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">class Buffer(ABC):</span><br><span class=\"line\">    @abstractmethod</span><br><span class=\"line\">    def sample(self) -&gt; list:</span><br><span class=\"line\">        pass</span><br><span class=\"line\"></span><br><span class=\"line\">class Sum_Tree(object):</span><br><span class=\"line\">    def __init__(self, capacity):</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        capacity = 5，设置经验池大小</span><br><span class=\"line\">        tree = [0,1,2,3,4,5,6,7,8,9,10,11,12] 8-12存放叶子结点p值，1-7存放父节点、根节点p值的和，0存放树节点的数量</span><br><span class=\"line\">        data = [0,1,2,3,4,5] 1-5存放数据， 0存放capacity</span><br><span class=\"line\">        Tree structure and array storage:</span><br><span class=\"line\">        Tree index:</span><br><span class=\"line\">                    1         -&gt; storing priority sum</span><br><span class=\"line\">              /          \\ </span><br><span class=\"line\">             2            3</span><br><span class=\"line\">            / \\          / \\</span><br><span class=\"line\">          4     5       6   7</span><br><span class=\"line\">         / \\   / \\     / \\  / \\</span><br><span class=\"line\">        8   9 10   11 12                   -&gt; storing priority for transitions</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        assert capacity &gt; 0</span><br><span class=\"line\">        self.now = 0</span><br><span class=\"line\">        self.parent_node_count = self.get_parent_node_count(capacity)</span><br><span class=\"line\">        print(self.parent_node_count)</span><br><span class=\"line\">        self.tree = np.zeros(self.parent_node_count + capacity + 1)</span><br><span class=\"line\">        self.tree[0] = len(self.tree) - 1</span><br><span class=\"line\">        self.data = np.zeros(capacity + 1, dtype=object)</span><br><span class=\"line\">        self.data[0] = capacity</span><br><span class=\"line\"></span><br><span class=\"line\">    def add(self, p, data):</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        p : property</span><br><span class=\"line\">        data : [s, a, r, s_, done]</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        tree_index = self.now + self.parent_node_count + 1</span><br><span class=\"line\">        self.data[self.now + 1] = data</span><br><span class=\"line\">        self._updatetree(tree_index, p)</span><br><span class=\"line\">        self.now += 1</span><br><span class=\"line\">        if self.now &gt; self.data[0]:</span><br><span class=\"line\">            self.now = 0</span><br><span class=\"line\"></span><br><span class=\"line\">    def _updatetree(self, tree_index, p):</span><br><span class=\"line\">        diff = p - self.tree[tree_index]</span><br><span class=\"line\">        self._propagate(tree_index, diff)</span><br><span class=\"line\">        self.tree[tree_index] = p</span><br><span class=\"line\"></span><br><span class=\"line\">    def _propagate(self, tree_index, diff):</span><br><span class=\"line\">        parent = tree_index // 2</span><br><span class=\"line\">        self.tree[parent] += diff</span><br><span class=\"line\">        if parent != 1:</span><br><span class=\"line\">            self._propagate(parent, diff)</span><br><span class=\"line\"></span><br><span class=\"line\">    @property</span><br><span class=\"line\">    def total(self):</span><br><span class=\"line\">        return self.tree[1]</span><br><span class=\"line\"></span><br><span class=\"line\">    def get(self, seg_p_total):</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        seg_p_total : The value of priority to sample</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\">        tree_index = self._retrieve(1, seg_p_total)</span><br><span class=\"line\">        data_index = tree_index - self.parent_node_count</span><br><span class=\"line\">        return (tree_index, data_index, self.tree[tree_index], self.data[data_index])</span><br><span class=\"line\"></span><br><span class=\"line\">    def _retrieve(self, tree_index, seg_p_total):</span><br><span class=\"line\">        left = 2 * tree_index</span><br><span class=\"line\">        right = left + 1</span><br><span class=\"line\">#         left = 2 * tree_index + 1</span><br><span class=\"line\">#         right = 2 * (tree_index + 1)</span><br><span class=\"line\">        if left &gt;= self.tree[0]:</span><br><span class=\"line\">            return tree_index</span><br><span class=\"line\">        return self._retrieve(left, seg_p_total) if seg_p_total &lt;= self.tree[left] else self._retrieve(right, seg_p_total - self.tree[left])</span><br><span class=\"line\"></span><br><span class=\"line\">    def pp(self):</span><br><span class=\"line\">        print(self.tree, self.data)</span><br><span class=\"line\"></span><br><span class=\"line\">    def get_parent_node_count(self, capacity):</span><br><span class=\"line\">        i = 0</span><br><span class=\"line\">        while True:</span><br><span class=\"line\">            if pow(2, i) &lt; capacity &lt;= pow(2, i + 1):</span><br><span class=\"line\">                return pow(2, i + 1) - 1</span><br><span class=\"line\">            i += 1</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">class PrioritizedReplayBuffer(Buffer):</span><br><span class=\"line\">    def __init__(self, batch_size, capacity, alpha, beta, epsilon):</span><br><span class=\"line\">        self.batch_size = batch_size</span><br><span class=\"line\">        self.capacity = capacity</span><br><span class=\"line\">        self._size = 0</span><br><span class=\"line\">        self.alpha = alpha</span><br><span class=\"line\">        self.beta = beta</span><br><span class=\"line\">        self.tree = Sum_Tree(capacity)</span><br><span class=\"line\">        self.epsilon = epsilon</span><br><span class=\"line\">        self.min_p = np.inf</span><br><span class=\"line\"></span><br><span class=\"line\">    def add(self, p, *args):</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        input: priorities, [ss, as, rs, _ss, dones]</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        p = np.power(np.abs(p) + self.epsilon, self.alpha)</span><br><span class=\"line\">        min_p = p.min()</span><br><span class=\"line\">        if min_p &lt; self.min_p:</span><br><span class=\"line\">            self.min_p = min_p</span><br><span class=\"line\">        if hasattr(args[0], &apos;__len__&apos;):</span><br><span class=\"line\">            for i in range(len(args[0])):</span><br><span class=\"line\">                self.tree.add(p[i], tuple(arg[i] for arg in args))</span><br><span class=\"line\">                if self._size &lt; self.capacity:</span><br><span class=\"line\">                    self._size += 1</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            self.tree.add(p, args)</span><br><span class=\"line\">            if self._size &lt; self.capacity:</span><br><span class=\"line\">                self._size += 1</span><br><span class=\"line\"></span><br><span class=\"line\">    def sample(self):</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        output: weights, [ss, as, rs, _ss, dones]</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        n_sample = self.batch_size if self.is_lg_batch_size else self._size</span><br><span class=\"line\">        interval = self.tree.total / n_sample</span><br><span class=\"line\">        segment = [self.tree.total - i * interval for i in range(n_sample + 1)]</span><br><span class=\"line\">        t = [self.tree.get(np.random.uniform(segment[i], segment[i + 1], 1)) for i in range(n_sample)]</span><br><span class=\"line\">        t = [np.array(e) for e in zip(*t)]</span><br><span class=\"line\">        self.last_indexs = t[0]</span><br><span class=\"line\">        return np.power(self.min_p / t[-2], self.beta), t[-1]</span><br><span class=\"line\"></span><br><span class=\"line\">    @property</span><br><span class=\"line\">    def is_lg_batch_size(self):</span><br><span class=\"line\">        return self._size &gt; self.batch_size</span><br><span class=\"line\"></span><br><span class=\"line\">    def update_priority(self, priority):</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        input: priorities</span><br><span class=\"line\">        &apos;&apos;&apos;</span><br><span class=\"line\">        assert hasattr(priority, &apos;__len__&apos;)</span><br><span class=\"line\">        assert len(priority) == len(self.last_indexs)</span><br><span class=\"line\">        for i in range(len(priority)):</span><br><span class=\"line\">            self.tree._updatetree(self.last_indexs[i], priority[i])</span><br></pre></td></tr></table></figure>"},{"title":"Evolution Strategies as a Scalable Alternative to Reinforcement Learning","copyright":true,"mathjax":true,"top":1,"date":"2019-05-21T04:38:54.000Z","keywords":null,"description":null,"_content":"\n这一篇论文讲了强化学习算法的替代可解方案：进化策略。主要思想是对参数空间添加噪音而不是动作空间。\n\n不推荐这篇论文：\n\n- 公式没有详细推理，非常难懂\n- 文中进化策略其实跟强化学习并没有特别大的关系\n- 很多关于进化策略的性质、优势非常难懂，基本上都是文字解释，没有举例\n- 文中措辞不难，但想要理解其本质非常难\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1703.03864.pdf](https://arxiv.org/pdf/1703.03864.pdf)\n\n进化策略ES是一组/一类算法，而不是一个算法，它属于黑盒优化方法，它由自然进化中的启发式搜索过程而得来：每一代中都有突变的基因，环境对基因突变的效果给出适应性的判断，重组好的突变基因产生下一代，直到最优。\n\n进化策略算法的划分主要有三个依据：基因如何表示（神经网络参数）、突变如何产生（参数优化过程）、基因如何重组（参数重组）。\n\n进化策略ES这种方法通常分为[直接策略搜索](https://pdfs.semanticscholar.org/dd17/8d3f30d801922c98cec9c2d90db05395f244.pdf?_ga=2.257341323.183297583.1558416128-1251761365.1555224483)和[神经进化](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7307180&tag=1)，黑盒优化方法有很多很好的特性：\n\n1. 不关心奖励分布，奖励密集或稀疏都无所谓\n2. 不需要反向传播梯度\n3. tolerance of potentially arbitrarily long time horizons. 翻译为可以适应长期视野/回报，在长动作序列上有优势\n\n但是，进化策略ES往往不能解决像Q-Learning和PG这样可应用的难的强化学习问题，这篇论文旨在使进化策略可以解决DRL算法可解决的更难的问题。\n\n# 正文精要\n\n> A large source of difficulty in RL stems from the lack of informative gradients of policy performance: such gradients may not exist due to non-smoothness of the environment or policy, or may only be available as high-variance estimates because the environment usually can only be accessed via sampling.  \n\n指出强化学习的难题在于缺乏策略性能的有效梯度：梯度可能由于环境不光滑而不存在、可能由于只能采样环境而存在高方差。\n\n> For MDP-based reinforcement learning algorithms, on the other hand, it is well known that frameskip is a crucial parameter to get right for the optimization to succeed.\n\n对于基于MDP的强化学习算法，**跳帧**是算法优化的关键参数。\n\n> It is common practice in RL to have the agent decide on its actions in a lower frequency than is used in the simulator that runs the environment.\n\nRL通常使智能体在模拟环境中决策频率高于在实际环境中。\n\n---\n\n文中设定一个策略的期望奖励为：\n$$\n\\mathbb{E}_{\\epsilon \\sim N(0,I)}F(\\theta+\\sigma\\epsilon)\n$$\n关于网络参数$\\theta$的导数为：\n$$\n\\nabla_{\\theta}\\mathbb{E}_{\\epsilon \\sim N(0,I)}F(\\theta+\\sigma\\epsilon)=\\frac{1}{\\sigma}\\mathbb{E}_{\\epsilon \\sim N(0,I)} \\{F(\\theta+\\sigma\\epsilon)\\epsilon \\}\n$$\n其中，$\\theta$为网络参数，也可以认为是多变量高斯分布的均值，$\\sigma$为固定方差，$\\epsilon$为扰动向量，由各向同性、方差均为1的多变量高斯分布采样得到。**文中没有对该导数推导过程有介绍，好像是使用了Reinforce Trick的方法，但是却不知道具体如何推导出这个形式**。\n\n文中提到的算法1是对一个策略进行多次扰动，每扰动一次就与环境交互得到一个episode，最后只用各个扰动向量$\\epsilon_{i}$与对应的回报$F(\\theta)$相乘，根据该期望进行参数更新。\n\n![](./Evolution-Strategies-2017/algorithm1.png)\n\n算法2是对算法1的并行化处理，设置相同的随机种子，假设n个worker：\n\n1. 各个worker共用一个策略$\\pi$\n2. 每个worker根据高斯分布采样得到扰动向量$\\epsilon$\n3. 各个worker根据扰动后的策略参数采样一个episode\n4. 互相分发各自的回报\n5. **再采样n个扰动向量$\\epsilon$**，使用梯度上升更新参数，然后分发策略\n\n![](./Evolution-Strategies-2017/algorithm2.png)\n\n文中后边提到，其实不必每次都从高斯分布中采样出扰动向量$\\epsilon$，可以在开始训练前直接采样得到m个扰动向量，每次需要扰动向量时直接根据m的值生成一个随机数，取出以该随机数为下标的扰动向量即可。这么做可以减少更新时的时长消耗。\n\n---\n\n> Experiments on Atari and MuJoCo show that it is a viable option with some attractive features: it is invariant to action frequency and delayed rewards, and it does not need temporal discounting or value function approximation. Most importantly, ES is highly parallelizable, which allows us to make up for a decreased data efficiency by scaling to more parallel workers. \n\n文中使用的进化策略ES的优点：\n\n- 与决策间隔无关，也就是对于跳帧间隔的设置鲁棒性很高\n- 不关心延迟奖励\n- 不需要折扣计算回报\n- 不需要值函数近似\n- 可以高度并行化使，我们能够通过扩展到更多并行训练节点来弥补数据效率的下降。\n\n# 实验发现\n\n1. 使用[Virtual Batch Normalization](https://arxiv.org/pdf/1606.03498.pdf)和神经网络策略重参数（文中没有提到重参数的内容，只提到网络参数的影响）可以极大提升进化策略ES的可靠性。实验中，不使用这两种方法算法很“脆弱”，也就是不稳定。\n2. 进化策略ES可以高度并行化。通过引入一个基于通用随机数的新颖通讯策略，即是是1000个子节点也可以达到运行时间的线性加速。\n3. 进化策略ES的数据效率出奇的好。尽管相比A3C算法需要3-10倍的数据量，但是由于具有不需反向传播、没有值函数等特点，这些轻微的数据效率劣势可以被弥补。实验表明，相同计算量下，1小时ES与1天A3C的效果基本相同。\n4. 进化策略ES相比PG类算法的探索性更强。\n5. 进化策略ES的鲁棒性很好。多种不同训练环境可以使用同一组超参数。\n\n# 实验结果\n\n## MuJoCo\n\n与**高度优化**的TRPO算法相比，ES在离散动作更有优势，因为连续动作在参数扰动方面可能过于平滑并且可能妨碍探索。\n\nES和TRPO的网络结构都是：输入层→64，tanh→64，tanh→输出层。\n\n复杂环境如Hopper和Walker2d中，ES样本复杂性相比TRPO高不到10倍；简单场景中，相比低3倍。\n\nTRPO训练500W步，ES训练至TRPO训练过程中各阶段效果所需步长的比例如表所示：\n\n![](./Evolution-Strategies-2017/mujoco.png)\n\n虽然文中说是简单场景低三倍，其实根本就没有明确的低三倍，而且我对文中所提的简单场景复杂场景的划分也持怀疑态度。\n\n## Atari\n\n预处理、网络架构与Atari那篇论文的一模一样，用A3C使用3.2亿帧训练1天的结果与使用ES训练10亿帧的结果相同（保持计算量相同，因为ES不需要反向传播和值函数评估）。使用720块cpu，训练一个游戏只需1小时。\n\n最终，纯图像输入下，与A3C相比，23个游戏ES胜，28个游戏A3C胜。\n\n![](./Evolution-Strategies-2017/atari.png)\n\n## 并行化 Parallelization\n\nES特别适合并行化，因为其通讯低带宽特性（只需各个worker的回报和随机种子）。\n\n测试环境：3D Humanoid walking task\n\n结果：单机18核需11小时，与最先进的强化学习算法性能相当，80台机器1440个CPU核心只需10分钟。\n\n![](./Evolution-Strategies-2017/parallelization.png)\n\n随着核心数增加，训练性能线性加速。\n\n## “跳帧”测试\n\n将强化学习在模拟环境中训练出的模型用于实际环境中式，通常需要降低其决策频率，也就是加大决策间隔。\n\n如果跳帧设置过大，智能体所做的动作往往不够好，如果跳帧设置过小，会导致每个episode的步数过长，加大计算量，恶化训练过程（其实文中这么说并不严谨）。\n\nES的一个优势是梯度计算与回合长度无关，这间接增加了对跳帧间隔的鲁棒性。在Atari游戏Pong中使用四个不同跳帧间隔{1，2，3，4}的学习曲线如下：\n\n![](./Evolution-Strategies-2017/frame-skip.png)\n\n由曲线可以看出，不同的跳帧间隔，训练效果差不多。**但，我对该鲁棒性测试在复杂环境中的效果表示怀疑。我觉得前沿强化学习算法在该训练场景中使用不同的跳帧间隔也可以得到相同结果。**","source":"_posts/Evolution-Strategies-2017.md","raw":"---\ntitle: Evolution Strategies as a Scalable Alternative to Reinforcement Learning\ncopyright: true\nmathjax: true\ntop: 1\ndate: 2019-05-21 12:38:54\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n这一篇论文讲了强化学习算法的替代可解方案：进化策略。主要思想是对参数空间添加噪音而不是动作空间。\n\n不推荐这篇论文：\n\n- 公式没有详细推理，非常难懂\n- 文中进化策略其实跟强化学习并没有特别大的关系\n- 很多关于进化策略的性质、优势非常难懂，基本上都是文字解释，没有举例\n- 文中措辞不难，但想要理解其本质非常难\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/pdf/1703.03864.pdf](https://arxiv.org/pdf/1703.03864.pdf)\n\n进化策略ES是一组/一类算法，而不是一个算法，它属于黑盒优化方法，它由自然进化中的启发式搜索过程而得来：每一代中都有突变的基因，环境对基因突变的效果给出适应性的判断，重组好的突变基因产生下一代，直到最优。\n\n进化策略算法的划分主要有三个依据：基因如何表示（神经网络参数）、突变如何产生（参数优化过程）、基因如何重组（参数重组）。\n\n进化策略ES这种方法通常分为[直接策略搜索](https://pdfs.semanticscholar.org/dd17/8d3f30d801922c98cec9c2d90db05395f244.pdf?_ga=2.257341323.183297583.1558416128-1251761365.1555224483)和[神经进化](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7307180&tag=1)，黑盒优化方法有很多很好的特性：\n\n1. 不关心奖励分布，奖励密集或稀疏都无所谓\n2. 不需要反向传播梯度\n3. tolerance of potentially arbitrarily long time horizons. 翻译为可以适应长期视野/回报，在长动作序列上有优势\n\n但是，进化策略ES往往不能解决像Q-Learning和PG这样可应用的难的强化学习问题，这篇论文旨在使进化策略可以解决DRL算法可解决的更难的问题。\n\n# 正文精要\n\n> A large source of difficulty in RL stems from the lack of informative gradients of policy performance: such gradients may not exist due to non-smoothness of the environment or policy, or may only be available as high-variance estimates because the environment usually can only be accessed via sampling.  \n\n指出强化学习的难题在于缺乏策略性能的有效梯度：梯度可能由于环境不光滑而不存在、可能由于只能采样环境而存在高方差。\n\n> For MDP-based reinforcement learning algorithms, on the other hand, it is well known that frameskip is a crucial parameter to get right for the optimization to succeed.\n\n对于基于MDP的强化学习算法，**跳帧**是算法优化的关键参数。\n\n> It is common practice in RL to have the agent decide on its actions in a lower frequency than is used in the simulator that runs the environment.\n\nRL通常使智能体在模拟环境中决策频率高于在实际环境中。\n\n---\n\n文中设定一个策略的期望奖励为：\n$$\n\\mathbb{E}_{\\epsilon \\sim N(0,I)}F(\\theta+\\sigma\\epsilon)\n$$\n关于网络参数$\\theta$的导数为：\n$$\n\\nabla_{\\theta}\\mathbb{E}_{\\epsilon \\sim N(0,I)}F(\\theta+\\sigma\\epsilon)=\\frac{1}{\\sigma}\\mathbb{E}_{\\epsilon \\sim N(0,I)} \\{F(\\theta+\\sigma\\epsilon)\\epsilon \\}\n$$\n其中，$\\theta$为网络参数，也可以认为是多变量高斯分布的均值，$\\sigma$为固定方差，$\\epsilon$为扰动向量，由各向同性、方差均为1的多变量高斯分布采样得到。**文中没有对该导数推导过程有介绍，好像是使用了Reinforce Trick的方法，但是却不知道具体如何推导出这个形式**。\n\n文中提到的算法1是对一个策略进行多次扰动，每扰动一次就与环境交互得到一个episode，最后只用各个扰动向量$\\epsilon_{i}$与对应的回报$F(\\theta)$相乘，根据该期望进行参数更新。\n\n![](./Evolution-Strategies-2017/algorithm1.png)\n\n算法2是对算法1的并行化处理，设置相同的随机种子，假设n个worker：\n\n1. 各个worker共用一个策略$\\pi$\n2. 每个worker根据高斯分布采样得到扰动向量$\\epsilon$\n3. 各个worker根据扰动后的策略参数采样一个episode\n4. 互相分发各自的回报\n5. **再采样n个扰动向量$\\epsilon$**，使用梯度上升更新参数，然后分发策略\n\n![](./Evolution-Strategies-2017/algorithm2.png)\n\n文中后边提到，其实不必每次都从高斯分布中采样出扰动向量$\\epsilon$，可以在开始训练前直接采样得到m个扰动向量，每次需要扰动向量时直接根据m的值生成一个随机数，取出以该随机数为下标的扰动向量即可。这么做可以减少更新时的时长消耗。\n\n---\n\n> Experiments on Atari and MuJoCo show that it is a viable option with some attractive features: it is invariant to action frequency and delayed rewards, and it does not need temporal discounting or value function approximation. Most importantly, ES is highly parallelizable, which allows us to make up for a decreased data efficiency by scaling to more parallel workers. \n\n文中使用的进化策略ES的优点：\n\n- 与决策间隔无关，也就是对于跳帧间隔的设置鲁棒性很高\n- 不关心延迟奖励\n- 不需要折扣计算回报\n- 不需要值函数近似\n- 可以高度并行化使，我们能够通过扩展到更多并行训练节点来弥补数据效率的下降。\n\n# 实验发现\n\n1. 使用[Virtual Batch Normalization](https://arxiv.org/pdf/1606.03498.pdf)和神经网络策略重参数（文中没有提到重参数的内容，只提到网络参数的影响）可以极大提升进化策略ES的可靠性。实验中，不使用这两种方法算法很“脆弱”，也就是不稳定。\n2. 进化策略ES可以高度并行化。通过引入一个基于通用随机数的新颖通讯策略，即是是1000个子节点也可以达到运行时间的线性加速。\n3. 进化策略ES的数据效率出奇的好。尽管相比A3C算法需要3-10倍的数据量，但是由于具有不需反向传播、没有值函数等特点，这些轻微的数据效率劣势可以被弥补。实验表明，相同计算量下，1小时ES与1天A3C的效果基本相同。\n4. 进化策略ES相比PG类算法的探索性更强。\n5. 进化策略ES的鲁棒性很好。多种不同训练环境可以使用同一组超参数。\n\n# 实验结果\n\n## MuJoCo\n\n与**高度优化**的TRPO算法相比，ES在离散动作更有优势，因为连续动作在参数扰动方面可能过于平滑并且可能妨碍探索。\n\nES和TRPO的网络结构都是：输入层→64，tanh→64，tanh→输出层。\n\n复杂环境如Hopper和Walker2d中，ES样本复杂性相比TRPO高不到10倍；简单场景中，相比低3倍。\n\nTRPO训练500W步，ES训练至TRPO训练过程中各阶段效果所需步长的比例如表所示：\n\n![](./Evolution-Strategies-2017/mujoco.png)\n\n虽然文中说是简单场景低三倍，其实根本就没有明确的低三倍，而且我对文中所提的简单场景复杂场景的划分也持怀疑态度。\n\n## Atari\n\n预处理、网络架构与Atari那篇论文的一模一样，用A3C使用3.2亿帧训练1天的结果与使用ES训练10亿帧的结果相同（保持计算量相同，因为ES不需要反向传播和值函数评估）。使用720块cpu，训练一个游戏只需1小时。\n\n最终，纯图像输入下，与A3C相比，23个游戏ES胜，28个游戏A3C胜。\n\n![](./Evolution-Strategies-2017/atari.png)\n\n## 并行化 Parallelization\n\nES特别适合并行化，因为其通讯低带宽特性（只需各个worker的回报和随机种子）。\n\n测试环境：3D Humanoid walking task\n\n结果：单机18核需11小时，与最先进的强化学习算法性能相当，80台机器1440个CPU核心只需10分钟。\n\n![](./Evolution-Strategies-2017/parallelization.png)\n\n随着核心数增加，训练性能线性加速。\n\n## “跳帧”测试\n\n将强化学习在模拟环境中训练出的模型用于实际环境中式，通常需要降低其决策频率，也就是加大决策间隔。\n\n如果跳帧设置过大，智能体所做的动作往往不够好，如果跳帧设置过小，会导致每个episode的步数过长，加大计算量，恶化训练过程（其实文中这么说并不严谨）。\n\nES的一个优势是梯度计算与回合长度无关，这间接增加了对跳帧间隔的鲁棒性。在Atari游戏Pong中使用四个不同跳帧间隔{1，2，3，4}的学习曲线如下：\n\n![](./Evolution-Strategies-2017/frame-skip.png)\n\n由曲线可以看出，不同的跳帧间隔，训练效果差不多。**但，我对该鲁棒性测试在复杂环境中的效果表示怀疑。我觉得前沿强化学习算法在该训练场景中使用不同的跳帧间隔也可以得到相同结果。**","slug":"Evolution-Strategies-2017","published":1,"updated":"2019-05-22T11:47:25.386Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma3d002wekvercmn0xoe","content":"<p>这一篇论文讲了强化学习算法的替代可解方案：进化策略。主要思想是对参数空间添加噪音而不是动作空间。</p>\n<p>不推荐这篇论文：</p>\n<ul>\n<li>公式没有详细推理，非常难懂</li>\n<li>文中进化策略其实跟强化学习并没有特别大的关系</li>\n<li>很多关于进化策略的性质、优势非常难懂，基本上都是文字解释，没有举例</li>\n<li>文中措辞不难，但想要理解其本质非常难</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1703.03864.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1703.03864.pdf</a></p>\n<p>进化策略ES是一组/一类算法，而不是一个算法，它属于黑盒优化方法，它由自然进化中的启发式搜索过程而得来：每一代中都有突变的基因，环境对基因突变的效果给出适应性的判断，重组好的突变基因产生下一代，直到最优。</p>\n<p>进化策略算法的划分主要有三个依据：基因如何表示（神经网络参数）、突变如何产生（参数优化过程）、基因如何重组（参数重组）。</p>\n<p>进化策略ES这种方法通常分为<a href=\"https://pdfs.semanticscholar.org/dd17/8d3f30d801922c98cec9c2d90db05395f244.pdf?_ga=2.257341323.183297583.1558416128-1251761365.1555224483\" rel=\"external nofollow\" target=\"_blank\">直接策略搜索</a>和<a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7307180&amp;tag=1\" rel=\"external nofollow\" target=\"_blank\">神经进化</a>，黑盒优化方法有很多很好的特性：</p>\n<ol>\n<li>不关心奖励分布，奖励密集或稀疏都无所谓</li>\n<li>不需要反向传播梯度</li>\n<li>tolerance of potentially arbitrarily long time horizons. 翻译为可以适应长期视野/回报，在长动作序列上有优势</li>\n</ol>\n<p>但是，进化策略ES往往不能解决像Q-Learning和PG这样可应用的难的强化学习问题，这篇论文旨在使进化策略可以解决DRL算法可解决的更难的问题。</p>\n<h1 id=\"正文精要\"><a href=\"#正文精要\" class=\"headerlink\" title=\"正文精要\"></a>正文精要</h1><blockquote>\n<p>A large source of difficulty in RL stems from the lack of informative gradients of policy performance: such gradients may not exist due to non-smoothness of the environment or policy, or may only be available as high-variance estimates because the environment usually can only be accessed via sampling.  </p>\n</blockquote>\n<p>指出强化学习的难题在于缺乏策略性能的有效梯度：梯度可能由于环境不光滑而不存在、可能由于只能采样环境而存在高方差。</p>\n<blockquote>\n<p>For MDP-based reinforcement learning algorithms, on the other hand, it is well known that frameskip is a crucial parameter to get right for the optimization to succeed.</p>\n</blockquote>\n<p>对于基于MDP的强化学习算法，<strong>跳帧</strong>是算法优化的关键参数。</p>\n<blockquote>\n<p>It is common practice in RL to have the agent decide on its actions in a lower frequency than is used in the simulator that runs the environment.</p>\n</blockquote>\n<p>RL通常使智能体在模拟环境中决策频率高于在实际环境中。</p>\n<hr>\n<p>文中设定一个策略的期望奖励为：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbb{E}_{\\epsilon \\sim N(0,I)}F(\\theta+\\sigma\\epsilon)</script><p>关于网络参数$\\theta$的导数为：</p>\n<script type=\"math/tex; mode=display\">\n\\nabla_{\\theta}\\mathbb{E}_{\\epsilon \\sim N(0,I)}F(\\theta+\\sigma\\epsilon)=\\frac{1}{\\sigma}\\mathbb{E}_{\\epsilon \\sim N(0,I)} \\{F(\\theta+\\sigma\\epsilon)\\epsilon \\}</script><p>其中，$\\theta$为网络参数，也可以认为是多变量高斯分布的均值，$\\sigma$为固定方差，$\\epsilon$为扰动向量，由各向同性、方差均为1的多变量高斯分布采样得到。<strong>文中没有对该导数推导过程有介绍，好像是使用了Reinforce Trick的方法，但是却不知道具体如何推导出这个形式</strong>。</p>\n<p>文中提到的算法1是对一个策略进行多次扰动，每扰动一次就与环境交互得到一个episode，最后只用各个扰动向量$\\epsilon_{i}$与对应的回报$F(\\theta)$相乘，根据该期望进行参数更新。</p>\n<p><img src=\"./Evolution-Strategies-2017/algorithm1.png\" alt=\"\"></p>\n<p>算法2是对算法1的并行化处理，设置相同的随机种子，假设n个worker：</p>\n<ol>\n<li>各个worker共用一个策略$\\pi$</li>\n<li>每个worker根据高斯分布采样得到扰动向量$\\epsilon$</li>\n<li>各个worker根据扰动后的策略参数采样一个episode</li>\n<li>互相分发各自的回报</li>\n<li><strong>再采样n个扰动向量$\\epsilon$</strong>，使用梯度上升更新参数，然后分发策略</li>\n</ol>\n<p><img src=\"./Evolution-Strategies-2017/algorithm2.png\" alt=\"\"></p>\n<p>文中后边提到，其实不必每次都从高斯分布中采样出扰动向量$\\epsilon$，可以在开始训练前直接采样得到m个扰动向量，每次需要扰动向量时直接根据m的值生成一个随机数，取出以该随机数为下标的扰动向量即可。这么做可以减少更新时的时长消耗。</p>\n<hr>\n<blockquote>\n<p>Experiments on Atari and MuJoCo show that it is a viable option with some attractive features: it is invariant to action frequency and delayed rewards, and it does not need temporal discounting or value function approximation. Most importantly, ES is highly parallelizable, which allows us to make up for a decreased data efficiency by scaling to more parallel workers. </p>\n</blockquote>\n<p>文中使用的进化策略ES的优点：</p>\n<ul>\n<li>与决策间隔无关，也就是对于跳帧间隔的设置鲁棒性很高</li>\n<li>不关心延迟奖励</li>\n<li>不需要折扣计算回报</li>\n<li>不需要值函数近似</li>\n<li>可以高度并行化使，我们能够通过扩展到更多并行训练节点来弥补数据效率的下降。</li>\n</ul>\n<h1 id=\"实验发现\"><a href=\"#实验发现\" class=\"headerlink\" title=\"实验发现\"></a>实验发现</h1><ol>\n<li>使用<a href=\"https://arxiv.org/pdf/1606.03498.pdf\" rel=\"external nofollow\" target=\"_blank\">Virtual Batch Normalization</a>和神经网络策略重参数（文中没有提到重参数的内容，只提到网络参数的影响）可以极大提升进化策略ES的可靠性。实验中，不使用这两种方法算法很“脆弱”，也就是不稳定。</li>\n<li>进化策略ES可以高度并行化。通过引入一个基于通用随机数的新颖通讯策略，即是是1000个子节点也可以达到运行时间的线性加速。</li>\n<li>进化策略ES的数据效率出奇的好。尽管相比A3C算法需要3-10倍的数据量，但是由于具有不需反向传播、没有值函数等特点，这些轻微的数据效率劣势可以被弥补。实验表明，相同计算量下，1小时ES与1天A3C的效果基本相同。</li>\n<li>进化策略ES相比PG类算法的探索性更强。</li>\n<li>进化策略ES的鲁棒性很好。多种不同训练环境可以使用同一组超参数。</li>\n</ol>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><h2 id=\"MuJoCo\"><a href=\"#MuJoCo\" class=\"headerlink\" title=\"MuJoCo\"></a>MuJoCo</h2><p>与<strong>高度优化</strong>的TRPO算法相比，ES在离散动作更有优势，因为连续动作在参数扰动方面可能过于平滑并且可能妨碍探索。</p>\n<p>ES和TRPO的网络结构都是：输入层→64，tanh→64，tanh→输出层。</p>\n<p>复杂环境如Hopper和Walker2d中，ES样本复杂性相比TRPO高不到10倍；简单场景中，相比低3倍。</p>\n<p>TRPO训练500W步，ES训练至TRPO训练过程中各阶段效果所需步长的比例如表所示：</p>\n<p><img src=\"./Evolution-Strategies-2017/mujoco.png\" alt=\"\"></p>\n<p>虽然文中说是简单场景低三倍，其实根本就没有明确的低三倍，而且我对文中所提的简单场景复杂场景的划分也持怀疑态度。</p>\n<h2 id=\"Atari\"><a href=\"#Atari\" class=\"headerlink\" title=\"Atari\"></a>Atari</h2><p>预处理、网络架构与Atari那篇论文的一模一样，用A3C使用3.2亿帧训练1天的结果与使用ES训练10亿帧的结果相同（保持计算量相同，因为ES不需要反向传播和值函数评估）。使用720块cpu，训练一个游戏只需1小时。</p>\n<p>最终，纯图像输入下，与A3C相比，23个游戏ES胜，28个游戏A3C胜。</p>\n<p><img src=\"./Evolution-Strategies-2017/atari.png\" alt=\"\"></p>\n<h2 id=\"并行化-Parallelization\"><a href=\"#并行化-Parallelization\" class=\"headerlink\" title=\"并行化 Parallelization\"></a>并行化 Parallelization</h2><p>ES特别适合并行化，因为其通讯低带宽特性（只需各个worker的回报和随机种子）。</p>\n<p>测试环境：3D Humanoid walking task</p>\n<p>结果：单机18核需11小时，与最先进的强化学习算法性能相当，80台机器1440个CPU核心只需10分钟。</p>\n<p><img src=\"./Evolution-Strategies-2017/parallelization.png\" alt=\"\"></p>\n<p>随着核心数增加，训练性能线性加速。</p>\n<h2 id=\"“跳帧”测试\"><a href=\"#“跳帧”测试\" class=\"headerlink\" title=\"“跳帧”测试\"></a>“跳帧”测试</h2><p>将强化学习在模拟环境中训练出的模型用于实际环境中式，通常需要降低其决策频率，也就是加大决策间隔。</p>\n<p>如果跳帧设置过大，智能体所做的动作往往不够好，如果跳帧设置过小，会导致每个episode的步数过长，加大计算量，恶化训练过程（其实文中这么说并不严谨）。</p>\n<p>ES的一个优势是梯度计算与回合长度无关，这间接增加了对跳帧间隔的鲁棒性。在Atari游戏Pong中使用四个不同跳帧间隔{1，2，3，4}的学习曲线如下：</p>\n<p><img src=\"./Evolution-Strategies-2017/frame-skip.png\" alt=\"\"></p>\n<p>由曲线可以看出，不同的跳帧间隔，训练效果差不多。<strong>但，我对该鲁棒性测试在复杂环境中的效果表示怀疑。我觉得前沿强化学习算法在该训练场景中使用不同的跳帧间隔也可以得到相同结果。</strong></p>\n","site":{"data":{}},"excerpt":"<p>这一篇论文讲了强化学习算法的替代可解方案：进化策略。主要思想是对参数空间添加噪音而不是动作空间。</p>\n<p>不推荐这篇论文：</p>\n<ul>\n<li>公式没有详细推理，非常难懂</li>\n<li>文中进化策略其实跟强化学习并没有特别大的关系</li>\n<li>很多关于进化策略的性质、优势非常难懂，基本上都是文字解释，没有举例</li>\n<li>文中措辞不难，但想要理解其本质非常难</li>\n</ul>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/pdf/1703.03864.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1703.03864.pdf</a></p>\n<p>进化策略ES是一组/一类算法，而不是一个算法，它属于黑盒优化方法，它由自然进化中的启发式搜索过程而得来：每一代中都有突变的基因，环境对基因突变的效果给出适应性的判断，重组好的突变基因产生下一代，直到最优。</p>\n<p>进化策略算法的划分主要有三个依据：基因如何表示（神经网络参数）、突变如何产生（参数优化过程）、基因如何重组（参数重组）。</p>\n<p>进化策略ES这种方法通常分为<a href=\"https://pdfs.semanticscholar.org/dd17/8d3f30d801922c98cec9c2d90db05395f244.pdf?_ga=2.257341323.183297583.1558416128-1251761365.1555224483\" rel=\"external nofollow\" target=\"_blank\">直接策略搜索</a>和<a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7307180&amp;tag=1\" rel=\"external nofollow\" target=\"_blank\">神经进化</a>，黑盒优化方法有很多很好的特性：</p>\n<ol>\n<li>不关心奖励分布，奖励密集或稀疏都无所谓</li>\n<li>不需要反向传播梯度</li>\n<li>tolerance of potentially arbitrarily long time horizons. 翻译为可以适应长期视野/回报，在长动作序列上有优势</li>\n</ol>\n<p>但是，进化策略ES往往不能解决像Q-Learning和PG这样可应用的难的强化学习问题，这篇论文旨在使进化策略可以解决DRL算法可解决的更难的问题。</p>\n<h1 id=\"正文精要\"><a href=\"#正文精要\" class=\"headerlink\" title=\"正文精要\"></a>正文精要</h1><blockquote>\n<p>A large source of difficulty in RL stems from the lack of informative gradients of policy performance: such gradients may not exist due to non-smoothness of the environment or policy, or may only be available as high-variance estimates because the environment usually can only be accessed via sampling.  </p>\n</blockquote>\n<p>指出强化学习的难题在于缺乏策略性能的有效梯度：梯度可能由于环境不光滑而不存在、可能由于只能采样环境而存在高方差。</p>\n<blockquote>\n<p>For MDP-based reinforcement learning algorithms, on the other hand, it is well known that frameskip is a crucial parameter to get right for the optimization to succeed.</p>\n</blockquote>\n<p>对于基于MDP的强化学习算法，<strong>跳帧</strong>是算法优化的关键参数。</p>\n<blockquote>\n<p>It is common practice in RL to have the agent decide on its actions in a lower frequency than is used in the simulator that runs the environment.</p>\n</blockquote>\n<p>RL通常使智能体在模拟环境中决策频率高于在实际环境中。</p>\n<hr>\n<p>文中设定一个策略的期望奖励为：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbb{E}_{\\epsilon \\sim N(0,I)}F(\\theta+\\sigma\\epsilon)</script><p>关于网络参数$\\theta$的导数为：</p>\n<script type=\"math/tex; mode=display\">\n\\nabla_{\\theta}\\mathbb{E}_{\\epsilon \\sim N(0,I)}F(\\theta+\\sigma\\epsilon)=\\frac{1}{\\sigma}\\mathbb{E}_{\\epsilon \\sim N(0,I)} \\{F(\\theta+\\sigma\\epsilon)\\epsilon \\}</script><p>其中，$\\theta$为网络参数，也可以认为是多变量高斯分布的均值，$\\sigma$为固定方差，$\\epsilon$为扰动向量，由各向同性、方差均为1的多变量高斯分布采样得到。<strong>文中没有对该导数推导过程有介绍，好像是使用了Reinforce Trick的方法，但是却不知道具体如何推导出这个形式</strong>。</p>\n<p>文中提到的算法1是对一个策略进行多次扰动，每扰动一次就与环境交互得到一个episode，最后只用各个扰动向量$\\epsilon_{i}$与对应的回报$F(\\theta)$相乘，根据该期望进行参数更新。</p>\n<p><img src=\"./Evolution-Strategies-2017/algorithm1.png\" alt=\"\"></p>\n<p>算法2是对算法1的并行化处理，设置相同的随机种子，假设n个worker：</p>\n<ol>\n<li>各个worker共用一个策略$\\pi$</li>\n<li>每个worker根据高斯分布采样得到扰动向量$\\epsilon$</li>\n<li>各个worker根据扰动后的策略参数采样一个episode</li>\n<li>互相分发各自的回报</li>\n<li><strong>再采样n个扰动向量$\\epsilon$</strong>，使用梯度上升更新参数，然后分发策略</li>\n</ol>\n<p><img src=\"./Evolution-Strategies-2017/algorithm2.png\" alt=\"\"></p>\n<p>文中后边提到，其实不必每次都从高斯分布中采样出扰动向量$\\epsilon$，可以在开始训练前直接采样得到m个扰动向量，每次需要扰动向量时直接根据m的值生成一个随机数，取出以该随机数为下标的扰动向量即可。这么做可以减少更新时的时长消耗。</p>\n<hr>\n<blockquote>\n<p>Experiments on Atari and MuJoCo show that it is a viable option with some attractive features: it is invariant to action frequency and delayed rewards, and it does not need temporal discounting or value function approximation. Most importantly, ES is highly parallelizable, which allows us to make up for a decreased data efficiency by scaling to more parallel workers. </p>\n</blockquote>\n<p>文中使用的进化策略ES的优点：</p>\n<ul>\n<li>与决策间隔无关，也就是对于跳帧间隔的设置鲁棒性很高</li>\n<li>不关心延迟奖励</li>\n<li>不需要折扣计算回报</li>\n<li>不需要值函数近似</li>\n<li>可以高度并行化使，我们能够通过扩展到更多并行训练节点来弥补数据效率的下降。</li>\n</ul>\n<h1 id=\"实验发现\"><a href=\"#实验发现\" class=\"headerlink\" title=\"实验发现\"></a>实验发现</h1><ol>\n<li>使用<a href=\"https://arxiv.org/pdf/1606.03498.pdf\" rel=\"external nofollow\" target=\"_blank\">Virtual Batch Normalization</a>和神经网络策略重参数（文中没有提到重参数的内容，只提到网络参数的影响）可以极大提升进化策略ES的可靠性。实验中，不使用这两种方法算法很“脆弱”，也就是不稳定。</li>\n<li>进化策略ES可以高度并行化。通过引入一个基于通用随机数的新颖通讯策略，即是是1000个子节点也可以达到运行时间的线性加速。</li>\n<li>进化策略ES的数据效率出奇的好。尽管相比A3C算法需要3-10倍的数据量，但是由于具有不需反向传播、没有值函数等特点，这些轻微的数据效率劣势可以被弥补。实验表明，相同计算量下，1小时ES与1天A3C的效果基本相同。</li>\n<li>进化策略ES相比PG类算法的探索性更强。</li>\n<li>进化策略ES的鲁棒性很好。多种不同训练环境可以使用同一组超参数。</li>\n</ol>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><h2 id=\"MuJoCo\"><a href=\"#MuJoCo\" class=\"headerlink\" title=\"MuJoCo\"></a>MuJoCo</h2><p>与<strong>高度优化</strong>的TRPO算法相比，ES在离散动作更有优势，因为连续动作在参数扰动方面可能过于平滑并且可能妨碍探索。</p>\n<p>ES和TRPO的网络结构都是：输入层→64，tanh→64，tanh→输出层。</p>\n<p>复杂环境如Hopper和Walker2d中，ES样本复杂性相比TRPO高不到10倍；简单场景中，相比低3倍。</p>\n<p>TRPO训练500W步，ES训练至TRPO训练过程中各阶段效果所需步长的比例如表所示：</p>\n<p><img src=\"./Evolution-Strategies-2017/mujoco.png\" alt=\"\"></p>\n<p>虽然文中说是简单场景低三倍，其实根本就没有明确的低三倍，而且我对文中所提的简单场景复杂场景的划分也持怀疑态度。</p>\n<h2 id=\"Atari\"><a href=\"#Atari\" class=\"headerlink\" title=\"Atari\"></a>Atari</h2><p>预处理、网络架构与Atari那篇论文的一模一样，用A3C使用3.2亿帧训练1天的结果与使用ES训练10亿帧的结果相同（保持计算量相同，因为ES不需要反向传播和值函数评估）。使用720块cpu，训练一个游戏只需1小时。</p>\n<p>最终，纯图像输入下，与A3C相比，23个游戏ES胜，28个游戏A3C胜。</p>\n<p><img src=\"./Evolution-Strategies-2017/atari.png\" alt=\"\"></p>\n<h2 id=\"并行化-Parallelization\"><a href=\"#并行化-Parallelization\" class=\"headerlink\" title=\"并行化 Parallelization\"></a>并行化 Parallelization</h2><p>ES特别适合并行化，因为其通讯低带宽特性（只需各个worker的回报和随机种子）。</p>\n<p>测试环境：3D Humanoid walking task</p>\n<p>结果：单机18核需11小时，与最先进的强化学习算法性能相当，80台机器1440个CPU核心只需10分钟。</p>\n<p><img src=\"./Evolution-Strategies-2017/parallelization.png\" alt=\"\"></p>\n<p>随着核心数增加，训练性能线性加速。</p>\n<h2 id=\"“跳帧”测试\"><a href=\"#“跳帧”测试\" class=\"headerlink\" title=\"“跳帧”测试\"></a>“跳帧”测试</h2><p>将强化学习在模拟环境中训练出的模型用于实际环境中式，通常需要降低其决策频率，也就是加大决策间隔。</p>\n<p>如果跳帧设置过大，智能体所做的动作往往不够好，如果跳帧设置过小，会导致每个episode的步数过长，加大计算量，恶化训练过程（其实文中这么说并不严谨）。</p>\n<p>ES的一个优势是梯度计算与回合长度无关，这间接增加了对跳帧间隔的鲁棒性。在Atari游戏Pong中使用四个不同跳帧间隔{1，2，3，4}的学习曲线如下：</p>\n<p><img src=\"./Evolution-Strategies-2017/frame-skip.png\" alt=\"\"></p>\n<p>由曲线可以看出，不同的跳帧间隔，训练效果差不多。<strong>但，我对该鲁棒性测试在复杂环境中的效果表示怀疑。我觉得前沿强化学习算法在该训练场景中使用不同的跳帧间隔也可以得到相同结果。</strong></p>"},{"title":"创建ML-Agents的Docker镜像","copyright":true,"top":1,"date":"2019-01-04T02:38:59.000Z","_content":"\n# 创建ML-Agents的Docker镜像\n\n<!--more-->\n\n## 前言\n\n  如果需要在镜像中使用GPU训练,可以将Nvidia的官方镜像作为基础镜像,`Dockerfile`如下:\n```\nFROM nvidia/cuda:9.0-base-ubuntu16.04\nLABEL maintainer \"NVIDIA CORPORATION <cudatools@nvidia.com>\"\n\nENV NCCL_VERSION 2.3.7\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n\tapt-utils \\\n        cuda-libraries-$CUDA_PKG_VERSION \\\n        cuda-cublas-9-0=9.0.176.4-1 \\\n        libnccl2=$NCCL_VERSION-1+cuda9.0 && \\\n    apt-mark hold libnccl2 && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN apt-get update && apt-get install -y openssh-server\n\nRUN apt-get install -y nano\n\nRUN mkdir /var/run/sshd\n\nRUN echo \"root:1234\" | chpasswd\n\nRUN sed -i 's/prohibit-password/yes/g' /etc/ssh/sshd_config\n\nEXPOSE 22\n\nENTRYPOINT [\"/usr/sbin/sshd\",\"-D\"]\n```\n\n## ML-Agents v0.6.0\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-44-58.png)\n\n### 环境\n\n本机环境\n\n- ML-Agents 0.6.0\n- Windows 10 专业版\n- docker client version 18.09.0\n- docker server version 18.09.0\n\n平台\n- [机器学习平台](http://10.0.4.228)\n\n### 创建镜像\n\n1. 打开`~/ml-agents-0.6.0/`目录,看到有一个官方给定的`Dockerfile`\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-52-49.png)\n2. 直接`Build`,在该目录下运行`docker build -t [name]:[tag] .`,一定要注意最后的`.`,很**重要**\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-29-30.png)\n3. 新建一个`sources.list`文件,为镜像内换源,因为将来有可能需要在容器内安装某些包,有一些国外的资源往往会下载失败,所以需要**换源**\n- 新建一个`sources.list`\n- 用文本编辑器打开,写入以下内容\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-36-31.png)\n```\n# deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted\ndeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties\ndeb http://archive.canonical.com/ubuntu xenial partner\ndeb-src http://archive.canonical.com/ubuntu xenial partner\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse\n```\n- 保存退出\n4. 新建一个`DockerfilePlus`,在官方生成的基础镜像上安装一些可以在平台上运行的包,`openssh-server`,联网工具`net-tools`,心爱的`apt-file`等等\n- 新建一个`DockerfilePlus`\n- 用文本编辑器打开,输入以下内容\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-01-32.png)\n```\nFROM hub.hoc.ccshu.net/wjs/mlunityv060:v0.1\n\nRUN cp /etc/apt/sources.list /etc/apt/sources.list.bak\nCOPY sources.list /etc/apt/sources.list\n\nENV PYTHONPATH /ml-agents:$PYTHONPATH\n\nRUN apt-get update && apt-get install -y \\\n        apt-file \\\n        nano \\\n        net-tools \\\n        iputils-ping \\\n        openssh-server \\\n        apt-utils \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && mkdir /var/run/sshd \\\n    && echo \"root:1234\" | chpasswd \\\n    && sed -i 's/prohibit-password/yes/g' /etc/ssh/sshd_config\n\nEXPOSE 22\n\nENTRYPOINT [\"/usr/sbin/sshd\",\"-D\"]\n```\n5. 在`DockerfilePlus`所在文件夹下,执行`build -t [name]:[tag] -f DockerfilePlus .`\n\n**为了使用GPU.改写完的Dockerfile如下(不需要看):**\n```\nFROM nvidia/cuda:9.0-base-ubuntu16.04\nLABEL maintainer \"Keavnn <https://stepneverstop.github.io>\"\n\nENV NCCL_VERSION 2.3.7\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n\tapt-utils \\\n        cuda-libraries-$CUDA_PKG_VERSION \\\n        cuda-cublas-9-0=9.0.176.4-1 \\\n        libnccl2=$NCCL_VERSION-1+cuda9.0 && \\\n    apt-mark hold libnccl2 && \\\n    rm -rf /var/lib/apt/lists/*\n\n\n# ensure local python is preferred over distribution python\nENV PATH /usr/local/bin:$PATH\n\n# http://bugs.python.org/issue19846\n# > At the moment, setting \"LANG=C\" on a Linux system *fundamentally breaks Python 3*, and that's not OK.\nENV LANG C.UTF-8\n\n# runtime dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n\t\tca-certificates \\\n\t\tlibexpat1 \\\n\t\tlibffi6 \\\n\t\tlibgdbm3 \\\n\t\tlibreadline6 \\\n\t\tlibsqlite3-0 \\\n\t\tlibssl1.0.0 \\\n\t&& rm -rf /var/lib/apt/lists/*\n\nENV GPG_KEY 0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D\nENV PYTHON_VERSION 3.6.4\n\nRUN set -ex \\\n\t&& buildDeps=\" \\\n\t\tdpkg-dev \\\n\t\tgcc \\\n\t\tlibbz2-dev \\\n\t\tlibc6-dev \\\n\t\tlibexpat1-dev \\\n\t\tlibffi-dev \\\n\t\tlibgdbm-dev \\\n\t\tliblzma-dev \\\n\t\tlibncursesw5-dev \\\n\t\tlibreadline-dev \\\n\t\tlibsqlite3-dev \\\n\t\tlibssl-dev \\\n\t\tmake \\\n\t\ttcl-dev \\\n\t\ttk-dev \\\n\t\twget \\\n\t\txz-utils \\\n\t\tzlib1g-dev \\\n# as of Stretch, \"gpg\" is no longer included by default\n\t\t$(command -v gpg > /dev/null || echo 'gnupg dirmngr') \\\n\t\" \\\n\t&& apt-get update && apt-get install -y $buildDeps --no-install-recommends && rm -rf /var/lib/apt/lists/* \\\n\t\\\n\t&& wget -O python.tar.xz \"https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz\" \\\n\t&& wget -O python.tar.xz.asc \"https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz.asc\" \\\n\t&& export GNUPGHOME=\"$(mktemp -d)\" \\\n\t&& gpg --keyserver ha.pool.sks-keyservers.net --recv-keys \"$GPG_KEY\" \\\n\t&& gpg --batch --verify python.tar.xz.asc python.tar.xz \\\n\t&& rm -rf \"$GNUPGHOME\" python.tar.xz.asc \\\n\t&& mkdir -p /usr/src/python \\\n\t&& tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz \\\n\t&& rm python.tar.xz \\\n\t\\\n\t&& cd /usr/src/python \\\n\t&& gnuArch=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" \\\n\t&& ./configure \\\n\t\t--build=\"$gnuArch\" \\\n\t\t--enable-loadable-sqlite-extensions \\\n\t\t--enable-shared \\\n\t\t--with-system-expat \\\n\t\t--with-system-ffi \\\n\t\t--without-ensurepip \\\n\t&& make -j \"$(nproc)\" \\\n\t&& make install \\\n\t&& ldconfig \\\n\t\\\n\t&& apt-get purge -y --auto-remove $buildDeps \\\n\t\\\n\t&& find /usr/local -depth \\\n\t\t\\( \\\n\t\t\t\\( -type d -a \\( -name test -o -name tests \\) \\) \\\n\t\t\t-o \\\n\t\t\t\\( -type f -a \\( -name '*.pyc' -o -name '*.pyo' \\) \\) \\\n\t\t\\) -exec rm -rf '{}' + \\\n\t&& rm -rf /usr/src/python\n\n# make some useful symlinks that are expected to exist\nRUN cd /usr/local/bin \\\n\t&& ln -s idle3 idle \\\n\t&& ln -s pydoc3 pydoc \\\n\t&& ln -s python3 python \\\n\t&& ln -s python3-config python-config\n\n# if this is called \"PIP_VERSION\", pip explodes with \"ValueError: invalid truth value '<VERSION>'\"\nENV PYTHON_PIP_VERSION 9.0.3\n\nRUN set -ex; \\\n\t\\\n\tapt-get update; \\\n\tapt-get install -y --no-install-recommends wget; \\\n\trm -rf /var/lib/apt/lists/*; \\\n\t\\\n\twget -O get-pip.py 'https://bootstrap.pypa.io/get-pip.py'; \\\n\t\\\n\tapt-get purge -y --auto-remove wget; \\\n\t\\\n\tpython get-pip.py \\\n\t\t--disable-pip-version-check \\\n\t\t--no-cache-dir \\\n\t\t\"pip==$PYTHON_PIP_VERSION\" \\\n\t; \\\n\tpip --version; \\\n\t\\\n\tfind /usr/local -depth \\\n\t\t\\( \\\n\t\t\t\\( -type d -a \\( -name test -o -name tests \\) \\) \\\n\t\t\t-o \\\n\t\t\t\\( -type f -a \\( -name '*.pyc' -o -name '*.pyo' \\) \\) \\\n\t\t\\) -exec rm -rf '{}' +; \\\n\trm -f get-pip.py\n\n\nRUN apt-get update && apt-get -y upgrade\n\n# xvfb is used to do CPU based rendering of Unity\nRUN apt-get install -y xvfb\n\nCOPY ml-agents /ml-agents\nWORKDIR /ml-agents\nRUN pip install .\n\n# port 5005 is the port used in in Editor training.\nEXPOSE 5005\n\nRUN cp /etc/apt/sources.list /etc/apt/sources.list.bak\nCOPY sources.list /etc/apt/sources.list\n\nENV PYTHONPATH /ml-agents:$PYTHONPATH\n\nRUN apt-get update && apt-get install -y \\\n        apt-file \\\n        nano \\\n        net-tools \\\n        iputils-ping \\\n        openssh-server \\\n        apt-utils \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && mkdir /var/run/sshd \\\n    && echo \"root:1234\" | chpasswd \\\n    && sed -i 's/prohibit-password/yes/g' /etc/ssh/sshd_config\n\nEXPOSE 22\n\nENTRYPOINT [\"/usr/sbin/sshd\",\"-D\"]\n```\n\n### PUSH镜像\n\n`docker push [name]:[tag]`\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-02-47.png)\n\n### 测试镜像\n\n- 登录[机器学习平台](http://10.0.4.228),没有使用平台的可以在本地使用`docker run`直接开启容器\n- 先测试使用容器的方式\n  - 创建容器\n  ![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-28-19.png)\n  将要运行和储存的文件夹放在数据卷`data`下,这个目录要在运行时由`--docker-target-name`指定\n  - 等待容器创建成功\n  ![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-15.png)\n  - 容器创建成功后进入容器\n  ![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-52.png)\n  - 执行`mlagents-learn trainer_config.yaml --docker-target-name=data/unity-volume --env=3dball --train --run-id=test --save-freq=5000 | tee /data/unity-volume/log.txt`,如果不想在屏幕输出,可以在后边加上`>/dev/null`\n  ![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-16-44.png)\n  ![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-18-40.png)\n  \n---\n\n## 安装Miniconda\n\n确保你的安装包放在了data文件夹下\n`apt-get update && apt-get install bzip2 -y && cd /data && bash Miniconda3-latest-Linux-x86_64.sh`\n\n一路按回车、yes等等就成功了.如果需要安装到指定目录,在安装过程中会有提示告诉你让你指定安装路径\n\n*注意*\n\n在[学校机器学习平台](http://10.0.4.228/)上使用时,如果是使用容器的方式,那么新开的容器就可以使用`conda`命令,不存在`conda:command not found`的错误信息.\n\n但是,如果在平台上以**提交任务**的形式来使用带conda的镜像所产生的容器时,就算是在镜像中配置了`echo 'export PATH=\"~/anaconda3/bin:$PATH\"' >> ~/.bashrc`,当提交任务时环境变量中仍然没有`~/anaconda3/bin`,这个问题目前没有找到比较方便的解决办法,目前所采用的方式是:\n\n在提交任务时, 首先加上命令`export PATH=\"~/anaconda3/bin:$PATH\" && `\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-25-17.png)\n\n接着又实验了一下`echo 'export PATH=\"~/anaconda3/bin:$PATH\"' >> /etc/profile`, 正常来说, 如果在容器中这样设置环境变量, 等待下次从镜像创建容器时, 这个环境变量一般并不会生效, 但是不知道这样设置对于提交任务方式来说有没有效, 索性试了一下\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-35-56.png)\n\n根据输出日志来看, 这种方式也并没有奏效\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-39-26.png)\n\n当然, 如果觉得上述配置比较麻烦的话,可以使用`Dockerfile`的`ENV`命令来设置环境变量, 这样设置99%是不会有问题的\n\n```\nFROM hub.hoc.ccshu.net/wjs/mlunityv060:v1.4.5\nENV PATH /usr/miniconda3/bin:$PATH\n```\n实验了一下,\n结果如下:\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_15-02-16.png)\n\n表示可以使用`conda`命令, 但是不能使用`conda activate`命令激活环境\n\n根据错误信息, 在`Dockerfile`中写入以下代码也不可行:\n\n`RUN ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh`\n\n查了一下相关资料,发现在4.5版本的conda是无解的\n[https://github.com/ContinuumIO/docker-images/issues/89](https://github.com/ContinuumIO/docker-images/issues/89)\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_22-38-42.png)\n\n希望4.6版本可以解决吧\n\n**更新2019年1月14日14:26:01**\n**已解决**\n\n4.6版本的确可以解决以提交任务模式运行时的问题, 需要使用命令`conda run -n [环境名字] [要执行的命令]`, 而不是使用`conda activate [环境名字]`先激活一个环境.\n\n不过, 更新至4.6版本需要相关配置\n```\nconda config --add channels conda-canary\nconda update conda\n```\n\n我使用Dockerfile来生成镜像, 代码如下:\n```\nFROM hub.hoc.ccshu.net/wjs/mlunityv060:v1.4.6\n\nENV PATH /usr/miniconda3/bin:$PATH\nRUN conda config --add channels conda-canary && conda update conda -y\n```\n原因是, 使用Dockerfile比较容易设置环境变量, 减少出错\n","source":"_posts/创建ML-Agents的Docker镜像.md","raw":"---\ntitle: 创建ML-Agents的Docker镜像\ncopyright: true\ntop: 1\ndate: 2019-01-04 10:38:59\ncategories:\n- Docker\n- Unity\ntags:\n- docker\n- unity\n- ml-agents\n\n---\n\n# 创建ML-Agents的Docker镜像\n\n<!--more-->\n\n## 前言\n\n  如果需要在镜像中使用GPU训练,可以将Nvidia的官方镜像作为基础镜像,`Dockerfile`如下:\n```\nFROM nvidia/cuda:9.0-base-ubuntu16.04\nLABEL maintainer \"NVIDIA CORPORATION <cudatools@nvidia.com>\"\n\nENV NCCL_VERSION 2.3.7\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n\tapt-utils \\\n        cuda-libraries-$CUDA_PKG_VERSION \\\n        cuda-cublas-9-0=9.0.176.4-1 \\\n        libnccl2=$NCCL_VERSION-1+cuda9.0 && \\\n    apt-mark hold libnccl2 && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN apt-get update && apt-get install -y openssh-server\n\nRUN apt-get install -y nano\n\nRUN mkdir /var/run/sshd\n\nRUN echo \"root:1234\" | chpasswd\n\nRUN sed -i 's/prohibit-password/yes/g' /etc/ssh/sshd_config\n\nEXPOSE 22\n\nENTRYPOINT [\"/usr/sbin/sshd\",\"-D\"]\n```\n\n## ML-Agents v0.6.0\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-44-58.png)\n\n### 环境\n\n本机环境\n\n- ML-Agents 0.6.0\n- Windows 10 专业版\n- docker client version 18.09.0\n- docker server version 18.09.0\n\n平台\n- [机器学习平台](http://10.0.4.228)\n\n### 创建镜像\n\n1. 打开`~/ml-agents-0.6.0/`目录,看到有一个官方给定的`Dockerfile`\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-52-49.png)\n2. 直接`Build`,在该目录下运行`docker build -t [name]:[tag] .`,一定要注意最后的`.`,很**重要**\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-29-30.png)\n3. 新建一个`sources.list`文件,为镜像内换源,因为将来有可能需要在容器内安装某些包,有一些国外的资源往往会下载失败,所以需要**换源**\n- 新建一个`sources.list`\n- 用文本编辑器打开,写入以下内容\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-36-31.png)\n```\n# deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted\ndeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties\ndeb http://archive.canonical.com/ubuntu xenial partner\ndeb-src http://archive.canonical.com/ubuntu xenial partner\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse\n```\n- 保存退出\n4. 新建一个`DockerfilePlus`,在官方生成的基础镜像上安装一些可以在平台上运行的包,`openssh-server`,联网工具`net-tools`,心爱的`apt-file`等等\n- 新建一个`DockerfilePlus`\n- 用文本编辑器打开,输入以下内容\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-01-32.png)\n```\nFROM hub.hoc.ccshu.net/wjs/mlunityv060:v0.1\n\nRUN cp /etc/apt/sources.list /etc/apt/sources.list.bak\nCOPY sources.list /etc/apt/sources.list\n\nENV PYTHONPATH /ml-agents:$PYTHONPATH\n\nRUN apt-get update && apt-get install -y \\\n        apt-file \\\n        nano \\\n        net-tools \\\n        iputils-ping \\\n        openssh-server \\\n        apt-utils \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && mkdir /var/run/sshd \\\n    && echo \"root:1234\" | chpasswd \\\n    && sed -i 's/prohibit-password/yes/g' /etc/ssh/sshd_config\n\nEXPOSE 22\n\nENTRYPOINT [\"/usr/sbin/sshd\",\"-D\"]\n```\n5. 在`DockerfilePlus`所在文件夹下,执行`build -t [name]:[tag] -f DockerfilePlus .`\n\n**为了使用GPU.改写完的Dockerfile如下(不需要看):**\n```\nFROM nvidia/cuda:9.0-base-ubuntu16.04\nLABEL maintainer \"Keavnn <https://stepneverstop.github.io>\"\n\nENV NCCL_VERSION 2.3.7\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n\tapt-utils \\\n        cuda-libraries-$CUDA_PKG_VERSION \\\n        cuda-cublas-9-0=9.0.176.4-1 \\\n        libnccl2=$NCCL_VERSION-1+cuda9.0 && \\\n    apt-mark hold libnccl2 && \\\n    rm -rf /var/lib/apt/lists/*\n\n\n# ensure local python is preferred over distribution python\nENV PATH /usr/local/bin:$PATH\n\n# http://bugs.python.org/issue19846\n# > At the moment, setting \"LANG=C\" on a Linux system *fundamentally breaks Python 3*, and that's not OK.\nENV LANG C.UTF-8\n\n# runtime dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n\t\tca-certificates \\\n\t\tlibexpat1 \\\n\t\tlibffi6 \\\n\t\tlibgdbm3 \\\n\t\tlibreadline6 \\\n\t\tlibsqlite3-0 \\\n\t\tlibssl1.0.0 \\\n\t&& rm -rf /var/lib/apt/lists/*\n\nENV GPG_KEY 0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D\nENV PYTHON_VERSION 3.6.4\n\nRUN set -ex \\\n\t&& buildDeps=\" \\\n\t\tdpkg-dev \\\n\t\tgcc \\\n\t\tlibbz2-dev \\\n\t\tlibc6-dev \\\n\t\tlibexpat1-dev \\\n\t\tlibffi-dev \\\n\t\tlibgdbm-dev \\\n\t\tliblzma-dev \\\n\t\tlibncursesw5-dev \\\n\t\tlibreadline-dev \\\n\t\tlibsqlite3-dev \\\n\t\tlibssl-dev \\\n\t\tmake \\\n\t\ttcl-dev \\\n\t\ttk-dev \\\n\t\twget \\\n\t\txz-utils \\\n\t\tzlib1g-dev \\\n# as of Stretch, \"gpg\" is no longer included by default\n\t\t$(command -v gpg > /dev/null || echo 'gnupg dirmngr') \\\n\t\" \\\n\t&& apt-get update && apt-get install -y $buildDeps --no-install-recommends && rm -rf /var/lib/apt/lists/* \\\n\t\\\n\t&& wget -O python.tar.xz \"https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz\" \\\n\t&& wget -O python.tar.xz.asc \"https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz.asc\" \\\n\t&& export GNUPGHOME=\"$(mktemp -d)\" \\\n\t&& gpg --keyserver ha.pool.sks-keyservers.net --recv-keys \"$GPG_KEY\" \\\n\t&& gpg --batch --verify python.tar.xz.asc python.tar.xz \\\n\t&& rm -rf \"$GNUPGHOME\" python.tar.xz.asc \\\n\t&& mkdir -p /usr/src/python \\\n\t&& tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz \\\n\t&& rm python.tar.xz \\\n\t\\\n\t&& cd /usr/src/python \\\n\t&& gnuArch=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" \\\n\t&& ./configure \\\n\t\t--build=\"$gnuArch\" \\\n\t\t--enable-loadable-sqlite-extensions \\\n\t\t--enable-shared \\\n\t\t--with-system-expat \\\n\t\t--with-system-ffi \\\n\t\t--without-ensurepip \\\n\t&& make -j \"$(nproc)\" \\\n\t&& make install \\\n\t&& ldconfig \\\n\t\\\n\t&& apt-get purge -y --auto-remove $buildDeps \\\n\t\\\n\t&& find /usr/local -depth \\\n\t\t\\( \\\n\t\t\t\\( -type d -a \\( -name test -o -name tests \\) \\) \\\n\t\t\t-o \\\n\t\t\t\\( -type f -a \\( -name '*.pyc' -o -name '*.pyo' \\) \\) \\\n\t\t\\) -exec rm -rf '{}' + \\\n\t&& rm -rf /usr/src/python\n\n# make some useful symlinks that are expected to exist\nRUN cd /usr/local/bin \\\n\t&& ln -s idle3 idle \\\n\t&& ln -s pydoc3 pydoc \\\n\t&& ln -s python3 python \\\n\t&& ln -s python3-config python-config\n\n# if this is called \"PIP_VERSION\", pip explodes with \"ValueError: invalid truth value '<VERSION>'\"\nENV PYTHON_PIP_VERSION 9.0.3\n\nRUN set -ex; \\\n\t\\\n\tapt-get update; \\\n\tapt-get install -y --no-install-recommends wget; \\\n\trm -rf /var/lib/apt/lists/*; \\\n\t\\\n\twget -O get-pip.py 'https://bootstrap.pypa.io/get-pip.py'; \\\n\t\\\n\tapt-get purge -y --auto-remove wget; \\\n\t\\\n\tpython get-pip.py \\\n\t\t--disable-pip-version-check \\\n\t\t--no-cache-dir \\\n\t\t\"pip==$PYTHON_PIP_VERSION\" \\\n\t; \\\n\tpip --version; \\\n\t\\\n\tfind /usr/local -depth \\\n\t\t\\( \\\n\t\t\t\\( -type d -a \\( -name test -o -name tests \\) \\) \\\n\t\t\t-o \\\n\t\t\t\\( -type f -a \\( -name '*.pyc' -o -name '*.pyo' \\) \\) \\\n\t\t\\) -exec rm -rf '{}' +; \\\n\trm -f get-pip.py\n\n\nRUN apt-get update && apt-get -y upgrade\n\n# xvfb is used to do CPU based rendering of Unity\nRUN apt-get install -y xvfb\n\nCOPY ml-agents /ml-agents\nWORKDIR /ml-agents\nRUN pip install .\n\n# port 5005 is the port used in in Editor training.\nEXPOSE 5005\n\nRUN cp /etc/apt/sources.list /etc/apt/sources.list.bak\nCOPY sources.list /etc/apt/sources.list\n\nENV PYTHONPATH /ml-agents:$PYTHONPATH\n\nRUN apt-get update && apt-get install -y \\\n        apt-file \\\n        nano \\\n        net-tools \\\n        iputils-ping \\\n        openssh-server \\\n        apt-utils \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && mkdir /var/run/sshd \\\n    && echo \"root:1234\" | chpasswd \\\n    && sed -i 's/prohibit-password/yes/g' /etc/ssh/sshd_config\n\nEXPOSE 22\n\nENTRYPOINT [\"/usr/sbin/sshd\",\"-D\"]\n```\n\n### PUSH镜像\n\n`docker push [name]:[tag]`\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-02-47.png)\n\n### 测试镜像\n\n- 登录[机器学习平台](http://10.0.4.228),没有使用平台的可以在本地使用`docker run`直接开启容器\n- 先测试使用容器的方式\n  - 创建容器\n  ![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-28-19.png)\n  将要运行和储存的文件夹放在数据卷`data`下,这个目录要在运行时由`--docker-target-name`指定\n  - 等待容器创建成功\n  ![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-15.png)\n  - 容器创建成功后进入容器\n  ![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-52.png)\n  - 执行`mlagents-learn trainer_config.yaml --docker-target-name=data/unity-volume --env=3dball --train --run-id=test --save-freq=5000 | tee /data/unity-volume/log.txt`,如果不想在屏幕输出,可以在后边加上`>/dev/null`\n  ![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-16-44.png)\n  ![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-18-40.png)\n  \n---\n\n## 安装Miniconda\n\n确保你的安装包放在了data文件夹下\n`apt-get update && apt-get install bzip2 -y && cd /data && bash Miniconda3-latest-Linux-x86_64.sh`\n\n一路按回车、yes等等就成功了.如果需要安装到指定目录,在安装过程中会有提示告诉你让你指定安装路径\n\n*注意*\n\n在[学校机器学习平台](http://10.0.4.228/)上使用时,如果是使用容器的方式,那么新开的容器就可以使用`conda`命令,不存在`conda:command not found`的错误信息.\n\n但是,如果在平台上以**提交任务**的形式来使用带conda的镜像所产生的容器时,就算是在镜像中配置了`echo 'export PATH=\"~/anaconda3/bin:$PATH\"' >> ~/.bashrc`,当提交任务时环境变量中仍然没有`~/anaconda3/bin`,这个问题目前没有找到比较方便的解决办法,目前所采用的方式是:\n\n在提交任务时, 首先加上命令`export PATH=\"~/anaconda3/bin:$PATH\" && `\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-25-17.png)\n\n接着又实验了一下`echo 'export PATH=\"~/anaconda3/bin:$PATH\"' >> /etc/profile`, 正常来说, 如果在容器中这样设置环境变量, 等待下次从镜像创建容器时, 这个环境变量一般并不会生效, 但是不知道这样设置对于提交任务方式来说有没有效, 索性试了一下\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-35-56.png)\n\n根据输出日志来看, 这种方式也并没有奏效\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-39-26.png)\n\n当然, 如果觉得上述配置比较麻烦的话,可以使用`Dockerfile`的`ENV`命令来设置环境变量, 这样设置99%是不会有问题的\n\n```\nFROM hub.hoc.ccshu.net/wjs/mlunityv060:v1.4.5\nENV PATH /usr/miniconda3/bin:$PATH\n```\n实验了一下,\n结果如下:\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_15-02-16.png)\n\n表示可以使用`conda`命令, 但是不能使用`conda activate`命令激活环境\n\n根据错误信息, 在`Dockerfile`中写入以下代码也不可行:\n\n`RUN ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh`\n\n查了一下相关资料,发现在4.5版本的conda是无解的\n[https://github.com/ContinuumIO/docker-images/issues/89](https://github.com/ContinuumIO/docker-images/issues/89)\n\n![](./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_22-38-42.png)\n\n希望4.6版本可以解决吧\n\n**更新2019年1月14日14:26:01**\n**已解决**\n\n4.6版本的确可以解决以提交任务模式运行时的问题, 需要使用命令`conda run -n [环境名字] [要执行的命令]`, 而不是使用`conda activate [环境名字]`先激活一个环境.\n\n不过, 更新至4.6版本需要相关配置\n```\nconda config --add channels conda-canary\nconda update conda\n```\n\n我使用Dockerfile来生成镜像, 代码如下:\n```\nFROM hub.hoc.ccshu.net/wjs/mlunityv060:v1.4.6\n\nENV PATH /usr/miniconda3/bin:$PATH\nRUN conda config --add channels conda-canary && conda update conda -y\n```\n原因是, 使用Dockerfile比较容易设置环境变量, 减少出错\n","slug":"创建ML-Agents的Docker镜像","published":1,"updated":"2019-05-10T01:25:04.774Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma3n0030ekveq9584kkb","content":"<h1 id=\"创建ML-Agents的Docker镜像\"><a href=\"#创建ML-Agents的Docker镜像\" class=\"headerlink\" title=\"创建ML-Agents的Docker镜像\"></a>创建ML-Agents的Docker镜像</h1><a id=\"more\"></a>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>  如果需要在镜像中使用GPU训练,可以将Nvidia的官方镜像作为基础镜像,<code>Dockerfile</code>如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM nvidia/cuda:9.0-base-ubuntu16.04</span><br><span class=\"line\">LABEL maintainer &quot;NVIDIA CORPORATION &lt;cudatools@nvidia.com&gt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">ENV NCCL_VERSION 2.3.7</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\</span><br><span class=\"line\">\tapt-utils \\</span><br><span class=\"line\">        cuda-libraries-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-cublas-9-0=9.0.176.4-1 \\</span><br><span class=\"line\">        libnccl2=$NCCL_VERSION-1+cuda9.0 &amp;&amp; \\</span><br><span class=\"line\">    apt-mark hold libnccl2 &amp;&amp; \\</span><br><span class=\"line\">    rm -rf /var/lib/apt/lists/*</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y openssh-server</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get install -y nano</span><br><span class=\"line\"></span><br><span class=\"line\">RUN mkdir /var/run/sshd</span><br><span class=\"line\"></span><br><span class=\"line\">RUN echo &quot;root:1234&quot; | chpasswd</span><br><span class=\"line\"></span><br><span class=\"line\">RUN sed -i &apos;s/prohibit-password/yes/g&apos; /etc/ssh/sshd_config</span><br><span class=\"line\"></span><br><span class=\"line\">EXPOSE 22</span><br><span class=\"line\"></span><br><span class=\"line\">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"ML-Agents-v0-6-0\"><a href=\"#ML-Agents-v0-6-0\" class=\"headerlink\" title=\"ML-Agents v0.6.0\"></a>ML-Agents v0.6.0</h2><p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-44-58.png\" alt=\"\"></p>\n<h3 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h3><p>本机环境</p>\n<ul>\n<li>ML-Agents 0.6.0</li>\n<li>Windows 10 专业版</li>\n<li>docker client version 18.09.0</li>\n<li>docker server version 18.09.0</li>\n</ul>\n<p>平台</p>\n<ul>\n<li><a href=\"http://10.0.4.228\" rel=\"external nofollow\" target=\"_blank\">机器学习平台</a></li>\n</ul>\n<h3 id=\"创建镜像\"><a href=\"#创建镜像\" class=\"headerlink\" title=\"创建镜像\"></a>创建镜像</h3><ol>\n<li>打开<code>~/ml-agents-0.6.0/</code>目录,看到有一个官方给定的<code>Dockerfile</code><br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-52-49.png\" alt=\"\"></li>\n<li>直接<code>Build</code>,在该目录下运行<code>docker build -t [name]:[tag] .</code>,一定要注意最后的<code>.</code>,很<strong>重要</strong><br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-29-30.png\" alt=\"\"></li>\n<li>新建一个<code>sources.list</code>文件,为镜像内换源,因为将来有可能需要在容器内安装某些包,有一些国外的资源往往会下载失败,所以需要<strong>换源</strong></li>\n</ol>\n<ul>\n<li>新建一个<code>sources.list</code></li>\n<li><p>用文本编辑器打开,写入以下内容<br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-36-31.png\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted</span><br><span class=\"line\">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class=\"line\">deb http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb-src http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>保存退出</p>\n</li>\n</ul>\n<ol>\n<li>新建一个<code>DockerfilePlus</code>,在官方生成的基础镜像上安装一些可以在平台上运行的包,<code>openssh-server</code>,联网工具<code>net-tools</code>,心爱的<code>apt-file</code>等等</li>\n</ol>\n<ul>\n<li>新建一个<code>DockerfilePlus</code></li>\n<li>用文本编辑器打开,输入以下内容<br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-01-32.png\" alt=\"\"><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM hub.hoc.ccshu.net/wjs/mlunityv060:v0.1</span><br><span class=\"line\"></span><br><span class=\"line\">RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class=\"line\">COPY sources.list /etc/apt/sources.list</span><br><span class=\"line\"></span><br><span class=\"line\">ENV PYTHONPATH /ml-agents:$PYTHONPATH</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y \\</span><br><span class=\"line\">        apt-file \\</span><br><span class=\"line\">        nano \\</span><br><span class=\"line\">        net-tools \\</span><br><span class=\"line\">        iputils-ping \\</span><br><span class=\"line\">        openssh-server \\</span><br><span class=\"line\">        apt-utils \\</span><br><span class=\"line\">    &amp;&amp; rm -rf /var/lib/apt/lists/* \\</span><br><span class=\"line\">    &amp;&amp; mkdir /var/run/sshd \\</span><br><span class=\"line\">    &amp;&amp; echo &quot;root:1234&quot; | chpasswd \\</span><br><span class=\"line\">    &amp;&amp; sed -i &apos;s/prohibit-password/yes/g&apos; /etc/ssh/sshd_config</span><br><span class=\"line\"></span><br><span class=\"line\">EXPOSE 22</span><br><span class=\"line\"></span><br><span class=\"line\">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ol>\n<li>在<code>DockerfilePlus</code>所在文件夹下,执行<code>build -t [name]:[tag] -f DockerfilePlus .</code></li>\n</ol>\n<p><strong>为了使用GPU.改写完的Dockerfile如下(不需要看):</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM nvidia/cuda:9.0-base-ubuntu16.04</span><br><span class=\"line\">LABEL maintainer &quot;Keavnn &lt;https://stepneverstop.github.io&gt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">ENV NCCL_VERSION 2.3.7</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\</span><br><span class=\"line\">\tapt-utils \\</span><br><span class=\"line\">        cuda-libraries-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-cublas-9-0=9.0.176.4-1 \\</span><br><span class=\"line\">        libnccl2=$NCCL_VERSION-1+cuda9.0 &amp;&amp; \\</span><br><span class=\"line\">    apt-mark hold libnccl2 &amp;&amp; \\</span><br><span class=\"line\">    rm -rf /var/lib/apt/lists/*</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># ensure local python is preferred over distribution python</span><br><span class=\"line\">ENV PATH /usr/local/bin:$PATH</span><br><span class=\"line\"></span><br><span class=\"line\"># http://bugs.python.org/issue19846</span><br><span class=\"line\"># &gt; At the moment, setting &quot;LANG=C&quot; on a Linux system *fundamentally breaks Python 3*, and that&apos;s not OK.</span><br><span class=\"line\">ENV LANG C.UTF-8</span><br><span class=\"line\"></span><br><span class=\"line\"># runtime dependencies</span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\</span><br><span class=\"line\">\t\tca-certificates \\</span><br><span class=\"line\">\t\tlibexpat1 \\</span><br><span class=\"line\">\t\tlibffi6 \\</span><br><span class=\"line\">\t\tlibgdbm3 \\</span><br><span class=\"line\">\t\tlibreadline6 \\</span><br><span class=\"line\">\t\tlibsqlite3-0 \\</span><br><span class=\"line\">\t\tlibssl1.0.0 \\</span><br><span class=\"line\">\t&amp;&amp; rm -rf /var/lib/apt/lists/*</span><br><span class=\"line\"></span><br><span class=\"line\">ENV GPG_KEY 0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D</span><br><span class=\"line\">ENV PYTHON_VERSION 3.6.4</span><br><span class=\"line\"></span><br><span class=\"line\">RUN set -ex \\</span><br><span class=\"line\">\t&amp;&amp; buildDeps=&quot; \\</span><br><span class=\"line\">\t\tdpkg-dev \\</span><br><span class=\"line\">\t\tgcc \\</span><br><span class=\"line\">\t\tlibbz2-dev \\</span><br><span class=\"line\">\t\tlibc6-dev \\</span><br><span class=\"line\">\t\tlibexpat1-dev \\</span><br><span class=\"line\">\t\tlibffi-dev \\</span><br><span class=\"line\">\t\tlibgdbm-dev \\</span><br><span class=\"line\">\t\tliblzma-dev \\</span><br><span class=\"line\">\t\tlibncursesw5-dev \\</span><br><span class=\"line\">\t\tlibreadline-dev \\</span><br><span class=\"line\">\t\tlibsqlite3-dev \\</span><br><span class=\"line\">\t\tlibssl-dev \\</span><br><span class=\"line\">\t\tmake \\</span><br><span class=\"line\">\t\ttcl-dev \\</span><br><span class=\"line\">\t\ttk-dev \\</span><br><span class=\"line\">\t\twget \\</span><br><span class=\"line\">\t\txz-utils \\</span><br><span class=\"line\">\t\tzlib1g-dev \\</span><br><span class=\"line\"># as of Stretch, &quot;gpg&quot; is no longer included by default</span><br><span class=\"line\">\t\t$(command -v gpg &gt; /dev/null || echo &apos;gnupg dirmngr&apos;) \\</span><br><span class=\"line\">\t&quot; \\</span><br><span class=\"line\">\t&amp;&amp; apt-get update &amp;&amp; apt-get install -y $buildDeps --no-install-recommends &amp;&amp; rm -rf /var/lib/apt/lists/* \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\t&amp;&amp; wget -O python.tar.xz &quot;https://www.python.org/ftp/python/$&#123;PYTHON_VERSION%%[a-z]*&#125;/Python-$PYTHON_VERSION.tar.xz&quot; \\</span><br><span class=\"line\">\t&amp;&amp; wget -O python.tar.xz.asc &quot;https://www.python.org/ftp/python/$&#123;PYTHON_VERSION%%[a-z]*&#125;/Python-$PYTHON_VERSION.tar.xz.asc&quot; \\</span><br><span class=\"line\">\t&amp;&amp; export GNUPGHOME=&quot;$(mktemp -d)&quot; \\</span><br><span class=\"line\">\t&amp;&amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-keys &quot;$GPG_KEY&quot; \\</span><br><span class=\"line\">\t&amp;&amp; gpg --batch --verify python.tar.xz.asc python.tar.xz \\</span><br><span class=\"line\">\t&amp;&amp; rm -rf &quot;$GNUPGHOME&quot; python.tar.xz.asc \\</span><br><span class=\"line\">\t&amp;&amp; mkdir -p /usr/src/python \\</span><br><span class=\"line\">\t&amp;&amp; tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz \\</span><br><span class=\"line\">\t&amp;&amp; rm python.tar.xz \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\t&amp;&amp; cd /usr/src/python \\</span><br><span class=\"line\">\t&amp;&amp; gnuArch=&quot;$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)&quot; \\</span><br><span class=\"line\">\t&amp;&amp; ./configure \\</span><br><span class=\"line\">\t\t--build=&quot;$gnuArch&quot; \\</span><br><span class=\"line\">\t\t--enable-loadable-sqlite-extensions \\</span><br><span class=\"line\">\t\t--enable-shared \\</span><br><span class=\"line\">\t\t--with-system-expat \\</span><br><span class=\"line\">\t\t--with-system-ffi \\</span><br><span class=\"line\">\t\t--without-ensurepip \\</span><br><span class=\"line\">\t&amp;&amp; make -j &quot;$(nproc)&quot; \\</span><br><span class=\"line\">\t&amp;&amp; make install \\</span><br><span class=\"line\">\t&amp;&amp; ldconfig \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\t&amp;&amp; apt-get purge -y --auto-remove $buildDeps \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\t&amp;&amp; find /usr/local -depth \\</span><br><span class=\"line\">\t\t\\( \\</span><br><span class=\"line\">\t\t\t\\( -type d -a \\( -name test -o -name tests \\) \\) \\</span><br><span class=\"line\">\t\t\t-o \\</span><br><span class=\"line\">\t\t\t\\( -type f -a \\( -name &apos;*.pyc&apos; -o -name &apos;*.pyo&apos; \\) \\) \\</span><br><span class=\"line\">\t\t\\) -exec rm -rf &apos;&#123;&#125;&apos; + \\</span><br><span class=\"line\">\t&amp;&amp; rm -rf /usr/src/python</span><br><span class=\"line\"></span><br><span class=\"line\"># make some useful symlinks that are expected to exist</span><br><span class=\"line\">RUN cd /usr/local/bin \\</span><br><span class=\"line\">\t&amp;&amp; ln -s idle3 idle \\</span><br><span class=\"line\">\t&amp;&amp; ln -s pydoc3 pydoc \\</span><br><span class=\"line\">\t&amp;&amp; ln -s python3 python \\</span><br><span class=\"line\">\t&amp;&amp; ln -s python3-config python-config</span><br><span class=\"line\"></span><br><span class=\"line\"># if this is called &quot;PIP_VERSION&quot;, pip explodes with &quot;ValueError: invalid truth value &apos;&lt;VERSION&gt;&apos;&quot;</span><br><span class=\"line\">ENV PYTHON_PIP_VERSION 9.0.3</span><br><span class=\"line\"></span><br><span class=\"line\">RUN set -ex; \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\tapt-get update; \\</span><br><span class=\"line\">\tapt-get install -y --no-install-recommends wget; \\</span><br><span class=\"line\">\trm -rf /var/lib/apt/lists/*; \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\twget -O get-pip.py &apos;https://bootstrap.pypa.io/get-pip.py&apos;; \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\tapt-get purge -y --auto-remove wget; \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\tpython get-pip.py \\</span><br><span class=\"line\">\t\t--disable-pip-version-check \\</span><br><span class=\"line\">\t\t--no-cache-dir \\</span><br><span class=\"line\">\t\t&quot;pip==$PYTHON_PIP_VERSION&quot; \\</span><br><span class=\"line\">\t; \\</span><br><span class=\"line\">\tpip --version; \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\tfind /usr/local -depth \\</span><br><span class=\"line\">\t\t\\( \\</span><br><span class=\"line\">\t\t\t\\( -type d -a \\( -name test -o -name tests \\) \\) \\</span><br><span class=\"line\">\t\t\t-o \\</span><br><span class=\"line\">\t\t\t\\( -type f -a \\( -name &apos;*.pyc&apos; -o -name &apos;*.pyo&apos; \\) \\) \\</span><br><span class=\"line\">\t\t\\) -exec rm -rf &apos;&#123;&#125;&apos; +; \\</span><br><span class=\"line\">\trm -f get-pip.py</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get -y upgrade</span><br><span class=\"line\"></span><br><span class=\"line\"># xvfb is used to do CPU based rendering of Unity</span><br><span class=\"line\">RUN apt-get install -y xvfb</span><br><span class=\"line\"></span><br><span class=\"line\">COPY ml-agents /ml-agents</span><br><span class=\"line\">WORKDIR /ml-agents</span><br><span class=\"line\">RUN pip install .</span><br><span class=\"line\"></span><br><span class=\"line\"># port 5005 is the port used in in Editor training.</span><br><span class=\"line\">EXPOSE 5005</span><br><span class=\"line\"></span><br><span class=\"line\">RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class=\"line\">COPY sources.list /etc/apt/sources.list</span><br><span class=\"line\"></span><br><span class=\"line\">ENV PYTHONPATH /ml-agents:$PYTHONPATH</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y \\</span><br><span class=\"line\">        apt-file \\</span><br><span class=\"line\">        nano \\</span><br><span class=\"line\">        net-tools \\</span><br><span class=\"line\">        iputils-ping \\</span><br><span class=\"line\">        openssh-server \\</span><br><span class=\"line\">        apt-utils \\</span><br><span class=\"line\">    &amp;&amp; rm -rf /var/lib/apt/lists/* \\</span><br><span class=\"line\">    &amp;&amp; mkdir /var/run/sshd \\</span><br><span class=\"line\">    &amp;&amp; echo &quot;root:1234&quot; | chpasswd \\</span><br><span class=\"line\">    &amp;&amp; sed -i &apos;s/prohibit-password/yes/g&apos; /etc/ssh/sshd_config</span><br><span class=\"line\"></span><br><span class=\"line\">EXPOSE 22</span><br><span class=\"line\"></span><br><span class=\"line\">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"PUSH镜像\"><a href=\"#PUSH镜像\" class=\"headerlink\" title=\"PUSH镜像\"></a>PUSH镜像</h3><p><code>docker push [name]:[tag]</code><br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-02-47.png\" alt=\"\"></p>\n<h3 id=\"测试镜像\"><a href=\"#测试镜像\" class=\"headerlink\" title=\"测试镜像\"></a>测试镜像</h3><ul>\n<li>登录<a href=\"http://10.0.4.228\" rel=\"external nofollow\" target=\"_blank\">机器学习平台</a>,没有使用平台的可以在本地使用<code>docker run</code>直接开启容器</li>\n<li>先测试使用容器的方式<ul>\n<li>创建容器<br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-28-19.png\" alt=\"\"><br>将要运行和储存的文件夹放在数据卷<code>data</code>下,这个目录要在运行时由<code>--docker-target-name</code>指定</li>\n<li>等待容器创建成功<br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-15.png\" alt=\"\"></li>\n<li>容器创建成功后进入容器<br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-52.png\" alt=\"\"></li>\n<li>执行<code>mlagents-learn trainer_config.yaml --docker-target-name=data/unity-volume --env=3dball --train --run-id=test --save-freq=5000 | tee /data/unity-volume/log.txt</code>,如果不想在屏幕输出,可以在后边加上<code>&gt;/dev/null</code><br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-16-44.png\" alt=\"\"><br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-18-40.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"安装Miniconda\"><a href=\"#安装Miniconda\" class=\"headerlink\" title=\"安装Miniconda\"></a>安装Miniconda</h2><p>确保你的安装包放在了data文件夹下<br><code>apt-get update &amp;&amp; apt-get install bzip2 -y &amp;&amp; cd /data &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh</code></p>\n<p>一路按回车、yes等等就成功了.如果需要安装到指定目录,在安装过程中会有提示告诉你让你指定安装路径</p>\n<p><em>注意</em></p>\n<p>在<a href=\"http://10.0.4.228/\" rel=\"external nofollow\" target=\"_blank\">学校机器学习平台</a>上使用时,如果是使用容器的方式,那么新开的容器就可以使用<code>conda</code>命令,不存在<code>conda:command not found</code>的错误信息.</p>\n<p>但是,如果在平台上以<strong>提交任务</strong>的形式来使用带conda的镜像所产生的容器时,就算是在镜像中配置了<code>echo &#39;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&#39; &gt;&gt; ~/.bashrc</code>,当提交任务时环境变量中仍然没有<code>~/anaconda3/bin</code>,这个问题目前没有找到比较方便的解决办法,目前所采用的方式是:</p>\n<p>在提交任务时, 首先加上命令<code>export PATH=&quot;~/anaconda3/bin:$PATH&quot; &amp;&amp;</code></p>\n<p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-25-17.png\" alt=\"\"></p>\n<p>接着又实验了一下<code>echo &#39;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&#39; &gt;&gt; /etc/profile</code>, 正常来说, 如果在容器中这样设置环境变量, 等待下次从镜像创建容器时, 这个环境变量一般并不会生效, 但是不知道这样设置对于提交任务方式来说有没有效, 索性试了一下</p>\n<p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-35-56.png\" alt=\"\"></p>\n<p>根据输出日志来看, 这种方式也并没有奏效</p>\n<p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-39-26.png\" alt=\"\"></p>\n<p>当然, 如果觉得上述配置比较麻烦的话,可以使用<code>Dockerfile</code>的<code>ENV</code>命令来设置环境变量, 这样设置99%是不会有问题的</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM hub.hoc.ccshu.net/wjs/mlunityv060:v1.4.5</span><br><span class=\"line\">ENV PATH /usr/miniconda3/bin:$PATH</span><br></pre></td></tr></table></figure>\n<p>实验了一下,<br>结果如下:</p>\n<p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_15-02-16.png\" alt=\"\"></p>\n<p>表示可以使用<code>conda</code>命令, 但是不能使用<code>conda activate</code>命令激活环境</p>\n<p>根据错误信息, 在<code>Dockerfile</code>中写入以下代码也不可行:</p>\n<p><code>RUN ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh</code></p>\n<p>查了一下相关资料,发现在4.5版本的conda是无解的<br><a href=\"https://github.com/ContinuumIO/docker-images/issues/89\" rel=\"external nofollow\" target=\"_blank\">https://github.com/ContinuumIO/docker-images/issues/89</a></p>\n<p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_22-38-42.png\" alt=\"\"></p>\n<p>希望4.6版本可以解决吧</p>\n<p><strong>更新2019年1月14日14:26:01</strong><br><strong>已解决</strong></p>\n<p>4.6版本的确可以解决以提交任务模式运行时的问题, 需要使用命令<code>conda run -n [环境名字] [要执行的命令]</code>, 而不是使用<code>conda activate [环境名字]</code>先激活一个环境.</p>\n<p>不过, 更新至4.6版本需要相关配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --add channels conda-canary</span><br><span class=\"line\">conda update conda</span><br></pre></td></tr></table></figure></p>\n<p>我使用Dockerfile来生成镜像, 代码如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM hub.hoc.ccshu.net/wjs/mlunityv060:v1.4.6</span><br><span class=\"line\"></span><br><span class=\"line\">ENV PATH /usr/miniconda3/bin:$PATH</span><br><span class=\"line\">RUN conda config --add channels conda-canary &amp;&amp; conda update conda -y</span><br></pre></td></tr></table></figure></p>\n<p>原因是, 使用Dockerfile比较容易设置环境变量, 减少出错</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"创建ML-Agents的Docker镜像\"><a href=\"#创建ML-Agents的Docker镜像\" class=\"headerlink\" title=\"创建ML-Agents的Docker镜像\"></a>创建ML-Agents的Docker镜像</h1>","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>  如果需要在镜像中使用GPU训练,可以将Nvidia的官方镜像作为基础镜像,<code>Dockerfile</code>如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM nvidia/cuda:9.0-base-ubuntu16.04</span><br><span class=\"line\">LABEL maintainer &quot;NVIDIA CORPORATION &lt;cudatools@nvidia.com&gt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">ENV NCCL_VERSION 2.3.7</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\</span><br><span class=\"line\">\tapt-utils \\</span><br><span class=\"line\">        cuda-libraries-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-cublas-9-0=9.0.176.4-1 \\</span><br><span class=\"line\">        libnccl2=$NCCL_VERSION-1+cuda9.0 &amp;&amp; \\</span><br><span class=\"line\">    apt-mark hold libnccl2 &amp;&amp; \\</span><br><span class=\"line\">    rm -rf /var/lib/apt/lists/*</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y openssh-server</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get install -y nano</span><br><span class=\"line\"></span><br><span class=\"line\">RUN mkdir /var/run/sshd</span><br><span class=\"line\"></span><br><span class=\"line\">RUN echo &quot;root:1234&quot; | chpasswd</span><br><span class=\"line\"></span><br><span class=\"line\">RUN sed -i &apos;s/prohibit-password/yes/g&apos; /etc/ssh/sshd_config</span><br><span class=\"line\"></span><br><span class=\"line\">EXPOSE 22</span><br><span class=\"line\"></span><br><span class=\"line\">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"ML-Agents-v0-6-0\"><a href=\"#ML-Agents-v0-6-0\" class=\"headerlink\" title=\"ML-Agents v0.6.0\"></a>ML-Agents v0.6.0</h2><p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-44-58.png\" alt=\"\"></p>\n<h3 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h3><p>本机环境</p>\n<ul>\n<li>ML-Agents 0.6.0</li>\n<li>Windows 10 专业版</li>\n<li>docker client version 18.09.0</li>\n<li>docker server version 18.09.0</li>\n</ul>\n<p>平台</p>\n<ul>\n<li><a href=\"http://10.0.4.228\" rel=\"external nofollow\" target=\"_blank\">机器学习平台</a></li>\n</ul>\n<h3 id=\"创建镜像\"><a href=\"#创建镜像\" class=\"headerlink\" title=\"创建镜像\"></a>创建镜像</h3><ol>\n<li>打开<code>~/ml-agents-0.6.0/</code>目录,看到有一个官方给定的<code>Dockerfile</code><br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-52-49.png\" alt=\"\"></li>\n<li>直接<code>Build</code>,在该目录下运行<code>docker build -t [name]:[tag] .</code>,一定要注意最后的<code>.</code>,很<strong>重要</strong><br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-29-30.png\" alt=\"\"></li>\n<li>新建一个<code>sources.list</code>文件,为镜像内换源,因为将来有可能需要在容器内安装某些包,有一些国外的资源往往会下载失败,所以需要<strong>换源</strong></li>\n</ol>\n<ul>\n<li>新建一个<code>sources.list</code></li>\n<li><p>用文本编辑器打开,写入以下内容<br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-36-31.png\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted</span><br><span class=\"line\">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class=\"line\">deb http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb-src http://archive.canonical.com/ubuntu xenial partner</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</span><br><span class=\"line\">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class=\"line\">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>保存退出</p>\n</li>\n</ul>\n<ol>\n<li>新建一个<code>DockerfilePlus</code>,在官方生成的基础镜像上安装一些可以在平台上运行的包,<code>openssh-server</code>,联网工具<code>net-tools</code>,心爱的<code>apt-file</code>等等</li>\n</ol>\n<ul>\n<li>新建一个<code>DockerfilePlus</code></li>\n<li>用文本编辑器打开,输入以下内容<br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-01-32.png\" alt=\"\"><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM hub.hoc.ccshu.net/wjs/mlunityv060:v0.1</span><br><span class=\"line\"></span><br><span class=\"line\">RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class=\"line\">COPY sources.list /etc/apt/sources.list</span><br><span class=\"line\"></span><br><span class=\"line\">ENV PYTHONPATH /ml-agents:$PYTHONPATH</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y \\</span><br><span class=\"line\">        apt-file \\</span><br><span class=\"line\">        nano \\</span><br><span class=\"line\">        net-tools \\</span><br><span class=\"line\">        iputils-ping \\</span><br><span class=\"line\">        openssh-server \\</span><br><span class=\"line\">        apt-utils \\</span><br><span class=\"line\">    &amp;&amp; rm -rf /var/lib/apt/lists/* \\</span><br><span class=\"line\">    &amp;&amp; mkdir /var/run/sshd \\</span><br><span class=\"line\">    &amp;&amp; echo &quot;root:1234&quot; | chpasswd \\</span><br><span class=\"line\">    &amp;&amp; sed -i &apos;s/prohibit-password/yes/g&apos; /etc/ssh/sshd_config</span><br><span class=\"line\"></span><br><span class=\"line\">EXPOSE 22</span><br><span class=\"line\"></span><br><span class=\"line\">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ol>\n<li>在<code>DockerfilePlus</code>所在文件夹下,执行<code>build -t [name]:[tag] -f DockerfilePlus .</code></li>\n</ol>\n<p><strong>为了使用GPU.改写完的Dockerfile如下(不需要看):</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM nvidia/cuda:9.0-base-ubuntu16.04</span><br><span class=\"line\">LABEL maintainer &quot;Keavnn &lt;https://stepneverstop.github.io&gt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">ENV NCCL_VERSION 2.3.7</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\</span><br><span class=\"line\">\tapt-utils \\</span><br><span class=\"line\">        cuda-libraries-$CUDA_PKG_VERSION \\</span><br><span class=\"line\">        cuda-cublas-9-0=9.0.176.4-1 \\</span><br><span class=\"line\">        libnccl2=$NCCL_VERSION-1+cuda9.0 &amp;&amp; \\</span><br><span class=\"line\">    apt-mark hold libnccl2 &amp;&amp; \\</span><br><span class=\"line\">    rm -rf /var/lib/apt/lists/*</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># ensure local python is preferred over distribution python</span><br><span class=\"line\">ENV PATH /usr/local/bin:$PATH</span><br><span class=\"line\"></span><br><span class=\"line\"># http://bugs.python.org/issue19846</span><br><span class=\"line\"># &gt; At the moment, setting &quot;LANG=C&quot; on a Linux system *fundamentally breaks Python 3*, and that&apos;s not OK.</span><br><span class=\"line\">ENV LANG C.UTF-8</span><br><span class=\"line\"></span><br><span class=\"line\"># runtime dependencies</span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\</span><br><span class=\"line\">\t\tca-certificates \\</span><br><span class=\"line\">\t\tlibexpat1 \\</span><br><span class=\"line\">\t\tlibffi6 \\</span><br><span class=\"line\">\t\tlibgdbm3 \\</span><br><span class=\"line\">\t\tlibreadline6 \\</span><br><span class=\"line\">\t\tlibsqlite3-0 \\</span><br><span class=\"line\">\t\tlibssl1.0.0 \\</span><br><span class=\"line\">\t&amp;&amp; rm -rf /var/lib/apt/lists/*</span><br><span class=\"line\"></span><br><span class=\"line\">ENV GPG_KEY 0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D</span><br><span class=\"line\">ENV PYTHON_VERSION 3.6.4</span><br><span class=\"line\"></span><br><span class=\"line\">RUN set -ex \\</span><br><span class=\"line\">\t&amp;&amp; buildDeps=&quot; \\</span><br><span class=\"line\">\t\tdpkg-dev \\</span><br><span class=\"line\">\t\tgcc \\</span><br><span class=\"line\">\t\tlibbz2-dev \\</span><br><span class=\"line\">\t\tlibc6-dev \\</span><br><span class=\"line\">\t\tlibexpat1-dev \\</span><br><span class=\"line\">\t\tlibffi-dev \\</span><br><span class=\"line\">\t\tlibgdbm-dev \\</span><br><span class=\"line\">\t\tliblzma-dev \\</span><br><span class=\"line\">\t\tlibncursesw5-dev \\</span><br><span class=\"line\">\t\tlibreadline-dev \\</span><br><span class=\"line\">\t\tlibsqlite3-dev \\</span><br><span class=\"line\">\t\tlibssl-dev \\</span><br><span class=\"line\">\t\tmake \\</span><br><span class=\"line\">\t\ttcl-dev \\</span><br><span class=\"line\">\t\ttk-dev \\</span><br><span class=\"line\">\t\twget \\</span><br><span class=\"line\">\t\txz-utils \\</span><br><span class=\"line\">\t\tzlib1g-dev \\</span><br><span class=\"line\"># as of Stretch, &quot;gpg&quot; is no longer included by default</span><br><span class=\"line\">\t\t$(command -v gpg &gt; /dev/null || echo &apos;gnupg dirmngr&apos;) \\</span><br><span class=\"line\">\t&quot; \\</span><br><span class=\"line\">\t&amp;&amp; apt-get update &amp;&amp; apt-get install -y $buildDeps --no-install-recommends &amp;&amp; rm -rf /var/lib/apt/lists/* \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\t&amp;&amp; wget -O python.tar.xz &quot;https://www.python.org/ftp/python/$&#123;PYTHON_VERSION%%[a-z]*&#125;/Python-$PYTHON_VERSION.tar.xz&quot; \\</span><br><span class=\"line\">\t&amp;&amp; wget -O python.tar.xz.asc &quot;https://www.python.org/ftp/python/$&#123;PYTHON_VERSION%%[a-z]*&#125;/Python-$PYTHON_VERSION.tar.xz.asc&quot; \\</span><br><span class=\"line\">\t&amp;&amp; export GNUPGHOME=&quot;$(mktemp -d)&quot; \\</span><br><span class=\"line\">\t&amp;&amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-keys &quot;$GPG_KEY&quot; \\</span><br><span class=\"line\">\t&amp;&amp; gpg --batch --verify python.tar.xz.asc python.tar.xz \\</span><br><span class=\"line\">\t&amp;&amp; rm -rf &quot;$GNUPGHOME&quot; python.tar.xz.asc \\</span><br><span class=\"line\">\t&amp;&amp; mkdir -p /usr/src/python \\</span><br><span class=\"line\">\t&amp;&amp; tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz \\</span><br><span class=\"line\">\t&amp;&amp; rm python.tar.xz \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\t&amp;&amp; cd /usr/src/python \\</span><br><span class=\"line\">\t&amp;&amp; gnuArch=&quot;$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)&quot; \\</span><br><span class=\"line\">\t&amp;&amp; ./configure \\</span><br><span class=\"line\">\t\t--build=&quot;$gnuArch&quot; \\</span><br><span class=\"line\">\t\t--enable-loadable-sqlite-extensions \\</span><br><span class=\"line\">\t\t--enable-shared \\</span><br><span class=\"line\">\t\t--with-system-expat \\</span><br><span class=\"line\">\t\t--with-system-ffi \\</span><br><span class=\"line\">\t\t--without-ensurepip \\</span><br><span class=\"line\">\t&amp;&amp; make -j &quot;$(nproc)&quot; \\</span><br><span class=\"line\">\t&amp;&amp; make install \\</span><br><span class=\"line\">\t&amp;&amp; ldconfig \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\t&amp;&amp; apt-get purge -y --auto-remove $buildDeps \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\t&amp;&amp; find /usr/local -depth \\</span><br><span class=\"line\">\t\t\\( \\</span><br><span class=\"line\">\t\t\t\\( -type d -a \\( -name test -o -name tests \\) \\) \\</span><br><span class=\"line\">\t\t\t-o \\</span><br><span class=\"line\">\t\t\t\\( -type f -a \\( -name &apos;*.pyc&apos; -o -name &apos;*.pyo&apos; \\) \\) \\</span><br><span class=\"line\">\t\t\\) -exec rm -rf &apos;&#123;&#125;&apos; + \\</span><br><span class=\"line\">\t&amp;&amp; rm -rf /usr/src/python</span><br><span class=\"line\"></span><br><span class=\"line\"># make some useful symlinks that are expected to exist</span><br><span class=\"line\">RUN cd /usr/local/bin \\</span><br><span class=\"line\">\t&amp;&amp; ln -s idle3 idle \\</span><br><span class=\"line\">\t&amp;&amp; ln -s pydoc3 pydoc \\</span><br><span class=\"line\">\t&amp;&amp; ln -s python3 python \\</span><br><span class=\"line\">\t&amp;&amp; ln -s python3-config python-config</span><br><span class=\"line\"></span><br><span class=\"line\"># if this is called &quot;PIP_VERSION&quot;, pip explodes with &quot;ValueError: invalid truth value &apos;&lt;VERSION&gt;&apos;&quot;</span><br><span class=\"line\">ENV PYTHON_PIP_VERSION 9.0.3</span><br><span class=\"line\"></span><br><span class=\"line\">RUN set -ex; \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\tapt-get update; \\</span><br><span class=\"line\">\tapt-get install -y --no-install-recommends wget; \\</span><br><span class=\"line\">\trm -rf /var/lib/apt/lists/*; \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\twget -O get-pip.py &apos;https://bootstrap.pypa.io/get-pip.py&apos;; \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\tapt-get purge -y --auto-remove wget; \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\tpython get-pip.py \\</span><br><span class=\"line\">\t\t--disable-pip-version-check \\</span><br><span class=\"line\">\t\t--no-cache-dir \\</span><br><span class=\"line\">\t\t&quot;pip==$PYTHON_PIP_VERSION&quot; \\</span><br><span class=\"line\">\t; \\</span><br><span class=\"line\">\tpip --version; \\</span><br><span class=\"line\">\t\\</span><br><span class=\"line\">\tfind /usr/local -depth \\</span><br><span class=\"line\">\t\t\\( \\</span><br><span class=\"line\">\t\t\t\\( -type d -a \\( -name test -o -name tests \\) \\) \\</span><br><span class=\"line\">\t\t\t-o \\</span><br><span class=\"line\">\t\t\t\\( -type f -a \\( -name &apos;*.pyc&apos; -o -name &apos;*.pyo&apos; \\) \\) \\</span><br><span class=\"line\">\t\t\\) -exec rm -rf &apos;&#123;&#125;&apos; +; \\</span><br><span class=\"line\">\trm -f get-pip.py</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get -y upgrade</span><br><span class=\"line\"></span><br><span class=\"line\"># xvfb is used to do CPU based rendering of Unity</span><br><span class=\"line\">RUN apt-get install -y xvfb</span><br><span class=\"line\"></span><br><span class=\"line\">COPY ml-agents /ml-agents</span><br><span class=\"line\">WORKDIR /ml-agents</span><br><span class=\"line\">RUN pip install .</span><br><span class=\"line\"></span><br><span class=\"line\"># port 5005 is the port used in in Editor training.</span><br><span class=\"line\">EXPOSE 5005</span><br><span class=\"line\"></span><br><span class=\"line\">RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class=\"line\">COPY sources.list /etc/apt/sources.list</span><br><span class=\"line\"></span><br><span class=\"line\">ENV PYTHONPATH /ml-agents:$PYTHONPATH</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt-get update &amp;&amp; apt-get install -y \\</span><br><span class=\"line\">        apt-file \\</span><br><span class=\"line\">        nano \\</span><br><span class=\"line\">        net-tools \\</span><br><span class=\"line\">        iputils-ping \\</span><br><span class=\"line\">        openssh-server \\</span><br><span class=\"line\">        apt-utils \\</span><br><span class=\"line\">    &amp;&amp; rm -rf /var/lib/apt/lists/* \\</span><br><span class=\"line\">    &amp;&amp; mkdir /var/run/sshd \\</span><br><span class=\"line\">    &amp;&amp; echo &quot;root:1234&quot; | chpasswd \\</span><br><span class=\"line\">    &amp;&amp; sed -i &apos;s/prohibit-password/yes/g&apos; /etc/ssh/sshd_config</span><br><span class=\"line\"></span><br><span class=\"line\">EXPOSE 22</span><br><span class=\"line\"></span><br><span class=\"line\">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"PUSH镜像\"><a href=\"#PUSH镜像\" class=\"headerlink\" title=\"PUSH镜像\"></a>PUSH镜像</h3><p><code>docker push [name]:[tag]</code><br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-02-47.png\" alt=\"\"></p>\n<h3 id=\"测试镜像\"><a href=\"#测试镜像\" class=\"headerlink\" title=\"测试镜像\"></a>测试镜像</h3><ul>\n<li>登录<a href=\"http://10.0.4.228\" rel=\"external nofollow\" target=\"_blank\">机器学习平台</a>,没有使用平台的可以在本地使用<code>docker run</code>直接开启容器</li>\n<li>先测试使用容器的方式<ul>\n<li>创建容器<br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-28-19.png\" alt=\"\"><br>将要运行和储存的文件夹放在数据卷<code>data</code>下,这个目录要在运行时由<code>--docker-target-name</code>指定</li>\n<li>等待容器创建成功<br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-15.png\" alt=\"\"></li>\n<li>容器创建成功后进入容器<br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-52.png\" alt=\"\"></li>\n<li>执行<code>mlagents-learn trainer_config.yaml --docker-target-name=data/unity-volume --env=3dball --train --run-id=test --save-freq=5000 | tee /data/unity-volume/log.txt</code>,如果不想在屏幕输出,可以在后边加上<code>&gt;/dev/null</code><br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-16-44.png\" alt=\"\"><br><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-18-40.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"安装Miniconda\"><a href=\"#安装Miniconda\" class=\"headerlink\" title=\"安装Miniconda\"></a>安装Miniconda</h2><p>确保你的安装包放在了data文件夹下<br><code>apt-get update &amp;&amp; apt-get install bzip2 -y &amp;&amp; cd /data &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh</code></p>\n<p>一路按回车、yes等等就成功了.如果需要安装到指定目录,在安装过程中会有提示告诉你让你指定安装路径</p>\n<p><em>注意</em></p>\n<p>在<a href=\"http://10.0.4.228/\" rel=\"external nofollow\" target=\"_blank\">学校机器学习平台</a>上使用时,如果是使用容器的方式,那么新开的容器就可以使用<code>conda</code>命令,不存在<code>conda:command not found</code>的错误信息.</p>\n<p>但是,如果在平台上以<strong>提交任务</strong>的形式来使用带conda的镜像所产生的容器时,就算是在镜像中配置了<code>echo &#39;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&#39; &gt;&gt; ~/.bashrc</code>,当提交任务时环境变量中仍然没有<code>~/anaconda3/bin</code>,这个问题目前没有找到比较方便的解决办法,目前所采用的方式是:</p>\n<p>在提交任务时, 首先加上命令<code>export PATH=&quot;~/anaconda3/bin:$PATH&quot; &amp;&amp;</code></p>\n<p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-25-17.png\" alt=\"\"></p>\n<p>接着又实验了一下<code>echo &#39;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&#39; &gt;&gt; /etc/profile</code>, 正常来说, 如果在容器中这样设置环境变量, 等待下次从镜像创建容器时, 这个环境变量一般并不会生效, 但是不知道这样设置对于提交任务方式来说有没有效, 索性试了一下</p>\n<p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-35-56.png\" alt=\"\"></p>\n<p>根据输出日志来看, 这种方式也并没有奏效</p>\n<p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-39-26.png\" alt=\"\"></p>\n<p>当然, 如果觉得上述配置比较麻烦的话,可以使用<code>Dockerfile</code>的<code>ENV</code>命令来设置环境变量, 这样设置99%是不会有问题的</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM hub.hoc.ccshu.net/wjs/mlunityv060:v1.4.5</span><br><span class=\"line\">ENV PATH /usr/miniconda3/bin:$PATH</span><br></pre></td></tr></table></figure>\n<p>实验了一下,<br>结果如下:</p>\n<p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_15-02-16.png\" alt=\"\"></p>\n<p>表示可以使用<code>conda</code>命令, 但是不能使用<code>conda activate</code>命令激活环境</p>\n<p>根据错误信息, 在<code>Dockerfile</code>中写入以下代码也不可行:</p>\n<p><code>RUN ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh</code></p>\n<p>查了一下相关资料,发现在4.5版本的conda是无解的<br><a href=\"https://github.com/ContinuumIO/docker-images/issues/89\" rel=\"external nofollow\" target=\"_blank\">https://github.com/ContinuumIO/docker-images/issues/89</a></p>\n<p><img src=\"./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_22-38-42.png\" alt=\"\"></p>\n<p>希望4.6版本可以解决吧</p>\n<p><strong>更新2019年1月14日14:26:01</strong><br><strong>已解决</strong></p>\n<p>4.6版本的确可以解决以提交任务模式运行时的问题, 需要使用命令<code>conda run -n [环境名字] [要执行的命令]</code>, 而不是使用<code>conda activate [环境名字]</code>先激活一个环境.</p>\n<p>不过, 更新至4.6版本需要相关配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --add channels conda-canary</span><br><span class=\"line\">conda update conda</span><br></pre></td></tr></table></figure></p>\n<p>我使用Dockerfile来生成镜像, 代码如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM hub.hoc.ccshu.net/wjs/mlunityv060:v1.4.6</span><br><span class=\"line\"></span><br><span class=\"line\">ENV PATH /usr/miniconda3/bin:$PATH</span><br><span class=\"line\">RUN conda config --add channels conda-canary &amp;&amp; conda update conda -y</span><br></pre></td></tr></table></figure></p>\n<p>原因是, 使用Dockerfile比较容易设置环境变量, 减少出错</p>"},{"title":"价值与贝尔曼方程","copyright":true,"top":1,"date":"2019-05-09T10:09:02.000Z","mathjax":true,"_content":"\n# 价值与贝尔曼方程\n\n我们人在做决策的时候往往会判断做这件事的价值和后果，就像失恋了去喝不喝闷酒一样，不同的人有不同的选择，但是选择前肯定会判断这么做能给自己带来什么。\n\n选择去喝酒的人觉得这可以缓解自己的痛苦，这就是判断喝酒这个动作的价值。因为身体原因不选择去喝酒的人觉得喝醉之后身体很不舒服，还会说胡话、闹事，这就是衡量后果、判断喝酒后状态的价值。\n\n在乎过程的会根据动作的价值进行抉择，在乎结果的会根据状态的价值进行抉择。总之，衡量价值，毫无疑问是我们做决策的重要评判标准。\n\n机器也一样，我们想教会机器学会自主决策，必然得让它们有一个价值导向，毕竟它可不会、也决不能像人一样\"没有原因呀，就随便选择了一个而已\"。\n\n本文介绍了**绝大部分强化学习问题及算法**中值函数与贝尔曼方程的定义。因为有一些研究探索的，如好奇心、信息熵等方向的算法对值函数的定义有稍许不同。\n\n<!--more-->\n\n---\n\n注：以下公式及推导过程可能与其他博客、论文、书本上有稍许不同，不过都是经过细细分析，一步步推导的，或许有些公式难以理解，但都是尽可能细化每一处细节。使读者可以更清楚地了解每一个值的来龙去脉。\n\n---\n\n## 值函数\n\n值函数分为状态值函数与动作值函数，分别用来表示状态和状态下执行某动作的好坏程度、优劣程度。\n\n回顾一下回报：\n$$\n\\begin{align*}\nG_{t} &\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\gamma^{3}R_{t+4}+...\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\gamma^{2}R_{t+4}+...)\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma (R_{t+3}+\\gamma R_{t+4}+...))\\\\\n&=R_{t+1}+\\gamma G_{t+1}\n\\end{align*}\n$$\n\n$$\nG_{t}\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+...=\\begin{cases}\n\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\\\\n\\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_{k}\n\\end{cases}\n$$\n\n回顾一下之前的MDP例子：\n\n![](./强化学习之MDP马尔科夫决策过程/MDP.jpg)\n\n将状态用符号表示为\n$$\n\\begin{bmatrix}\n玩游戏 & A\\\\ \n语文 & B\\\\ \n数学 & C\\\\ \n英语 & D\\\\ \n\\mathcal{Pass} & E\\\\ \n睡觉 & F\n\\end{bmatrix}\n$$\n将转移概率矩阵$\\mathcal{P}$写成如下形式\n\n\n\n\n\n|        |  A   |  B   |  C   |  D   |  E   |  F   |\n| :----: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Reward |  -1  |  -2  |  -2  |  -2  |  10  |  0   |\n|   A    | 0.9  | 0.1  |      |      |      |      |\n|   B    | 0.5  |      | 0.5  |      |      |      |\n|   C    |      |      |      | 0.8  |      | 0.2  |\n| D,0.4  |      | 0.2  | 0.4  | 0.4  |      |      |\n| D,0.6  |      |      |      |      | 0.6  |      |\n|   E    |      |      |      |      |      | 1.0  |\n\n其中，D状态有两个动作，但是其0.4概率选到的动作并不一定确定地转移到另一个状态，所以将两个动作分开写，其实除了Reward的每一行都是一个$(s,a)$的状态-动作对，但是除了D状态有特殊外，其他状态的转移都是确定的，于是省略了动作。后续将会看到如果根据$(D,0.4)$这个状态-动作对去进行相应的计算。\n\n### 状态值函数$V(s)$\n\n$\\pi$策略下$s$状态的价值函数可以表示为$v_{\\pi}(s)$，由**期望回报**表示\n\n$$\nv_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}[G_{t}|S_{t}=s] = \\mathbb{E}_{\\pi}\\left [ \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\mid S_{t}=s \\right ], \\ for \\ all \\ s\\in S\n$$\n\n有了这个公式，我们能根据上述表格计算出每个状态的价值吗？当然可以，只是很麻烦，如果对于连续状态空间的问题就不只是麻烦的问题，而是不能计算。\n\n为什么呢？因为要求期望需要遍历所有可能性的episode，连续状态空间根本无法遍历所有的情况。\n\n### 动作值函数$Q(s,a)$\n\n动作值函数与状态值函数在公式表示上差别不大，$\\pi$策略$s$状态下执行a动作的价值函数可以表示为$Q_{\\pi}(s，a)$，由**期望回报**表示\n$$\nQ_{\\pi}(s,a) \\doteq \\mathbb{E}_{\\pi}[G_{t}|S_{t}=s,A_{t}=a] = \\mathbb{E}_{\\pi}\\left [ \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\mid S_{t}=s,A_{t}=a \\right ]\n$$\n\n## 贝尔曼方程\n\n> [贝尔曼方程（Bellman Equation）(百度百科)](https://baike.baidu.com/item/贝尔曼方程/5500990?fr=aladdin)也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。\n>\n> 贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。此方程把“决策问题在特定时间怎么的值”以“来自初始选择的报酬比从初始选择衍生的决策问题的值”的形式表示。借此这个方式把动态最佳化问题变成简单的子问题，而这些子问题遵守从贝尔曼所提出来的“最佳化还原理”。\n\n\n\n**贝尔曼方程将状态值函数$V(s)$与动作值函数$Q(s,a)$、将当前的值函数与之后状态$V(s‘)$或动作的值函数$Q(s’,a‘)$联系起来。**\n\n\n\n### 状态值函数$V(s)$与动作值函数$Q(s,a)$的关系\n\n![](./价值与贝尔曼方程/vs.jpg)\n$$\nv_{\\pi}(s)=\\sum_{a}\\pi(a\\mid s) q_{\\pi}(s,a)\n$$\n![](./价值与贝尔曼方程/qsa.jpg)\n$$\nq_{\\pi}(s,a) = \\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\n$$\n\n### 贝尔曼期望方程\n\n状态值函数$V(s)$可以写成如下形式：\n$$\n\\begin{align*}\nv_{\\pi}(s) & \\doteq \\mathbb{E}_{\\pi}\\left [ G_{t}\\mid S_{t}=s \\right ]\\\\\n&=\\mathbb{E}_{t} \\left [R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s \\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s)\\sum_{s'}\\sum_{r}p(s',r\\mid s,a)\\left[r+\\gamma \\mathbb{E}\\left[G_{t+1}\\mid S_{t+1}=s' \\right]\\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s)\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s) q_{\\pi}(s,a)\n\\end{align*},\nfor \\ all \\ s\\in S\n$$\n\n![](./价值与贝尔曼方程/v.jpg)\n\n看到没有，此时可以将当前状态的状态值$v_{\\pi}(s)$与下一个可到达状态的状态值$v_{\\pi}(s')$联系起来！\n\n动作值函数$Q_{\\pi}(s,a)$也可以进行类似推导：\n$$\n\\begin{align*}\nq_{\\pi}(s,a) & \\doteq \\mathbb{E}_{\\pi}\\left [ G_{t}\\mid S_{t}=s,A_{t}=a \\right ]\\\\\n&=\\mathbb{E}_{t} \\left [R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s,A_{t}=a \\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma \\sum_{a'}\\pi(a'\\mid s') \\mathbb{E}\\left[G_{t+1}\\mid S_{t+1}=s',A_{t+1}=a' \\right]\\right]\\\\\n&=\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma \\sum_{a'}\\pi(a'\\mid s') q_{\\pi}(s',a')\\right]\\\\\n&=\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\n\\end{align*}\n$$\n\n![](./价值与贝尔曼方程/q.jpg)\n\n### 最优值函数\n\n解决一个强化学习问题也就是意味着找到一种选择动作的策略能够获得足够多的回报。如果执行每个动作所产生的转移都是确定的（有限MDP），那么能够定义出一个最优策略，如果一个策略$\\pi'$的所有状态值函数都大于$\\pi$，那么就说策略$\\pi'$更好，但不一定是最好的，我们把最优策略用$\\pi_{*}$表示。\n\n最优状态值函数：\n\n$$\nv_{*}(s)=\\max_{\\pi} v_{\\pi}(s)\n$$\n\n最优动作值函数：\n\n$$\nq_{*}(s,a)=\\max_{\\pi} q_{\\pi}(s,a)\n$$\n\n### 贝尔曼最优方程\n\n$$\n\\begin{align*}\nv_{*}(s) &= \\max_{a} q_{*}(s,a)\\\\\n&=\\max_{a}\\mathbb{E}\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\max_{a}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\right]\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\nq_{*}(s,a) &= \\mathbb{E}\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\mathbb{E}\\left[r+\\gamma \\max_{a'} q_{*}(s',a')\\mid s,a\\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma \\max_{a'}q_{*}(s',a')\\right]\n\\end{align*}\n$$\n\n![](./价值与贝尔曼方程/vq.jpg)\n\n虽然我们已经定义出了最优值函数和最优策略，而且理论上也可以直接计算出来。但是通常情况下我们没法得到这么多的计算资源。与此同时内存溢出也是一个很大的问题，因为很多问题的状态数量太多超过存储范围。对于这些情况我们就不能够使用直接存储每个状态的值函数而是必须使用一种更精简的参数型函数表示的方法。\n\n强化学习的框架迫使我们进行近似求解，而且这个框架同时也很容易进行近似，比如对于很多小概率出现的状态，选择最优解和次优解区别不大。\n\n### 最优策略\n\n定义策略之间的偏序关系\n$$\n\\pi \\geq \\pi' \\ if \\ v_{\\pi}(s) \\geq v_{\\pi'}(s) \\ , \\ \\forall s\n$$\n那么有如下定理成立：\n\n对任意MDP：\n\n- 存在最优策略$\\pi_{\\ast}$，满足$\\pi_{\\ast} \\geq \\pi,\\forall \\pi$\n- 所有最优策略的状态值函数都等于最优状态值函数$v_{\\pi_{\\ast}}(s)=v_{\\ast}(s)$\n- 所有最优策略的动作值函数都等于最优动作值函数$q_{\\pi_{\\ast}}(s,a)=q_{\\ast}(s,a)$\n\n## 例子\n\n对于上述例子和表格，我们来试着计算一下$V(S)、Q(S,A)$。\n\n第一个问题，怎么计算这些值？初始化终态的状态值为0，然后从后向前递归？我们来试一下！\n\n### 只初始化终态\n\n根据上述公式，设$\\gamma =1$：\n$$\n\\begin{align*}\n&v(F)=r=0\\\\\n&q(E，)=1\\times (0+v(F))=0\\\\\n&1表示选择这个动作转移至另一个状态的概率\\\\\n&v(E)=1\\times q(E,)=0\\\\\n&q(D,0.6)=1\\times (10+v(E))=10\\\\\n&q(D,0.4)=0.2\\times(-2+v(B))+0.4\\times(-2+v(C))+0.4\\times(-2+v(D))=\\\\\n&v(D)=0.4\\times q(D,0.4)+0.6\\times q(D,0.6)=\\\\\n&q(C,0.8)=1\\times (-2+v(D))=\\\\\n&q(C,0.2)=1\\times (0+v(F))=0\\\\\n&v(C)=0.2\\times q(C,0.2)+0.8\\times q(C,0.8)=\\\\\n&q(B,0.5_{C})=1\\times (-2+v(C))=\\\\\n&q(B,0.5_{A})=1\\times (-1+v(A))=\\\\\n&v(B)=0.5\\times q(B,0.5_{C})+0.5 \\times q(B,0.5_{A})=\\\\\n&q(A,0.9)=1\\times (-1+v(A))=\\\\\n&q(A,0.1)=1\\times(-2+v(B))=\\\\\n&v(A)=0.1\\times q(A,0.1)+0.9\\times q(A,0.9)=\n\\end{align*}\n$$\n哎呀，卡住了，解不出来，$v(D)、v(A)、v(B)$互相依赖，解不出来，看来这样计算是行不通了。其实，很多问题中终态都很难定义，更别说使用这种方法了。\n\n### 初始化全部状态值\n\n初始化所有状态的值函数为0，即\n$$\nv(s)=0,\\ for\\ all\\ s\\in S\n$$\n\n先试验一下$\\gamma =0.5$，\n\n\n\n\n\n\n|                     V和Q，$\\gamma =0.5$                      | 初始化V计算Q | 迭代→V→Q | 第46轮完全收敛 |\n| :----------------------------------------------------------: | :----------: | :------: | -------------- |\n|                              A                               |      0       |    ……    | -2.171         |\n|                              B                               |      0       |    ……    | -1.880         |\n|                              C                               |      0       |    ……    | 0.651          |\n|                              D                               |      0       |    ……    | 5.627          |\n|                              E                               |      0       |    ……    | 0              |\n|                              F                               |      0       |    ……    | 0              |\n|                    $(A,0.1)\\rightarrow B$                    |      0       |    ……    | -2.940         |\n|                    $(A,0.9)\\rightarrow A$                    |      0       |    ……    | -2.085         |\n|                  $(B,0.5_{A})\\rightarrow A$                  |      0       |    ……    | -2.085         |\n|                  $(B,0.5_{C})\\rightarrow C$                  |      0       |    ……    | -1.675         |\n|                    $(C,0.2)\\rightarrow F$                    |      0       |    ……    | 0              |\n|                    $(C,0.8)\\rightarrow D$                    |      0       |    ……    | 0.814          |\n| $(D,0.4)\\rightarrow \\begin{cases}B,0.2\\\\C,0.4\\\\D,0.4\\end{cases}$ |      0       |    ……    | -0.932         |\n|                    $(D,0.6)\\rightarrow E$                    |      0       |    ……    | 10             |\n|                    $(E,1.0)\\rightarrow F$                    |      0       |    ……    | 0              |\n\n这是代码计算的结果，接下来我使$\\gamma =1$，计算结果如下，每迭代100次输出一下：\n\n![](./价值与贝尔曼方程/example1.png)\n\n可以发现，在700至800次迭代后值函数最终收敛。\n\n如果我将$\\gamma $设置为0.1呢？来看一下结果：\n\n![](./价值与贝尔曼方程/example2.png)\n\n仅仅需要十几次就可以迭代至收敛。\n\n如果设置为0呢？会怎么样？看结果：\n\n![](./价值与贝尔曼方程/example3.png)\n\n仅需一次迭代就可以收敛，而且就是转移状态的立即奖励值，这下可以理解$\\gamma$为什么表示对未来的看重程度了吧。\n\n一般我们是不会将$\\gamma$设置为0的，从这个例子的直观感受也可以得到，就拿$\\gamma =0$与$\\gamma =0.5来比较$：\n\n- $\\gamma =0$状态值最高的是$v(D)=5.2$，这很容易理解，D状态距离最大奖励值10最近，理应最好，这点与$\\gamma =0.5$时相同。\n\n- 但是对于状态C，$\\gamma =0$时认为这个状态最差，$v(C)=-1.6$，$\\gamma =0$时认为这个状态次优，$v(C)=0.651$，其实这就是目光短浅与目光长远的不同，$\\gamma =0$并没有考虑到其附近状态的临近状态的价值，导致其主观的认为最接近我的都是负的，于是状态肯定差。\n\n- 对于动作值也是一样，一个认为次优，一个认为最差。\n\n  \n\n**注意：并不是说$\\gamma$越接近于1越好，因为在有些问题上，$\\gamma=1$时其值函数永远不收敛，必须设置$0 \\leq \\gamma \\lt 1$，值函数才能收敛。为什么呢？试着计算一下$\\gamma^{n}$，看看对不同的$\\gamma$值，$n$取什么值时结果接近0。**\n\n试着计算一下这个例子，红色代表立即奖励，蓝色代表选择动作的概率以及状态转移的概率，小写字母代表动作，大写字母代表状态。\n\n![](./价值与贝尔曼方程/example4.png)\n\n$\\gamma =1$时，迭代100W次也不收敛：\n\n![](./价值与贝尔曼方程/example5.png)\n\n$\\gamma =0.5$时，迭代50多次即可收敛：\n\n![](./价值与贝尔曼方程/example6.png)\n\n","source":"_posts/价值与贝尔曼方程.md","raw":"---\ntitle: 价值与贝尔曼方程\ncopyright: true\ntop: 1\ndate: 2019-05-09 18:09:02\nmathjax: true\ncategories: ReinforcementLearning\ntags:\n- rl\n---\n\n# 价值与贝尔曼方程\n\n我们人在做决策的时候往往会判断做这件事的价值和后果，就像失恋了去喝不喝闷酒一样，不同的人有不同的选择，但是选择前肯定会判断这么做能给自己带来什么。\n\n选择去喝酒的人觉得这可以缓解自己的痛苦，这就是判断喝酒这个动作的价值。因为身体原因不选择去喝酒的人觉得喝醉之后身体很不舒服，还会说胡话、闹事，这就是衡量后果、判断喝酒后状态的价值。\n\n在乎过程的会根据动作的价值进行抉择，在乎结果的会根据状态的价值进行抉择。总之，衡量价值，毫无疑问是我们做决策的重要评判标准。\n\n机器也一样，我们想教会机器学会自主决策，必然得让它们有一个价值导向，毕竟它可不会、也决不能像人一样\"没有原因呀，就随便选择了一个而已\"。\n\n本文介绍了**绝大部分强化学习问题及算法**中值函数与贝尔曼方程的定义。因为有一些研究探索的，如好奇心、信息熵等方向的算法对值函数的定义有稍许不同。\n\n<!--more-->\n\n---\n\n注：以下公式及推导过程可能与其他博客、论文、书本上有稍许不同，不过都是经过细细分析，一步步推导的，或许有些公式难以理解，但都是尽可能细化每一处细节。使读者可以更清楚地了解每一个值的来龙去脉。\n\n---\n\n## 值函数\n\n值函数分为状态值函数与动作值函数，分别用来表示状态和状态下执行某动作的好坏程度、优劣程度。\n\n回顾一下回报：\n$$\n\\begin{align*}\nG_{t} &\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\gamma^{3}R_{t+4}+...\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\gamma^{2}R_{t+4}+...)\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma (R_{t+3}+\\gamma R_{t+4}+...))\\\\\n&=R_{t+1}+\\gamma G_{t+1}\n\\end{align*}\n$$\n\n$$\nG_{t}\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+...=\\begin{cases}\n\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\\\\n\\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_{k}\n\\end{cases}\n$$\n\n回顾一下之前的MDP例子：\n\n![](./强化学习之MDP马尔科夫决策过程/MDP.jpg)\n\n将状态用符号表示为\n$$\n\\begin{bmatrix}\n玩游戏 & A\\\\ \n语文 & B\\\\ \n数学 & C\\\\ \n英语 & D\\\\ \n\\mathcal{Pass} & E\\\\ \n睡觉 & F\n\\end{bmatrix}\n$$\n将转移概率矩阵$\\mathcal{P}$写成如下形式\n\n\n\n\n\n|        |  A   |  B   |  C   |  D   |  E   |  F   |\n| :----: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Reward |  -1  |  -2  |  -2  |  -2  |  10  |  0   |\n|   A    | 0.9  | 0.1  |      |      |      |      |\n|   B    | 0.5  |      | 0.5  |      |      |      |\n|   C    |      |      |      | 0.8  |      | 0.2  |\n| D,0.4  |      | 0.2  | 0.4  | 0.4  |      |      |\n| D,0.6  |      |      |      |      | 0.6  |      |\n|   E    |      |      |      |      |      | 1.0  |\n\n其中，D状态有两个动作，但是其0.4概率选到的动作并不一定确定地转移到另一个状态，所以将两个动作分开写，其实除了Reward的每一行都是一个$(s,a)$的状态-动作对，但是除了D状态有特殊外，其他状态的转移都是确定的，于是省略了动作。后续将会看到如果根据$(D,0.4)$这个状态-动作对去进行相应的计算。\n\n### 状态值函数$V(s)$\n\n$\\pi$策略下$s$状态的价值函数可以表示为$v_{\\pi}(s)$，由**期望回报**表示\n\n$$\nv_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}[G_{t}|S_{t}=s] = \\mathbb{E}_{\\pi}\\left [ \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\mid S_{t}=s \\right ], \\ for \\ all \\ s\\in S\n$$\n\n有了这个公式，我们能根据上述表格计算出每个状态的价值吗？当然可以，只是很麻烦，如果对于连续状态空间的问题就不只是麻烦的问题，而是不能计算。\n\n为什么呢？因为要求期望需要遍历所有可能性的episode，连续状态空间根本无法遍历所有的情况。\n\n### 动作值函数$Q(s,a)$\n\n动作值函数与状态值函数在公式表示上差别不大，$\\pi$策略$s$状态下执行a动作的价值函数可以表示为$Q_{\\pi}(s，a)$，由**期望回报**表示\n$$\nQ_{\\pi}(s,a) \\doteq \\mathbb{E}_{\\pi}[G_{t}|S_{t}=s,A_{t}=a] = \\mathbb{E}_{\\pi}\\left [ \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\mid S_{t}=s,A_{t}=a \\right ]\n$$\n\n## 贝尔曼方程\n\n> [贝尔曼方程（Bellman Equation）(百度百科)](https://baike.baidu.com/item/贝尔曼方程/5500990?fr=aladdin)也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。\n>\n> 贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。此方程把“决策问题在特定时间怎么的值”以“来自初始选择的报酬比从初始选择衍生的决策问题的值”的形式表示。借此这个方式把动态最佳化问题变成简单的子问题，而这些子问题遵守从贝尔曼所提出来的“最佳化还原理”。\n\n\n\n**贝尔曼方程将状态值函数$V(s)$与动作值函数$Q(s,a)$、将当前的值函数与之后状态$V(s‘)$或动作的值函数$Q(s’,a‘)$联系起来。**\n\n\n\n### 状态值函数$V(s)$与动作值函数$Q(s,a)$的关系\n\n![](./价值与贝尔曼方程/vs.jpg)\n$$\nv_{\\pi}(s)=\\sum_{a}\\pi(a\\mid s) q_{\\pi}(s,a)\n$$\n![](./价值与贝尔曼方程/qsa.jpg)\n$$\nq_{\\pi}(s,a) = \\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\n$$\n\n### 贝尔曼期望方程\n\n状态值函数$V(s)$可以写成如下形式：\n$$\n\\begin{align*}\nv_{\\pi}(s) & \\doteq \\mathbb{E}_{\\pi}\\left [ G_{t}\\mid S_{t}=s \\right ]\\\\\n&=\\mathbb{E}_{t} \\left [R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s \\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s)\\sum_{s'}\\sum_{r}p(s',r\\mid s,a)\\left[r+\\gamma \\mathbb{E}\\left[G_{t+1}\\mid S_{t+1}=s' \\right]\\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s)\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s) q_{\\pi}(s,a)\n\\end{align*},\nfor \\ all \\ s\\in S\n$$\n\n![](./价值与贝尔曼方程/v.jpg)\n\n看到没有，此时可以将当前状态的状态值$v_{\\pi}(s)$与下一个可到达状态的状态值$v_{\\pi}(s')$联系起来！\n\n动作值函数$Q_{\\pi}(s,a)$也可以进行类似推导：\n$$\n\\begin{align*}\nq_{\\pi}(s,a) & \\doteq \\mathbb{E}_{\\pi}\\left [ G_{t}\\mid S_{t}=s,A_{t}=a \\right ]\\\\\n&=\\mathbb{E}_{t} \\left [R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s,A_{t}=a \\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma \\sum_{a'}\\pi(a'\\mid s') \\mathbb{E}\\left[G_{t+1}\\mid S_{t+1}=s',A_{t+1}=a' \\right]\\right]\\\\\n&=\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma \\sum_{a'}\\pi(a'\\mid s') q_{\\pi}(s',a')\\right]\\\\\n&=\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\n\\end{align*}\n$$\n\n![](./价值与贝尔曼方程/q.jpg)\n\n### 最优值函数\n\n解决一个强化学习问题也就是意味着找到一种选择动作的策略能够获得足够多的回报。如果执行每个动作所产生的转移都是确定的（有限MDP），那么能够定义出一个最优策略，如果一个策略$\\pi'$的所有状态值函数都大于$\\pi$，那么就说策略$\\pi'$更好，但不一定是最好的，我们把最优策略用$\\pi_{*}$表示。\n\n最优状态值函数：\n\n$$\nv_{*}(s)=\\max_{\\pi} v_{\\pi}(s)\n$$\n\n最优动作值函数：\n\n$$\nq_{*}(s,a)=\\max_{\\pi} q_{\\pi}(s,a)\n$$\n\n### 贝尔曼最优方程\n\n$$\n\\begin{align*}\nv_{*}(s) &= \\max_{a} q_{*}(s,a)\\\\\n&=\\max_{a}\\mathbb{E}\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\max_{a}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\right]\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\nq_{*}(s,a) &= \\mathbb{E}\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\mathbb{E}\\left[r+\\gamma \\max_{a'} q_{*}(s',a')\\mid s,a\\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma \\max_{a'}q_{*}(s',a')\\right]\n\\end{align*}\n$$\n\n![](./价值与贝尔曼方程/vq.jpg)\n\n虽然我们已经定义出了最优值函数和最优策略，而且理论上也可以直接计算出来。但是通常情况下我们没法得到这么多的计算资源。与此同时内存溢出也是一个很大的问题，因为很多问题的状态数量太多超过存储范围。对于这些情况我们就不能够使用直接存储每个状态的值函数而是必须使用一种更精简的参数型函数表示的方法。\n\n强化学习的框架迫使我们进行近似求解，而且这个框架同时也很容易进行近似，比如对于很多小概率出现的状态，选择最优解和次优解区别不大。\n\n### 最优策略\n\n定义策略之间的偏序关系\n$$\n\\pi \\geq \\pi' \\ if \\ v_{\\pi}(s) \\geq v_{\\pi'}(s) \\ , \\ \\forall s\n$$\n那么有如下定理成立：\n\n对任意MDP：\n\n- 存在最优策略$\\pi_{\\ast}$，满足$\\pi_{\\ast} \\geq \\pi,\\forall \\pi$\n- 所有最优策略的状态值函数都等于最优状态值函数$v_{\\pi_{\\ast}}(s)=v_{\\ast}(s)$\n- 所有最优策略的动作值函数都等于最优动作值函数$q_{\\pi_{\\ast}}(s,a)=q_{\\ast}(s,a)$\n\n## 例子\n\n对于上述例子和表格，我们来试着计算一下$V(S)、Q(S,A)$。\n\n第一个问题，怎么计算这些值？初始化终态的状态值为0，然后从后向前递归？我们来试一下！\n\n### 只初始化终态\n\n根据上述公式，设$\\gamma =1$：\n$$\n\\begin{align*}\n&v(F)=r=0\\\\\n&q(E，)=1\\times (0+v(F))=0\\\\\n&1表示选择这个动作转移至另一个状态的概率\\\\\n&v(E)=1\\times q(E,)=0\\\\\n&q(D,0.6)=1\\times (10+v(E))=10\\\\\n&q(D,0.4)=0.2\\times(-2+v(B))+0.4\\times(-2+v(C))+0.4\\times(-2+v(D))=\\\\\n&v(D)=0.4\\times q(D,0.4)+0.6\\times q(D,0.6)=\\\\\n&q(C,0.8)=1\\times (-2+v(D))=\\\\\n&q(C,0.2)=1\\times (0+v(F))=0\\\\\n&v(C)=0.2\\times q(C,0.2)+0.8\\times q(C,0.8)=\\\\\n&q(B,0.5_{C})=1\\times (-2+v(C))=\\\\\n&q(B,0.5_{A})=1\\times (-1+v(A))=\\\\\n&v(B)=0.5\\times q(B,0.5_{C})+0.5 \\times q(B,0.5_{A})=\\\\\n&q(A,0.9)=1\\times (-1+v(A))=\\\\\n&q(A,0.1)=1\\times(-2+v(B))=\\\\\n&v(A)=0.1\\times q(A,0.1)+0.9\\times q(A,0.9)=\n\\end{align*}\n$$\n哎呀，卡住了，解不出来，$v(D)、v(A)、v(B)$互相依赖，解不出来，看来这样计算是行不通了。其实，很多问题中终态都很难定义，更别说使用这种方法了。\n\n### 初始化全部状态值\n\n初始化所有状态的值函数为0，即\n$$\nv(s)=0,\\ for\\ all\\ s\\in S\n$$\n\n先试验一下$\\gamma =0.5$，\n\n\n\n\n\n\n|                     V和Q，$\\gamma =0.5$                      | 初始化V计算Q | 迭代→V→Q | 第46轮完全收敛 |\n| :----------------------------------------------------------: | :----------: | :------: | -------------- |\n|                              A                               |      0       |    ……    | -2.171         |\n|                              B                               |      0       |    ……    | -1.880         |\n|                              C                               |      0       |    ……    | 0.651          |\n|                              D                               |      0       |    ……    | 5.627          |\n|                              E                               |      0       |    ……    | 0              |\n|                              F                               |      0       |    ……    | 0              |\n|                    $(A,0.1)\\rightarrow B$                    |      0       |    ……    | -2.940         |\n|                    $(A,0.9)\\rightarrow A$                    |      0       |    ……    | -2.085         |\n|                  $(B,0.5_{A})\\rightarrow A$                  |      0       |    ……    | -2.085         |\n|                  $(B,0.5_{C})\\rightarrow C$                  |      0       |    ……    | -1.675         |\n|                    $(C,0.2)\\rightarrow F$                    |      0       |    ……    | 0              |\n|                    $(C,0.8)\\rightarrow D$                    |      0       |    ……    | 0.814          |\n| $(D,0.4)\\rightarrow \\begin{cases}B,0.2\\\\C,0.4\\\\D,0.4\\end{cases}$ |      0       |    ……    | -0.932         |\n|                    $(D,0.6)\\rightarrow E$                    |      0       |    ……    | 10             |\n|                    $(E,1.0)\\rightarrow F$                    |      0       |    ……    | 0              |\n\n这是代码计算的结果，接下来我使$\\gamma =1$，计算结果如下，每迭代100次输出一下：\n\n![](./价值与贝尔曼方程/example1.png)\n\n可以发现，在700至800次迭代后值函数最终收敛。\n\n如果我将$\\gamma $设置为0.1呢？来看一下结果：\n\n![](./价值与贝尔曼方程/example2.png)\n\n仅仅需要十几次就可以迭代至收敛。\n\n如果设置为0呢？会怎么样？看结果：\n\n![](./价值与贝尔曼方程/example3.png)\n\n仅需一次迭代就可以收敛，而且就是转移状态的立即奖励值，这下可以理解$\\gamma$为什么表示对未来的看重程度了吧。\n\n一般我们是不会将$\\gamma$设置为0的，从这个例子的直观感受也可以得到，就拿$\\gamma =0$与$\\gamma =0.5来比较$：\n\n- $\\gamma =0$状态值最高的是$v(D)=5.2$，这很容易理解，D状态距离最大奖励值10最近，理应最好，这点与$\\gamma =0.5$时相同。\n\n- 但是对于状态C，$\\gamma =0$时认为这个状态最差，$v(C)=-1.6$，$\\gamma =0$时认为这个状态次优，$v(C)=0.651$，其实这就是目光短浅与目光长远的不同，$\\gamma =0$并没有考虑到其附近状态的临近状态的价值，导致其主观的认为最接近我的都是负的，于是状态肯定差。\n\n- 对于动作值也是一样，一个认为次优，一个认为最差。\n\n  \n\n**注意：并不是说$\\gamma$越接近于1越好，因为在有些问题上，$\\gamma=1$时其值函数永远不收敛，必须设置$0 \\leq \\gamma \\lt 1$，值函数才能收敛。为什么呢？试着计算一下$\\gamma^{n}$，看看对不同的$\\gamma$值，$n$取什么值时结果接近0。**\n\n试着计算一下这个例子，红色代表立即奖励，蓝色代表选择动作的概率以及状态转移的概率，小写字母代表动作，大写字母代表状态。\n\n![](./价值与贝尔曼方程/example4.png)\n\n$\\gamma =1$时，迭代100W次也不收敛：\n\n![](./价值与贝尔曼方程/example5.png)\n\n$\\gamma =0.5$时，迭代50多次即可收敛：\n\n![](./价值与贝尔曼方程/example6.png)\n\n","slug":"价值与贝尔曼方程","published":1,"updated":"2019-05-13T10:11:12.683Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma3r0033ekve0yhbtcsx","content":"<h1 id=\"价值与贝尔曼方程\"><a href=\"#价值与贝尔曼方程\" class=\"headerlink\" title=\"价值与贝尔曼方程\"></a>价值与贝尔曼方程</h1><p>我们人在做决策的时候往往会判断做这件事的价值和后果，就像失恋了去喝不喝闷酒一样，不同的人有不同的选择，但是选择前肯定会判断这么做能给自己带来什么。</p>\n<p>选择去喝酒的人觉得这可以缓解自己的痛苦，这就是判断喝酒这个动作的价值。因为身体原因不选择去喝酒的人觉得喝醉之后身体很不舒服，还会说胡话、闹事，这就是衡量后果、判断喝酒后状态的价值。</p>\n<p>在乎过程的会根据动作的价值进行抉择，在乎结果的会根据状态的价值进行抉择。总之，衡量价值，毫无疑问是我们做决策的重要评判标准。</p>\n<p>机器也一样，我们想教会机器学会自主决策，必然得让它们有一个价值导向，毕竟它可不会、也决不能像人一样”没有原因呀，就随便选择了一个而已”。</p>\n<p>本文介绍了<strong>绝大部分强化学习问题及算法</strong>中值函数与贝尔曼方程的定义。因为有一些研究探索的，如好奇心、信息熵等方向的算法对值函数的定义有稍许不同。</p>\n<a id=\"more\"></a>\n<hr>\n<p>注：以下公式及推导过程可能与其他博客、论文、书本上有稍许不同，不过都是经过细细分析，一步步推导的，或许有些公式难以理解，但都是尽可能细化每一处细节。使读者可以更清楚地了解每一个值的来龙去脉。</p>\n<hr>\n<h2 id=\"值函数\"><a href=\"#值函数\" class=\"headerlink\" title=\"值函数\"></a>值函数</h2><p>值函数分为状态值函数与动作值函数，分别用来表示状态和状态下执行某动作的好坏程度、优劣程度。</p>\n<p>回顾一下回报：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nG_{t} &\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\gamma^{3}R_{t+4}+...\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\gamma^{2}R_{t+4}+...)\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma (R_{t+3}+\\gamma R_{t+4}+...))\\\\\n&=R_{t+1}+\\gamma G_{t+1}\n\\end{align*}</script><script type=\"math/tex; mode=display\">\nG_{t}\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+...=\\begin{cases}\n\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\\\\n\\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_{k}\n\\end{cases}</script><p>回顾一下之前的MDP例子：</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/MDP.jpg\" alt=\"\"></p>\n<p>将状态用符号表示为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n玩游戏 & A\\\\ \n语文 & B\\\\ \n数学 & C\\\\ \n英语 & D\\\\ \n\\mathcal{Pass} & E\\\\ \n睡觉 & F\n\\end{bmatrix}</script><p>将转移概率矩阵$\\mathcal{P}$写成如下形式</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th style=\"text-align:center\">A</th>\n<th style=\"text-align:center\">B</th>\n<th style=\"text-align:center\">C</th>\n<th style=\"text-align:center\">D</th>\n<th style=\"text-align:center\">E</th>\n<th style=\"text-align:center\">F</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Reward</td>\n<td style=\"text-align:center\">-1</td>\n<td style=\"text-align:center\">-2</td>\n<td style=\"text-align:center\">-2</td>\n<td style=\"text-align:center\">-2</td>\n<td style=\"text-align:center\">10</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">0.9</td>\n<td style=\"text-align:center\">0.1</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">B</td>\n<td style=\"text-align:center\">0.5</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">0.5</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">C</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">0.8</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">0.2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">D,0.4</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">0.2</td>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">D,0.6</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">E</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">1.0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>其中，D状态有两个动作，但是其0.4概率选到的动作并不一定确定地转移到另一个状态，所以将两个动作分开写，其实除了Reward的每一行都是一个$(s,a)$的状态-动作对，但是除了D状态有特殊外，其他状态的转移都是确定的，于是省略了动作。后续将会看到如果根据$(D,0.4)$这个状态-动作对去进行相应的计算。</p>\n<h3 id=\"状态值函数-V-s\"><a href=\"#状态值函数-V-s\" class=\"headerlink\" title=\"状态值函数$V(s)$\"></a>状态值函数$V(s)$</h3><p>$\\pi$策略下$s$状态的价值函数可以表示为$v_{\\pi}(s)$，由<strong>期望回报</strong>表示</p>\n<script type=\"math/tex; mode=display\">\nv_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}[G_{t}|S_{t}=s] = \\mathbb{E}_{\\pi}\\left [ \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\mid S_{t}=s \\right ], \\ for \\ all \\ s\\in S</script><p>有了这个公式，我们能根据上述表格计算出每个状态的价值吗？当然可以，只是很麻烦，如果对于连续状态空间的问题就不只是麻烦的问题，而是不能计算。</p>\n<p>为什么呢？因为要求期望需要遍历所有可能性的episode，连续状态空间根本无法遍历所有的情况。</p>\n<h3 id=\"动作值函数-Q-s-a\"><a href=\"#动作值函数-Q-s-a\" class=\"headerlink\" title=\"动作值函数$Q(s,a)$\"></a>动作值函数$Q(s,a)$</h3><p>动作值函数与状态值函数在公式表示上差别不大，$\\pi$策略$s$状态下执行a动作的价值函数可以表示为$Q_{\\pi}(s，a)$，由<strong>期望回报</strong>表示</p>\n<script type=\"math/tex; mode=display\">\nQ_{\\pi}(s,a) \\doteq \\mathbb{E}_{\\pi}[G_{t}|S_{t}=s,A_{t}=a] = \\mathbb{E}_{\\pi}\\left [ \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\mid S_{t}=s,A_{t}=a \\right ]</script><h2 id=\"贝尔曼方程\"><a href=\"#贝尔曼方程\" class=\"headerlink\" title=\"贝尔曼方程\"></a>贝尔曼方程</h2><blockquote>\n<p><a href=\"https://baike.baidu.com/item/贝尔曼方程/5500990?fr=aladdin\" rel=\"external nofollow\" target=\"_blank\">贝尔曼方程（Bellman Equation）(百度百科)</a>也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。</p>\n<p>贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。此方程把“决策问题在特定时间怎么的值”以“来自初始选择的报酬比从初始选择衍生的决策问题的值”的形式表示。借此这个方式把动态最佳化问题变成简单的子问题，而这些子问题遵守从贝尔曼所提出来的“最佳化还原理”。</p>\n</blockquote>\n<p><strong>贝尔曼方程将状态值函数$V(s)$与动作值函数$Q(s,a)$、将当前的值函数与之后状态$V(s‘)$或动作的值函数$Q(s’,a‘)$联系起来。</strong></p>\n<h3 id=\"状态值函数-V-s-与动作值函数-Q-s-a-的关系\"><a href=\"#状态值函数-V-s-与动作值函数-Q-s-a-的关系\" class=\"headerlink\" title=\"状态值函数$V(s)$与动作值函数$Q(s,a)$的关系\"></a>状态值函数$V(s)$与动作值函数$Q(s,a)$的关系</h3><p><img src=\"./价值与贝尔曼方程/vs.jpg\" alt=\"\"></p>\n<script type=\"math/tex; mode=display\">\nv_{\\pi}(s)=\\sum_{a}\\pi(a\\mid s) q_{\\pi}(s,a)</script><p><img src=\"./价值与贝尔曼方程/qsa.jpg\" alt=\"\"></p>\n<script type=\"math/tex; mode=display\">\nq_{\\pi}(s,a) = \\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]</script><h3 id=\"贝尔曼期望方程\"><a href=\"#贝尔曼期望方程\" class=\"headerlink\" title=\"贝尔曼期望方程\"></a>贝尔曼期望方程</h3><p>状态值函数$V(s)$可以写成如下形式：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nv_{\\pi}(s) & \\doteq \\mathbb{E}_{\\pi}\\left [ G_{t}\\mid S_{t}=s \\right ]\\\\\n&=\\mathbb{E}_{t} \\left [R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s \\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s)\\sum_{s'}\\sum_{r}p(s',r\\mid s,a)\\left[r+\\gamma \\mathbb{E}\\left[G_{t+1}\\mid S_{t+1}=s' \\right]\\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s)\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s) q_{\\pi}(s,a)\n\\end{align*},\nfor \\ all \\ s\\in S</script><p><img src=\"./价值与贝尔曼方程/v.jpg\" alt=\"\"></p>\n<p>看到没有，此时可以将当前状态的状态值$v_{\\pi}(s)$与下一个可到达状态的状态值$v_{\\pi}(s’)$联系起来！</p>\n<p>动作值函数$Q_{\\pi}(s,a)$也可以进行类似推导：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nq_{\\pi}(s,a) & \\doteq \\mathbb{E}_{\\pi}\\left [ G_{t}\\mid S_{t}=s,A_{t}=a \\right ]\\\\\n&=\\mathbb{E}_{t} \\left [R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s,A_{t}=a \\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma \\sum_{a'}\\pi(a'\\mid s') \\mathbb{E}\\left[G_{t+1}\\mid S_{t+1}=s',A_{t+1}=a' \\right]\\right]\\\\\n&=\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma \\sum_{a'}\\pi(a'\\mid s') q_{\\pi}(s',a')\\right]\\\\\n&=\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\n\\end{align*}</script><p><img src=\"./价值与贝尔曼方程/q.jpg\" alt=\"\"></p>\n<h3 id=\"最优值函数\"><a href=\"#最优值函数\" class=\"headerlink\" title=\"最优值函数\"></a>最优值函数</h3><p>解决一个强化学习问题也就是意味着找到一种选择动作的策略能够获得足够多的回报。如果执行每个动作所产生的转移都是确定的（有限MDP），那么能够定义出一个最优策略，如果一个策略$\\pi’$的所有状态值函数都大于$\\pi$，那么就说策略$\\pi’$更好，但不一定是最好的，我们把最优策略用$\\pi_{*}$表示。</p>\n<p>最优状态值函数：</p>\n<script type=\"math/tex; mode=display\">\nv_{*}(s)=\\max_{\\pi} v_{\\pi}(s)</script><p>最优动作值函数：</p>\n<script type=\"math/tex; mode=display\">\nq_{*}(s,a)=\\max_{\\pi} q_{\\pi}(s,a)</script><h3 id=\"贝尔曼最优方程\"><a href=\"#贝尔曼最优方程\" class=\"headerlink\" title=\"贝尔曼最优方程\"></a>贝尔曼最优方程</h3><script type=\"math/tex; mode=display\">\n\\begin{align*}\nv_{*}(s) &= \\max_{a} q_{*}(s,a)\\\\\n&=\\max_{a}\\mathbb{E}\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\max_{a}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\right]\n\\end{align*}</script><script type=\"math/tex; mode=display\">\n\\begin{align*}\nq_{*}(s,a) &= \\mathbb{E}\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\mathbb{E}\\left[r+\\gamma \\max_{a'} q_{*}(s',a')\\mid s,a\\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma \\max_{a'}q_{*}(s',a')\\right]\n\\end{align*}</script><p><img src=\"./价值与贝尔曼方程/vq.jpg\" alt=\"\"></p>\n<p>虽然我们已经定义出了最优值函数和最优策略，而且理论上也可以直接计算出来。但是通常情况下我们没法得到这么多的计算资源。与此同时内存溢出也是一个很大的问题，因为很多问题的状态数量太多超过存储范围。对于这些情况我们就不能够使用直接存储每个状态的值函数而是必须使用一种更精简的参数型函数表示的方法。</p>\n<p>强化学习的框架迫使我们进行近似求解，而且这个框架同时也很容易进行近似，比如对于很多小概率出现的状态，选择最优解和次优解区别不大。</p>\n<h3 id=\"最优策略\"><a href=\"#最优策略\" class=\"headerlink\" title=\"最优策略\"></a>最优策略</h3><p>定义策略之间的偏序关系</p>\n<script type=\"math/tex; mode=display\">\n\\pi \\geq \\pi' \\ if \\ v_{\\pi}(s) \\geq v_{\\pi'}(s) \\ , \\ \\forall s</script><p>那么有如下定理成立：</p>\n<p>对任意MDP：</p>\n<ul>\n<li>存在最优策略$\\pi_{\\ast}$，满足$\\pi_{\\ast} \\geq \\pi,\\forall \\pi$</li>\n<li>所有最优策略的状态值函数都等于最优状态值函数$v_{\\pi_{\\ast}}(s)=v_{\\ast}(s)$</li>\n<li>所有最优策略的动作值函数都等于最优动作值函数$q_{\\pi_{\\ast}}(s,a)=q_{\\ast}(s,a)$</li>\n</ul>\n<h2 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h2><p>对于上述例子和表格，我们来试着计算一下$V(S)、Q(S,A)$。</p>\n<p>第一个问题，怎么计算这些值？初始化终态的状态值为0，然后从后向前递归？我们来试一下！</p>\n<h3 id=\"只初始化终态\"><a href=\"#只初始化终态\" class=\"headerlink\" title=\"只初始化终态\"></a>只初始化终态</h3><p>根据上述公式，设$\\gamma =1$：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\n&v(F)=r=0\\\\\n&q(E，)=1\\times (0+v(F))=0\\\\\n&1表示选择这个动作转移至另一个状态的概率\\\\\n&v(E)=1\\times q(E,)=0\\\\\n&q(D,0.6)=1\\times (10+v(E))=10\\\\\n&q(D,0.4)=0.2\\times(-2+v(B))+0.4\\times(-2+v(C))+0.4\\times(-2+v(D))=\\\\\n&v(D)=0.4\\times q(D,0.4)+0.6\\times q(D,0.6)=\\\\\n&q(C,0.8)=1\\times (-2+v(D))=\\\\\n&q(C,0.2)=1\\times (0+v(F))=0\\\\\n&v(C)=0.2\\times q(C,0.2)+0.8\\times q(C,0.8)=\\\\\n&q(B,0.5_{C})=1\\times (-2+v(C))=\\\\\n&q(B,0.5_{A})=1\\times (-1+v(A))=\\\\\n&v(B)=0.5\\times q(B,0.5_{C})+0.5 \\times q(B,0.5_{A})=\\\\\n&q(A,0.9)=1\\times (-1+v(A))=\\\\\n&q(A,0.1)=1\\times(-2+v(B))=\\\\\n&v(A)=0.1\\times q(A,0.1)+0.9\\times q(A,0.9)=\n\\end{align*}</script><p>哎呀，卡住了，解不出来，$v(D)、v(A)、v(B)$互相依赖，解不出来，看来这样计算是行不通了。其实，很多问题中终态都很难定义，更别说使用这种方法了。</p>\n<h3 id=\"初始化全部状态值\"><a href=\"#初始化全部状态值\" class=\"headerlink\" title=\"初始化全部状态值\"></a>初始化全部状态值</h3><p>初始化所有状态的值函数为0，即</p>\n<script type=\"math/tex; mode=display\">\nv(s)=0,\\ for\\ all\\ s\\in S</script><p>先试验一下$\\gamma =0.5$，</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">V和Q，$\\gamma =0.5$</th>\n<th style=\"text-align:center\">初始化V计算Q</th>\n<th style=\"text-align:center\">迭代→V→Q</th>\n<th>第46轮完全收敛</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-2.171</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">B</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-1.880</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">C</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0.651</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">D</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>5.627</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">E</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">F</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(A,0.1)\\rightarrow B$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-2.940</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(A,0.9)\\rightarrow A$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-2.085</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(B,0.5_{A})\\rightarrow A$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-2.085</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(B,0.5_{C})\\rightarrow C$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-1.675</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(C,0.2)\\rightarrow F$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(C,0.8)\\rightarrow D$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0.814</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(D,0.4)\\rightarrow \\begin{cases}B,0.2\\\\C,0.4\\\\D,0.4\\end{cases}$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-0.932</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(D,0.6)\\rightarrow E$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>10</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(E,1.0)\\rightarrow F$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>这是代码计算的结果，接下来我使$\\gamma =1$，计算结果如下，每迭代100次输出一下：</p>\n<p><img src=\"./价值与贝尔曼方程/example1.png\" alt=\"\"></p>\n<p>可以发现，在700至800次迭代后值函数最终收敛。</p>\n<p>如果我将$\\gamma $设置为0.1呢？来看一下结果：</p>\n<p><img src=\"./价值与贝尔曼方程/example2.png\" alt=\"\"></p>\n<p>仅仅需要十几次就可以迭代至收敛。</p>\n<p>如果设置为0呢？会怎么样？看结果：</p>\n<p><img src=\"./价值与贝尔曼方程/example3.png\" alt=\"\"></p>\n<p>仅需一次迭代就可以收敛，而且就是转移状态的立即奖励值，这下可以理解$\\gamma$为什么表示对未来的看重程度了吧。</p>\n<p>一般我们是不会将$\\gamma$设置为0的，从这个例子的直观感受也可以得到，就拿$\\gamma =0$与$\\gamma =0.5来比较$：</p>\n<ul>\n<li><p>$\\gamma =0$状态值最高的是$v(D)=5.2$，这很容易理解，D状态距离最大奖励值10最近，理应最好，这点与$\\gamma =0.5$时相同。</p>\n</li>\n<li><p>但是对于状态C，$\\gamma =0$时认为这个状态最差，$v(C)=-1.6$，$\\gamma =0$时认为这个状态次优，$v(C)=0.651$，其实这就是目光短浅与目光长远的不同，$\\gamma =0$并没有考虑到其附近状态的临近状态的价值，导致其主观的认为最接近我的都是负的，于是状态肯定差。</p>\n</li>\n<li><p>对于动作值也是一样，一个认为次优，一个认为最差。</p>\n</li>\n</ul>\n<p><strong>注意：并不是说$\\gamma$越接近于1越好，因为在有些问题上，$\\gamma=1$时其值函数永远不收敛，必须设置$0 \\leq \\gamma \\lt 1$，值函数才能收敛。为什么呢？试着计算一下$\\gamma^{n}$，看看对不同的$\\gamma$值，$n$取什么值时结果接近0。</strong></p>\n<p>试着计算一下这个例子，红色代表立即奖励，蓝色代表选择动作的概率以及状态转移的概率，小写字母代表动作，大写字母代表状态。</p>\n<p><img src=\"./价值与贝尔曼方程/example4.png\" alt=\"\"></p>\n<p>$\\gamma =1$时，迭代100W次也不收敛：</p>\n<p><img src=\"./价值与贝尔曼方程/example5.png\" alt=\"\"></p>\n<p>$\\gamma =0.5$时，迭代50多次即可收敛：</p>\n<p><img src=\"./价值与贝尔曼方程/example6.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"价值与贝尔曼方程\"><a href=\"#价值与贝尔曼方程\" class=\"headerlink\" title=\"价值与贝尔曼方程\"></a>价值与贝尔曼方程</h1><p>我们人在做决策的时候往往会判断做这件事的价值和后果，就像失恋了去喝不喝闷酒一样，不同的人有不同的选择，但是选择前肯定会判断这么做能给自己带来什么。</p>\n<p>选择去喝酒的人觉得这可以缓解自己的痛苦，这就是判断喝酒这个动作的价值。因为身体原因不选择去喝酒的人觉得喝醉之后身体很不舒服，还会说胡话、闹事，这就是衡量后果、判断喝酒后状态的价值。</p>\n<p>在乎过程的会根据动作的价值进行抉择，在乎结果的会根据状态的价值进行抉择。总之，衡量价值，毫无疑问是我们做决策的重要评判标准。</p>\n<p>机器也一样，我们想教会机器学会自主决策，必然得让它们有一个价值导向，毕竟它可不会、也决不能像人一样”没有原因呀，就随便选择了一个而已”。</p>\n<p>本文介绍了<strong>绝大部分强化学习问题及算法</strong>中值函数与贝尔曼方程的定义。因为有一些研究探索的，如好奇心、信息熵等方向的算法对值函数的定义有稍许不同。</p>","more":"<hr>\n<p>注：以下公式及推导过程可能与其他博客、论文、书本上有稍许不同，不过都是经过细细分析，一步步推导的，或许有些公式难以理解，但都是尽可能细化每一处细节。使读者可以更清楚地了解每一个值的来龙去脉。</p>\n<hr>\n<h2 id=\"值函数\"><a href=\"#值函数\" class=\"headerlink\" title=\"值函数\"></a>值函数</h2><p>值函数分为状态值函数与动作值函数，分别用来表示状态和状态下执行某动作的好坏程度、优劣程度。</p>\n<p>回顾一下回报：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nG_{t} &\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\gamma^{3}R_{t+4}+...\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\gamma^{2}R_{t+4}+...)\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma (R_{t+3}+\\gamma R_{t+4}+...))\\\\\n&=R_{t+1}+\\gamma G_{t+1}\n\\end{align*}</script><script type=\"math/tex; mode=display\">\nG_{t}\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+...=\\begin{cases}\n\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\\\\n\\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_{k}\n\\end{cases}</script><p>回顾一下之前的MDP例子：</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/MDP.jpg\" alt=\"\"></p>\n<p>将状态用符号表示为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n玩游戏 & A\\\\ \n语文 & B\\\\ \n数学 & C\\\\ \n英语 & D\\\\ \n\\mathcal{Pass} & E\\\\ \n睡觉 & F\n\\end{bmatrix}</script><p>将转移概率矩阵$\\mathcal{P}$写成如下形式</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th style=\"text-align:center\">A</th>\n<th style=\"text-align:center\">B</th>\n<th style=\"text-align:center\">C</th>\n<th style=\"text-align:center\">D</th>\n<th style=\"text-align:center\">E</th>\n<th style=\"text-align:center\">F</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Reward</td>\n<td style=\"text-align:center\">-1</td>\n<td style=\"text-align:center\">-2</td>\n<td style=\"text-align:center\">-2</td>\n<td style=\"text-align:center\">-2</td>\n<td style=\"text-align:center\">10</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">0.9</td>\n<td style=\"text-align:center\">0.1</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">B</td>\n<td style=\"text-align:center\">0.5</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">0.5</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">C</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">0.8</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">0.2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">D,0.4</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">0.2</td>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">D,0.6</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">E</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">1.0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>其中，D状态有两个动作，但是其0.4概率选到的动作并不一定确定地转移到另一个状态，所以将两个动作分开写，其实除了Reward的每一行都是一个$(s,a)$的状态-动作对，但是除了D状态有特殊外，其他状态的转移都是确定的，于是省略了动作。后续将会看到如果根据$(D,0.4)$这个状态-动作对去进行相应的计算。</p>\n<h3 id=\"状态值函数-V-s\"><a href=\"#状态值函数-V-s\" class=\"headerlink\" title=\"状态值函数$V(s)$\"></a>状态值函数$V(s)$</h3><p>$\\pi$策略下$s$状态的价值函数可以表示为$v_{\\pi}(s)$，由<strong>期望回报</strong>表示</p>\n<script type=\"math/tex; mode=display\">\nv_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}[G_{t}|S_{t}=s] = \\mathbb{E}_{\\pi}\\left [ \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\mid S_{t}=s \\right ], \\ for \\ all \\ s\\in S</script><p>有了这个公式，我们能根据上述表格计算出每个状态的价值吗？当然可以，只是很麻烦，如果对于连续状态空间的问题就不只是麻烦的问题，而是不能计算。</p>\n<p>为什么呢？因为要求期望需要遍历所有可能性的episode，连续状态空间根本无法遍历所有的情况。</p>\n<h3 id=\"动作值函数-Q-s-a\"><a href=\"#动作值函数-Q-s-a\" class=\"headerlink\" title=\"动作值函数$Q(s,a)$\"></a>动作值函数$Q(s,a)$</h3><p>动作值函数与状态值函数在公式表示上差别不大，$\\pi$策略$s$状态下执行a动作的价值函数可以表示为$Q_{\\pi}(s，a)$，由<strong>期望回报</strong>表示</p>\n<script type=\"math/tex; mode=display\">\nQ_{\\pi}(s,a) \\doteq \\mathbb{E}_{\\pi}[G_{t}|S_{t}=s,A_{t}=a] = \\mathbb{E}_{\\pi}\\left [ \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\mid S_{t}=s,A_{t}=a \\right ]</script><h2 id=\"贝尔曼方程\"><a href=\"#贝尔曼方程\" class=\"headerlink\" title=\"贝尔曼方程\"></a>贝尔曼方程</h2><blockquote>\n<p><a href=\"https://baike.baidu.com/item/贝尔曼方程/5500990?fr=aladdin\" rel=\"external nofollow\" target=\"_blank\">贝尔曼方程（Bellman Equation）(百度百科)</a>也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。</p>\n<p>贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。此方程把“决策问题在特定时间怎么的值”以“来自初始选择的报酬比从初始选择衍生的决策问题的值”的形式表示。借此这个方式把动态最佳化问题变成简单的子问题，而这些子问题遵守从贝尔曼所提出来的“最佳化还原理”。</p>\n</blockquote>\n<p><strong>贝尔曼方程将状态值函数$V(s)$与动作值函数$Q(s,a)$、将当前的值函数与之后状态$V(s‘)$或动作的值函数$Q(s’,a‘)$联系起来。</strong></p>\n<h3 id=\"状态值函数-V-s-与动作值函数-Q-s-a-的关系\"><a href=\"#状态值函数-V-s-与动作值函数-Q-s-a-的关系\" class=\"headerlink\" title=\"状态值函数$V(s)$与动作值函数$Q(s,a)$的关系\"></a>状态值函数$V(s)$与动作值函数$Q(s,a)$的关系</h3><p><img src=\"./价值与贝尔曼方程/vs.jpg\" alt=\"\"></p>\n<script type=\"math/tex; mode=display\">\nv_{\\pi}(s)=\\sum_{a}\\pi(a\\mid s) q_{\\pi}(s,a)</script><p><img src=\"./价值与贝尔曼方程/qsa.jpg\" alt=\"\"></p>\n<script type=\"math/tex; mode=display\">\nq_{\\pi}(s,a) = \\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]</script><h3 id=\"贝尔曼期望方程\"><a href=\"#贝尔曼期望方程\" class=\"headerlink\" title=\"贝尔曼期望方程\"></a>贝尔曼期望方程</h3><p>状态值函数$V(s)$可以写成如下形式：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nv_{\\pi}(s) & \\doteq \\mathbb{E}_{\\pi}\\left [ G_{t}\\mid S_{t}=s \\right ]\\\\\n&=\\mathbb{E}_{t} \\left [R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s \\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s)\\sum_{s'}\\sum_{r}p(s',r\\mid s,a)\\left[r+\\gamma \\mathbb{E}\\left[G_{t+1}\\mid S_{t+1}=s' \\right]\\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s)\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\\\\\n&=\\sum_{a}\\pi(a\\mid s) q_{\\pi}(s,a)\n\\end{align*},\nfor \\ all \\ s\\in S</script><p><img src=\"./价值与贝尔曼方程/v.jpg\" alt=\"\"></p>\n<p>看到没有，此时可以将当前状态的状态值$v_{\\pi}(s)$与下一个可到达状态的状态值$v_{\\pi}(s’)$联系起来！</p>\n<p>动作值函数$Q_{\\pi}(s,a)$也可以进行类似推导：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nq_{\\pi}(s,a) & \\doteq \\mathbb{E}_{\\pi}\\left [ G_{t}\\mid S_{t}=s,A_{t}=a \\right ]\\\\\n&=\\mathbb{E}_{t} \\left [R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s,A_{t}=a \\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma \\sum_{a'}\\pi(a'\\mid s') \\mathbb{E}\\left[G_{t+1}\\mid S_{t+1}=s',A_{t+1}=a' \\right]\\right]\\\\\n&=\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma \\sum_{a'}\\pi(a'\\mid s') q_{\\pi}(s',a')\\right]\\\\\n&=\\sum_{s',r}p(s',r \\mid s,a)\\left[r+\\gamma v_{\\pi}(s')\\right]\n\\end{align*}</script><p><img src=\"./价值与贝尔曼方程/q.jpg\" alt=\"\"></p>\n<h3 id=\"最优值函数\"><a href=\"#最优值函数\" class=\"headerlink\" title=\"最优值函数\"></a>最优值函数</h3><p>解决一个强化学习问题也就是意味着找到一种选择动作的策略能够获得足够多的回报。如果执行每个动作所产生的转移都是确定的（有限MDP），那么能够定义出一个最优策略，如果一个策略$\\pi’$的所有状态值函数都大于$\\pi$，那么就说策略$\\pi’$更好，但不一定是最好的，我们把最优策略用$\\pi_{*}$表示。</p>\n<p>最优状态值函数：</p>\n<script type=\"math/tex; mode=display\">\nv_{*}(s)=\\max_{\\pi} v_{\\pi}(s)</script><p>最优动作值函数：</p>\n<script type=\"math/tex; mode=display\">\nq_{*}(s,a)=\\max_{\\pi} q_{\\pi}(s,a)</script><h3 id=\"贝尔曼最优方程\"><a href=\"#贝尔曼最优方程\" class=\"headerlink\" title=\"贝尔曼最优方程\"></a>贝尔曼最优方程</h3><script type=\"math/tex; mode=display\">\n\\begin{align*}\nv_{*}(s) &= \\max_{a} q_{*}(s,a)\\\\\n&=\\max_{a}\\mathbb{E}\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\max_{a}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\right]\n\\end{align*}</script><script type=\"math/tex; mode=display\">\n\\begin{align*}\nq_{*}(s,a) &= \\mathbb{E}\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\mathbb{E}\\left[r+\\gamma \\max_{a'} q_{*}(s',a')\\mid s,a\\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma v_{*}(s')\\mid s,a\\right]\\\\\n&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma \\max_{a'}q_{*}(s',a')\\right]\n\\end{align*}</script><p><img src=\"./价值与贝尔曼方程/vq.jpg\" alt=\"\"></p>\n<p>虽然我们已经定义出了最优值函数和最优策略，而且理论上也可以直接计算出来。但是通常情况下我们没法得到这么多的计算资源。与此同时内存溢出也是一个很大的问题，因为很多问题的状态数量太多超过存储范围。对于这些情况我们就不能够使用直接存储每个状态的值函数而是必须使用一种更精简的参数型函数表示的方法。</p>\n<p>强化学习的框架迫使我们进行近似求解，而且这个框架同时也很容易进行近似，比如对于很多小概率出现的状态，选择最优解和次优解区别不大。</p>\n<h3 id=\"最优策略\"><a href=\"#最优策略\" class=\"headerlink\" title=\"最优策略\"></a>最优策略</h3><p>定义策略之间的偏序关系</p>\n<script type=\"math/tex; mode=display\">\n\\pi \\geq \\pi' \\ if \\ v_{\\pi}(s) \\geq v_{\\pi'}(s) \\ , \\ \\forall s</script><p>那么有如下定理成立：</p>\n<p>对任意MDP：</p>\n<ul>\n<li>存在最优策略$\\pi_{\\ast}$，满足$\\pi_{\\ast} \\geq \\pi,\\forall \\pi$</li>\n<li>所有最优策略的状态值函数都等于最优状态值函数$v_{\\pi_{\\ast}}(s)=v_{\\ast}(s)$</li>\n<li>所有最优策略的动作值函数都等于最优动作值函数$q_{\\pi_{\\ast}}(s,a)=q_{\\ast}(s,a)$</li>\n</ul>\n<h2 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h2><p>对于上述例子和表格，我们来试着计算一下$V(S)、Q(S,A)$。</p>\n<p>第一个问题，怎么计算这些值？初始化终态的状态值为0，然后从后向前递归？我们来试一下！</p>\n<h3 id=\"只初始化终态\"><a href=\"#只初始化终态\" class=\"headerlink\" title=\"只初始化终态\"></a>只初始化终态</h3><p>根据上述公式，设$\\gamma =1$：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\n&v(F)=r=0\\\\\n&q(E，)=1\\times (0+v(F))=0\\\\\n&1表示选择这个动作转移至另一个状态的概率\\\\\n&v(E)=1\\times q(E,)=0\\\\\n&q(D,0.6)=1\\times (10+v(E))=10\\\\\n&q(D,0.4)=0.2\\times(-2+v(B))+0.4\\times(-2+v(C))+0.4\\times(-2+v(D))=\\\\\n&v(D)=0.4\\times q(D,0.4)+0.6\\times q(D,0.6)=\\\\\n&q(C,0.8)=1\\times (-2+v(D))=\\\\\n&q(C,0.2)=1\\times (0+v(F))=0\\\\\n&v(C)=0.2\\times q(C,0.2)+0.8\\times q(C,0.8)=\\\\\n&q(B,0.5_{C})=1\\times (-2+v(C))=\\\\\n&q(B,0.5_{A})=1\\times (-1+v(A))=\\\\\n&v(B)=0.5\\times q(B,0.5_{C})+0.5 \\times q(B,0.5_{A})=\\\\\n&q(A,0.9)=1\\times (-1+v(A))=\\\\\n&q(A,0.1)=1\\times(-2+v(B))=\\\\\n&v(A)=0.1\\times q(A,0.1)+0.9\\times q(A,0.9)=\n\\end{align*}</script><p>哎呀，卡住了，解不出来，$v(D)、v(A)、v(B)$互相依赖，解不出来，看来这样计算是行不通了。其实，很多问题中终态都很难定义，更别说使用这种方法了。</p>\n<h3 id=\"初始化全部状态值\"><a href=\"#初始化全部状态值\" class=\"headerlink\" title=\"初始化全部状态值\"></a>初始化全部状态值</h3><p>初始化所有状态的值函数为0，即</p>\n<script type=\"math/tex; mode=display\">\nv(s)=0,\\ for\\ all\\ s\\in S</script><p>先试验一下$\\gamma =0.5$，</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">V和Q，$\\gamma =0.5$</th>\n<th style=\"text-align:center\">初始化V计算Q</th>\n<th style=\"text-align:center\">迭代→V→Q</th>\n<th>第46轮完全收敛</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-2.171</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">B</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-1.880</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">C</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0.651</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">D</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>5.627</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">E</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">F</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(A,0.1)\\rightarrow B$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-2.940</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(A,0.9)\\rightarrow A$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-2.085</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(B,0.5_{A})\\rightarrow A$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-2.085</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(B,0.5_{C})\\rightarrow C$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-1.675</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(C,0.2)\\rightarrow F$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(C,0.8)\\rightarrow D$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0.814</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(D,0.4)\\rightarrow \\begin{cases}B,0.2\\\\C,0.4\\\\D,0.4\\end{cases}$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>-0.932</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(D,0.6)\\rightarrow E$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>10</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">$(E,1.0)\\rightarrow F$</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">……</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>这是代码计算的结果，接下来我使$\\gamma =1$，计算结果如下，每迭代100次输出一下：</p>\n<p><img src=\"./价值与贝尔曼方程/example1.png\" alt=\"\"></p>\n<p>可以发现，在700至800次迭代后值函数最终收敛。</p>\n<p>如果我将$\\gamma $设置为0.1呢？来看一下结果：</p>\n<p><img src=\"./价值与贝尔曼方程/example2.png\" alt=\"\"></p>\n<p>仅仅需要十几次就可以迭代至收敛。</p>\n<p>如果设置为0呢？会怎么样？看结果：</p>\n<p><img src=\"./价值与贝尔曼方程/example3.png\" alt=\"\"></p>\n<p>仅需一次迭代就可以收敛，而且就是转移状态的立即奖励值，这下可以理解$\\gamma$为什么表示对未来的看重程度了吧。</p>\n<p>一般我们是不会将$\\gamma$设置为0的，从这个例子的直观感受也可以得到，就拿$\\gamma =0$与$\\gamma =0.5来比较$：</p>\n<ul>\n<li><p>$\\gamma =0$状态值最高的是$v(D)=5.2$，这很容易理解，D状态距离最大奖励值10最近，理应最好，这点与$\\gamma =0.5$时相同。</p>\n</li>\n<li><p>但是对于状态C，$\\gamma =0$时认为这个状态最差，$v(C)=-1.6$，$\\gamma =0$时认为这个状态次优，$v(C)=0.651$，其实这就是目光短浅与目光长远的不同，$\\gamma =0$并没有考虑到其附近状态的临近状态的价值，导致其主观的认为最接近我的都是负的，于是状态肯定差。</p>\n</li>\n<li><p>对于动作值也是一样，一个认为次优，一个认为最差。</p>\n</li>\n</ul>\n<p><strong>注意：并不是说$\\gamma$越接近于1越好，因为在有些问题上，$\\gamma=1$时其值函数永远不收敛，必须设置$0 \\leq \\gamma \\lt 1$，值函数才能收敛。为什么呢？试着计算一下$\\gamma^{n}$，看看对不同的$\\gamma$值，$n$取什么值时结果接近0。</strong></p>\n<p>试着计算一下这个例子，红色代表立即奖励，蓝色代表选择动作的概率以及状态转移的概率，小写字母代表动作，大写字母代表状态。</p>\n<p><img src=\"./价值与贝尔曼方程/example4.png\" alt=\"\"></p>\n<p>$\\gamma =1$时，迭代100W次也不收敛：</p>\n<p><img src=\"./价值与贝尔曼方程/example5.png\" alt=\"\"></p>\n<p>$\\gamma =0.5$时，迭代50多次即可收敛：</p>\n<p><img src=\"./价值与贝尔曼方程/example6.png\" alt=\"\"></p>"},{"title":"强化学习基本概念","copyright":true,"top":1,"date":"2019-04-08T11:23:16.000Z","mathjax":true,"_content":"\n# 强化学习基本概念\n\n学习了这么久的强化学习, 不做笔记总是会忘记, 于是写在博客里方便自己复习, 也与同路人分享.\n\n## 强化学习是什么?\n\n强化学习是什么? 它的英文名字是*Reinforcement Learning*, 和*Machine Learning*一样, 都是以*'ing'*结尾的. 它是一个问题、一组解决这个问题的方案以及探求这些解决方案的方法. 对于问题和方法一定要有清晰的认识, 很多人在学习强化学习时遇到的各种困惑与不解都是因为不能清晰的认识问题和方法的区别和联系.\n\n<!--more-->\n\n强化学习与有监督学习(*supervised learning*)不同. 有监督学习是目前机器学习领域研究最多的方向, 它从由经验丰富的、学识渊博的专家(监督者)提供一系列带有标签(如每个样本被正确分类的类别)的样本数据中进行学习, 这种方法通常被用于分类问题. 有监督学习的目标是当给定一个没有在训练样本集出现的数据时, 可以准确推断出它的标签/类别. 这种有监督学习非常重要而且有用, 但是它没有能力从**交互**中进行学习, 而强化学习在智能体与环境进行交互的过程中进行学习. 为什么有监督学习不能从交互中学习呢? 因为有监督学习需要的近乎完全的样本以及其准确的信息都是在交互问题中很难获得的(不现实的). 在未知的交互场景中, 我们往往只能根据智能体的经验进行学习.\n\n强化学习与无监督学习(*unsupervised learning*)也是不同的. 无监督学习通常被用于发现无标签样本集的隐藏结构. 我们一般任务机器学习只分为有、无监督学习两种, 而且将强化学习分为无监督学习一类. 但其实强化学习与无监督学习有本质的区别. **强化学习的目的是最大化可获得的奖励值**, 而无监督学习是发现隐藏结构. 当然, 如果在强化学习问题中可以发现其样本内的隐藏结构, 这对于强化学习肯定是很有帮助的, 但是仅仅这些隐藏结构并不能处理强化学习最大化奖励值方法的问题. 因此, 我们通常将强化学习归为机器学习的第三个类别, 与有、无监督学习并列.\n\n*注: 在强化学习问题中, 任何可以反映当前动作所带来的影响的元素都可以被理解为奖励值.(个人见解)*\n\n> Reinforcement learning is learning what to do——how to map situations to actions——so as to maxmize a numerical reward signal.\t——《Reinforcement Learning: An Introduction》\n\n强化学习学习的是从状态*s*到要执行的最优动作*a*之间的映射关系, 也就是找到一个策略(函数/逻辑规则)使得在给定状态下通过该策略所产生的决策可以最终带来最大的回报. 学习者不被告知应该采取什么动作, 而是通过训练使它们发现采取什么样的动作可以产生最高的奖励值. 这与婴儿学习的方式很像, 你可能会说:\"瞎讲, 婴儿可以模仿你的动作进行学习.\". 但你要知道, 当你对婴儿的动作进行批评(吵)和奖励(笑)时, 这就已经是一个强化学习的过程了.\n\n## 强化学习的两个要素\n\n强化学习必不可少的两个要素是智能体`Agent`和环境`Environment`.\n既然强化学习是在交互过程中进行学习, 那么交互必定是双方或者多方的, 在强化学习问题中, 交互的双方是智能体和环境.\n\n1. 智能体\n- 智能体是环境的观察者\n- 智能体是策略的载体\n- 智能体是动作的执行者\n3. 环境\n- 环境是对智能体动作的评判者, 即给出立即奖励\n- 环境是智能体进行运动等行为的基本空间\n- 环境给出当前时刻的观察信息, 供智能体进行采集\n\n## 强化学习的两个特点\n\n1. **trial-and-error/试错学习**\n智能体在与环境交互的过程中进行学习时, 不会得到任何人为的或者示例的指导(如果进行指导, 则为有监督学习/模仿学习/逆强化学习等), 智能体只能通过在环境中不断地**试错**, 积累经验, 最终学到可以完成目标并获得最大奖励值的策略.\n2. **delayed reward/延迟奖励**\n在大多数强化学习问题中, 某一状态*s*下执行的动作*a*不仅会影响当前的立即奖励*r*, 而且还会影响后续的状态序列, 以及后续的奖励值. 当前的立即奖励值并不能反映出在这个动作对(*s,a*)对整个决策过程的影响, 只有等到这一决策过程结束时, 才能判断其在这个状态序列的奖励(价值), 所以, 延迟奖励也是强化学习过程中的一个特点.\n\n## 强化学习的难点(Challenge)\n\n相比其他学习, 强化学习中的一大难点是**探索与利用**, 也就是**exploration and exploitation**, 这个难题已经被数学家研究了几十年了, 但仍然没有解决. 为了获得尽量多的奖励, 智能体需要根据过去学习的经验选择产生立即奖励值最高的动作, 但是给定状态下可供选择的动作有很多, 有些被执行过, 有些没有被执行过, 为了去发现产生立即奖励值最高的动作, 必须尝试选择之前被选择过动作. 这个问题就出现了, **智能体必须利用它已经探索过的产生大奖励值的动作, 也必须探索未知奖励值的动作(有可能很小)为了以后可以选择更好的动作**. 只探索不利用、只利用不探索在强化学习问题中都是独木难支. 在随机任务中, 一个同样的动作往往需要被探索很多次才可能对它的期望奖励值有较准确的估计. \n\n## 强化学习的四个元素\n\n除了智能体与环境两个要素之外, 强化学习系统/框架中还有四个子元素: 策略、奖励机制、值函数、模型(未必有).\n\n1. 策略 Policy\n策略定义了智能体在当前时刻应该做出的行为. 与人类的刺激-反应机制很像, 策略是从感知到的环境信息到执行的行为之间的映射, 策略是强化学习智能体的**核心**, 它决定了智能体的行为. 在一般的强化学习问题中, 策略可能是随机的、非确定的, 它通常给出可选择执行的动作的概率或概率分布.\n\n2. 奖励机制 Reward Signal\n奖励机制定义了强化学习问题的**目标**, 在交互的每一步, 环境都会向智能体传递一个数字信息, 我们称之为\"奖励\". 智能体的唯一目标就是在整个交互过程中最大化总的奖励之和. 因此, 奖励定义了某个动作的好坏(但并不意味着坏的动作在交互过程中是坏的, 其作用由值函数来定义). 类比于我们人类, 奖励就像我们高兴或者痛苦一样, 它们是我们对当前环境-动作的立即反应和评价. \n奖励机制是智能体更新策略Policy的基础, 如果智能体成功进行了学习, 当在当前策略选择了一个较低回报的动作时, 之后它可能会选择其他动作. \n通常, 奖励机制由状态*s*和动作*a*的随机函数表示`R(s,a)`\n\n3. 值函数 Value Function\n立即奖励表示着当前动作或状态带来的立即效果是好是坏, 但是值函数表示这个动作在整个交互过程中扮演的角色是好是坏. 一个状态的值是从该状态开始到交互结束所积累的立即奖励的总和.\n一个状态可能总是产生很低的立即奖励, 但是它有很高的值, 因为该状态之后的后续状态中会产生很大的立即奖励. 相反也是一样. 类比于我们人类, 立即奖励的高低相当于我们高兴或痛苦, 但是值函数给出的值则表示了在整个事件过程中我们有多高兴或不高兴的深刻判断. \n引入值函数的唯一目的就是为了训练智能体以获得更大的奖励, 当智能体做决策以及评估决策时, 我们一直关心的都是值函数而不是立即奖励, 对于动作的选择也是基于对值函数的判断/评估. 值比立即奖励更难以确定, 因为立即奖励可以由环境准确的给出, 但是值却需要评估甚至多次评估才可能相对准确(因为有可能交互过程永远不结束, 那么对值的估计会有偏差). 我们希望选择的动作带来最高的值, 而不是最高的立即奖励, 实际上, 几乎所有强化学习算法中最重要的部分就是对于值函数的有效估计方法. 关于值函数估计所扮演的核心角色在近60年被广泛研究. \n\n4. 模型 Model\n模型是对环境行为的仿真, 我们可以通过模型推断出动作对环境的改变, 给出准确的立即奖励和状态信息. 例如, 给定一个状态和动作, 模型可以预测出下一个要转移的状态以及下一个立即奖励值. 如果模型是确定的, 我们一般使用规划(*planning*)的方法来选择最优动作, 对于这种方式我们称之为基于模型**model-based**的方法, 相反, 如果模型是不确定的, 也就是**model-free**, 我们只能通过试错的方式进行学习并选择动作. \n\n*注: 对于什么是model-based和model-free将在以后进行深入讨论.*\n\n## 强化学习的目标\n\n与目标识别为了最小化误差损失不同，强化学习的目的是寻到一个策略，使得期望（折扣）奖励最大化。\n\n## 强化学习的通用符号表示 Notation\n\n$←$\t赋值\n\n$\\varepsilon$ 在$\\varepsilon-greedy$策略中随机选择动作的概率\n\n$\\gamma$ 计算总奖励的折扣因子\n\n$\\lambda$ 资格迹的衰减率或者GAE的权重因子\n\n$s,s'$ 状态，下一个状态\n\n$a$ 一个动作\n\n$r$ 一个奖励值（标量）\n\n$S$ 状态集（不包含终态）\n\n$S^{+}$ 状态集（包含终态）\n\n$A(s)$ $s$状态下可选择的动作\n\n$R$ 奖励集合\n\n$|S|$ 状态集中的元素数\n\n$t$ 单个时间步\n\n$T$ 一个episode的终态时间点\n\n$A_{t}$ $t$时刻选择的动作\n\n$S_{t}$ $t$时刻所在的状态\n\n$R_{t}$ $t$时刻获得的奖励\n\n$\\pi$ 策略（从状态到动作的映射）\n\n$\\pi(s)$ 在$s$状态下使用$\\pi$策略所选择的动作\n\n$\\pi(a|s)$ 在$s$状态下使用$\\pi$策略选择到动作$a$的概率\n\n$G_{t}$ 以$t$时刻为起始时间点，到终态所能获得的总奖励（回报）\n\n$p(s',r|s,a)$ 在$s$状态执行a动作转移到$s‘$状态并获得奖励值为$r$的概率\n\n$p(s'|s,a)$ 在$s$状态执行$a$动作转移到$s’$状态的概率\n\n$r(s,a)$ 在$s$状态执行$a$动作所获得的**期望**立即奖励（即时奖励）\n\n$r(s,a,s')$ 在$s$状态执行$a$动作转移到$s'$状态所获得的**期望**立即奖励\n\n$v_{\\pi}(s)$ $\\pi$策略下状态$s$的值（以该状态为始态的期望奖励回报）\n\n$v_{*}(s)$ **最优**策略下状态s的值\n\n$q_{\\pi}(s,a)$ $\\pi$策略下状态-行动对$(s,a)$的值\n\n$q_{*}(s,a)$ **最优**策略下状态-行动对$(s,a)$的值\n\n$V,V_{t}$ 状态值的矩阵估计，行和列分别是时间点$t$和每个状态的估计值$v_{\\pi}$或$v_{*}$\n\n$Q,Q_{t}$ 状态-行动对$(s,a)$的矩阵估计，一般为一个3维矩阵,行、列和深度分别为状态、动作、时间点\n\n$V_{t}(s)$ 状态$s$的期望估计值\n\n$\\delta_{t}$ $t$时刻的TD-error时间差分量\n\n$\\theta,\\theta_{t}$ 目标策略的参数（向量）\n\n$\\pi(a|s,\\theta)$ 目标策略的参数为$\\theta$时，在$s$状态选择到$a$动作的概率\n\n$\\pi_{\\theta}$ 表示参数为$\\theta$的策略\n\n$\\nabla{\\pi(a|s,\\theta)}$ $\\pi(a|s,\\theta)$对于参数$\\theta$的偏微分\n\n$J(\\theta)$ 参数为$\\theta$的策略的性能度量、期望奖励(performance measure)\n\n$\\nabla{J(\\theta)}$ 性能度量对于策略参数$\\theta$的偏导数","source":"_posts/强化学习基本概念.md","raw":"---\ntitle: 强化学习基本概念\ncopyright: true\ntop: 1\ndate: 2019-04-08 19:23:16\nmathjax: true\ncategories: ReinforcementLearning\ntags:\n- rl\n---\n\n# 强化学习基本概念\n\n学习了这么久的强化学习, 不做笔记总是会忘记, 于是写在博客里方便自己复习, 也与同路人分享.\n\n## 强化学习是什么?\n\n强化学习是什么? 它的英文名字是*Reinforcement Learning*, 和*Machine Learning*一样, 都是以*'ing'*结尾的. 它是一个问题、一组解决这个问题的方案以及探求这些解决方案的方法. 对于问题和方法一定要有清晰的认识, 很多人在学习强化学习时遇到的各种困惑与不解都是因为不能清晰的认识问题和方法的区别和联系.\n\n<!--more-->\n\n强化学习与有监督学习(*supervised learning*)不同. 有监督学习是目前机器学习领域研究最多的方向, 它从由经验丰富的、学识渊博的专家(监督者)提供一系列带有标签(如每个样本被正确分类的类别)的样本数据中进行学习, 这种方法通常被用于分类问题. 有监督学习的目标是当给定一个没有在训练样本集出现的数据时, 可以准确推断出它的标签/类别. 这种有监督学习非常重要而且有用, 但是它没有能力从**交互**中进行学习, 而强化学习在智能体与环境进行交互的过程中进行学习. 为什么有监督学习不能从交互中学习呢? 因为有监督学习需要的近乎完全的样本以及其准确的信息都是在交互问题中很难获得的(不现实的). 在未知的交互场景中, 我们往往只能根据智能体的经验进行学习.\n\n强化学习与无监督学习(*unsupervised learning*)也是不同的. 无监督学习通常被用于发现无标签样本集的隐藏结构. 我们一般任务机器学习只分为有、无监督学习两种, 而且将强化学习分为无监督学习一类. 但其实强化学习与无监督学习有本质的区别. **强化学习的目的是最大化可获得的奖励值**, 而无监督学习是发现隐藏结构. 当然, 如果在强化学习问题中可以发现其样本内的隐藏结构, 这对于强化学习肯定是很有帮助的, 但是仅仅这些隐藏结构并不能处理强化学习最大化奖励值方法的问题. 因此, 我们通常将强化学习归为机器学习的第三个类别, 与有、无监督学习并列.\n\n*注: 在强化学习问题中, 任何可以反映当前动作所带来的影响的元素都可以被理解为奖励值.(个人见解)*\n\n> Reinforcement learning is learning what to do——how to map situations to actions——so as to maxmize a numerical reward signal.\t——《Reinforcement Learning: An Introduction》\n\n强化学习学习的是从状态*s*到要执行的最优动作*a*之间的映射关系, 也就是找到一个策略(函数/逻辑规则)使得在给定状态下通过该策略所产生的决策可以最终带来最大的回报. 学习者不被告知应该采取什么动作, 而是通过训练使它们发现采取什么样的动作可以产生最高的奖励值. 这与婴儿学习的方式很像, 你可能会说:\"瞎讲, 婴儿可以模仿你的动作进行学习.\". 但你要知道, 当你对婴儿的动作进行批评(吵)和奖励(笑)时, 这就已经是一个强化学习的过程了.\n\n## 强化学习的两个要素\n\n强化学习必不可少的两个要素是智能体`Agent`和环境`Environment`.\n既然强化学习是在交互过程中进行学习, 那么交互必定是双方或者多方的, 在强化学习问题中, 交互的双方是智能体和环境.\n\n1. 智能体\n- 智能体是环境的观察者\n- 智能体是策略的载体\n- 智能体是动作的执行者\n3. 环境\n- 环境是对智能体动作的评判者, 即给出立即奖励\n- 环境是智能体进行运动等行为的基本空间\n- 环境给出当前时刻的观察信息, 供智能体进行采集\n\n## 强化学习的两个特点\n\n1. **trial-and-error/试错学习**\n智能体在与环境交互的过程中进行学习时, 不会得到任何人为的或者示例的指导(如果进行指导, 则为有监督学习/模仿学习/逆强化学习等), 智能体只能通过在环境中不断地**试错**, 积累经验, 最终学到可以完成目标并获得最大奖励值的策略.\n2. **delayed reward/延迟奖励**\n在大多数强化学习问题中, 某一状态*s*下执行的动作*a*不仅会影响当前的立即奖励*r*, 而且还会影响后续的状态序列, 以及后续的奖励值. 当前的立即奖励值并不能反映出在这个动作对(*s,a*)对整个决策过程的影响, 只有等到这一决策过程结束时, 才能判断其在这个状态序列的奖励(价值), 所以, 延迟奖励也是强化学习过程中的一个特点.\n\n## 强化学习的难点(Challenge)\n\n相比其他学习, 强化学习中的一大难点是**探索与利用**, 也就是**exploration and exploitation**, 这个难题已经被数学家研究了几十年了, 但仍然没有解决. 为了获得尽量多的奖励, 智能体需要根据过去学习的经验选择产生立即奖励值最高的动作, 但是给定状态下可供选择的动作有很多, 有些被执行过, 有些没有被执行过, 为了去发现产生立即奖励值最高的动作, 必须尝试选择之前被选择过动作. 这个问题就出现了, **智能体必须利用它已经探索过的产生大奖励值的动作, 也必须探索未知奖励值的动作(有可能很小)为了以后可以选择更好的动作**. 只探索不利用、只利用不探索在强化学习问题中都是独木难支. 在随机任务中, 一个同样的动作往往需要被探索很多次才可能对它的期望奖励值有较准确的估计. \n\n## 强化学习的四个元素\n\n除了智能体与环境两个要素之外, 强化学习系统/框架中还有四个子元素: 策略、奖励机制、值函数、模型(未必有).\n\n1. 策略 Policy\n策略定义了智能体在当前时刻应该做出的行为. 与人类的刺激-反应机制很像, 策略是从感知到的环境信息到执行的行为之间的映射, 策略是强化学习智能体的**核心**, 它决定了智能体的行为. 在一般的强化学习问题中, 策略可能是随机的、非确定的, 它通常给出可选择执行的动作的概率或概率分布.\n\n2. 奖励机制 Reward Signal\n奖励机制定义了强化学习问题的**目标**, 在交互的每一步, 环境都会向智能体传递一个数字信息, 我们称之为\"奖励\". 智能体的唯一目标就是在整个交互过程中最大化总的奖励之和. 因此, 奖励定义了某个动作的好坏(但并不意味着坏的动作在交互过程中是坏的, 其作用由值函数来定义). 类比于我们人类, 奖励就像我们高兴或者痛苦一样, 它们是我们对当前环境-动作的立即反应和评价. \n奖励机制是智能体更新策略Policy的基础, 如果智能体成功进行了学习, 当在当前策略选择了一个较低回报的动作时, 之后它可能会选择其他动作. \n通常, 奖励机制由状态*s*和动作*a*的随机函数表示`R(s,a)`\n\n3. 值函数 Value Function\n立即奖励表示着当前动作或状态带来的立即效果是好是坏, 但是值函数表示这个动作在整个交互过程中扮演的角色是好是坏. 一个状态的值是从该状态开始到交互结束所积累的立即奖励的总和.\n一个状态可能总是产生很低的立即奖励, 但是它有很高的值, 因为该状态之后的后续状态中会产生很大的立即奖励. 相反也是一样. 类比于我们人类, 立即奖励的高低相当于我们高兴或痛苦, 但是值函数给出的值则表示了在整个事件过程中我们有多高兴或不高兴的深刻判断. \n引入值函数的唯一目的就是为了训练智能体以获得更大的奖励, 当智能体做决策以及评估决策时, 我们一直关心的都是值函数而不是立即奖励, 对于动作的选择也是基于对值函数的判断/评估. 值比立即奖励更难以确定, 因为立即奖励可以由环境准确的给出, 但是值却需要评估甚至多次评估才可能相对准确(因为有可能交互过程永远不结束, 那么对值的估计会有偏差). 我们希望选择的动作带来最高的值, 而不是最高的立即奖励, 实际上, 几乎所有强化学习算法中最重要的部分就是对于值函数的有效估计方法. 关于值函数估计所扮演的核心角色在近60年被广泛研究. \n\n4. 模型 Model\n模型是对环境行为的仿真, 我们可以通过模型推断出动作对环境的改变, 给出准确的立即奖励和状态信息. 例如, 给定一个状态和动作, 模型可以预测出下一个要转移的状态以及下一个立即奖励值. 如果模型是确定的, 我们一般使用规划(*planning*)的方法来选择最优动作, 对于这种方式我们称之为基于模型**model-based**的方法, 相反, 如果模型是不确定的, 也就是**model-free**, 我们只能通过试错的方式进行学习并选择动作. \n\n*注: 对于什么是model-based和model-free将在以后进行深入讨论.*\n\n## 强化学习的目标\n\n与目标识别为了最小化误差损失不同，强化学习的目的是寻到一个策略，使得期望（折扣）奖励最大化。\n\n## 强化学习的通用符号表示 Notation\n\n$←$\t赋值\n\n$\\varepsilon$ 在$\\varepsilon-greedy$策略中随机选择动作的概率\n\n$\\gamma$ 计算总奖励的折扣因子\n\n$\\lambda$ 资格迹的衰减率或者GAE的权重因子\n\n$s,s'$ 状态，下一个状态\n\n$a$ 一个动作\n\n$r$ 一个奖励值（标量）\n\n$S$ 状态集（不包含终态）\n\n$S^{+}$ 状态集（包含终态）\n\n$A(s)$ $s$状态下可选择的动作\n\n$R$ 奖励集合\n\n$|S|$ 状态集中的元素数\n\n$t$ 单个时间步\n\n$T$ 一个episode的终态时间点\n\n$A_{t}$ $t$时刻选择的动作\n\n$S_{t}$ $t$时刻所在的状态\n\n$R_{t}$ $t$时刻获得的奖励\n\n$\\pi$ 策略（从状态到动作的映射）\n\n$\\pi(s)$ 在$s$状态下使用$\\pi$策略所选择的动作\n\n$\\pi(a|s)$ 在$s$状态下使用$\\pi$策略选择到动作$a$的概率\n\n$G_{t}$ 以$t$时刻为起始时间点，到终态所能获得的总奖励（回报）\n\n$p(s',r|s,a)$ 在$s$状态执行a动作转移到$s‘$状态并获得奖励值为$r$的概率\n\n$p(s'|s,a)$ 在$s$状态执行$a$动作转移到$s’$状态的概率\n\n$r(s,a)$ 在$s$状态执行$a$动作所获得的**期望**立即奖励（即时奖励）\n\n$r(s,a,s')$ 在$s$状态执行$a$动作转移到$s'$状态所获得的**期望**立即奖励\n\n$v_{\\pi}(s)$ $\\pi$策略下状态$s$的值（以该状态为始态的期望奖励回报）\n\n$v_{*}(s)$ **最优**策略下状态s的值\n\n$q_{\\pi}(s,a)$ $\\pi$策略下状态-行动对$(s,a)$的值\n\n$q_{*}(s,a)$ **最优**策略下状态-行动对$(s,a)$的值\n\n$V,V_{t}$ 状态值的矩阵估计，行和列分别是时间点$t$和每个状态的估计值$v_{\\pi}$或$v_{*}$\n\n$Q,Q_{t}$ 状态-行动对$(s,a)$的矩阵估计，一般为一个3维矩阵,行、列和深度分别为状态、动作、时间点\n\n$V_{t}(s)$ 状态$s$的期望估计值\n\n$\\delta_{t}$ $t$时刻的TD-error时间差分量\n\n$\\theta,\\theta_{t}$ 目标策略的参数（向量）\n\n$\\pi(a|s,\\theta)$ 目标策略的参数为$\\theta$时，在$s$状态选择到$a$动作的概率\n\n$\\pi_{\\theta}$ 表示参数为$\\theta$的策略\n\n$\\nabla{\\pi(a|s,\\theta)}$ $\\pi(a|s,\\theta)$对于参数$\\theta$的偏微分\n\n$J(\\theta)$ 参数为$\\theta$的策略的性能度量、期望奖励(performance measure)\n\n$\\nabla{J(\\theta)}$ 性能度量对于策略参数$\\theta$的偏导数","slug":"强化学习基本概念","published":1,"updated":"2019-05-12T10:38:59.847Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma80003jekvehf1wgcry","content":"<h1 id=\"强化学习基本概念\"><a href=\"#强化学习基本概念\" class=\"headerlink\" title=\"强化学习基本概念\"></a>强化学习基本概念</h1><p>学习了这么久的强化学习, 不做笔记总是会忘记, 于是写在博客里方便自己复习, 也与同路人分享.</p>\n<h2 id=\"强化学习是什么\"><a href=\"#强化学习是什么\" class=\"headerlink\" title=\"强化学习是什么?\"></a>强化学习是什么?</h2><p>强化学习是什么? 它的英文名字是<em>Reinforcement Learning</em>, 和<em>Machine Learning</em>一样, 都是以<em>‘ing’</em>结尾的. 它是一个问题、一组解决这个问题的方案以及探求这些解决方案的方法. 对于问题和方法一定要有清晰的认识, 很多人在学习强化学习时遇到的各种困惑与不解都是因为不能清晰的认识问题和方法的区别和联系.</p>\n<a id=\"more\"></a>\n<p>强化学习与有监督学习(<em>supervised learning</em>)不同. 有监督学习是目前机器学习领域研究最多的方向, 它从由经验丰富的、学识渊博的专家(监督者)提供一系列带有标签(如每个样本被正确分类的类别)的样本数据中进行学习, 这种方法通常被用于分类问题. 有监督学习的目标是当给定一个没有在训练样本集出现的数据时, 可以准确推断出它的标签/类别. 这种有监督学习非常重要而且有用, 但是它没有能力从<strong>交互</strong>中进行学习, 而强化学习在智能体与环境进行交互的过程中进行学习. 为什么有监督学习不能从交互中学习呢? 因为有监督学习需要的近乎完全的样本以及其准确的信息都是在交互问题中很难获得的(不现实的). 在未知的交互场景中, 我们往往只能根据智能体的经验进行学习.</p>\n<p>强化学习与无监督学习(<em>unsupervised learning</em>)也是不同的. 无监督学习通常被用于发现无标签样本集的隐藏结构. 我们一般任务机器学习只分为有、无监督学习两种, 而且将强化学习分为无监督学习一类. 但其实强化学习与无监督学习有本质的区别. <strong>强化学习的目的是最大化可获得的奖励值</strong>, 而无监督学习是发现隐藏结构. 当然, 如果在强化学习问题中可以发现其样本内的隐藏结构, 这对于强化学习肯定是很有帮助的, 但是仅仅这些隐藏结构并不能处理强化学习最大化奖励值方法的问题. 因此, 我们通常将强化学习归为机器学习的第三个类别, 与有、无监督学习并列.</p>\n<p><em>注: 在强化学习问题中, 任何可以反映当前动作所带来的影响的元素都可以被理解为奖励值.(个人见解)</em></p>\n<blockquote>\n<p>Reinforcement learning is learning what to do——how to map situations to actions——so as to maxmize a numerical reward signal.    ——《Reinforcement Learning: An Introduction》</p>\n</blockquote>\n<p>强化学习学习的是从状态<em>s</em>到要执行的最优动作<em>a</em>之间的映射关系, 也就是找到一个策略(函数/逻辑规则)使得在给定状态下通过该策略所产生的决策可以最终带来最大的回报. 学习者不被告知应该采取什么动作, 而是通过训练使它们发现采取什么样的动作可以产生最高的奖励值. 这与婴儿学习的方式很像, 你可能会说:”瞎讲, 婴儿可以模仿你的动作进行学习.”. 但你要知道, 当你对婴儿的动作进行批评(吵)和奖励(笑)时, 这就已经是一个强化学习的过程了.</p>\n<h2 id=\"强化学习的两个要素\"><a href=\"#强化学习的两个要素\" class=\"headerlink\" title=\"强化学习的两个要素\"></a>强化学习的两个要素</h2><p>强化学习必不可少的两个要素是智能体<code>Agent</code>和环境<code>Environment</code>.<br>既然强化学习是在交互过程中进行学习, 那么交互必定是双方或者多方的, 在强化学习问题中, 交互的双方是智能体和环境.</p>\n<ol>\n<li>智能体</li>\n</ol>\n<ul>\n<li>智能体是环境的观察者</li>\n<li>智能体是策略的载体</li>\n<li>智能体是动作的执行者</li>\n</ul>\n<ol>\n<li>环境</li>\n</ol>\n<ul>\n<li>环境是对智能体动作的评判者, 即给出立即奖励</li>\n<li>环境是智能体进行运动等行为的基本空间</li>\n<li>环境给出当前时刻的观察信息, 供智能体进行采集</li>\n</ul>\n<h2 id=\"强化学习的两个特点\"><a href=\"#强化学习的两个特点\" class=\"headerlink\" title=\"强化学习的两个特点\"></a>强化学习的两个特点</h2><ol>\n<li><strong>trial-and-error/试错学习</strong><br>智能体在与环境交互的过程中进行学习时, 不会得到任何人为的或者示例的指导(如果进行指导, 则为有监督学习/模仿学习/逆强化学习等), 智能体只能通过在环境中不断地<strong>试错</strong>, 积累经验, 最终学到可以完成目标并获得最大奖励值的策略.</li>\n<li><strong>delayed reward/延迟奖励</strong><br>在大多数强化学习问题中, 某一状态<em>s</em>下执行的动作<em>a</em>不仅会影响当前的立即奖励<em>r</em>, 而且还会影响后续的状态序列, 以及后续的奖励值. 当前的立即奖励值并不能反映出在这个动作对(<em>s,a</em>)对整个决策过程的影响, 只有等到这一决策过程结束时, 才能判断其在这个状态序列的奖励(价值), 所以, 延迟奖励也是强化学习过程中的一个特点.</li>\n</ol>\n<h2 id=\"强化学习的难点-Challenge\"><a href=\"#强化学习的难点-Challenge\" class=\"headerlink\" title=\"强化学习的难点(Challenge)\"></a>强化学习的难点(Challenge)</h2><p>相比其他学习, 强化学习中的一大难点是<strong>探索与利用</strong>, 也就是<strong>exploration and exploitation</strong>, 这个难题已经被数学家研究了几十年了, 但仍然没有解决. 为了获得尽量多的奖励, 智能体需要根据过去学习的经验选择产生立即奖励值最高的动作, 但是给定状态下可供选择的动作有很多, 有些被执行过, 有些没有被执行过, 为了去发现产生立即奖励值最高的动作, 必须尝试选择之前被选择过动作. 这个问题就出现了, <strong>智能体必须利用它已经探索过的产生大奖励值的动作, 也必须探索未知奖励值的动作(有可能很小)为了以后可以选择更好的动作</strong>. 只探索不利用、只利用不探索在强化学习问题中都是独木难支. 在随机任务中, 一个同样的动作往往需要被探索很多次才可能对它的期望奖励值有较准确的估计. </p>\n<h2 id=\"强化学习的四个元素\"><a href=\"#强化学习的四个元素\" class=\"headerlink\" title=\"强化学习的四个元素\"></a>强化学习的四个元素</h2><p>除了智能体与环境两个要素之外, 强化学习系统/框架中还有四个子元素: 策略、奖励机制、值函数、模型(未必有).</p>\n<ol>\n<li><p>策略 Policy<br>策略定义了智能体在当前时刻应该做出的行为. 与人类的刺激-反应机制很像, 策略是从感知到的环境信息到执行的行为之间的映射, 策略是强化学习智能体的<strong>核心</strong>, 它决定了智能体的行为. 在一般的强化学习问题中, 策略可能是随机的、非确定的, 它通常给出可选择执行的动作的概率或概率分布.</p>\n</li>\n<li><p>奖励机制 Reward Signal<br>奖励机制定义了强化学习问题的<strong>目标</strong>, 在交互的每一步, 环境都会向智能体传递一个数字信息, 我们称之为”奖励”. 智能体的唯一目标就是在整个交互过程中最大化总的奖励之和. 因此, 奖励定义了某个动作的好坏(但并不意味着坏的动作在交互过程中是坏的, 其作用由值函数来定义). 类比于我们人类, 奖励就像我们高兴或者痛苦一样, 它们是我们对当前环境-动作的立即反应和评价.<br>奖励机制是智能体更新策略Policy的基础, 如果智能体成功进行了学习, 当在当前策略选择了一个较低回报的动作时, 之后它可能会选择其他动作.<br>通常, 奖励机制由状态<em>s</em>和动作<em>a</em>的随机函数表示<code>R(s,a)</code></p>\n</li>\n<li><p>值函数 Value Function<br>立即奖励表示着当前动作或状态带来的立即效果是好是坏, 但是值函数表示这个动作在整个交互过程中扮演的角色是好是坏. 一个状态的值是从该状态开始到交互结束所积累的立即奖励的总和.<br>一个状态可能总是产生很低的立即奖励, 但是它有很高的值, 因为该状态之后的后续状态中会产生很大的立即奖励. 相反也是一样. 类比于我们人类, 立即奖励的高低相当于我们高兴或痛苦, 但是值函数给出的值则表示了在整个事件过程中我们有多高兴或不高兴的深刻判断.<br>引入值函数的唯一目的就是为了训练智能体以获得更大的奖励, 当智能体做决策以及评估决策时, 我们一直关心的都是值函数而不是立即奖励, 对于动作的选择也是基于对值函数的判断/评估. 值比立即奖励更难以确定, 因为立即奖励可以由环境准确的给出, 但是值却需要评估甚至多次评估才可能相对准确(因为有可能交互过程永远不结束, 那么对值的估计会有偏差). 我们希望选择的动作带来最高的值, 而不是最高的立即奖励, 实际上, 几乎所有强化学习算法中最重要的部分就是对于值函数的有效估计方法. 关于值函数估计所扮演的核心角色在近60年被广泛研究. </p>\n</li>\n<li><p>模型 Model<br>模型是对环境行为的仿真, 我们可以通过模型推断出动作对环境的改变, 给出准确的立即奖励和状态信息. 例如, 给定一个状态和动作, 模型可以预测出下一个要转移的状态以及下一个立即奖励值. 如果模型是确定的, 我们一般使用规划(<em>planning</em>)的方法来选择最优动作, 对于这种方式我们称之为基于模型<strong>model-based</strong>的方法, 相反, 如果模型是不确定的, 也就是<strong>model-free</strong>, 我们只能通过试错的方式进行学习并选择动作. </p>\n</li>\n</ol>\n<p><em>注: 对于什么是model-based和model-free将在以后进行深入讨论.</em></p>\n<h2 id=\"强化学习的目标\"><a href=\"#强化学习的目标\" class=\"headerlink\" title=\"强化学习的目标\"></a>强化学习的目标</h2><p>与目标识别为了最小化误差损失不同，强化学习的目的是寻到一个策略，使得期望（折扣）奖励最大化。</p>\n<h2 id=\"强化学习的通用符号表示-Notation\"><a href=\"#强化学习的通用符号表示-Notation\" class=\"headerlink\" title=\"强化学习的通用符号表示 Notation\"></a>强化学习的通用符号表示 Notation</h2><p>$←$    赋值</p>\n<p>$\\varepsilon$ 在$\\varepsilon-greedy$策略中随机选择动作的概率</p>\n<p>$\\gamma$ 计算总奖励的折扣因子</p>\n<p>$\\lambda$ 资格迹的衰减率或者GAE的权重因子</p>\n<p>$s,s’$ 状态，下一个状态</p>\n<p>$a$ 一个动作</p>\n<p>$r$ 一个奖励值（标量）</p>\n<p>$S$ 状态集（不包含终态）</p>\n<p>$S^{+}$ 状态集（包含终态）</p>\n<p>$A(s)$ $s$状态下可选择的动作</p>\n<p>$R$ 奖励集合</p>\n<p>$|S|$ 状态集中的元素数</p>\n<p>$t$ 单个时间步</p>\n<p>$T$ 一个episode的终态时间点</p>\n<p>$A_{t}$ $t$时刻选择的动作</p>\n<p>$S_{t}$ $t$时刻所在的状态</p>\n<p>$R_{t}$ $t$时刻获得的奖励</p>\n<p>$\\pi$ 策略（从状态到动作的映射）</p>\n<p>$\\pi(s)$ 在$s$状态下使用$\\pi$策略所选择的动作</p>\n<p>$\\pi(a|s)$ 在$s$状态下使用$\\pi$策略选择到动作$a$的概率</p>\n<p>$G_{t}$ 以$t$时刻为起始时间点，到终态所能获得的总奖励（回报）</p>\n<p>$p(s’,r|s,a)$ 在$s$状态执行a动作转移到$s‘$状态并获得奖励值为$r$的概率</p>\n<p>$p(s’|s,a)$ 在$s$状态执行$a$动作转移到$s’$状态的概率</p>\n<p>$r(s,a)$ 在$s$状态执行$a$动作所获得的<strong>期望</strong>立即奖励（即时奖励）</p>\n<p>$r(s,a,s’)$ 在$s$状态执行$a$动作转移到$s’$状态所获得的<strong>期望</strong>立即奖励</p>\n<p>$v_{\\pi}(s)$ $\\pi$策略下状态$s$的值（以该状态为始态的期望奖励回报）</p>\n<p>$v_{<em>}(s)$ <em>*最优</em></em>策略下状态s的值</p>\n<p>$q_{\\pi}(s,a)$ $\\pi$策略下状态-行动对$(s,a)$的值</p>\n<p>$q_{<em>}(s,a)$ <em>*最优</em></em>策略下状态-行动对$(s,a)$的值</p>\n<p>$V,V_{t}$ 状态值的矩阵估计，行和列分别是时间点$t$和每个状态的估计值$v_{\\pi}$或$v_{*}$</p>\n<p>$Q,Q_{t}$ 状态-行动对$(s,a)$的矩阵估计，一般为一个3维矩阵,行、列和深度分别为状态、动作、时间点</p>\n<p>$V_{t}(s)$ 状态$s$的期望估计值</p>\n<p>$\\delta_{t}$ $t$时刻的TD-error时间差分量</p>\n<p>$\\theta,\\theta_{t}$ 目标策略的参数（向量）</p>\n<p>$\\pi(a|s,\\theta)$ 目标策略的参数为$\\theta$时，在$s$状态选择到$a$动作的概率</p>\n<p>$\\pi_{\\theta}$ 表示参数为$\\theta$的策略</p>\n<p>$\\nabla{\\pi(a|s,\\theta)}$ $\\pi(a|s,\\theta)$对于参数$\\theta$的偏微分</p>\n<p>$J(\\theta)$ 参数为$\\theta$的策略的性能度量、期望奖励(performance measure)</p>\n<p>$\\nabla{J(\\theta)}$ 性能度量对于策略参数$\\theta$的偏导数</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"强化学习基本概念\"><a href=\"#强化学习基本概念\" class=\"headerlink\" title=\"强化学习基本概念\"></a>强化学习基本概念</h1><p>学习了这么久的强化学习, 不做笔记总是会忘记, 于是写在博客里方便自己复习, 也与同路人分享.</p>\n<h2 id=\"强化学习是什么\"><a href=\"#强化学习是什么\" class=\"headerlink\" title=\"强化学习是什么?\"></a>强化学习是什么?</h2><p>强化学习是什么? 它的英文名字是<em>Reinforcement Learning</em>, 和<em>Machine Learning</em>一样, 都是以<em>‘ing’</em>结尾的. 它是一个问题、一组解决这个问题的方案以及探求这些解决方案的方法. 对于问题和方法一定要有清晰的认识, 很多人在学习强化学习时遇到的各种困惑与不解都是因为不能清晰的认识问题和方法的区别和联系.</p>","more":"<p>强化学习与有监督学习(<em>supervised learning</em>)不同. 有监督学习是目前机器学习领域研究最多的方向, 它从由经验丰富的、学识渊博的专家(监督者)提供一系列带有标签(如每个样本被正确分类的类别)的样本数据中进行学习, 这种方法通常被用于分类问题. 有监督学习的目标是当给定一个没有在训练样本集出现的数据时, 可以准确推断出它的标签/类别. 这种有监督学习非常重要而且有用, 但是它没有能力从<strong>交互</strong>中进行学习, 而强化学习在智能体与环境进行交互的过程中进行学习. 为什么有监督学习不能从交互中学习呢? 因为有监督学习需要的近乎完全的样本以及其准确的信息都是在交互问题中很难获得的(不现实的). 在未知的交互场景中, 我们往往只能根据智能体的经验进行学习.</p>\n<p>强化学习与无监督学习(<em>unsupervised learning</em>)也是不同的. 无监督学习通常被用于发现无标签样本集的隐藏结构. 我们一般任务机器学习只分为有、无监督学习两种, 而且将强化学习分为无监督学习一类. 但其实强化学习与无监督学习有本质的区别. <strong>强化学习的目的是最大化可获得的奖励值</strong>, 而无监督学习是发现隐藏结构. 当然, 如果在强化学习问题中可以发现其样本内的隐藏结构, 这对于强化学习肯定是很有帮助的, 但是仅仅这些隐藏结构并不能处理强化学习最大化奖励值方法的问题. 因此, 我们通常将强化学习归为机器学习的第三个类别, 与有、无监督学习并列.</p>\n<p><em>注: 在强化学习问题中, 任何可以反映当前动作所带来的影响的元素都可以被理解为奖励值.(个人见解)</em></p>\n<blockquote>\n<p>Reinforcement learning is learning what to do——how to map situations to actions——so as to maxmize a numerical reward signal.    ——《Reinforcement Learning: An Introduction》</p>\n</blockquote>\n<p>强化学习学习的是从状态<em>s</em>到要执行的最优动作<em>a</em>之间的映射关系, 也就是找到一个策略(函数/逻辑规则)使得在给定状态下通过该策略所产生的决策可以最终带来最大的回报. 学习者不被告知应该采取什么动作, 而是通过训练使它们发现采取什么样的动作可以产生最高的奖励值. 这与婴儿学习的方式很像, 你可能会说:”瞎讲, 婴儿可以模仿你的动作进行学习.”. 但你要知道, 当你对婴儿的动作进行批评(吵)和奖励(笑)时, 这就已经是一个强化学习的过程了.</p>\n<h2 id=\"强化学习的两个要素\"><a href=\"#强化学习的两个要素\" class=\"headerlink\" title=\"强化学习的两个要素\"></a>强化学习的两个要素</h2><p>强化学习必不可少的两个要素是智能体<code>Agent</code>和环境<code>Environment</code>.<br>既然强化学习是在交互过程中进行学习, 那么交互必定是双方或者多方的, 在强化学习问题中, 交互的双方是智能体和环境.</p>\n<ol>\n<li>智能体</li>\n</ol>\n<ul>\n<li>智能体是环境的观察者</li>\n<li>智能体是策略的载体</li>\n<li>智能体是动作的执行者</li>\n</ul>\n<ol>\n<li>环境</li>\n</ol>\n<ul>\n<li>环境是对智能体动作的评判者, 即给出立即奖励</li>\n<li>环境是智能体进行运动等行为的基本空间</li>\n<li>环境给出当前时刻的观察信息, 供智能体进行采集</li>\n</ul>\n<h2 id=\"强化学习的两个特点\"><a href=\"#强化学习的两个特点\" class=\"headerlink\" title=\"强化学习的两个特点\"></a>强化学习的两个特点</h2><ol>\n<li><strong>trial-and-error/试错学习</strong><br>智能体在与环境交互的过程中进行学习时, 不会得到任何人为的或者示例的指导(如果进行指导, 则为有监督学习/模仿学习/逆强化学习等), 智能体只能通过在环境中不断地<strong>试错</strong>, 积累经验, 最终学到可以完成目标并获得最大奖励值的策略.</li>\n<li><strong>delayed reward/延迟奖励</strong><br>在大多数强化学习问题中, 某一状态<em>s</em>下执行的动作<em>a</em>不仅会影响当前的立即奖励<em>r</em>, 而且还会影响后续的状态序列, 以及后续的奖励值. 当前的立即奖励值并不能反映出在这个动作对(<em>s,a</em>)对整个决策过程的影响, 只有等到这一决策过程结束时, 才能判断其在这个状态序列的奖励(价值), 所以, 延迟奖励也是强化学习过程中的一个特点.</li>\n</ol>\n<h2 id=\"强化学习的难点-Challenge\"><a href=\"#强化学习的难点-Challenge\" class=\"headerlink\" title=\"强化学习的难点(Challenge)\"></a>强化学习的难点(Challenge)</h2><p>相比其他学习, 强化学习中的一大难点是<strong>探索与利用</strong>, 也就是<strong>exploration and exploitation</strong>, 这个难题已经被数学家研究了几十年了, 但仍然没有解决. 为了获得尽量多的奖励, 智能体需要根据过去学习的经验选择产生立即奖励值最高的动作, 但是给定状态下可供选择的动作有很多, 有些被执行过, 有些没有被执行过, 为了去发现产生立即奖励值最高的动作, 必须尝试选择之前被选择过动作. 这个问题就出现了, <strong>智能体必须利用它已经探索过的产生大奖励值的动作, 也必须探索未知奖励值的动作(有可能很小)为了以后可以选择更好的动作</strong>. 只探索不利用、只利用不探索在强化学习问题中都是独木难支. 在随机任务中, 一个同样的动作往往需要被探索很多次才可能对它的期望奖励值有较准确的估计. </p>\n<h2 id=\"强化学习的四个元素\"><a href=\"#强化学习的四个元素\" class=\"headerlink\" title=\"强化学习的四个元素\"></a>强化学习的四个元素</h2><p>除了智能体与环境两个要素之外, 强化学习系统/框架中还有四个子元素: 策略、奖励机制、值函数、模型(未必有).</p>\n<ol>\n<li><p>策略 Policy<br>策略定义了智能体在当前时刻应该做出的行为. 与人类的刺激-反应机制很像, 策略是从感知到的环境信息到执行的行为之间的映射, 策略是强化学习智能体的<strong>核心</strong>, 它决定了智能体的行为. 在一般的强化学习问题中, 策略可能是随机的、非确定的, 它通常给出可选择执行的动作的概率或概率分布.</p>\n</li>\n<li><p>奖励机制 Reward Signal<br>奖励机制定义了强化学习问题的<strong>目标</strong>, 在交互的每一步, 环境都会向智能体传递一个数字信息, 我们称之为”奖励”. 智能体的唯一目标就是在整个交互过程中最大化总的奖励之和. 因此, 奖励定义了某个动作的好坏(但并不意味着坏的动作在交互过程中是坏的, 其作用由值函数来定义). 类比于我们人类, 奖励就像我们高兴或者痛苦一样, 它们是我们对当前环境-动作的立即反应和评价.<br>奖励机制是智能体更新策略Policy的基础, 如果智能体成功进行了学习, 当在当前策略选择了一个较低回报的动作时, 之后它可能会选择其他动作.<br>通常, 奖励机制由状态<em>s</em>和动作<em>a</em>的随机函数表示<code>R(s,a)</code></p>\n</li>\n<li><p>值函数 Value Function<br>立即奖励表示着当前动作或状态带来的立即效果是好是坏, 但是值函数表示这个动作在整个交互过程中扮演的角色是好是坏. 一个状态的值是从该状态开始到交互结束所积累的立即奖励的总和.<br>一个状态可能总是产生很低的立即奖励, 但是它有很高的值, 因为该状态之后的后续状态中会产生很大的立即奖励. 相反也是一样. 类比于我们人类, 立即奖励的高低相当于我们高兴或痛苦, 但是值函数给出的值则表示了在整个事件过程中我们有多高兴或不高兴的深刻判断.<br>引入值函数的唯一目的就是为了训练智能体以获得更大的奖励, 当智能体做决策以及评估决策时, 我们一直关心的都是值函数而不是立即奖励, 对于动作的选择也是基于对值函数的判断/评估. 值比立即奖励更难以确定, 因为立即奖励可以由环境准确的给出, 但是值却需要评估甚至多次评估才可能相对准确(因为有可能交互过程永远不结束, 那么对值的估计会有偏差). 我们希望选择的动作带来最高的值, 而不是最高的立即奖励, 实际上, 几乎所有强化学习算法中最重要的部分就是对于值函数的有效估计方法. 关于值函数估计所扮演的核心角色在近60年被广泛研究. </p>\n</li>\n<li><p>模型 Model<br>模型是对环境行为的仿真, 我们可以通过模型推断出动作对环境的改变, 给出准确的立即奖励和状态信息. 例如, 给定一个状态和动作, 模型可以预测出下一个要转移的状态以及下一个立即奖励值. 如果模型是确定的, 我们一般使用规划(<em>planning</em>)的方法来选择最优动作, 对于这种方式我们称之为基于模型<strong>model-based</strong>的方法, 相反, 如果模型是不确定的, 也就是<strong>model-free</strong>, 我们只能通过试错的方式进行学习并选择动作. </p>\n</li>\n</ol>\n<p><em>注: 对于什么是model-based和model-free将在以后进行深入讨论.</em></p>\n<h2 id=\"强化学习的目标\"><a href=\"#强化学习的目标\" class=\"headerlink\" title=\"强化学习的目标\"></a>强化学习的目标</h2><p>与目标识别为了最小化误差损失不同，强化学习的目的是寻到一个策略，使得期望（折扣）奖励最大化。</p>\n<h2 id=\"强化学习的通用符号表示-Notation\"><a href=\"#强化学习的通用符号表示-Notation\" class=\"headerlink\" title=\"强化学习的通用符号表示 Notation\"></a>强化学习的通用符号表示 Notation</h2><p>$←$    赋值</p>\n<p>$\\varepsilon$ 在$\\varepsilon-greedy$策略中随机选择动作的概率</p>\n<p>$\\gamma$ 计算总奖励的折扣因子</p>\n<p>$\\lambda$ 资格迹的衰减率或者GAE的权重因子</p>\n<p>$s,s’$ 状态，下一个状态</p>\n<p>$a$ 一个动作</p>\n<p>$r$ 一个奖励值（标量）</p>\n<p>$S$ 状态集（不包含终态）</p>\n<p>$S^{+}$ 状态集（包含终态）</p>\n<p>$A(s)$ $s$状态下可选择的动作</p>\n<p>$R$ 奖励集合</p>\n<p>$|S|$ 状态集中的元素数</p>\n<p>$t$ 单个时间步</p>\n<p>$T$ 一个episode的终态时间点</p>\n<p>$A_{t}$ $t$时刻选择的动作</p>\n<p>$S_{t}$ $t$时刻所在的状态</p>\n<p>$R_{t}$ $t$时刻获得的奖励</p>\n<p>$\\pi$ 策略（从状态到动作的映射）</p>\n<p>$\\pi(s)$ 在$s$状态下使用$\\pi$策略所选择的动作</p>\n<p>$\\pi(a|s)$ 在$s$状态下使用$\\pi$策略选择到动作$a$的概率</p>\n<p>$G_{t}$ 以$t$时刻为起始时间点，到终态所能获得的总奖励（回报）</p>\n<p>$p(s’,r|s,a)$ 在$s$状态执行a动作转移到$s‘$状态并获得奖励值为$r$的概率</p>\n<p>$p(s’|s,a)$ 在$s$状态执行$a$动作转移到$s’$状态的概率</p>\n<p>$r(s,a)$ 在$s$状态执行$a$动作所获得的<strong>期望</strong>立即奖励（即时奖励）</p>\n<p>$r(s,a,s’)$ 在$s$状态执行$a$动作转移到$s’$状态所获得的<strong>期望</strong>立即奖励</p>\n<p>$v_{\\pi}(s)$ $\\pi$策略下状态$s$的值（以该状态为始态的期望奖励回报）</p>\n<p>$v_{<em>}(s)$ <em>*最优</em></em>策略下状态s的值</p>\n<p>$q_{\\pi}(s,a)$ $\\pi$策略下状态-行动对$(s,a)$的值</p>\n<p>$q_{<em>}(s,a)$ <em>*最优</em></em>策略下状态-行动对$(s,a)$的值</p>\n<p>$V,V_{t}$ 状态值的矩阵估计，行和列分别是时间点$t$和每个状态的估计值$v_{\\pi}$或$v_{*}$</p>\n<p>$Q,Q_{t}$ 状态-行动对$(s,a)$的矩阵估计，一般为一个3维矩阵,行、列和深度分别为状态、动作、时间点</p>\n<p>$V_{t}(s)$ 状态$s$的期望估计值</p>\n<p>$\\delta_{t}$ $t$时刻的TD-error时间差分量</p>\n<p>$\\theta,\\theta_{t}$ 目标策略的参数（向量）</p>\n<p>$\\pi(a|s,\\theta)$ 目标策略的参数为$\\theta$时，在$s$状态选择到$a$动作的概率</p>\n<p>$\\pi_{\\theta}$ 表示参数为$\\theta$的策略</p>\n<p>$\\nabla{\\pi(a|s,\\theta)}$ $\\pi(a|s,\\theta)$对于参数$\\theta$的偏微分</p>\n<p>$J(\\theta)$ 参数为$\\theta$的策略的性能度量、期望奖励(performance measure)</p>\n<p>$\\nabla{J(\\theta)}$ 性能度量对于策略参数$\\theta$的偏导数</p>"},{"title":"Reinforcement Learning with Deep Energy-Based Policies","copyright":true,"mathjax":true,"top":1,"date":"2019-06-26T07:12:39.000Z","keywords":null,"description":null,"_content":"\n本文提出了一个算法，用于学习连续空间下基于能量的策略：SQL，不是数据库的SQL，而是soft Q-Learning。该算法应用了最大熵理论，并且使用能量模型（EBM，Energy-Based Model）作为决策模型。\n\n推荐阅读该论文：\n\n- 公式复杂，但详尽吃透可以学习到SVGD、EBM等概念与算法\n- 文章充实，可以继续阅读后续算法SAC\n- 拓展在强化学习与熵进行结合方面的知识\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/abs/1702.08165](https://arxiv.org/abs/1702.08165)\n\n源代码：[https://github.com/rail-berkeley/softlearning](https://github.com/rail-berkeley/softlearning)\n\n该论文与2017年发于第34次ICML会议上，本文对v2版本进行分析。该论文作者为Tuomas Haarnoja，是伯克利大学BAIR实验室的博士生，SAC算法也是他的杰作。\n\n传统的RL方法主要是用分布拟合单峰分布，即\n\n![](./rl-with-deep-energy-based-policies/unimodal-policy.png)\n\n也有许多算法想要根据Q函数的值拟合出多峰分布，即\n\n![](./rl-with-deep-energy-based-policies/multimodal-policy.png)\n\n本文中就是针对拟合多峰分布提出了算法SQL。\n\n为什么要拟合多峰分布呢？当我们考虑最优控制和概率推理之间的联系时，随机策略才是最优解。\n\n> As discussed in prior work, a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference.\n\n随机策略有一些优点：\n\n- 如果可以全面地学习给定任务中的目标策略，那么结果策略可以作为很好的初始化策略，微调后以学习更高级的策略\n- 这种随机的探索机制，可以更好地寻求多峰任务中的最佳决策模型\n- 更好的鲁棒性，环境有干扰或者噪音时，有多种完成目标的行动可以选择，可以从干扰中“脱身”\n\n## 算法效果\n\n> The applications of training such stochastic policies include improved exploration in the case of multimodal objectives and compositionality via pretraining general-purpose stochastic policies that can then be efficiently finetuned into task-specific behaviors. \n\n在多峰目标任务中训练随机策略可以提升探索，也可以预训练出通用目的的随机策略以微调后运用至指定任务中进行训练（迁移学习、元学习）。\n\n# 文中精要\n\n## 标准强化学习的最优策略\n\n$$\n\\pi_{\\mathrm{std}}^{*}=\\arg \\max _{\\pi} \\sum_{t} \\mathbb{E}_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\sim \\rho_{\\pi}}\\left[r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right] \\tag{1}\n$$\n\n- `std`下标代表标准的意思：standard，星号$\\ast$代表最优\n\n- $\\rho_{\\pi}$代表策略$\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right)$下的迹分布，$\\rho_{\\pi}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)$代表状态-行动对的边缘分布。\n\n> We will also use $\\rho_{\\pi}\\left(\\mathbf{s}_{t}\\right)$ and $\\rho_{\\pi}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)$ to denote the state and state-action marginals of the trajectory distribution induced by a policy $\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right)$. \n\n## 最大熵强化学习的最优策略\n\n最大熵强化学习在标准RL的目标函数上加入了一个关于状态下可选动作分布熵的项，这种目标希望智能体不仅能以获得最大奖励的方式完成目标，而且能够决策地尽可能随机。因为通过这种目标函数学到的策略**更具鲁棒性**，可以更好适用于环境的突然变化，或者从前没有遇到过得场景。\n$$\n\\pi_{\\mathrm{MaxEnt}}^{*}=\\arg \\max _{\\pi} \\sum_{t} \\mathbb{E}_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\sim \\rho_{\\pi}}\\left[r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)+\\color{red}{\\alpha \\mathcal{H}\\left(\\pi\\left(\\cdot | \\mathbf{s}_{t}\\right)\\right)}\\right]\n\\tag{2}\n$$\n\n- `MaxEnt`下标代表最大熵的意思：Maximum entropy\n- 式中的系数$\\alpha$可以用来调节奖励项与熵值项的重要性比率。**一般将$\\alpha$表示为奖励范围（reward scale）的倒数，但在实际中通常将其作为超参数手动调节。**SAC算法中有介绍在训练过程中自动调节该系数的方法。\n- 本文中的SQL算法也是为了优化该目标函数\n\n## 最大熵目标的优点\n\n- 在多峰（即一个状态下有多个最优动作选择）问题中提升探索能力\n- 可以用于迁移学习，因为其“预训练”模型更好地适应之后的任务\n\n## soft 值函数\n\n文中，***定义***了最大熵RL下的Q函数与V函数，注意，是定义，不是推导出来的。\n\nsoft Q函数定义如下：\n$$\n{Q_{\\text { soft }}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+}  {\\mathbb{E}_{\\left(\\mathbf{s}_{t+1}, \\ldots\\right) \\sim \\rho_{\\pi}}\\left[\\sum_{l=1}^{\\infty} \\gamma^{l}\\left(r_{t+l}+\\alpha \\mathcal{H}\\left(\\pi_{\\text { MaxEnt }}^{*}\\left(\\cdot | \\mathbf{s}_{t+l}\\right)\\right)\\right)\\right]}\n\\tag{3}\n$$\nsoft V函数定义如下：\n$$\nV_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)=\\alpha \\log \\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}\n\\tag{4}\n$$\n乍一看这个值函数$V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)$的形式定义的很奇怪，的确很奇怪，严格来说，它的真实意义并不是为了构造状态值函数，而是构造一个配分函数使得后面推导最优策略时可以化简过程。当然，算法中也不需要用它的值去衡量状态的价值，只是作为计算的中间过程。\n\n作者说，值函数满足soft 贝尔曼方程，即\n$$\nQ_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t+1}\\right)\\right]\n\\tag{5}\n$$\n\n## 能量模型与策略\n\n文中提出能量模型（Energy-Based Models）的初衷是之前很多人在研究中使用了多项式分布（discrete multinomial distributions）、高斯分布（Gaussian distributions）来表示策略，这样的分布通常用来表示动作价值分布是单峰（unimodal）的情况，而且最终收敛结果往往是接近确定性（near-deterministic）的。即使拓展出多峰的形式，也各自有或多或少的不足。基于此，作者想使用更广泛、通用的分布用来表示复杂、多峰的动作选择。\n\n所以，作者选择使用基于能量的通用策略：\n$$\n\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right) \\propto \\exp \\left(-\\mathcal{E}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)\n\\tag{6}\n$$\n\n- $\\mathcal{E}$是字母E的花体形式，代表能量函数，其可以被深度神经网络表示，如果使用通用值函数近似来表示能量函数，那么可以表示任意策略$\\pi\\left(\\boldsymbol{a}_{t} | \\mathbf{s}_{t}\\right)$\n\n- > where $\\mathcal{E}$ is an energy function that could be represented, for example, by a deep neural network. If we use a universal function approximator for $\\mathcal{E}$, we can represent any distribution $\\pi\\left(\\boldsymbol{a}_{t} | \\mathbf{s}_{t}\\right)$. \n\n- 文中将该能量函数设置为\n  $$\n  \\mathcal{E}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=-\\frac{1}{\\alpha} Q_{\\operatorname{soft}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\n  \\tag{7}\n  $$\n  其实也很容易就能理解，将负号抵消掉之后，Q值大的动作能量高嘛，指数分布又能更好的放大较大的值，使Q值大的动作更为突出，这样完全可以作为选择动作的策略\n\n但是有一个问题是，不能使能量无限大呀，假如超过了计算能力那就不好了，当然这种情况几乎不会发生。于是，可以将能量给归一化，即\n$$\n\\begin{aligned} \n\\pi_{\\text { MaxEnt }}^{*}\\left(a_{t} | s_{t}\\right) \n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right) \\mathrm{d} a^{\\prime}} \\\\ \n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\exp \\left(\\frac{1}{\\alpha} V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)} \\\\\n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\color{blue}{\\exp \\log} \\exp \\left(\\frac{1}{\\alpha} V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)} \\\\\n&=\\color{red}{\\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)-V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)\\right)}\n\\end{aligned}\n\\tag{8}\n$$\n\n<p align=\"center\" style=\"color:blue\" ><a href=\"https://bluefisher.github.io/2018/11/13/Reinforcement-Learning-with-Deep-Energy-Based-Policies/\">BlueFisher's Blog</a></p>\n论文中只给出了红色字体的部分，其实这才是作者想要表达的意思，文中就是基于此定义了状态值函数$V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)$的形式。\n\n在这个公式中就可以看出，策略是对动作值函数Q进行了一个softmax操作，这也是文中soft的含义。\n\n## 使用SQL优化目标函数\n\n像使用Q-Learning对网格世界问题进行优化求解一样，我们也可以使用迭代的方式进行优化，交互计算两个值函数，使其各自收敛，就可以导出最优策略。\n\n于是，作者定义了soft Q-Iteration。\n\n### Soft Q-Iteration\n\n先要假设值函数$Q_{\\mathrm{soft}}(\\cdot, \\cdot)$、$V_{\\text { soft }}(\\cdot)$有界，\n$$\n\\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}\\left(\\cdot, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}<\\infty \\ ，\\ Q_{\\mathrm{soft}}^{*}<\\infty\n\\tag{9}\n$$\n文中定义的交互迭代至收敛的方式其实跟SARSA算法比较像：\n$$\n\\begin{array}{c}\n{Q_{\\text { soft }}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\leftarrow r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\text { soft }}\\left(\\mathbf{s}_{t+1}\\right)\\right], \\forall \\mathbf{s}_{t}, \\mathbf{a}_{t}} \\\\ \n{V_{\\text { soft }}\\left(\\mathbf{s}_{t}\\right) \\leftarrow \\alpha \\log \\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}, \\forall \\mathbf{s}_{t}}\n\\end{array}\n\\tag{10}\n$$\n这种优化方式在理论上是可行的，但是在实际应用中存在两个问题：\n\n1. 连续空间无法求期望，或者计算不准确。**解决方案是重要性采样，使用采样多次后计算来代替积分，在初期进行随机均匀采样，后期根据policy来采样。**\n2. 迭代过程需要不断选择动作，问题是式（8）的分布形式无法进行采样。**解决方案是使用SVGD算法拟合后验分布，并输出采样的动作。**\n\n### Soft Q-Learning\n\n文中在这一部分引用了重要性采样，解决了上面提到的第一个问题，即，使用分布$q_{\\mathrm{a}^{\\prime}}$来代替真实策略分布\n$$\n\\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)-V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)\\right\n)\n$$\n进行采样。\n$$\nV_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}\\right)=\\alpha \\log \\mathbb{E}_{\\color{red}{q_{\\mathrm{a}^{\\prime}}}}\\left[\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right)}{q_{\\mathrm{a}^{\\prime}}\\left(\\mathbf{a}^{\\prime}\\right)}\\right]\n\\tag{11}\n$$\n采样分布$q_{\\mathrm{a}^{\\prime}}$可以使用任意的分布，但是由于重要性采样的性质，采样分布与原分布越接近，效果越好。式子中的$\\theta$为Q神经网络的参数。\n\n因为在训练初期，我们估计的真实分布是偏差很大的，几乎可以说是错误的，因此在训练初期将采样分布设置为均匀分布比较合理，在训练一段时间之后，可以将采样分布设置为接近原分布，甚至是原分布（如果原分布可以采样，如，使用神经网络等“黑匣子”进行表示）\n\n由此，可以定义Q神经网络的损失函数为：\n$$\nJ_{Q}(\\theta)=\\mathbb{E}_{\\mathbf{s}_{t} \\sim q_{\\mathbf{s}_{t}}, \\mathbf{a}_{t} \\sim q_{\\mathbf{a}_{t}}}\\left[\\frac{1}{2}\\left(\\hat{Q}_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)-Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)^{2}\\right]\n\\tag{12}\n$$\n上式中期望的下标为环境和真实策略分布，$\\overline{\\theta}$代表target网络的参数，目标是最小化这个损失函数，其中，\n$$\n\\hat{Q}_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t+1}\\right)\\right]\n\\tag{13}\n$$\n### 近似采样与SVGD\n\nSVGD：Stein Vairational Gradient Descent，SVGD是一种确定性的、基于梯度的近似推理采样算法。\n\n文中在这一部分引用了SVGD的优化算法，并且使用SVGD近似策略的后验分布以进行采样，解决了上文提到的第二个问题。在百度上完全搜不到关于SVGD算法的信息，但是了解了这个算法之后，感觉它的能力还是很强的，最终，搜集了Google、Bing的检索结果，发现了原作者在SVGD方法上的一些资源分享，[Stein’s Method for Practical Machine Learning](https://www.cs.utexas.edu/~lqiang/stein.html)\n\n对于基于能量的模型、分布，有两类采样方式：\n\n1. MCMC采样，即马尔科夫链蒙特卡洛采样\n2. 学习一个采样网络去近似采样出符合目标分布的样本\n\n在需要不断更新策略的在线学习任务中，使用MCMC采样是不可行的，于是作者使用了基于SVGD和Amortized SVGD的采样网络。\n\nSVGD论文：[https://arxiv.org/abs/1608.04471](https://arxiv.org/abs/1608.04471)\n\nAmortized SVGD论文：[https://arxiv.org/abs/1707.06626](https://arxiv.org/abs/1707.06626)\n\n![](./rl-with-deep-energy-based-policies/1dgmm.gif)\n![](./rl-with-deep-energy-based-policies/vp.gif)\n\nAmortized SVGD有一些有趣的性质：\n\n- 可以训练随机采样网络非常快地采样\n- 可以准确收敛至EBM能量模型的后验估计分布\n- 文中结合了Amortized SVGD后，算法形式很像A-C模式\n\nSVGD算法的更新形式是这样的，\n$$\nx_{i} \\leftarrow x_{i}+\\frac{\\epsilon}{n} \\sum_{j=1}^{n}\\left[k\\left(x_{j}, x_{i}\\right) \\nabla_{x_{j}} \\log p\\left(x_{j}\\right)+\\nabla_{x_{j}} k\\left(x_{j}, x_{i}\\right)\\right], \\qquad \\forall i=1, \\ldots, n\n\\tag{14}\n$$\n- $\\epsilon$代表学习率\n- $k\\left(x_{j}, x_{i}\\right)$代表正定核，如径向基（RBF，Radial Basis Function）函数$k\\left(x, x^{\\prime}\\right)=\\exp \\left(-\\frac{1}{h}\\left\\|x-x^{\\prime}\\right\\|_{2}^{2}\\right)$，它可以被认为是变量之间的相似性度量\n- 包含对数项$\\log p\\left(x_{j}\\right)$的梯度驱使采样器朝着$p(x)$分布中高概率区域进行采样\n- 第二项核函数梯度驱使样本点之间产生间隙，相当于用一个排斥力使样本点尽可能分散开\n- 对数项梯度不依赖分布$p(x)$的归一化常数，使SVGD易于应用于图模型、贝叶斯推理和深层生成模型中出现的难以处理的分布。\n\n先定义我们采样网络（其实就是Actor）的目标函数：\n$$\nJ_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)=D_{\\mathrm{KL}}\\left(\\pi^{\\phi}\\left(\\cdot | \\mathbf{s}_{t}\\right) \\| \\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\cdot\\right)-V_{\\mathrm{soft}}^{\\theta}\\right)\\right)\\right)\n\\tag{15}\n$$\n- $\\phi$表示采样网络中的参数\n- 将产生动作的函数简写成$\\mathbf{a}_{t}^{(i)}=f^{\\phi}\\left(\\xi^{(i)} ; \\mathbf{s}_{t}\\right)$，也就是说神经网络的输入分为两部分，一部分是状态$s$，一部分是噪声扰乱“perturb”$\\xi$，一般从标准正态分布中采样，而且最好使噪声的维度与动作的维度一致\n\n计算梯度方向：\n$$\n\\begin{aligned}\n\\Delta f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)=& \\mathbb{E}_{\\mathbf{a}_{t} \\sim \\pi^{\\phi}}\\left[\\kappa\\left(\\mathbf{a}_{t}, f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)\\right) \\nabla_{\\mathbf{a}^{\\prime}} Q_{\\mathrm{soft}}^{\\theta}\\left.\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{t}}\\right.\\\\ &+\\alpha \\nabla_{\\mathbf{a}^{\\prime}} \\kappa\\left.\\left(\\mathbf{a}^{\\prime}, f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{t}} ] \\end{aligned}\n\\tag{16}\n$$\n- 严格来说，$\\Delta f^{\\phi}$是希尔伯特空间的最优梯度方向，并不是Actor目标函数$J_{\\pi}$的梯度\n\n- > To be precise, $\\Delta f^{\\phi}$ is the optimal direction in the reproducing kernel Hilbert space of $\\kappa$, and is thus not strictly speaking the gradient of $J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)$ \n\n根据链式法则，Stein变分梯度SVG为\n$$\n\\frac{\\partial J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)}{\\partial \\phi} \\propto \\mathbb{E}_{\\xi}\\left[\\Delta f^{\\phi}\\left(\\xi ; \\mathbf{s}_{t}\\right) \\frac{\\partial f^{\\phi}\\left(\\xi ; \\mathbf{s}_{t}\\right)}{\\partial \\phi}\\right]\n\\tag{17}\n$$\n## 伪代码\n\n![](./rl-with-deep-energy-based-policies/pseudo.png)\n\n解析：\n\n算法中更新Actor网络时，其实是使用了如下梯度公式：\n$$\n\\hat{\\nabla}_{\\phi} J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)=\\frac{1}{K M} \\sum_{j=1}^{K} \\sum_{i=1}^{M}\\left(\\kappa\\left(\\mathbf{a}_{t}^{(i)}, \\tilde{\\mathbf{a}}_{t}^{(j)}\\right) \\nabla_{\\mathbf{a}^{\\prime}} Q_{\\mathrm{soft}}\\left.\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{i}^{(i)}}+\\nabla_{\\mathbf{a}^{\\prime}} \\kappa\\left.\\left(\\mathbf{a}^{\\prime}, \\tilde{\\mathbf{a}}_{t}^{(j)}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{i}^{(i)}}\\right) \\nabla_{\\phi} f^{\\phi}\\left(\\tilde{\\xi}^{(j)} ; \\mathbf{s}_{t}\\right)\n\\tag{18}\n$$\n更新方向为mini-batch经验的梯度平均值，而不是累加和\n\n- 伪代码中定义了Actor的target网络，参数为$\\overline{\\theta}$。但是伪代码中并没有显示出其在何处使用，我**猜测**该网络代表采样分布$q_{\\mathbf{a}^{\\prime}}$\n  - $q_{\\mathbf{a}^{\\prime}}$在训练初期使用均匀分布\n  - $q_{\\mathbf{a}^{\\prime}}$在一段时间之后使用Actor真实分布，我猜测这里使用的就是Actor目标网络\n- 噪音$\\xi$从多维标准正态分布中采样，维度最好与动作空间维度一致\n- $\\left\\{\\mathbf{a}^{(i, j)}\\right\\}_{j=0}^{M} \\sim q_{\\mathbf{a}^{\\prime}}$其中的M用于设置采样多少个样本，以使用公式（11）计算V值，使用的网络为Q目标网络\n- 在更新Q网络时，使用了从经验池采样到的真实执行过的动作\n- $\\left\\{\\xi^{(i, j)}\\right\\}_{j=0}^{M} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{I})$中其实少写了一个参数$K$，但实际上$K=M$，在这一步中需要采样两组噪音，当然也可以采样一组，使用两次。\n- 在更新Actor网络时，没有使用经验池中采样到的动作，而模拟采样了两组动作，即根据两组噪音生成的动作，用它们来计算梯度并更新。\n- Q网络的输入为状态与动作的连接，$(s||a)$，输出为Q值\n- Actor网络的输入为状态与噪音的连接，$(s||\\xi)$，输出为动作$a$\n- 伪代码中的式(10)、(11)、(13)、(14)分别代表本文中的式(11)、(12)、(16)、(17)\n\n# 实验\n\n## 实验设置\n\n- 比较算法：DDPG vs SQL\n- Actor和Q网络使用Adam优化器\n- Actor学习率为0.0001，Q网络学习率为0.001\n- 经验池大小为100W\n- 经验池填充1W条经验后开始训练\n- batch_size=64\n- Actor和Q网络都是2层隐藏层，每层200个隐藏节点，激活函数为ReLU\n- DDPG和SQL都使用了Ornstein-Uhlenbeck随机过程产生噪音来增加探索，它是一种序贯相关的随机过程，$\\theta=0.15 \\ , \\ \\sigma=0.3$\n  - OU随机过程可以在序贯模型中添加与时间相关的随机噪音，而且噪音也满足强马尔可夫性\n  \n  - 形式为$d x_{t}=\\theta\\left(\\mu-x_{t}\\right) d t+\\sigma d W_{t}$，是一个具有均值恢复属性的随机过程\n  \n  - $\\theta$表示变量$x$以多大幅度、多块恢复到平均值，$\\mu$代表平均值，$\\sigma$代表波动程度，$d W_{t}$代表维纳过程，一般通过高斯分布实现\n  \n  - OU随机过程产生的噪音只与上一次产生的噪音相关，它可以用于增加探索，也能够柔顺控制。比如在相邻的两个决策动作，一个为10，一个为-10，反复如此，智能体会产生震荡。在此使用OU过程可以使智能体在一个方向保持一定时间，不会瞬间过大地改变智能体的状态，相当于增加了时滞性。\n  \n  - 代码\n  \n    ```\n    x = 10\n    dx = theta * (mu - x) + sigma * numpy.random.randn(len(x))\n    x = x + dx\n    ```\n  \n  - 参考：\n  \n    - [https://www.quora.com/Why-do-we-use-the-Ornstein-Uhlenbeck-Process-in-the-exploration-of-DDPG](https://www.quora.com/Why-do-we-use-the-Ornstein-Uhlenbeck-Process-in-the-exploration-of-DDPG)\n    - [https://github.com/floodsung/DDPG-tensorflow/blob/master/ou_noise.py](https://github.com/floodsung/DDPG-tensorflow/blob/master/ou_noise.py)\n    - [https://zhuanlan.zhihu.com/p/51333694](https://zhuanlan.zhihu.com/p/51333694)\n- 核函数使用了径向基函数RBF，$\\kappa\\left(\\mathbf{a}, \\mathbf{a}^{\\prime}\\right)=\\exp \\left(-\\frac{1}{h}\\left\\|\\mathbf{a}-\\mathbf{a}^{\\prime}\\right\\|_{2}^{2}\\right)$，其中，$h=\\frac{d}{2 \\log (M+1)}$，$d$为各变量对之间距离的中位数\n- 目标网络的更新采样硬覆盖的模式\n- 超参数$\\alpha$根据任务设置为10，0.1等等\n- 训练的epoch、步长、系数$\\alpha$，采样动作的数量$K 和 M$根据任务（多目标，单目标，微调）的不同而不同，具体请看原论文附录D部分\n\n## 实验结果\n\n未完待续\n\n# 个人感想\n\n虽然文中用大量公式、篇幅结合最大熵进行介绍、推理，但是在伪代码以及目标函数中似乎并没有看到关于熵的影子，包含熵项的Q值也是通过Q网络的输出将熵的值包含在内，并没有显式地计算它。\n\n虽然算法的名字为soft Q-Learning，但其实它跟传统的Q-Learning算法思想并不相同，如果说有一点相同，那也是都是想使Q值收敛以推导出最优策略，但是这个优化过程也跟SARSA算法比较像，并没有使用传统Q-Learning中贪婪的选择最有价值下一个动作以自举的方法。","source":"_posts/rl-with-deep-energy-based-policies.md","raw":"---\ntitle: Reinforcement Learning with Deep Energy-Based Policies\ncopyright: true\nmathjax: true\ntop: 1\ndate: 2019-06-26 15:12:39\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n本文提出了一个算法，用于学习连续空间下基于能量的策略：SQL，不是数据库的SQL，而是soft Q-Learning。该算法应用了最大熵理论，并且使用能量模型（EBM，Energy-Based Model）作为决策模型。\n\n推荐阅读该论文：\n\n- 公式复杂，但详尽吃透可以学习到SVGD、EBM等概念与算法\n- 文章充实，可以继续阅读后续算法SAC\n- 拓展在强化学习与熵进行结合方面的知识\n\n<!--more-->\n\n# 简介\n\n论文地址：[https://arxiv.org/abs/1702.08165](https://arxiv.org/abs/1702.08165)\n\n源代码：[https://github.com/rail-berkeley/softlearning](https://github.com/rail-berkeley/softlearning)\n\n该论文与2017年发于第34次ICML会议上，本文对v2版本进行分析。该论文作者为Tuomas Haarnoja，是伯克利大学BAIR实验室的博士生，SAC算法也是他的杰作。\n\n传统的RL方法主要是用分布拟合单峰分布，即\n\n![](./rl-with-deep-energy-based-policies/unimodal-policy.png)\n\n也有许多算法想要根据Q函数的值拟合出多峰分布，即\n\n![](./rl-with-deep-energy-based-policies/multimodal-policy.png)\n\n本文中就是针对拟合多峰分布提出了算法SQL。\n\n为什么要拟合多峰分布呢？当我们考虑最优控制和概率推理之间的联系时，随机策略才是最优解。\n\n> As discussed in prior work, a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference.\n\n随机策略有一些优点：\n\n- 如果可以全面地学习给定任务中的目标策略，那么结果策略可以作为很好的初始化策略，微调后以学习更高级的策略\n- 这种随机的探索机制，可以更好地寻求多峰任务中的最佳决策模型\n- 更好的鲁棒性，环境有干扰或者噪音时，有多种完成目标的行动可以选择，可以从干扰中“脱身”\n\n## 算法效果\n\n> The applications of training such stochastic policies include improved exploration in the case of multimodal objectives and compositionality via pretraining general-purpose stochastic policies that can then be efficiently finetuned into task-specific behaviors. \n\n在多峰目标任务中训练随机策略可以提升探索，也可以预训练出通用目的的随机策略以微调后运用至指定任务中进行训练（迁移学习、元学习）。\n\n# 文中精要\n\n## 标准强化学习的最优策略\n\n$$\n\\pi_{\\mathrm{std}}^{*}=\\arg \\max _{\\pi} \\sum_{t} \\mathbb{E}_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\sim \\rho_{\\pi}}\\left[r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right] \\tag{1}\n$$\n\n- `std`下标代表标准的意思：standard，星号$\\ast$代表最优\n\n- $\\rho_{\\pi}$代表策略$\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right)$下的迹分布，$\\rho_{\\pi}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)$代表状态-行动对的边缘分布。\n\n> We will also use $\\rho_{\\pi}\\left(\\mathbf{s}_{t}\\right)$ and $\\rho_{\\pi}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)$ to denote the state and state-action marginals of the trajectory distribution induced by a policy $\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right)$. \n\n## 最大熵强化学习的最优策略\n\n最大熵强化学习在标准RL的目标函数上加入了一个关于状态下可选动作分布熵的项，这种目标希望智能体不仅能以获得最大奖励的方式完成目标，而且能够决策地尽可能随机。因为通过这种目标函数学到的策略**更具鲁棒性**，可以更好适用于环境的突然变化，或者从前没有遇到过得场景。\n$$\n\\pi_{\\mathrm{MaxEnt}}^{*}=\\arg \\max _{\\pi} \\sum_{t} \\mathbb{E}_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\sim \\rho_{\\pi}}\\left[r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)+\\color{red}{\\alpha \\mathcal{H}\\left(\\pi\\left(\\cdot | \\mathbf{s}_{t}\\right)\\right)}\\right]\n\\tag{2}\n$$\n\n- `MaxEnt`下标代表最大熵的意思：Maximum entropy\n- 式中的系数$\\alpha$可以用来调节奖励项与熵值项的重要性比率。**一般将$\\alpha$表示为奖励范围（reward scale）的倒数，但在实际中通常将其作为超参数手动调节。**SAC算法中有介绍在训练过程中自动调节该系数的方法。\n- 本文中的SQL算法也是为了优化该目标函数\n\n## 最大熵目标的优点\n\n- 在多峰（即一个状态下有多个最优动作选择）问题中提升探索能力\n- 可以用于迁移学习，因为其“预训练”模型更好地适应之后的任务\n\n## soft 值函数\n\n文中，***定义***了最大熵RL下的Q函数与V函数，注意，是定义，不是推导出来的。\n\nsoft Q函数定义如下：\n$$\n{Q_{\\text { soft }}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+}  {\\mathbb{E}_{\\left(\\mathbf{s}_{t+1}, \\ldots\\right) \\sim \\rho_{\\pi}}\\left[\\sum_{l=1}^{\\infty} \\gamma^{l}\\left(r_{t+l}+\\alpha \\mathcal{H}\\left(\\pi_{\\text { MaxEnt }}^{*}\\left(\\cdot | \\mathbf{s}_{t+l}\\right)\\right)\\right)\\right]}\n\\tag{3}\n$$\nsoft V函数定义如下：\n$$\nV_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)=\\alpha \\log \\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}\n\\tag{4}\n$$\n乍一看这个值函数$V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)$的形式定义的很奇怪，的确很奇怪，严格来说，它的真实意义并不是为了构造状态值函数，而是构造一个配分函数使得后面推导最优策略时可以化简过程。当然，算法中也不需要用它的值去衡量状态的价值，只是作为计算的中间过程。\n\n作者说，值函数满足soft 贝尔曼方程，即\n$$\nQ_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t+1}\\right)\\right]\n\\tag{5}\n$$\n\n## 能量模型与策略\n\n文中提出能量模型（Energy-Based Models）的初衷是之前很多人在研究中使用了多项式分布（discrete multinomial distributions）、高斯分布（Gaussian distributions）来表示策略，这样的分布通常用来表示动作价值分布是单峰（unimodal）的情况，而且最终收敛结果往往是接近确定性（near-deterministic）的。即使拓展出多峰的形式，也各自有或多或少的不足。基于此，作者想使用更广泛、通用的分布用来表示复杂、多峰的动作选择。\n\n所以，作者选择使用基于能量的通用策略：\n$$\n\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right) \\propto \\exp \\left(-\\mathcal{E}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)\n\\tag{6}\n$$\n\n- $\\mathcal{E}$是字母E的花体形式，代表能量函数，其可以被深度神经网络表示，如果使用通用值函数近似来表示能量函数，那么可以表示任意策略$\\pi\\left(\\boldsymbol{a}_{t} | \\mathbf{s}_{t}\\right)$\n\n- > where $\\mathcal{E}$ is an energy function that could be represented, for example, by a deep neural network. If we use a universal function approximator for $\\mathcal{E}$, we can represent any distribution $\\pi\\left(\\boldsymbol{a}_{t} | \\mathbf{s}_{t}\\right)$. \n\n- 文中将该能量函数设置为\n  $$\n  \\mathcal{E}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=-\\frac{1}{\\alpha} Q_{\\operatorname{soft}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\n  \\tag{7}\n  $$\n  其实也很容易就能理解，将负号抵消掉之后，Q值大的动作能量高嘛，指数分布又能更好的放大较大的值，使Q值大的动作更为突出，这样完全可以作为选择动作的策略\n\n但是有一个问题是，不能使能量无限大呀，假如超过了计算能力那就不好了，当然这种情况几乎不会发生。于是，可以将能量给归一化，即\n$$\n\\begin{aligned} \n\\pi_{\\text { MaxEnt }}^{*}\\left(a_{t} | s_{t}\\right) \n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right) \\mathrm{d} a^{\\prime}} \\\\ \n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\exp \\left(\\frac{1}{\\alpha} V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)} \\\\\n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\color{blue}{\\exp \\log} \\exp \\left(\\frac{1}{\\alpha} V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)} \\\\\n&=\\color{red}{\\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)-V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)\\right)}\n\\end{aligned}\n\\tag{8}\n$$\n\n<p align=\"center\" style=\"color:blue\" ><a href=\"https://bluefisher.github.io/2018/11/13/Reinforcement-Learning-with-Deep-Energy-Based-Policies/\">BlueFisher's Blog</a></p>\n论文中只给出了红色字体的部分，其实这才是作者想要表达的意思，文中就是基于此定义了状态值函数$V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)$的形式。\n\n在这个公式中就可以看出，策略是对动作值函数Q进行了一个softmax操作，这也是文中soft的含义。\n\n## 使用SQL优化目标函数\n\n像使用Q-Learning对网格世界问题进行优化求解一样，我们也可以使用迭代的方式进行优化，交互计算两个值函数，使其各自收敛，就可以导出最优策略。\n\n于是，作者定义了soft Q-Iteration。\n\n### Soft Q-Iteration\n\n先要假设值函数$Q_{\\mathrm{soft}}(\\cdot, \\cdot)$、$V_{\\text { soft }}(\\cdot)$有界，\n$$\n\\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}\\left(\\cdot, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}<\\infty \\ ，\\ Q_{\\mathrm{soft}}^{*}<\\infty\n\\tag{9}\n$$\n文中定义的交互迭代至收敛的方式其实跟SARSA算法比较像：\n$$\n\\begin{array}{c}\n{Q_{\\text { soft }}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\leftarrow r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\text { soft }}\\left(\\mathbf{s}_{t+1}\\right)\\right], \\forall \\mathbf{s}_{t}, \\mathbf{a}_{t}} \\\\ \n{V_{\\text { soft }}\\left(\\mathbf{s}_{t}\\right) \\leftarrow \\alpha \\log \\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}, \\forall \\mathbf{s}_{t}}\n\\end{array}\n\\tag{10}\n$$\n这种优化方式在理论上是可行的，但是在实际应用中存在两个问题：\n\n1. 连续空间无法求期望，或者计算不准确。**解决方案是重要性采样，使用采样多次后计算来代替积分，在初期进行随机均匀采样，后期根据policy来采样。**\n2. 迭代过程需要不断选择动作，问题是式（8）的分布形式无法进行采样。**解决方案是使用SVGD算法拟合后验分布，并输出采样的动作。**\n\n### Soft Q-Learning\n\n文中在这一部分引用了重要性采样，解决了上面提到的第一个问题，即，使用分布$q_{\\mathrm{a}^{\\prime}}$来代替真实策略分布\n$$\n\\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)-V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)\\right\n)\n$$\n进行采样。\n$$\nV_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}\\right)=\\alpha \\log \\mathbb{E}_{\\color{red}{q_{\\mathrm{a}^{\\prime}}}}\\left[\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right)}{q_{\\mathrm{a}^{\\prime}}\\left(\\mathbf{a}^{\\prime}\\right)}\\right]\n\\tag{11}\n$$\n采样分布$q_{\\mathrm{a}^{\\prime}}$可以使用任意的分布，但是由于重要性采样的性质，采样分布与原分布越接近，效果越好。式子中的$\\theta$为Q神经网络的参数。\n\n因为在训练初期，我们估计的真实分布是偏差很大的，几乎可以说是错误的，因此在训练初期将采样分布设置为均匀分布比较合理，在训练一段时间之后，可以将采样分布设置为接近原分布，甚至是原分布（如果原分布可以采样，如，使用神经网络等“黑匣子”进行表示）\n\n由此，可以定义Q神经网络的损失函数为：\n$$\nJ_{Q}(\\theta)=\\mathbb{E}_{\\mathbf{s}_{t} \\sim q_{\\mathbf{s}_{t}}, \\mathbf{a}_{t} \\sim q_{\\mathbf{a}_{t}}}\\left[\\frac{1}{2}\\left(\\hat{Q}_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)-Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)^{2}\\right]\n\\tag{12}\n$$\n上式中期望的下标为环境和真实策略分布，$\\overline{\\theta}$代表target网络的参数，目标是最小化这个损失函数，其中，\n$$\n\\hat{Q}_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t+1}\\right)\\right]\n\\tag{13}\n$$\n### 近似采样与SVGD\n\nSVGD：Stein Vairational Gradient Descent，SVGD是一种确定性的、基于梯度的近似推理采样算法。\n\n文中在这一部分引用了SVGD的优化算法，并且使用SVGD近似策略的后验分布以进行采样，解决了上文提到的第二个问题。在百度上完全搜不到关于SVGD算法的信息，但是了解了这个算法之后，感觉它的能力还是很强的，最终，搜集了Google、Bing的检索结果，发现了原作者在SVGD方法上的一些资源分享，[Stein’s Method for Practical Machine Learning](https://www.cs.utexas.edu/~lqiang/stein.html)\n\n对于基于能量的模型、分布，有两类采样方式：\n\n1. MCMC采样，即马尔科夫链蒙特卡洛采样\n2. 学习一个采样网络去近似采样出符合目标分布的样本\n\n在需要不断更新策略的在线学习任务中，使用MCMC采样是不可行的，于是作者使用了基于SVGD和Amortized SVGD的采样网络。\n\nSVGD论文：[https://arxiv.org/abs/1608.04471](https://arxiv.org/abs/1608.04471)\n\nAmortized SVGD论文：[https://arxiv.org/abs/1707.06626](https://arxiv.org/abs/1707.06626)\n\n![](./rl-with-deep-energy-based-policies/1dgmm.gif)\n![](./rl-with-deep-energy-based-policies/vp.gif)\n\nAmortized SVGD有一些有趣的性质：\n\n- 可以训练随机采样网络非常快地采样\n- 可以准确收敛至EBM能量模型的后验估计分布\n- 文中结合了Amortized SVGD后，算法形式很像A-C模式\n\nSVGD算法的更新形式是这样的，\n$$\nx_{i} \\leftarrow x_{i}+\\frac{\\epsilon}{n} \\sum_{j=1}^{n}\\left[k\\left(x_{j}, x_{i}\\right) \\nabla_{x_{j}} \\log p\\left(x_{j}\\right)+\\nabla_{x_{j}} k\\left(x_{j}, x_{i}\\right)\\right], \\qquad \\forall i=1, \\ldots, n\n\\tag{14}\n$$\n- $\\epsilon$代表学习率\n- $k\\left(x_{j}, x_{i}\\right)$代表正定核，如径向基（RBF，Radial Basis Function）函数$k\\left(x, x^{\\prime}\\right)=\\exp \\left(-\\frac{1}{h}\\left\\|x-x^{\\prime}\\right\\|_{2}^{2}\\right)$，它可以被认为是变量之间的相似性度量\n- 包含对数项$\\log p\\left(x_{j}\\right)$的梯度驱使采样器朝着$p(x)$分布中高概率区域进行采样\n- 第二项核函数梯度驱使样本点之间产生间隙，相当于用一个排斥力使样本点尽可能分散开\n- 对数项梯度不依赖分布$p(x)$的归一化常数，使SVGD易于应用于图模型、贝叶斯推理和深层生成模型中出现的难以处理的分布。\n\n先定义我们采样网络（其实就是Actor）的目标函数：\n$$\nJ_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)=D_{\\mathrm{KL}}\\left(\\pi^{\\phi}\\left(\\cdot | \\mathbf{s}_{t}\\right) \\| \\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\cdot\\right)-V_{\\mathrm{soft}}^{\\theta}\\right)\\right)\\right)\n\\tag{15}\n$$\n- $\\phi$表示采样网络中的参数\n- 将产生动作的函数简写成$\\mathbf{a}_{t}^{(i)}=f^{\\phi}\\left(\\xi^{(i)} ; \\mathbf{s}_{t}\\right)$，也就是说神经网络的输入分为两部分，一部分是状态$s$，一部分是噪声扰乱“perturb”$\\xi$，一般从标准正态分布中采样，而且最好使噪声的维度与动作的维度一致\n\n计算梯度方向：\n$$\n\\begin{aligned}\n\\Delta f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)=& \\mathbb{E}_{\\mathbf{a}_{t} \\sim \\pi^{\\phi}}\\left[\\kappa\\left(\\mathbf{a}_{t}, f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)\\right) \\nabla_{\\mathbf{a}^{\\prime}} Q_{\\mathrm{soft}}^{\\theta}\\left.\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{t}}\\right.\\\\ &+\\alpha \\nabla_{\\mathbf{a}^{\\prime}} \\kappa\\left.\\left(\\mathbf{a}^{\\prime}, f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{t}} ] \\end{aligned}\n\\tag{16}\n$$\n- 严格来说，$\\Delta f^{\\phi}$是希尔伯特空间的最优梯度方向，并不是Actor目标函数$J_{\\pi}$的梯度\n\n- > To be precise, $\\Delta f^{\\phi}$ is the optimal direction in the reproducing kernel Hilbert space of $\\kappa$, and is thus not strictly speaking the gradient of $J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)$ \n\n根据链式法则，Stein变分梯度SVG为\n$$\n\\frac{\\partial J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)}{\\partial \\phi} \\propto \\mathbb{E}_{\\xi}\\left[\\Delta f^{\\phi}\\left(\\xi ; \\mathbf{s}_{t}\\right) \\frac{\\partial f^{\\phi}\\left(\\xi ; \\mathbf{s}_{t}\\right)}{\\partial \\phi}\\right]\n\\tag{17}\n$$\n## 伪代码\n\n![](./rl-with-deep-energy-based-policies/pseudo.png)\n\n解析：\n\n算法中更新Actor网络时，其实是使用了如下梯度公式：\n$$\n\\hat{\\nabla}_{\\phi} J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)=\\frac{1}{K M} \\sum_{j=1}^{K} \\sum_{i=1}^{M}\\left(\\kappa\\left(\\mathbf{a}_{t}^{(i)}, \\tilde{\\mathbf{a}}_{t}^{(j)}\\right) \\nabla_{\\mathbf{a}^{\\prime}} Q_{\\mathrm{soft}}\\left.\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{i}^{(i)}}+\\nabla_{\\mathbf{a}^{\\prime}} \\kappa\\left.\\left(\\mathbf{a}^{\\prime}, \\tilde{\\mathbf{a}}_{t}^{(j)}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{i}^{(i)}}\\right) \\nabla_{\\phi} f^{\\phi}\\left(\\tilde{\\xi}^{(j)} ; \\mathbf{s}_{t}\\right)\n\\tag{18}\n$$\n更新方向为mini-batch经验的梯度平均值，而不是累加和\n\n- 伪代码中定义了Actor的target网络，参数为$\\overline{\\theta}$。但是伪代码中并没有显示出其在何处使用，我**猜测**该网络代表采样分布$q_{\\mathbf{a}^{\\prime}}$\n  - $q_{\\mathbf{a}^{\\prime}}$在训练初期使用均匀分布\n  - $q_{\\mathbf{a}^{\\prime}}$在一段时间之后使用Actor真实分布，我猜测这里使用的就是Actor目标网络\n- 噪音$\\xi$从多维标准正态分布中采样，维度最好与动作空间维度一致\n- $\\left\\{\\mathbf{a}^{(i, j)}\\right\\}_{j=0}^{M} \\sim q_{\\mathbf{a}^{\\prime}}$其中的M用于设置采样多少个样本，以使用公式（11）计算V值，使用的网络为Q目标网络\n- 在更新Q网络时，使用了从经验池采样到的真实执行过的动作\n- $\\left\\{\\xi^{(i, j)}\\right\\}_{j=0}^{M} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{I})$中其实少写了一个参数$K$，但实际上$K=M$，在这一步中需要采样两组噪音，当然也可以采样一组，使用两次。\n- 在更新Actor网络时，没有使用经验池中采样到的动作，而模拟采样了两组动作，即根据两组噪音生成的动作，用它们来计算梯度并更新。\n- Q网络的输入为状态与动作的连接，$(s||a)$，输出为Q值\n- Actor网络的输入为状态与噪音的连接，$(s||\\xi)$，输出为动作$a$\n- 伪代码中的式(10)、(11)、(13)、(14)分别代表本文中的式(11)、(12)、(16)、(17)\n\n# 实验\n\n## 实验设置\n\n- 比较算法：DDPG vs SQL\n- Actor和Q网络使用Adam优化器\n- Actor学习率为0.0001，Q网络学习率为0.001\n- 经验池大小为100W\n- 经验池填充1W条经验后开始训练\n- batch_size=64\n- Actor和Q网络都是2层隐藏层，每层200个隐藏节点，激活函数为ReLU\n- DDPG和SQL都使用了Ornstein-Uhlenbeck随机过程产生噪音来增加探索，它是一种序贯相关的随机过程，$\\theta=0.15 \\ , \\ \\sigma=0.3$\n  - OU随机过程可以在序贯模型中添加与时间相关的随机噪音，而且噪音也满足强马尔可夫性\n  \n  - 形式为$d x_{t}=\\theta\\left(\\mu-x_{t}\\right) d t+\\sigma d W_{t}$，是一个具有均值恢复属性的随机过程\n  \n  - $\\theta$表示变量$x$以多大幅度、多块恢复到平均值，$\\mu$代表平均值，$\\sigma$代表波动程度，$d W_{t}$代表维纳过程，一般通过高斯分布实现\n  \n  - OU随机过程产生的噪音只与上一次产生的噪音相关，它可以用于增加探索，也能够柔顺控制。比如在相邻的两个决策动作，一个为10，一个为-10，反复如此，智能体会产生震荡。在此使用OU过程可以使智能体在一个方向保持一定时间，不会瞬间过大地改变智能体的状态，相当于增加了时滞性。\n  \n  - 代码\n  \n    ```\n    x = 10\n    dx = theta * (mu - x) + sigma * numpy.random.randn(len(x))\n    x = x + dx\n    ```\n  \n  - 参考：\n  \n    - [https://www.quora.com/Why-do-we-use-the-Ornstein-Uhlenbeck-Process-in-the-exploration-of-DDPG](https://www.quora.com/Why-do-we-use-the-Ornstein-Uhlenbeck-Process-in-the-exploration-of-DDPG)\n    - [https://github.com/floodsung/DDPG-tensorflow/blob/master/ou_noise.py](https://github.com/floodsung/DDPG-tensorflow/blob/master/ou_noise.py)\n    - [https://zhuanlan.zhihu.com/p/51333694](https://zhuanlan.zhihu.com/p/51333694)\n- 核函数使用了径向基函数RBF，$\\kappa\\left(\\mathbf{a}, \\mathbf{a}^{\\prime}\\right)=\\exp \\left(-\\frac{1}{h}\\left\\|\\mathbf{a}-\\mathbf{a}^{\\prime}\\right\\|_{2}^{2}\\right)$，其中，$h=\\frac{d}{2 \\log (M+1)}$，$d$为各变量对之间距离的中位数\n- 目标网络的更新采样硬覆盖的模式\n- 超参数$\\alpha$根据任务设置为10，0.1等等\n- 训练的epoch、步长、系数$\\alpha$，采样动作的数量$K 和 M$根据任务（多目标，单目标，微调）的不同而不同，具体请看原论文附录D部分\n\n## 实验结果\n\n未完待续\n\n# 个人感想\n\n虽然文中用大量公式、篇幅结合最大熵进行介绍、推理，但是在伪代码以及目标函数中似乎并没有看到关于熵的影子，包含熵项的Q值也是通过Q网络的输出将熵的值包含在内，并没有显式地计算它。\n\n虽然算法的名字为soft Q-Learning，但其实它跟传统的Q-Learning算法思想并不相同，如果说有一点相同，那也是都是想使Q值收敛以推导出最优策略，但是这个优化过程也跟SARSA算法比较像，并没有使用传统Q-Learning中贪婪的选择最有价值下一个动作以自举的方法。","slug":"rl-with-deep-energy-based-policies","published":1,"updated":"2019-06-27T01:29:59.095Z","_id":"cjxd6ma81003kekvekgs6f8q9","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本文提出了一个算法，用于学习连续空间下基于能量的策略：SQL，不是数据库的SQL，而是soft Q-Learning。该算法应用了最大熵理论，并且使用能量模型（EBM，Energy-Based Model）作为决策模型。</p>\n<p>推荐阅读该论文：</p>\n<ul>\n<li>公式复杂，但详尽吃透可以学习到SVGD、EBM等概念与算法</li>\n<li>文章充实，可以继续阅读后续算法SAC</li>\n<li>拓展在强化学习与熵进行结合方面的知识</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/abs/1702.08165\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/abs/1702.08165</a></p>\n<p>源代码：<a href=\"https://github.com/rail-berkeley/softlearning\" rel=\"external nofollow\" target=\"_blank\">https://github.com/rail-berkeley/softlearning</a></p>\n<p>该论文与2017年发于第34次ICML会议上，本文对v2版本进行分析。该论文作者为Tuomas Haarnoja，是伯克利大学BAIR实验室的博士生，SAC算法也是他的杰作。</p>\n<p>传统的RL方法主要是用分布拟合单峰分布，即</p>\n<p><img src=\"./rl-with-deep-energy-based-policies/unimodal-policy.png\" alt=\"\"></p>\n<p>也有许多算法想要根据Q函数的值拟合出多峰分布，即</p>\n<p><img src=\"./rl-with-deep-energy-based-policies/multimodal-policy.png\" alt=\"\"></p>\n<p>本文中就是针对拟合多峰分布提出了算法SQL。</p>\n<p>为什么要拟合多峰分布呢？当我们考虑最优控制和概率推理之间的联系时，随机策略才是最优解。</p>\n<blockquote>\n<p>As discussed in prior work, a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference.</p>\n</blockquote>\n<p>随机策略有一些优点：</p>\n<ul>\n<li>如果可以全面地学习给定任务中的目标策略，那么结果策略可以作为很好的初始化策略，微调后以学习更高级的策略</li>\n<li>这种随机的探索机制，可以更好地寻求多峰任务中的最佳决策模型</li>\n<li>更好的鲁棒性，环境有干扰或者噪音时，有多种完成目标的行动可以选择，可以从干扰中“脱身”</li>\n</ul>\n<h2 id=\"算法效果\"><a href=\"#算法效果\" class=\"headerlink\" title=\"算法效果\"></a>算法效果</h2><blockquote>\n<p>The applications of training such stochastic policies include improved exploration in the case of multimodal objectives and compositionality via pretraining general-purpose stochastic policies that can then be efficiently finetuned into task-specific behaviors. </p>\n</blockquote>\n<p>在多峰目标任务中训练随机策略可以提升探索，也可以预训练出通用目的的随机策略以微调后运用至指定任务中进行训练（迁移学习、元学习）。</p>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><h2 id=\"标准强化学习的最优策略\"><a href=\"#标准强化学习的最优策略\" class=\"headerlink\" title=\"标准强化学习的最优策略\"></a>标准强化学习的最优策略</h2><script type=\"math/tex; mode=display\">\n\\pi_{\\mathrm{std}}^{*}=\\arg \\max _{\\pi} \\sum_{t} \\mathbb{E}_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\sim \\rho_{\\pi}}\\left[r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right] \\tag{1}</script><ul>\n<li><p><code>std</code>下标代表标准的意思：standard，星号$\\ast$代表最优</p>\n</li>\n<li><p>$\\rho_{\\pi}$代表策略$\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right)$下的迹分布，$\\rho_{\\pi}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)$代表状态-行动对的边缘分布。</p>\n</li>\n</ul>\n<blockquote>\n<p>We will also use $\\rho_{\\pi}\\left(\\mathbf{s}_{t}\\right)$ and $\\rho_{\\pi}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)$ to denote the state and state-action marginals of the trajectory distribution induced by a policy $\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right)$. </p>\n</blockquote>\n<h2 id=\"最大熵强化学习的最优策略\"><a href=\"#最大熵强化学习的最优策略\" class=\"headerlink\" title=\"最大熵强化学习的最优策略\"></a>最大熵强化学习的最优策略</h2><p>最大熵强化学习在标准RL的目标函数上加入了一个关于状态下可选动作分布熵的项，这种目标希望智能体不仅能以获得最大奖励的方式完成目标，而且能够决策地尽可能随机。因为通过这种目标函数学到的策略<strong>更具鲁棒性</strong>，可以更好适用于环境的突然变化，或者从前没有遇到过得场景。</p>\n<script type=\"math/tex; mode=display\">\n\\pi_{\\mathrm{MaxEnt}}^{*}=\\arg \\max _{\\pi} \\sum_{t} \\mathbb{E}_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\sim \\rho_{\\pi}}\\left[r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)+\\color{red}{\\alpha \\mathcal{H}\\left(\\pi\\left(\\cdot | \\mathbf{s}_{t}\\right)\\right)}\\right]\n\\tag{2}</script><ul>\n<li><code>MaxEnt</code>下标代表最大熵的意思：Maximum entropy</li>\n<li>式中的系数$\\alpha$可以用来调节奖励项与熵值项的重要性比率。<strong>一般将$\\alpha$表示为奖励范围（reward scale）的倒数，但在实际中通常将其作为超参数手动调节。</strong>SAC算法中有介绍在训练过程中自动调节该系数的方法。</li>\n<li>本文中的SQL算法也是为了优化该目标函数</li>\n</ul>\n<h2 id=\"最大熵目标的优点\"><a href=\"#最大熵目标的优点\" class=\"headerlink\" title=\"最大熵目标的优点\"></a>最大熵目标的优点</h2><ul>\n<li>在多峰（即一个状态下有多个最优动作选择）问题中提升探索能力</li>\n<li>可以用于迁移学习，因为其“预训练”模型更好地适应之后的任务</li>\n</ul>\n<h2 id=\"soft-值函数\"><a href=\"#soft-值函数\" class=\"headerlink\" title=\"soft 值函数\"></a>soft 值函数</h2><p>文中，<strong><em>定义</em></strong>了最大熵RL下的Q函数与V函数，注意，是定义，不是推导出来的。</p>\n<p>soft Q函数定义如下：</p>\n<script type=\"math/tex; mode=display\">\n{Q_{\\text { soft }}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+}  {\\mathbb{E}_{\\left(\\mathbf{s}_{t+1}, \\ldots\\right) \\sim \\rho_{\\pi}}\\left[\\sum_{l=1}^{\\infty} \\gamma^{l}\\left(r_{t+l}+\\alpha \\mathcal{H}\\left(\\pi_{\\text { MaxEnt }}^{*}\\left(\\cdot | \\mathbf{s}_{t+l}\\right)\\right)\\right)\\right]}\n\\tag{3}</script><p>soft V函数定义如下：</p>\n<script type=\"math/tex; mode=display\">\nV_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)=\\alpha \\log \\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}\n\\tag{4}</script><p>乍一看这个值函数$V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)$的形式定义的很奇怪，的确很奇怪，严格来说，它的真实意义并不是为了构造状态值函数，而是构造一个配分函数使得后面推导最优策略时可以化简过程。当然，算法中也不需要用它的值去衡量状态的价值，只是作为计算的中间过程。</p>\n<p>作者说，值函数满足soft 贝尔曼方程，即</p>\n<script type=\"math/tex; mode=display\">\nQ_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t+1}\\right)\\right]\n\\tag{5}</script><h2 id=\"能量模型与策略\"><a href=\"#能量模型与策略\" class=\"headerlink\" title=\"能量模型与策略\"></a>能量模型与策略</h2><p>文中提出能量模型（Energy-Based Models）的初衷是之前很多人在研究中使用了多项式分布（discrete multinomial distributions）、高斯分布（Gaussian distributions）来表示策略，这样的分布通常用来表示动作价值分布是单峰（unimodal）的情况，而且最终收敛结果往往是接近确定性（near-deterministic）的。即使拓展出多峰的形式，也各自有或多或少的不足。基于此，作者想使用更广泛、通用的分布用来表示复杂、多峰的动作选择。</p>\n<p>所以，作者选择使用基于能量的通用策略：</p>\n<script type=\"math/tex; mode=display\">\n\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right) \\propto \\exp \\left(-\\mathcal{E}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)\n\\tag{6}</script><ul>\n<li><p>$\\mathcal{E}$是字母E的花体形式，代表能量函数，其可以被深度神经网络表示，如果使用通用值函数近似来表示能量函数，那么可以表示任意策略$\\pi\\left(\\boldsymbol{a}_{t} | \\mathbf{s}_{t}\\right)$</p>\n</li>\n<li><blockquote>\n<p>where $\\mathcal{E}$ is an energy function that could be represented, for example, by a deep neural network. If we use a universal function approximator for $\\mathcal{E}$, we can represent any distribution $\\pi\\left(\\boldsymbol{a}_{t} | \\mathbf{s}_{t}\\right)$. </p>\n</blockquote>\n</li>\n<li><p>文中将该能量函数设置为</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{E}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=-\\frac{1}{\\alpha} Q_{\\operatorname{soft}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\n\\tag{7}</script><p>其实也很容易就能理解，将负号抵消掉之后，Q值大的动作能量高嘛，指数分布又能更好的放大较大的值，使Q值大的动作更为突出，这样完全可以作为选择动作的策略</p>\n</li>\n</ul>\n<p>但是有一个问题是，不能使能量无限大呀，假如超过了计算能力那就不好了，当然这种情况几乎不会发生。于是，可以将能量给归一化，即</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} \n\\pi_{\\text { MaxEnt }}^{*}\\left(a_{t} | s_{t}\\right) \n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right) \\mathrm{d} a^{\\prime}} \\\\ \n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\exp \\left(\\frac{1}{\\alpha} V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)} \\\\\n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\color{blue}{\\exp \\log} \\exp \\left(\\frac{1}{\\alpha} V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)} \\\\\n&=\\color{red}{\\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)-V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)\\right)}\n\\end{aligned}\n\\tag{8}</script><p></p><p align=\"center\" style=\"color:blue\"><a href=\"https://bluefisher.github.io/2018/11/13/Reinforcement-Learning-with-Deep-Energy-Based-Policies/\" rel=\"external nofollow\" target=\"_blank\">BlueFisher's Blog</a></p><br>论文中只给出了红色字体的部分，其实这才是作者想要表达的意思，文中就是基于此定义了状态值函数$V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)$的形式。<p></p>\n<p>在这个公式中就可以看出，策略是对动作值函数Q进行了一个softmax操作，这也是文中soft的含义。</p>\n<h2 id=\"使用SQL优化目标函数\"><a href=\"#使用SQL优化目标函数\" class=\"headerlink\" title=\"使用SQL优化目标函数\"></a>使用SQL优化目标函数</h2><p>像使用Q-Learning对网格世界问题进行优化求解一样，我们也可以使用迭代的方式进行优化，交互计算两个值函数，使其各自收敛，就可以导出最优策略。</p>\n<p>于是，作者定义了soft Q-Iteration。</p>\n<h3 id=\"Soft-Q-Iteration\"><a href=\"#Soft-Q-Iteration\" class=\"headerlink\" title=\"Soft Q-Iteration\"></a>Soft Q-Iteration</h3><p>先要假设值函数$Q_{\\mathrm{soft}}(\\cdot, \\cdot)$、$V_{\\text { soft }}(\\cdot)$有界，</p>\n<script type=\"math/tex; mode=display\">\n\\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}\\left(\\cdot, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}<\\infty \\ ，\\ Q_{\\mathrm{soft}}^{*}<\\infty\n\\tag{9}</script><p>文中定义的交互迭代至收敛的方式其实跟SARSA算法比较像：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n{Q_{\\text { soft }}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\leftarrow r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\text { soft }}\\left(\\mathbf{s}_{t+1}\\right)\\right], \\forall \\mathbf{s}_{t}, \\mathbf{a}_{t}} \\\\ \n{V_{\\text { soft }}\\left(\\mathbf{s}_{t}\\right) \\leftarrow \\alpha \\log \\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}, \\forall \\mathbf{s}_{t}}\n\\end{array}\n\\tag{10}</script><p>这种优化方式在理论上是可行的，但是在实际应用中存在两个问题：</p>\n<ol>\n<li>连续空间无法求期望，或者计算不准确。<strong>解决方案是重要性采样，使用采样多次后计算来代替积分，在初期进行随机均匀采样，后期根据policy来采样。</strong></li>\n<li>迭代过程需要不断选择动作，问题是式（8）的分布形式无法进行采样。<strong>解决方案是使用SVGD算法拟合后验分布，并输出采样的动作。</strong></li>\n</ol>\n<h3 id=\"Soft-Q-Learning\"><a href=\"#Soft-Q-Learning\" class=\"headerlink\" title=\"Soft Q-Learning\"></a>Soft Q-Learning</h3><p>文中在这一部分引用了重要性采样，解决了上面提到的第一个问题，即，使用分布$q_{\\mathrm{a}^{\\prime}}$来代替真实策略分布</p>\n<script type=\"math/tex; mode=display\">\n\\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)-V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)\\right\n)</script><p>进行采样。</p>\n<script type=\"math/tex; mode=display\">\nV_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}\\right)=\\alpha \\log \\mathbb{E}_{\\color{red}{q_{\\mathrm{a}^{\\prime}}}}\\left[\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right)}{q_{\\mathrm{a}^{\\prime}}\\left(\\mathbf{a}^{\\prime}\\right)}\\right]\n\\tag{11}</script><p>采样分布$q_{\\mathrm{a}^{\\prime}}$可以使用任意的分布，但是由于重要性采样的性质，采样分布与原分布越接近，效果越好。式子中的$\\theta$为Q神经网络的参数。</p>\n<p>因为在训练初期，我们估计的真实分布是偏差很大的，几乎可以说是错误的，因此在训练初期将采样分布设置为均匀分布比较合理，在训练一段时间之后，可以将采样分布设置为接近原分布，甚至是原分布（如果原分布可以采样，如，使用神经网络等“黑匣子”进行表示）</p>\n<p>由此，可以定义Q神经网络的损失函数为：</p>\n<script type=\"math/tex; mode=display\">\nJ_{Q}(\\theta)=\\mathbb{E}_{\\mathbf{s}_{t} \\sim q_{\\mathbf{s}_{t}}, \\mathbf{a}_{t} \\sim q_{\\mathbf{a}_{t}}}\\left[\\frac{1}{2}\\left(\\hat{Q}_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)-Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)^{2}\\right]\n\\tag{12}</script><p>上式中期望的下标为环境和真实策略分布，$\\overline{\\theta}$代表target网络的参数，目标是最小化这个损失函数，其中，</p>\n<script type=\"math/tex; mode=display\">\n\\hat{Q}_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t+1}\\right)\\right]\n\\tag{13}</script><h3 id=\"近似采样与SVGD\"><a href=\"#近似采样与SVGD\" class=\"headerlink\" title=\"近似采样与SVGD\"></a>近似采样与SVGD</h3><p>SVGD：Stein Vairational Gradient Descent，SVGD是一种确定性的、基于梯度的近似推理采样算法。</p>\n<p>文中在这一部分引用了SVGD的优化算法，并且使用SVGD近似策略的后验分布以进行采样，解决了上文提到的第二个问题。在百度上完全搜不到关于SVGD算法的信息，但是了解了这个算法之后，感觉它的能力还是很强的，最终，搜集了Google、Bing的检索结果，发现了原作者在SVGD方法上的一些资源分享，<a href=\"https://www.cs.utexas.edu/~lqiang/stein.html\" rel=\"external nofollow\" target=\"_blank\">Stein’s Method for Practical Machine Learning</a></p>\n<p>对于基于能量的模型、分布，有两类采样方式：</p>\n<ol>\n<li>MCMC采样，即马尔科夫链蒙特卡洛采样</li>\n<li>学习一个采样网络去近似采样出符合目标分布的样本</li>\n</ol>\n<p>在需要不断更新策略的在线学习任务中，使用MCMC采样是不可行的，于是作者使用了基于SVGD和Amortized SVGD的采样网络。</p>\n<p>SVGD论文：<a href=\"https://arxiv.org/abs/1608.04471\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/abs/1608.04471</a></p>\n<p>Amortized SVGD论文：<a href=\"https://arxiv.org/abs/1707.06626\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/abs/1707.06626</a></p>\n<p><img src=\"./rl-with-deep-energy-based-policies/1dgmm.gif\" alt=\"\"><br><img src=\"./rl-with-deep-energy-based-policies/vp.gif\" alt=\"\"></p>\n<p>Amortized SVGD有一些有趣的性质：</p>\n<ul>\n<li>可以训练随机采样网络非常快地采样</li>\n<li>可以准确收敛至EBM能量模型的后验估计分布</li>\n<li>文中结合了Amortized SVGD后，算法形式很像A-C模式</li>\n</ul>\n<p>SVGD算法的更新形式是这样的，</p>\n<script type=\"math/tex; mode=display\">\nx_{i} \\leftarrow x_{i}+\\frac{\\epsilon}{n} \\sum_{j=1}^{n}\\left[k\\left(x_{j}, x_{i}\\right) \\nabla_{x_{j}} \\log p\\left(x_{j}\\right)+\\nabla_{x_{j}} k\\left(x_{j}, x_{i}\\right)\\right], \\qquad \\forall i=1, \\ldots, n\n\\tag{14}</script><ul>\n<li>$\\epsilon$代表学习率</li>\n<li>$k\\left(x_{j}, x_{i}\\right)$代表正定核，如径向基（RBF，Radial Basis Function）函数$k\\left(x, x^{\\prime}\\right)=\\exp \\left(-\\frac{1}{h}\\left|x-x^{\\prime}\\right|_{2}^{2}\\right)$，它可以被认为是变量之间的相似性度量</li>\n<li>包含对数项$\\log p\\left(x_{j}\\right)$的梯度驱使采样器朝着$p(x)$分布中高概率区域进行采样</li>\n<li>第二项核函数梯度驱使样本点之间产生间隙，相当于用一个排斥力使样本点尽可能分散开</li>\n<li>对数项梯度不依赖分布$p(x)$的归一化常数，使SVGD易于应用于图模型、贝叶斯推理和深层生成模型中出现的难以处理的分布。</li>\n</ul>\n<p>先定义我们采样网络（其实就是Actor）的目标函数：</p>\n<script type=\"math/tex; mode=display\">\nJ_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)=D_{\\mathrm{KL}}\\left(\\pi^{\\phi}\\left(\\cdot | \\mathbf{s}_{t}\\right) \\| \\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\cdot\\right)-V_{\\mathrm{soft}}^{\\theta}\\right)\\right)\\right)\n\\tag{15}</script><ul>\n<li>$\\phi$表示采样网络中的参数</li>\n<li>将产生动作的函数简写成$\\mathbf{a}_{t}^{(i)}=f^{\\phi}\\left(\\xi^{(i)} ; \\mathbf{s}_{t}\\right)$，也就是说神经网络的输入分为两部分，一部分是状态$s$，一部分是噪声扰乱“perturb”$\\xi$，一般从标准正态分布中采样，而且最好使噪声的维度与动作的维度一致</li>\n</ul>\n<p>计算梯度方向：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)=& \\mathbb{E}_{\\mathbf{a}_{t} \\sim \\pi^{\\phi}}\\left[\\kappa\\left(\\mathbf{a}_{t}, f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)\\right) \\nabla_{\\mathbf{a}^{\\prime}} Q_{\\mathrm{soft}}^{\\theta}\\left.\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{t}}\\right.\\\\ &+\\alpha \\nabla_{\\mathbf{a}^{\\prime}} \\kappa\\left.\\left(\\mathbf{a}^{\\prime}, f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{t}} ] \\end{aligned}\n\\tag{16}</script><ul>\n<li><p>严格来说，$\\Delta f^{\\phi}$是希尔伯特空间的最优梯度方向，并不是Actor目标函数$J_{\\pi}$的梯度</p>\n</li>\n<li><blockquote>\n<p>To be precise, $\\Delta f^{\\phi}$ is the optimal direction in the reproducing kernel Hilbert space of $\\kappa$, and is thus not strictly speaking the gradient of $J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)$ </p>\n</blockquote>\n</li>\n</ul>\n<p>根据链式法则，Stein变分梯度SVG为</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)}{\\partial \\phi} \\propto \\mathbb{E}_{\\xi}\\left[\\Delta f^{\\phi}\\left(\\xi ; \\mathbf{s}_{t}\\right) \\frac{\\partial f^{\\phi}\\left(\\xi ; \\mathbf{s}_{t}\\right)}{\\partial \\phi}\\right]\n\\tag{17}</script><h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-with-deep-energy-based-policies/pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<p>算法中更新Actor网络时，其实是使用了如下梯度公式：</p>\n<script type=\"math/tex; mode=display\">\n\\hat{\\nabla}_{\\phi} J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)=\\frac{1}{K M} \\sum_{j=1}^{K} \\sum_{i=1}^{M}\\left(\\kappa\\left(\\mathbf{a}_{t}^{(i)}, \\tilde{\\mathbf{a}}_{t}^{(j)}\\right) \\nabla_{\\mathbf{a}^{\\prime}} Q_{\\mathrm{soft}}\\left.\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{i}^{(i)}}+\\nabla_{\\mathbf{a}^{\\prime}} \\kappa\\left.\\left(\\mathbf{a}^{\\prime}, \\tilde{\\mathbf{a}}_{t}^{(j)}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{i}^{(i)}}\\right) \\nabla_{\\phi} f^{\\phi}\\left(\\tilde{\\xi}^{(j)} ; \\mathbf{s}_{t}\\right)\n\\tag{18}</script><p>更新方向为mini-batch经验的梯度平均值，而不是累加和</p>\n<ul>\n<li>伪代码中定义了Actor的target网络，参数为$\\overline{\\theta}$。但是伪代码中并没有显示出其在何处使用，我<strong>猜测</strong>该网络代表采样分布$q_{\\mathbf{a}^{\\prime}}$<ul>\n<li>$q_{\\mathbf{a}^{\\prime}}$在训练初期使用均匀分布</li>\n<li>$q_{\\mathbf{a}^{\\prime}}$在一段时间之后使用Actor真实分布，我猜测这里使用的就是Actor目标网络</li>\n</ul>\n</li>\n<li>噪音$\\xi$从多维标准正态分布中采样，维度最好与动作空间维度一致</li>\n<li>$\\left\\{\\mathbf{a}^{(i, j)}\\right\\}_{j=0}^{M} \\sim q_{\\mathbf{a}^{\\prime}}$其中的M用于设置采样多少个样本，以使用公式（11）计算V值，使用的网络为Q目标网络</li>\n<li>在更新Q网络时，使用了从经验池采样到的真实执行过的动作</li>\n<li>$\\left\\{\\xi^{(i, j)}\\right\\}_{j=0}^{M} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{I})$中其实少写了一个参数$K$，但实际上$K=M$，在这一步中需要采样两组噪音，当然也可以采样一组，使用两次。</li>\n<li>在更新Actor网络时，没有使用经验池中采样到的动作，而模拟采样了两组动作，即根据两组噪音生成的动作，用它们来计算梯度并更新。</li>\n<li>Q网络的输入为状态与动作的连接，$(s||a)$，输出为Q值</li>\n<li>Actor网络的输入为状态与噪音的连接，$(s||\\xi)$，输出为动作$a$</li>\n<li>伪代码中的式(10)、(11)、(13)、(14)分别代表本文中的式(11)、(12)、(16)、(17)</li>\n</ul>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><h2 id=\"实验设置\"><a href=\"#实验设置\" class=\"headerlink\" title=\"实验设置\"></a>实验设置</h2><ul>\n<li>比较算法：DDPG vs SQL</li>\n<li>Actor和Q网络使用Adam优化器</li>\n<li>Actor学习率为0.0001，Q网络学习率为0.001</li>\n<li>经验池大小为100W</li>\n<li>经验池填充1W条经验后开始训练</li>\n<li>batch_size=64</li>\n<li>Actor和Q网络都是2层隐藏层，每层200个隐藏节点，激活函数为ReLU</li>\n<li><p>DDPG和SQL都使用了Ornstein-Uhlenbeck随机过程产生噪音来增加探索，它是一种序贯相关的随机过程，$\\theta=0.15 \\ , \\ \\sigma=0.3$</p>\n<ul>\n<li><p>OU随机过程可以在序贯模型中添加与时间相关的随机噪音，而且噪音也满足强马尔可夫性</p>\n</li>\n<li><p>形式为$d x_{t}=\\theta\\left(\\mu-x_{t}\\right) d t+\\sigma d W_{t}$，是一个具有均值恢复属性的随机过程</p>\n</li>\n<li><p>$\\theta$表示变量$x$以多大幅度、多块恢复到平均值，$\\mu$代表平均值，$\\sigma$代表波动程度，$d W_{t}$代表维纳过程，一般通过高斯分布实现</p>\n</li>\n<li><p>OU随机过程产生的噪音只与上一次产生的噪音相关，它可以用于增加探索，也能够柔顺控制。比如在相邻的两个决策动作，一个为10，一个为-10，反复如此，智能体会产生震荡。在此使用OU过程可以使智能体在一个方向保持一定时间，不会瞬间过大地改变智能体的状态，相当于增加了时滞性。</p>\n</li>\n<li><p>代码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = 10</span><br><span class=\"line\">dx = theta * (mu - x) + sigma * numpy.random.randn(len(x))</span><br><span class=\"line\">x = x + dx</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>参考：</p>\n<ul>\n<li><a href=\"https://www.quora.com/Why-do-we-use-the-Ornstein-Uhlenbeck-Process-in-the-exploration-of-DDPG\" rel=\"external nofollow\" target=\"_blank\">https://www.quora.com/Why-do-we-use-the-Ornstein-Uhlenbeck-Process-in-the-exploration-of-DDPG</a></li>\n<li><a href=\"https://github.com/floodsung/DDPG-tensorflow/blob/master/ou_noise.py\" rel=\"external nofollow\" target=\"_blank\">https://github.com/floodsung/DDPG-tensorflow/blob/master/ou_noise.py</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/51333694\" rel=\"external nofollow\" target=\"_blank\">https://zhuanlan.zhihu.com/p/51333694</a></li>\n<li>核函数使用了径向基函数RBF，$\\kappa\\left(\\mathbf{a}, \\mathbf{a}^{\\prime}\\right)=\\exp \\left(-\\frac{1}{h}\\left|\\mathbf{a}-\\mathbf{a}^{\\prime}\\right|_{2}^{2}\\right)$，其中，$h=\\frac{d}{2 \\log (M+1)}$，$d$为各变量对之间距离的中位数</li>\n<li>目标网络的更新采样硬覆盖的模式</li>\n<li>超参数$\\alpha$根据任务设置为10，0.1等等</li>\n<li>训练的epoch、步长、系数$\\alpha$，采样动作的数量$K 和 M$根据任务（多目标，单目标，微调）的不同而不同，具体请看原论文附录D部分</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><p>未完待续</p>\n<h1 id=\"个人感想\"><a href=\"#个人感想\" class=\"headerlink\" title=\"个人感想\"></a>个人感想</h1><p>虽然文中用大量公式、篇幅结合最大熵进行介绍、推理，但是在伪代码以及目标函数中似乎并没有看到关于熵的影子，包含熵项的Q值也是通过Q网络的输出将熵的值包含在内，并没有显式地计算它。</p>\n<p>虽然算法的名字为soft Q-Learning，但其实它跟传统的Q-Learning算法思想并不相同，如果说有一点相同，那也是都是想使Q值收敛以推导出最优策略，但是这个优化过程也跟SARSA算法比较像，并没有使用传统Q-Learning中贪婪的选择最有价值下一个动作以自举的方法。</p>\n","site":{"data":{}},"excerpt":"<p>本文提出了一个算法，用于学习连续空间下基于能量的策略：SQL，不是数据库的SQL，而是soft Q-Learning。该算法应用了最大熵理论，并且使用能量模型（EBM，Energy-Based Model）作为决策模型。</p>\n<p>推荐阅读该论文：</p>\n<ul>\n<li>公式复杂，但详尽吃透可以学习到SVGD、EBM等概念与算法</li>\n<li>文章充实，可以继续阅读后续算法SAC</li>\n<li>拓展在强化学习与熵进行结合方面的知识</li>\n</ul>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>论文地址：<a href=\"https://arxiv.org/abs/1702.08165\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/abs/1702.08165</a></p>\n<p>源代码：<a href=\"https://github.com/rail-berkeley/softlearning\" rel=\"external nofollow\" target=\"_blank\">https://github.com/rail-berkeley/softlearning</a></p>\n<p>该论文与2017年发于第34次ICML会议上，本文对v2版本进行分析。该论文作者为Tuomas Haarnoja，是伯克利大学BAIR实验室的博士生，SAC算法也是他的杰作。</p>\n<p>传统的RL方法主要是用分布拟合单峰分布，即</p>\n<p><img src=\"./rl-with-deep-energy-based-policies/unimodal-policy.png\" alt=\"\"></p>\n<p>也有许多算法想要根据Q函数的值拟合出多峰分布，即</p>\n<p><img src=\"./rl-with-deep-energy-based-policies/multimodal-policy.png\" alt=\"\"></p>\n<p>本文中就是针对拟合多峰分布提出了算法SQL。</p>\n<p>为什么要拟合多峰分布呢？当我们考虑最优控制和概率推理之间的联系时，随机策略才是最优解。</p>\n<blockquote>\n<p>As discussed in prior work, a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference.</p>\n</blockquote>\n<p>随机策略有一些优点：</p>\n<ul>\n<li>如果可以全面地学习给定任务中的目标策略，那么结果策略可以作为很好的初始化策略，微调后以学习更高级的策略</li>\n<li>这种随机的探索机制，可以更好地寻求多峰任务中的最佳决策模型</li>\n<li>更好的鲁棒性，环境有干扰或者噪音时，有多种完成目标的行动可以选择，可以从干扰中“脱身”</li>\n</ul>\n<h2 id=\"算法效果\"><a href=\"#算法效果\" class=\"headerlink\" title=\"算法效果\"></a>算法效果</h2><blockquote>\n<p>The applications of training such stochastic policies include improved exploration in the case of multimodal objectives and compositionality via pretraining general-purpose stochastic policies that can then be efficiently finetuned into task-specific behaviors. </p>\n</blockquote>\n<p>在多峰目标任务中训练随机策略可以提升探索，也可以预训练出通用目的的随机策略以微调后运用至指定任务中进行训练（迁移学习、元学习）。</p>\n<h1 id=\"文中精要\"><a href=\"#文中精要\" class=\"headerlink\" title=\"文中精要\"></a>文中精要</h1><h2 id=\"标准强化学习的最优策略\"><a href=\"#标准强化学习的最优策略\" class=\"headerlink\" title=\"标准强化学习的最优策略\"></a>标准强化学习的最优策略</h2><script type=\"math/tex; mode=display\">\n\\pi_{\\mathrm{std}}^{*}=\\arg \\max _{\\pi} \\sum_{t} \\mathbb{E}_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\sim \\rho_{\\pi}}\\left[r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right] \\tag{1}</script><ul>\n<li><p><code>std</code>下标代表标准的意思：standard，星号$\\ast$代表最优</p>\n</li>\n<li><p>$\\rho_{\\pi}$代表策略$\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right)$下的迹分布，$\\rho_{\\pi}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)$代表状态-行动对的边缘分布。</p>\n</li>\n</ul>\n<blockquote>\n<p>We will also use $\\rho_{\\pi}\\left(\\mathbf{s}_{t}\\right)$ and $\\rho_{\\pi}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)$ to denote the state and state-action marginals of the trajectory distribution induced by a policy $\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right)$. </p>\n</blockquote>\n<h2 id=\"最大熵强化学习的最优策略\"><a href=\"#最大熵强化学习的最优策略\" class=\"headerlink\" title=\"最大熵强化学习的最优策略\"></a>最大熵强化学习的最优策略</h2><p>最大熵强化学习在标准RL的目标函数上加入了一个关于状态下可选动作分布熵的项，这种目标希望智能体不仅能以获得最大奖励的方式完成目标，而且能够决策地尽可能随机。因为通过这种目标函数学到的策略<strong>更具鲁棒性</strong>，可以更好适用于环境的突然变化，或者从前没有遇到过得场景。</p>\n<script type=\"math/tex; mode=display\">\n\\pi_{\\mathrm{MaxEnt}}^{*}=\\arg \\max _{\\pi} \\sum_{t} \\mathbb{E}_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\sim \\rho_{\\pi}}\\left[r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)+\\color{red}{\\alpha \\mathcal{H}\\left(\\pi\\left(\\cdot | \\mathbf{s}_{t}\\right)\\right)}\\right]\n\\tag{2}</script><ul>\n<li><code>MaxEnt</code>下标代表最大熵的意思：Maximum entropy</li>\n<li>式中的系数$\\alpha$可以用来调节奖励项与熵值项的重要性比率。<strong>一般将$\\alpha$表示为奖励范围（reward scale）的倒数，但在实际中通常将其作为超参数手动调节。</strong>SAC算法中有介绍在训练过程中自动调节该系数的方法。</li>\n<li>本文中的SQL算法也是为了优化该目标函数</li>\n</ul>\n<h2 id=\"最大熵目标的优点\"><a href=\"#最大熵目标的优点\" class=\"headerlink\" title=\"最大熵目标的优点\"></a>最大熵目标的优点</h2><ul>\n<li>在多峰（即一个状态下有多个最优动作选择）问题中提升探索能力</li>\n<li>可以用于迁移学习，因为其“预训练”模型更好地适应之后的任务</li>\n</ul>\n<h2 id=\"soft-值函数\"><a href=\"#soft-值函数\" class=\"headerlink\" title=\"soft 值函数\"></a>soft 值函数</h2><p>文中，<strong><em>定义</em></strong>了最大熵RL下的Q函数与V函数，注意，是定义，不是推导出来的。</p>\n<p>soft Q函数定义如下：</p>\n<script type=\"math/tex; mode=display\">\n{Q_{\\text { soft }}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+}  {\\mathbb{E}_{\\left(\\mathbf{s}_{t+1}, \\ldots\\right) \\sim \\rho_{\\pi}}\\left[\\sum_{l=1}^{\\infty} \\gamma^{l}\\left(r_{t+l}+\\alpha \\mathcal{H}\\left(\\pi_{\\text { MaxEnt }}^{*}\\left(\\cdot | \\mathbf{s}_{t+l}\\right)\\right)\\right)\\right]}\n\\tag{3}</script><p>soft V函数定义如下：</p>\n<script type=\"math/tex; mode=display\">\nV_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)=\\alpha \\log \\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}\n\\tag{4}</script><p>乍一看这个值函数$V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)$的形式定义的很奇怪，的确很奇怪，严格来说，它的真实意义并不是为了构造状态值函数，而是构造一个配分函数使得后面推导最优策略时可以化简过程。当然，算法中也不需要用它的值去衡量状态的价值，只是作为计算的中间过程。</p>\n<p>作者说，值函数满足soft 贝尔曼方程，即</p>\n<script type=\"math/tex; mode=display\">\nQ_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t+1}\\right)\\right]\n\\tag{5}</script><h2 id=\"能量模型与策略\"><a href=\"#能量模型与策略\" class=\"headerlink\" title=\"能量模型与策略\"></a>能量模型与策略</h2><p>文中提出能量模型（Energy-Based Models）的初衷是之前很多人在研究中使用了多项式分布（discrete multinomial distributions）、高斯分布（Gaussian distributions）来表示策略，这样的分布通常用来表示动作价值分布是单峰（unimodal）的情况，而且最终收敛结果往往是接近确定性（near-deterministic）的。即使拓展出多峰的形式，也各自有或多或少的不足。基于此，作者想使用更广泛、通用的分布用来表示复杂、多峰的动作选择。</p>\n<p>所以，作者选择使用基于能量的通用策略：</p>\n<script type=\"math/tex; mode=display\">\n\\pi\\left(\\mathbf{a}_{t} | \\mathbf{s}_{t}\\right) \\propto \\exp \\left(-\\mathcal{E}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)\n\\tag{6}</script><ul>\n<li><p>$\\mathcal{E}$是字母E的花体形式，代表能量函数，其可以被深度神经网络表示，如果使用通用值函数近似来表示能量函数，那么可以表示任意策略$\\pi\\left(\\boldsymbol{a}_{t} | \\mathbf{s}_{t}\\right)$</p>\n</li>\n<li><blockquote>\n<p>where $\\mathcal{E}$ is an energy function that could be represented, for example, by a deep neural network. If we use a universal function approximator for $\\mathcal{E}$, we can represent any distribution $\\pi\\left(\\boldsymbol{a}_{t} | \\mathbf{s}_{t}\\right)$. </p>\n</blockquote>\n</li>\n<li><p>文中将该能量函数设置为</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{E}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=-\\frac{1}{\\alpha} Q_{\\operatorname{soft}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\n\\tag{7}</script><p>其实也很容易就能理解，将负号抵消掉之后，Q值大的动作能量高嘛，指数分布又能更好的放大较大的值，使Q值大的动作更为突出，这样完全可以作为选择动作的策略</p>\n</li>\n</ul>\n<p>但是有一个问题是，不能使能量无限大呀，假如超过了计算能力那就不好了，当然这种情况几乎不会发生。于是，可以将能量给归一化，即</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} \n\\pi_{\\text { MaxEnt }}^{*}\\left(a_{t} | s_{t}\\right) \n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right) \\mathrm{d} a^{\\prime}} \\\\ \n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\exp \\left(\\frac{1}{\\alpha} V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)} \\\\\n&=\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)\\right)}{\\color{blue}{\\exp \\log} \\exp \\left(\\frac{1}{\\alpha} V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)} \\\\\n&=\\color{red}{\\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)-V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)\\right)}\n\\end{aligned}\n\\tag{8}</script><p></p><p align=\"center\" style=\"color:blue\"><a href=\"https://bluefisher.github.io/2018/11/13/Reinforcement-Learning-with-Deep-Energy-Based-Policies/\" rel=\"external nofollow\" target=\"_blank\">BlueFisher's Blog</a></p><br>论文中只给出了红色字体的部分，其实这才是作者想要表达的意思，文中就是基于此定义了状态值函数$V_{\\mathrm{soft}}^{*}\\left(\\mathbf{s}_{t}\\right)$的形式。<p></p>\n<p>在这个公式中就可以看出，策略是对动作值函数Q进行了一个softmax操作，这也是文中soft的含义。</p>\n<h2 id=\"使用SQL优化目标函数\"><a href=\"#使用SQL优化目标函数\" class=\"headerlink\" title=\"使用SQL优化目标函数\"></a>使用SQL优化目标函数</h2><p>像使用Q-Learning对网格世界问题进行优化求解一样，我们也可以使用迭代的方式进行优化，交互计算两个值函数，使其各自收敛，就可以导出最优策略。</p>\n<p>于是，作者定义了soft Q-Iteration。</p>\n<h3 id=\"Soft-Q-Iteration\"><a href=\"#Soft-Q-Iteration\" class=\"headerlink\" title=\"Soft Q-Iteration\"></a>Soft Q-Iteration</h3><p>先要假设值函数$Q_{\\mathrm{soft}}(\\cdot, \\cdot)$、$V_{\\text { soft }}(\\cdot)$有界，</p>\n<script type=\"math/tex; mode=display\">\n\\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}\\left(\\cdot, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}<\\infty \\ ，\\ Q_{\\mathrm{soft}}^{*}<\\infty\n\\tag{9}</script><p>文中定义的交互迭代至收敛的方式其实跟SARSA算法比较像：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n{Q_{\\text { soft }}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\leftarrow r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\text { soft }}\\left(\\mathbf{s}_{t+1}\\right)\\right], \\forall \\mathbf{s}_{t}, \\mathbf{a}_{t}} \\\\ \n{V_{\\text { soft }}\\left(\\mathbf{s}_{t}\\right) \\leftarrow \\alpha \\log \\int_{\\mathcal{A}} \\exp \\left(\\frac{1}{\\alpha} Q_{\\text { soft }}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right) d \\mathbf{a}^{\\prime}, \\forall \\mathbf{s}_{t}}\n\\end{array}\n\\tag{10}</script><p>这种优化方式在理论上是可行的，但是在实际应用中存在两个问题：</p>\n<ol>\n<li>连续空间无法求期望，或者计算不准确。<strong>解决方案是重要性采样，使用采样多次后计算来代替积分，在初期进行随机均匀采样，后期根据policy来采样。</strong></li>\n<li>迭代过程需要不断选择动作，问题是式（8）的分布形式无法进行采样。<strong>解决方案是使用SVGD算法拟合后验分布，并输出采样的动作。</strong></li>\n</ol>\n<h3 id=\"Soft-Q-Learning\"><a href=\"#Soft-Q-Learning\" class=\"headerlink\" title=\"Soft Q-Learning\"></a>Soft Q-Learning</h3><p>文中在这一部分引用了重要性采样，解决了上面提到的第一个问题，即，使用分布$q_{\\mathrm{a}^{\\prime}}$来代替真实策略分布</p>\n<script type=\"math/tex; mode=display\">\n\\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\text { soft }}^{*}\\left(s_{t}, a_{t}\\right)-V_{\\text { soft }}^{*}\\left(s_{t}\\right)\\right)\\right\n)</script><p>进行采样。</p>\n<script type=\"math/tex; mode=display\">\nV_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}\\right)=\\alpha \\log \\mathbb{E}_{\\color{red}{q_{\\mathrm{a}^{\\prime}}}}\\left[\\frac{\\exp \\left(\\frac{1}{\\alpha} Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right)}{q_{\\mathrm{a}^{\\prime}}\\left(\\mathbf{a}^{\\prime}\\right)}\\right]\n\\tag{11}</script><p>采样分布$q_{\\mathrm{a}^{\\prime}}$可以使用任意的分布，但是由于重要性采样的性质，采样分布与原分布越接近，效果越好。式子中的$\\theta$为Q神经网络的参数。</p>\n<p>因为在训练初期，我们估计的真实分布是偏差很大的，几乎可以说是错误的，因此在训练初期将采样分布设置为均匀分布比较合理，在训练一段时间之后，可以将采样分布设置为接近原分布，甚至是原分布（如果原分布可以采样，如，使用神经网络等“黑匣子”进行表示）</p>\n<p>由此，可以定义Q神经网络的损失函数为：</p>\n<script type=\"math/tex; mode=display\">\nJ_{Q}(\\theta)=\\mathbb{E}_{\\mathbf{s}_{t} \\sim q_{\\mathbf{s}_{t}}, \\mathbf{a}_{t} \\sim q_{\\mathbf{a}_{t}}}\\left[\\frac{1}{2}\\left(\\hat{Q}_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)-Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)^{2}\\right]\n\\tag{12}</script><p>上式中期望的下标为环境和真实策略分布，$\\overline{\\theta}$代表target网络的参数，目标是最小化这个损失函数，其中，</p>\n<script type=\"math/tex; mode=display\">\n\\hat{Q}_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=r_{t}+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p_{\\mathbf{s}}}\\left[V_{\\mathrm{soft}}^{\\overline{\\theta}}\\left(\\mathbf{s}_{t+1}\\right)\\right]\n\\tag{13}</script><h3 id=\"近似采样与SVGD\"><a href=\"#近似采样与SVGD\" class=\"headerlink\" title=\"近似采样与SVGD\"></a>近似采样与SVGD</h3><p>SVGD：Stein Vairational Gradient Descent，SVGD是一种确定性的、基于梯度的近似推理采样算法。</p>\n<p>文中在这一部分引用了SVGD的优化算法，并且使用SVGD近似策略的后验分布以进行采样，解决了上文提到的第二个问题。在百度上完全搜不到关于SVGD算法的信息，但是了解了这个算法之后，感觉它的能力还是很强的，最终，搜集了Google、Bing的检索结果，发现了原作者在SVGD方法上的一些资源分享，<a href=\"https://www.cs.utexas.edu/~lqiang/stein.html\" rel=\"external nofollow\" target=\"_blank\">Stein’s Method for Practical Machine Learning</a></p>\n<p>对于基于能量的模型、分布，有两类采样方式：</p>\n<ol>\n<li>MCMC采样，即马尔科夫链蒙特卡洛采样</li>\n<li>学习一个采样网络去近似采样出符合目标分布的样本</li>\n</ol>\n<p>在需要不断更新策略的在线学习任务中，使用MCMC采样是不可行的，于是作者使用了基于SVGD和Amortized SVGD的采样网络。</p>\n<p>SVGD论文：<a href=\"https://arxiv.org/abs/1608.04471\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/abs/1608.04471</a></p>\n<p>Amortized SVGD论文：<a href=\"https://arxiv.org/abs/1707.06626\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/abs/1707.06626</a></p>\n<p><img src=\"./rl-with-deep-energy-based-policies/1dgmm.gif\" alt=\"\"><br><img src=\"./rl-with-deep-energy-based-policies/vp.gif\" alt=\"\"></p>\n<p>Amortized SVGD有一些有趣的性质：</p>\n<ul>\n<li>可以训练随机采样网络非常快地采样</li>\n<li>可以准确收敛至EBM能量模型的后验估计分布</li>\n<li>文中结合了Amortized SVGD后，算法形式很像A-C模式</li>\n</ul>\n<p>SVGD算法的更新形式是这样的，</p>\n<script type=\"math/tex; mode=display\">\nx_{i} \\leftarrow x_{i}+\\frac{\\epsilon}{n} \\sum_{j=1}^{n}\\left[k\\left(x_{j}, x_{i}\\right) \\nabla_{x_{j}} \\log p\\left(x_{j}\\right)+\\nabla_{x_{j}} k\\left(x_{j}, x_{i}\\right)\\right], \\qquad \\forall i=1, \\ldots, n\n\\tag{14}</script><ul>\n<li>$\\epsilon$代表学习率</li>\n<li>$k\\left(x_{j}, x_{i}\\right)$代表正定核，如径向基（RBF，Radial Basis Function）函数$k\\left(x, x^{\\prime}\\right)=\\exp \\left(-\\frac{1}{h}\\left|x-x^{\\prime}\\right|_{2}^{2}\\right)$，它可以被认为是变量之间的相似性度量</li>\n<li>包含对数项$\\log p\\left(x_{j}\\right)$的梯度驱使采样器朝着$p(x)$分布中高概率区域进行采样</li>\n<li>第二项核函数梯度驱使样本点之间产生间隙，相当于用一个排斥力使样本点尽可能分散开</li>\n<li>对数项梯度不依赖分布$p(x)$的归一化常数，使SVGD易于应用于图模型、贝叶斯推理和深层生成模型中出现的难以处理的分布。</li>\n</ul>\n<p>先定义我们采样网络（其实就是Actor）的目标函数：</p>\n<script type=\"math/tex; mode=display\">\nJ_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)=D_{\\mathrm{KL}}\\left(\\pi^{\\phi}\\left(\\cdot | \\mathbf{s}_{t}\\right) \\| \\exp \\left(\\frac{1}{\\alpha}\\left(Q_{\\mathrm{soft}}^{\\theta}\\left(\\mathbf{s}_{t}, \\cdot\\right)-V_{\\mathrm{soft}}^{\\theta}\\right)\\right)\\right)\n\\tag{15}</script><ul>\n<li>$\\phi$表示采样网络中的参数</li>\n<li>将产生动作的函数简写成$\\mathbf{a}_{t}^{(i)}=f^{\\phi}\\left(\\xi^{(i)} ; \\mathbf{s}_{t}\\right)$，也就是说神经网络的输入分为两部分，一部分是状态$s$，一部分是噪声扰乱“perturb”$\\xi$，一般从标准正态分布中采样，而且最好使噪声的维度与动作的维度一致</li>\n</ul>\n<p>计算梯度方向：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)=& \\mathbb{E}_{\\mathbf{a}_{t} \\sim \\pi^{\\phi}}\\left[\\kappa\\left(\\mathbf{a}_{t}, f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)\\right) \\nabla_{\\mathbf{a}^{\\prime}} Q_{\\mathrm{soft}}^{\\theta}\\left.\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{t}}\\right.\\\\ &+\\alpha \\nabla_{\\mathbf{a}^{\\prime}} \\kappa\\left.\\left(\\mathbf{a}^{\\prime}, f^{\\phi}\\left(\\cdot ; \\mathbf{s}_{t}\\right)\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{t}} ] \\end{aligned}\n\\tag{16}</script><ul>\n<li><p>严格来说，$\\Delta f^{\\phi}$是希尔伯特空间的最优梯度方向，并不是Actor目标函数$J_{\\pi}$的梯度</p>\n</li>\n<li><blockquote>\n<p>To be precise, $\\Delta f^{\\phi}$ is the optimal direction in the reproducing kernel Hilbert space of $\\kappa$, and is thus not strictly speaking the gradient of $J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)$ </p>\n</blockquote>\n</li>\n</ul>\n<p>根据链式法则，Stein变分梯度SVG为</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)}{\\partial \\phi} \\propto \\mathbb{E}_{\\xi}\\left[\\Delta f^{\\phi}\\left(\\xi ; \\mathbf{s}_{t}\\right) \\frac{\\partial f^{\\phi}\\left(\\xi ; \\mathbf{s}_{t}\\right)}{\\partial \\phi}\\right]\n\\tag{17}</script><h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-with-deep-energy-based-policies/pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<p>算法中更新Actor网络时，其实是使用了如下梯度公式：</p>\n<script type=\"math/tex; mode=display\">\n\\hat{\\nabla}_{\\phi} J_{\\pi}\\left(\\phi ; \\mathbf{s}_{t}\\right)=\\frac{1}{K M} \\sum_{j=1}^{K} \\sum_{i=1}^{M}\\left(\\kappa\\left(\\mathbf{a}_{t}^{(i)}, \\tilde{\\mathbf{a}}_{t}^{(j)}\\right) \\nabla_{\\mathbf{a}^{\\prime}} Q_{\\mathrm{soft}}\\left.\\left(\\mathbf{s}_{t}, \\mathbf{a}^{\\prime}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{i}^{(i)}}+\\nabla_{\\mathbf{a}^{\\prime}} \\kappa\\left.\\left(\\mathbf{a}^{\\prime}, \\tilde{\\mathbf{a}}_{t}^{(j)}\\right)\\right|_{\\mathbf{a}^{\\prime}=\\mathbf{a}_{i}^{(i)}}\\right) \\nabla_{\\phi} f^{\\phi}\\left(\\tilde{\\xi}^{(j)} ; \\mathbf{s}_{t}\\right)\n\\tag{18}</script><p>更新方向为mini-batch经验的梯度平均值，而不是累加和</p>\n<ul>\n<li>伪代码中定义了Actor的target网络，参数为$\\overline{\\theta}$。但是伪代码中并没有显示出其在何处使用，我<strong>猜测</strong>该网络代表采样分布$q_{\\mathbf{a}^{\\prime}}$<ul>\n<li>$q_{\\mathbf{a}^{\\prime}}$在训练初期使用均匀分布</li>\n<li>$q_{\\mathbf{a}^{\\prime}}$在一段时间之后使用Actor真实分布，我猜测这里使用的就是Actor目标网络</li>\n</ul>\n</li>\n<li>噪音$\\xi$从多维标准正态分布中采样，维度最好与动作空间维度一致</li>\n<li>$\\left\\{\\mathbf{a}^{(i, j)}\\right\\}_{j=0}^{M} \\sim q_{\\mathbf{a}^{\\prime}}$其中的M用于设置采样多少个样本，以使用公式（11）计算V值，使用的网络为Q目标网络</li>\n<li>在更新Q网络时，使用了从经验池采样到的真实执行过的动作</li>\n<li>$\\left\\{\\xi^{(i, j)}\\right\\}_{j=0}^{M} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{I})$中其实少写了一个参数$K$，但实际上$K=M$，在这一步中需要采样两组噪音，当然也可以采样一组，使用两次。</li>\n<li>在更新Actor网络时，没有使用经验池中采样到的动作，而模拟采样了两组动作，即根据两组噪音生成的动作，用它们来计算梯度并更新。</li>\n<li>Q网络的输入为状态与动作的连接，$(s||a)$，输出为Q值</li>\n<li>Actor网络的输入为状态与噪音的连接，$(s||\\xi)$，输出为动作$a$</li>\n<li>伪代码中的式(10)、(11)、(13)、(14)分别代表本文中的式(11)、(12)、(16)、(17)</li>\n</ul>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><h2 id=\"实验设置\"><a href=\"#实验设置\" class=\"headerlink\" title=\"实验设置\"></a>实验设置</h2><ul>\n<li>比较算法：DDPG vs SQL</li>\n<li>Actor和Q网络使用Adam优化器</li>\n<li>Actor学习率为0.0001，Q网络学习率为0.001</li>\n<li>经验池大小为100W</li>\n<li>经验池填充1W条经验后开始训练</li>\n<li>batch_size=64</li>\n<li>Actor和Q网络都是2层隐藏层，每层200个隐藏节点，激活函数为ReLU</li>\n<li><p>DDPG和SQL都使用了Ornstein-Uhlenbeck随机过程产生噪音来增加探索，它是一种序贯相关的随机过程，$\\theta=0.15 \\ , \\ \\sigma=0.3$</p>\n<ul>\n<li><p>OU随机过程可以在序贯模型中添加与时间相关的随机噪音，而且噪音也满足强马尔可夫性</p>\n</li>\n<li><p>形式为$d x_{t}=\\theta\\left(\\mu-x_{t}\\right) d t+\\sigma d W_{t}$，是一个具有均值恢复属性的随机过程</p>\n</li>\n<li><p>$\\theta$表示变量$x$以多大幅度、多块恢复到平均值，$\\mu$代表平均值，$\\sigma$代表波动程度，$d W_{t}$代表维纳过程，一般通过高斯分布实现</p>\n</li>\n<li><p>OU随机过程产生的噪音只与上一次产生的噪音相关，它可以用于增加探索，也能够柔顺控制。比如在相邻的两个决策动作，一个为10，一个为-10，反复如此，智能体会产生震荡。在此使用OU过程可以使智能体在一个方向保持一定时间，不会瞬间过大地改变智能体的状态，相当于增加了时滞性。</p>\n</li>\n<li><p>代码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = 10</span><br><span class=\"line\">dx = theta * (mu - x) + sigma * numpy.random.randn(len(x))</span><br><span class=\"line\">x = x + dx</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>参考：</p>\n<ul>\n<li><a href=\"https://www.quora.com/Why-do-we-use-the-Ornstein-Uhlenbeck-Process-in-the-exploration-of-DDPG\" rel=\"external nofollow\" target=\"_blank\">https://www.quora.com/Why-do-we-use-the-Ornstein-Uhlenbeck-Process-in-the-exploration-of-DDPG</a></li>\n<li><a href=\"https://github.com/floodsung/DDPG-tensorflow/blob/master/ou_noise.py\" rel=\"external nofollow\" target=\"_blank\">https://github.com/floodsung/DDPG-tensorflow/blob/master/ou_noise.py</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/51333694\" rel=\"external nofollow\" target=\"_blank\">https://zhuanlan.zhihu.com/p/51333694</a></li>\n<li>核函数使用了径向基函数RBF，$\\kappa\\left(\\mathbf{a}, \\mathbf{a}^{\\prime}\\right)=\\exp \\left(-\\frac{1}{h}\\left|\\mathbf{a}-\\mathbf{a}^{\\prime}\\right|_{2}^{2}\\right)$，其中，$h=\\frac{d}{2 \\log (M+1)}$，$d$为各变量对之间距离的中位数</li>\n<li>目标网络的更新采样硬覆盖的模式</li>\n<li>超参数$\\alpha$根据任务设置为10，0.1等等</li>\n<li>训练的epoch、步长、系数$\\alpha$，采样动作的数量$K 和 M$根据任务（多目标，单目标，微调）的不同而不同，具体请看原论文附录D部分</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><p>未完待续</p>\n<h1 id=\"个人感想\"><a href=\"#个人感想\" class=\"headerlink\" title=\"个人感想\"></a>个人感想</h1><p>虽然文中用大量公式、篇幅结合最大熵进行介绍、推理，但是在伪代码以及目标函数中似乎并没有看到关于熵的影子，包含熵项的Q值也是通过Q网络的输出将熵的值包含在内，并没有显式地计算它。</p>\n<p>虽然算法的名字为soft Q-Learning，但其实它跟传统的Q-Learning算法思想并不相同，如果说有一点相同，那也是都是想使Q值收敛以推导出最优策略，但是这个优化过程也跟SARSA算法比较像，并没有使用传统Q-Learning中贪婪的选择最有价值下一个动作以自举的方法。</p>"},{"title":"强化学习论文浅读集合","copyright":true,"mathjax":true,"top":1,"date":"2019-06-10T07:55:29.000Z","keywords":null,"description":null,"_content":"\n本文记录了一些粗读的强化学习相关的论文。\n\n<!--more-->\n\n<h1 align=\"center\" style=\"color:blue\" id=\"Gorila\">[DeepMind]Massively Parallel Methods for Deep Reinforcement Learning[Gorila]</h1>\n\n本文提出了一个分布式强化学习训练的架构：Gorila(General Reinforcement Learning Architecture)。2015年发于ICML，本文使用DQN算法进行分布式实现。\n\n论文地址：[https://arxiv.org/pdf/1507.04296.pdf](https://arxiv.org/pdf/1507.04296.pdf)\n\n## 模型示意图\n\n![](./rl-rough-reading/gorila.png)\n\n解析：\n\n- shard代表参数分片的意思，即模型过大、参数过多，需要将参数分片放置多台机器上\n- Bundled Mode模式指的是Actor中的Q网络与Learner中的Q网络一样，但是Learner比Actor多了一个目标Q网络，用于计算梯度\n\n## 特点\n\n- 并行Actor采数据\n- 并行Learner计算梯度，**不更新Learner中的模型**\n- 中心参数服务器，用于维持最新的网络模型。如果模型太大、参数过多，可以分片将网络模型放置多个参数服务器，每个参数服务器中的参数独立不关联，根据learner传的梯度更新相应的变量\n- 经验池机制，分为local与global两种\n  - local，即每个actor节点一个经验池\n  - global，将所有actor节点的经验存至一个分布式数据库中，这个**需要网络通信开销**\n\n## 伪代码\n\n![](./rl-rough-reading/gorila-pseudo.png)\n\n解析：\n\n- 伪代码中为一个actor节点的流程\n\n- 注意伪代码中出现两次`Update θ from parameters θ+ of the parameter server `，这句话的意思为从中心参数服务器拉取模型到actor和learner，拉取的时间点为：\n\n  - 每个episode开始前\n  - 每次执行动作$a_{t}$后，但是在计算梯度并将梯度传递至参数服务器之前\n\n- 伪代码中`equation 2`，代表$g_{i}=\\left(r+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime} ; \\theta_{i}^{-}\\right)-Q\\left(s, a ; \\theta_{i}\\right)\\right) \\nabla_{\\theta_{i}} Q(s, a ; \\theta)$，这是DQN中的损失函数\n\n- 注意，与传统DQN不同的是，**该分布式DQN中给Learner中的目标Q网络赋值时，是直接将更新N次的中心参数服务器中的模型进行拉取覆盖，而不是使用Learner中的Q网络**\n\n- 中心参数服务器中的参数梯度更新需要累计多个learner传来的梯度后进行更新，使用异步SGD即ASGD方法进行梯度下降。\n\n  - > The parameter server then applies the updates that are accumulated from many learners. \n\n- 因为每个actor都是阶段更新自己的模型，即从参数服务器中拉取。所以每个actor中的行为策略（采样策略）都不完全相同，事实上，每个actor节点可以采取不同的探索机制，这样可以更有效地探索环境\n\n## 稳定性\n\n为了应对节点退出、网速慢、节点机器运行慢等问题，该文章中指出使用了一个超参数用来控制actor和server之间最大延时。\n\n- 过时的梯度（低于时间阈值）将会被丢弃\n\n  - > All gradients older than the threshold are discarded by the parameter server. \n\n- 过高或过低的梯度也将被丢弃\n\n  - > each actor/learner keeps a running average and standard deviation of the absolute DQN loss for the data it sees and discards gradients with absolute loss higher than the mean plus several standard deviations. \n\n- 使用AdaGrad更新规则\n\n## 效果\n\n采用于提出DQN的论文中一样的网络结构，具体请见论文中第5部分。\n\n在Atari 2600 49个游戏中，41个明显优于单GPU DQN。\n\nGorila进一步实现了DRL的希望：一个可伸缩的架构，随着计算和内存的增加，它的性能会越来越好\n\n\n\n<h1 align=\"center\" style=\"color:blue\" id=\"MB-MPO\">[UCB/OpenAI]Model-Based Reinforcement Learning via Meta-Policy Optimization[MB-MPO]</h1>\n\n论文地址：[https://arxiv.org/pdf/1809.05214.pdf](https://arxiv.org/pdf/1809.05214.pdf)\n\n本文2018年发布于CoRL，提出了一个基于模型的元强化学习算法MB-MPO。相比于一般的元强化学习是从多个MDPs任务中学习一个通用模型加速以后特定任务的模型训练，该文中的方法是将一个model-free的任务学习多个不确定、不完全、不完美的动态模型，即一个模型集合，然后使用这个模型集合学习出该任务的通用模型。因为它有一个从model-free学习动态模型的过程，所以为model-based方法。\n\n## 元强化学习\n\n$$\n\\max _{\\theta} \\mathbb{E}_{\\mathcal{M}_{k} \\sim \\rho(\\mathcal{M}),\\boldsymbol{s}_{t+1} \\sim p_{k},\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}^{\\prime}}\\left(\\boldsymbol{a}_{t} | \\boldsymbol{s}_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r_{k}\\left(s_{t}, a_{t}\\right)\\right] \\\\ s.t.:\\boldsymbol{\\theta}^{\\prime}=\\boldsymbol{\\theta}+\\alpha\\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}_{\\boldsymbol{s}_{t+1} \\sim p_{k},\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{a}_{t} | \\boldsymbol{s}_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r_{k}\\left(s_{t}, a_{t}\\right)\\right]\n$$\n\n$\\mathcal{M}$为一系列MDP，共享相同的状态空间$\\mathcal{S}$与动作空间$\\mathcal{A}$，但是奖励函数可以不同\n\n## 学习环境动态模型\n\n$$\n\\min _{\\boldsymbol{\\phi}_{k}} \\frac{1}{\\left|\\mathcal{D}_{k}\\right|} \\sum_{\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}, \\boldsymbol{s}_{t+1}\\right) \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{s}_{t+1}-\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)\\right\\|_{2}^{2}\n$$\n\n解析：\n\n- $\\mathcal{D}_{k}$为第k个学习模型采样的“经验”\n\n- $\\phi$为用神经网络表示的环境模型的参数\n\n- $\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)$为第k个学习模型针对状态$s_{t}$执行动作$a_{t}$后转移状态的预测，其中，神经网络的输出不直接是预测的状态$\\color{red}{s_{t+1}}$，而是$\\color{red}{\\Delta s=s_{t+1}-s_{t}}$，所以$\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)=s_{t}+\\Delta s$\n\n  - > We follow the standard practice in model-based RL of training the neural network to predict the change in state $\\Delta s=s_{t+1}-s_{t}$ (rather than the next state $s_{t+1}$) \n\n为了防止过拟合，文中使用了3个trick：\n\n1. 早停\n2. 归一化神经网络输入与输出\n3. 权重归一化\n\n## 基于环境动态模型的元强化学习\n\n假设学到了K个近似模型$\\left\\{\\hat{f}_{\\phi_{1}}, \\hat{f}_{\\phi_{2}}, \\ldots, \\hat{f}_{\\phi_{K}}\\right\\}$，把每个模型转换成一个MDP过程，即$\\mathcal{M}_{k}=\\left(S, A, \\hat{f}_{\\phi_{k}}, r, \\gamma, p_{0}\\right)$，其中，**奖励函数相同**\n\n由此给每个学习到的动态模型分配的行为策略目标函数为：\n$$\nJ_{k}(\\boldsymbol{\\theta})=\\mathbb{E}_{\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{a}_{t} | s_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right) | \\boldsymbol{s}_{t+1}=\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)\\right]\n$$\n定义MB=MPO的最终目标函数为：\n$$\n\\max _{\\boldsymbol{\\theta}} \\frac{1}{K} \\sum_{k=0}^{K} J_{k}\\left(\\boldsymbol{\\theta}_{k}^{\\prime}\\right) \\quad \\text { s.t.: } \\quad \\boldsymbol{\\theta}_{k}^{\\prime}=\\boldsymbol{\\theta}+\\alpha \\nabla_{\\boldsymbol{\\theta}} J_{k}(\\boldsymbol{\\theta})\n$$\n小写k代表第k个学到的模型，大写K代表模型的总数。\n\n注意看，这里公式后边使用的是$\\color{red}{\\theta'_{k}}$，而不是$\\theta$。这里并没有写错，我起初以为写错了，具体请看后边的伪代码解释。\n\n## 伪代码\n\n![](./rl-rough-reading/mb-mpo-pseudo.png)\n\n解析：\n\n- MB-MPO分为两部分更新，第一部分更新每个模型分配的行为策略，第二部分更新元策略。**注意：行为策略的更新是不连贯的，即不是自身迭代，而是不断使用元策略进行稍加修改然后替换，所以叫做adapted policy**\n- 上一项提到的两次更新都是对元策略的参数$\\theta$进行更新，区别是，第一次更新将更新后的参数赋值给了行为策略，未更改元策略本身，第二次更新直接更新元策略本身\n- $\\alpha, \\beta$为两部分更新的学习率\n- 行为策略使用VPG，即传统策略梯度算法进行优化，元策略使用TRPO算法进行优化\n- 伪代码中的大致流程如下：\n  1. 初始化策略$\\pi_{\\theta}$并将其复制K份$\\pi_{\\theta_{1}^{\\prime}}, \\dots, \\pi_{\\boldsymbol{\\theta}_{K}^{\\prime}}$\n  2. 使用$\\pi_{\\theta_{1}^{\\prime}}, \\dots, \\pi_{\\boldsymbol{\\theta}_{K}^{\\prime}}$对**真实的环境模型进行采样（这一步是实际交互，即真实数据）**，将数据存入经验池\n  3. 根据经验池训练K个环境模型，即使用`学习环境动态模型`部分的公式\n  4. 对于每个更新后的环境模型，用元策略$\\color{red}{\\pi_{\\theta}}$进行**虚拟采样（这一步是预测采样，即不实际进行交互）**，采样到$\\mathcal{T}_{k}$以适应性修改行为策略$\\boldsymbol{\\theta}_{k}^{\\prime}$。这里也是前边提到的行为策略更新是不连贯的原因。\n  5. 再用适应性策略$\\boldsymbol{\\theta}_{k}^{\\prime}$进行**虚拟采样**，采样到$\\mathcal{T}_{k}^{\\prime}$以更新元策略$\\pi_{\\theta}$\n  6. 跳向第2步\n- 伪代码中虽然没有明确指出，但是其实使用了baseline的trick用来减少方差\n\n## 流程示意图\n\n![](./rl-rough-reading/mb-mpo-visio.png)\n\n## 效果\n\n1. 比之前的model-based方法效果好、收敛快\n2. 可以达到model-free算法的渐进性能\n3. 需要更少的经验，低采样复杂性。其实是使用了虚拟采样，提高了数据效率，减少了交互采样的代价。\n4. 对于模型偏差（model-bias，即环境模型没学到位）的情况，之前的算法不能有效处理，该算法对不完美、不完全、不完整的模型具有很好地鲁棒性\n\n<h1 align=\"center\" style=\"color:blue\" id=\"DIAYN\">[UCB/Google AI]Diversity is All Your Need: Learning Skills Without a Reward Function[DIAYN]</h1>\n\n论文地址：[https://arxiv.org/pdf/1802.06070.pdf](https://arxiv.org/pdf/1802.06070.pdf)\n\nGoogle网页：[https://sites.google.com/view/diayn/home](https://sites.google.com/view/diayn/home)\n\nGithub项目：[https://github.com/ben-eysenbach/sac/blob/master/DIAYN.md](https://github.com/ben-eysenbach/sac/blob/master/DIAYN.md)\n\n这篇文章使用信息论中最大熵的方法来构造强化学习的学习目标，**期望学习到具有多样性的技能（skills）**。\n\n个人认为，此文章中所提的方法虽然很新颖，但是不能作为优化一项任务的可用算法，因为虽然其可以学到以各种花样完成目标，但是没有奖励函数的控制使得无法规范、指引智能体“解题”过程的效果，如柔顺性、实用性、实际可行性等。从另一方面来讲，将这样虽然不规划、不严谨决策行为的策略用于元策略的预训练模式还是可用的。\n\n## 技能\n\n技能的定义在文中有如下表述：\n\n> A skill is a latent-conditioned policy that alters that state of the environment in a consistent\n> way. \n> we refer to a/the policy conditioned on a fixed Z as a “skill” .\n\n意思是，设定一个隐变量，以（状态$S$，隐变量$Z$）为条件进行动作选择，即为技能——skill。\n\nDIAYN就好像是要给每个状态赋予各个不同技能的概率，并且使其中一个技能的概率最大，这样就使得在整个状态空间中，不同的技能“占领”着状态空间的不同部分，每个技能在各自偏好的局部状态空间中作用，但是作者同样希望每个技能在各自的状态空间中尽可能随机决策。\n\n![](./rl-rough-reading/skill.png)\n\n假设以不同的颜色代表不同的技能skill，每个方格代表一个状态，那么每个状态对于每个技能到达此状态的“偏好”概率是不同的。总的来说，作者希望技能之间的重合度尽可能小，但每个技能在各自的领域内尽可能随机地完成目标。\n\n## 亮点与作用\n\n1. 去掉了奖励函数\n2. 修改了目标函数，$\\mathcal{F}(\\theta)\\triangleq \\mathcal{G}(\\color{red}{\\theta, \\phi})$\n   - $\\color{red}{\\theta}$代表Actor网络中的参数\n   - $\\color{red}{\\phi}$代表Critic网络中的参数\n3. 学习到的技能可以用于*分层强化学习、迁移学习、模仿学习*\n\n## 目标函数\n\n$$\n\\begin{aligned} \n\\mathcal{F}(\\theta) & \\triangleq \\color{red}{I(S ; Z)+\\mathcal{H}[A | S]-I(A ; Z | S)} \\\\ \n&=(\\mathcal{H}[Z]-\\mathcal{H}[Z | S])+\\mathcal{H}[A | S]-(\\mathcal{H}[A | S]-\\mathcal{H}[A | S, Z]) \\\\ \n&=\\color{blue}{\\mathcal{H}[Z]-\\mathcal{H}[Z | S]+\\mathcal{H}[A | S, Z]} \\\\\n&=\\mathcal{H}[A | S, Z]+\\mathbb{E}_{z \\sim p(z), s \\sim \\pi(z)}[\\log p(z | s)]-\\mathbb{E}_{z \\sim p(z)}[\\log p(z)] \\\\\n&{ \\color{orange}{\\geq} \\mathcal{H}[A | S, Z]+\\mathbb{E}_{z \\sim p(z), s \\sim \\pi(z)}\\left[\\log q_{\\phi}(z | s)-\\log p(z)\\right] \\\\ \n\\triangleq \\mathcal{G}(\\theta, \\phi)}\n\\end{aligned}\n$$\n\n解析：\n\n- 互信息，离散下为$I(X ; Y)=\\sum_{y \\in Y} \\sum_{x \\in X} p(x, y) \\log \\left(\\frac{p(x, y)}{p(x) p(y)}\\right)$，连续下为$I(X ; Y)=\\int_{Y} \\int_{X} p(x, y) \\log \\left(\\frac{p(x, y)}{p(x) p(y)}\\right) d x d y$\n\n- 信息熵表示为$H(X, Y)=-\\sum_{x, y} p(x, y) \\log p(x, y)=-\\sum_{i=1}^{n} \\sum_{j=1}^{m} p\\left(x_{i}, y_{i}\\right) \\log p\\left(x_{i}, y_{i}\\right)$\n\n- 推到中频繁使用了性质$I(X,Y)=H(X)-H(X | Y)$\n\n- 式中对数的底为自然指数$e$\n\n- 看红色部分，化简之前：\n\n  - **增大**：$I(S ; Z)$代表状态$S$与策略隐变量$Z$之间的互信息。因为作者希望可以通过策略所能到达的状态来判别其属于哪个技能，即将技能与状态挂钩。作者给出一个直观的解释：因为在有些状态下可以执行很多动作，但是却不改变环境（至少不明显改变），就像用机械手臂夹紧一个物体时，可使用力的大小、方向等都是很多的，不同技能选择不同动作导致的效果可能相同，所以作者不希望从动作的选择来区分学到的技能，而是通过可以明显观察到、数值化的状态$S$来作为区别不同技能的标准。**互信息$I(X,Y)$有一个直观的性质就是，它可以衡量两个随机变量的“相关性”，也就是说，互信息越大，代表知道$X$后对$Y$的不确定性减少，即知道其一可以加深对另一个的了解。**所以，目标函数希望最大化互信息$I(S ; Z)$，以将状态和技能相关联，使技能尽可能根据状态可以区分。\n  - **增大**：$\\mathcal{H}[A | S]$代表策略（不以隐变量$Z$区分技能，混合所有技能即为策略）的熵值。与SAC算法中想要使用熵增来使得动作的选择更加随机一样，作者希望随机性的动作同样可以完成目标，所以希望尽可能增大这一项。\n  - **减小**：$I(A ; Z | S)$代表动作$A$与策略隐变量$Z$在给定状态$S$时之间的互信息。为了避免歧义，应该写作为$I[(A ; Z) | S]$。作者希望技能根据状态可区分，而不是根据动作，所以需要最小化这一项。\n\n- 看蓝色部分，化简之后：\n\n  - **固定最大，为$\\ln n$**：$\\mathcal{H}[Z]$代表技能分布的不确定性，既然要最大化这个项，不如就固定它，使得技能从其中均匀采样，使熵为最大值。\n  - **减小**：$\\mathcal{H}[Z | S]$代表状态$S$条件下技能的不确定性，我们知道，熵越大，不确定性越大；熵越小，不确定性越小。作者希望技能根据状态可区分，可以需要使这一项最小，以减小给定状态下所属技能的不确定性，使其尽可能接近概率1。\n  - **增大**：$\\mathcal{H [ A | S}, Z ]$代表给定技能$(S,Z)$下动作的不确定性。因为作者希望动作的选择尽可能随机但又可以完成目标，所以需要最大化这一项。\n\n- 看橘色部分，使用Jensen不等式：\n\n  - 这一步推导使用了论文[《The IM Algorithm : A variational approach to Information Maximization》](https://pdfs.semanticscholar.org/f586/4b47b1d848e4426319a8bb28efeeaf55a52a.pdf)中的推导公式\n    $$\n    I(\\mathbf{x}, \\mathbf{y}) \\geq \\underbrace{H(\\mathbf{x})}_{\\text { ‘‘entrop’’ }}+\\underbrace{\\langle\\log q(\\mathbf{x} | \\mathbf{y})\\rangle_{p(\\mathbf{x}, \\mathbf{y})}}_{\\text { ‘‘energy’’ }} \\stackrel{\\mathrm{def}}{=} \\tilde{I}(\\mathbf{x}, \\mathbf{y})\n    $$\n\n  - ![](./rl-rough-reading/Agakov.png)\n\n  - 蓝色公式中，有$I(Z;S) = \\mathcal{H}[Z]-\\mathcal{H}[Z | S]$，可以应用上述性质进行推导，将真实分布$p(z | s)$替换为任意变分分布(variational distribution)$q(z | s)$\n\n  - 最后使用变分下界$\\mathcal{G}(\\theta, \\phi)$代替目标函数$\\mathcal{F}(\\theta)$\n\n- 至此，思路已经十分清晰。Actor网络以变量$\\theta$参数化，并使用SAC算法($\\alpha=0.1$)最大化$\\mathcal{G}(\\theta, \\phi)$中$\\mathcal{H}[A | S, Z]$部分；Critic网络以变量$\\phi$参数化，并最大化后半个期望部分。文中将期望内的元素定义为“伪奖励”：\n  $$\n  r_{z}(s, a) \\triangleq \\log q_{\\phi}(z | s)-\\log p(z)\n  $$\n  由于$p(z)$为均匀分布，是固定的；对数函数不改变原函数单调性，所以只需最大化$q_{\\phi}(z | s)$即可。\n\n## 伪代码\n\n![](./rl-rough-reading/diayn-pseudo.png)\n\n解析：\n\n- 每个episode都重新采样隐变量$z$\n- Actor网络的输入为$(S||Z)$，即状态与隐变量的连结(我猜的= =)\n- Critic网络的输入为状态$S$\n\n## 模型示意图\n\n![](./rl-rough-reading/diayn.png)\n\n解析：\n\n- 隐变量分布$p(z)$是固定的\n\n\n\n<h1 align=\"center\" style=\"color:blue\" id=\"CDP\">Curiosity-Driven Experience Prioritization via Density Estimation[CDP]</h1>\n论文地址：[https://arxiv.org/pdf/1902.08039.pdf](https://arxiv.org/pdf/1902.08039.pdf)\n\n这篇文章发于2018年的NIPS，作者为赵瑞，之前读过他的两篇论文，并写了博客，可以在论文精读里找，此处不贴链接了，分别是基于能量的HER和最大熵正则化多目标RL。\n\n这篇文章总的来说提出了**基于迹密度的优先经验回放**，人类的好奇心机制驱动他有了这样的想法，文中说受有监督学习使用过采样和降采样解决训练集样本不平衡问题的启发，想在强化学习中解决经验池中迹“不足（under-represented）”的问题。\n\n说起来也挺佩服这个作者的，目前（2019年6月21日14:59:05）总共发了三篇关于强化学习的论文，但都有很好地结果：\n\n1. 基于迹能量的优先级，发了CoRL\n2. 基于迹密度的优先级，也就是这篇，发了NIPS\n3. 基于迹最大熵的优先级，发了ICML\n\n我个人道行尚浅，对于几篇论文中的深奥精髓有些不能尽数参透，由于先验知识不足，对于文中内容也不敢完全苟同，但是从这几篇阅读总结下来，发得了这种高级别论文有以下几个“加分性”要求：\n\n1. 数学要好，这是必然的，数学公式写的越华丽，数学模型越复杂，当然越具有吸引力\n2. 工作要专一且连续，看这三篇论文虽然不是递进关系，但都是在解决经验池优先相关的工作，所以找准一个领域内的小角度也是很重要的\n3. 实验部分要做好，三篇都没用完整地、细节地比较各个算法，但是却新奇地比较了采样复杂性、数据利用率等等，总之，一定要用实验表明自己的方法在某方面有用\n4. 其他秘密因素\n\n## 流程\n\n这篇论文的方法流程如下：\n\n1. 计算**迹密度**$\\rho$\n2. 计算迹密度的补$\\overline{\\rho} \\propto 1-\\rho$\n3. 根据补排序，并设置优先级，补越大优先级越大\n4. 使用HER补充经验，设置相同的优先级和迹密度\n5. 优化算法\n\n## 迹密度的计算\n\n这一部分没有看太懂，主要是本人数学功底比较薄弱，感兴趣的可以亲自查看论文中2.4与3.2.1、3.2.2部分。\n\n根据文中的意思，思想大致如下：\n\n1. 用GMM（高斯混合模型）来估计迹密度\n   $$\n   \\rho(\\mathbf{x})=\\sum_{k=1}^{K} c_{k} \\mathcal{N}\\left(\\mathbf{x} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)\n   $$\n\n2. 在每个epoch，使用V-GMM（GMM的一个变体）+EM算法推断GMM参数($\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}$)的后验分布\n\n3. 在每个episode，使用如下公式计算迹密度\n   $$\n   \\rho=\\mathrm{V}-\\operatorname{GMM}(\\mathcal{T})=\\sum_{k=1}^{K} c_{k} \\mathcal{N}\\left(\\mathcal{T} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)\n   $$\n   其中，$\\mathcal{T}=\\left(s_{0}\\left\\|s_{1}\\right\\| \\ldots \\| s_{T}\\right)$，**每个迹的长度相同**，中间的符号代表连结操作的意思，然后进行归一化\n   $$\n   \\rho_{i}=\\frac{\\rho_{i}}{\\sum_{n=1}^{N} \\rho_{n}}\n   $$\n\n*注：我猜想上边符号表示的$s$其实包含了智能体的所在状态和要达到的真实目标，也就是$(s,g)$，文中有一段可能解释了这一部分，但是我没有太理解。*\n\n![](./rl-rough-reading/cdp-sg.png)\n\n## 优先级的设定\n\n作者说使用rank-based方法来设置优先级，因为其受异常点影响小而具有良好的鲁棒性。\n\n先计算迹密度的补\n$$\n\\overline{\\rho} \\propto 1-\\rho\n$$\n将补从小到大排序，并根据排名计算优先级，排名从0开始，即\n$$\n\\operatorname{rank}(\\cdot) \\in\\{0,1, \\ldots, N-1\\}\n$$\n\n$$\np\\left(\\mathcal{T}_{i}\\right)=\\frac{\\operatorname{rank}\\left(\\overline{\\rho}\\left(\\mathcal{T}_{i}\\right)\\right)}{\\sum_{n=1}^{N} \\operatorname{rank}\\left(\\left(\\overline{\\rho}\\left(\\mathcal{T}_{n}\\right)\\right)\\right.}\n$$\n\n## 伪代码\n\n![](./rl-rough-reading/cdp-pseudo.png)\n\n解析：\n\n- 每个epoch根据经验池中的样本数据重新拟合一次密度模型，也就是GMM中的参数\n- 每个episode都计算其迹密度\n- 红色框中的公式数字编号分别代表之间部分中关于计算迹密度和迹优先级的公式\n- 采样迹、采样经验转换之后，需要采样目标并存入经验池，重构后的经验其优先级及迹密度与真正目标下迹的相同\n\n## 优点\n\n实验部分的比较详见论文。\n\n1. 可以适用于任何Off-Policy算法\n2. 不使用TD-error计算优先级，而使用迹密度，减少了计算时间\n3. 提升了采样效率两倍左右\n4. 算法性能超过最新算法9%（这个结果看看即可，不必放在心上）\n\n<h1 align=\"center\" style=\"color:blue\" id=\"NAF\">[Google]Continuous Deep Q-Learning with Model-based Acceleration[NAF]</h1>\n论文地址：[https://arxiv.org/abs/1603.00748](https://arxiv.org/abs/1603.00748)\n\n本文介绍了标准化优势函数Normalized Advantage Function——NAF算法，该算法简化了A-C架构，将Q-Learning的思想应用于高维连续空间。\n\n本文的主要贡献是：\n\n1. 提出NAF，简化了Actor-Critic架构\n2. 将Q-Learning推广至高维连续空间\n3. 提出新的与已学模型结合的方法，提升了采样复杂性（也就是降低）和学习效率，不牺牲策略的最优性。原文中翻译意思是，评估了几种将已学习模型与Q-Learning结合的方案，提出将局部线性模型与局部On-Policy想定推演结合以加速Q-Learning算法在model-free、连续问题下的学习\n\n## 伪代码\n\n![](./rl-rough-reading/naf-pseudo.png)\n\n解析：\n\n- 。","source":"_posts/rl-rough-reading.md","raw":"---\ntitle: 强化学习论文浅读集合\ncopyright: true\nmathjax: true\ntop: 1\ndate: 2019-06-10 15:55:29\ncategories: ReinforcementLearning\ntags:\n- rl\nkeywords:\ndescription:\n---\n\n本文记录了一些粗读的强化学习相关的论文。\n\n<!--more-->\n\n<h1 align=\"center\" style=\"color:blue\" id=\"Gorila\">[DeepMind]Massively Parallel Methods for Deep Reinforcement Learning[Gorila]</h1>\n\n本文提出了一个分布式强化学习训练的架构：Gorila(General Reinforcement Learning Architecture)。2015年发于ICML，本文使用DQN算法进行分布式实现。\n\n论文地址：[https://arxiv.org/pdf/1507.04296.pdf](https://arxiv.org/pdf/1507.04296.pdf)\n\n## 模型示意图\n\n![](./rl-rough-reading/gorila.png)\n\n解析：\n\n- shard代表参数分片的意思，即模型过大、参数过多，需要将参数分片放置多台机器上\n- Bundled Mode模式指的是Actor中的Q网络与Learner中的Q网络一样，但是Learner比Actor多了一个目标Q网络，用于计算梯度\n\n## 特点\n\n- 并行Actor采数据\n- 并行Learner计算梯度，**不更新Learner中的模型**\n- 中心参数服务器，用于维持最新的网络模型。如果模型太大、参数过多，可以分片将网络模型放置多个参数服务器，每个参数服务器中的参数独立不关联，根据learner传的梯度更新相应的变量\n- 经验池机制，分为local与global两种\n  - local，即每个actor节点一个经验池\n  - global，将所有actor节点的经验存至一个分布式数据库中，这个**需要网络通信开销**\n\n## 伪代码\n\n![](./rl-rough-reading/gorila-pseudo.png)\n\n解析：\n\n- 伪代码中为一个actor节点的流程\n\n- 注意伪代码中出现两次`Update θ from parameters θ+ of the parameter server `，这句话的意思为从中心参数服务器拉取模型到actor和learner，拉取的时间点为：\n\n  - 每个episode开始前\n  - 每次执行动作$a_{t}$后，但是在计算梯度并将梯度传递至参数服务器之前\n\n- 伪代码中`equation 2`，代表$g_{i}=\\left(r+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime} ; \\theta_{i}^{-}\\right)-Q\\left(s, a ; \\theta_{i}\\right)\\right) \\nabla_{\\theta_{i}} Q(s, a ; \\theta)$，这是DQN中的损失函数\n\n- 注意，与传统DQN不同的是，**该分布式DQN中给Learner中的目标Q网络赋值时，是直接将更新N次的中心参数服务器中的模型进行拉取覆盖，而不是使用Learner中的Q网络**\n\n- 中心参数服务器中的参数梯度更新需要累计多个learner传来的梯度后进行更新，使用异步SGD即ASGD方法进行梯度下降。\n\n  - > The parameter server then applies the updates that are accumulated from many learners. \n\n- 因为每个actor都是阶段更新自己的模型，即从参数服务器中拉取。所以每个actor中的行为策略（采样策略）都不完全相同，事实上，每个actor节点可以采取不同的探索机制，这样可以更有效地探索环境\n\n## 稳定性\n\n为了应对节点退出、网速慢、节点机器运行慢等问题，该文章中指出使用了一个超参数用来控制actor和server之间最大延时。\n\n- 过时的梯度（低于时间阈值）将会被丢弃\n\n  - > All gradients older than the threshold are discarded by the parameter server. \n\n- 过高或过低的梯度也将被丢弃\n\n  - > each actor/learner keeps a running average and standard deviation of the absolute DQN loss for the data it sees and discards gradients with absolute loss higher than the mean plus several standard deviations. \n\n- 使用AdaGrad更新规则\n\n## 效果\n\n采用于提出DQN的论文中一样的网络结构，具体请见论文中第5部分。\n\n在Atari 2600 49个游戏中，41个明显优于单GPU DQN。\n\nGorila进一步实现了DRL的希望：一个可伸缩的架构，随着计算和内存的增加，它的性能会越来越好\n\n\n\n<h1 align=\"center\" style=\"color:blue\" id=\"MB-MPO\">[UCB/OpenAI]Model-Based Reinforcement Learning via Meta-Policy Optimization[MB-MPO]</h1>\n\n论文地址：[https://arxiv.org/pdf/1809.05214.pdf](https://arxiv.org/pdf/1809.05214.pdf)\n\n本文2018年发布于CoRL，提出了一个基于模型的元强化学习算法MB-MPO。相比于一般的元强化学习是从多个MDPs任务中学习一个通用模型加速以后特定任务的模型训练，该文中的方法是将一个model-free的任务学习多个不确定、不完全、不完美的动态模型，即一个模型集合，然后使用这个模型集合学习出该任务的通用模型。因为它有一个从model-free学习动态模型的过程，所以为model-based方法。\n\n## 元强化学习\n\n$$\n\\max _{\\theta} \\mathbb{E}_{\\mathcal{M}_{k} \\sim \\rho(\\mathcal{M}),\\boldsymbol{s}_{t+1} \\sim p_{k},\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}^{\\prime}}\\left(\\boldsymbol{a}_{t} | \\boldsymbol{s}_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r_{k}\\left(s_{t}, a_{t}\\right)\\right] \\\\ s.t.:\\boldsymbol{\\theta}^{\\prime}=\\boldsymbol{\\theta}+\\alpha\\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}_{\\boldsymbol{s}_{t+1} \\sim p_{k},\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{a}_{t} | \\boldsymbol{s}_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r_{k}\\left(s_{t}, a_{t}\\right)\\right]\n$$\n\n$\\mathcal{M}$为一系列MDP，共享相同的状态空间$\\mathcal{S}$与动作空间$\\mathcal{A}$，但是奖励函数可以不同\n\n## 学习环境动态模型\n\n$$\n\\min _{\\boldsymbol{\\phi}_{k}} \\frac{1}{\\left|\\mathcal{D}_{k}\\right|} \\sum_{\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}, \\boldsymbol{s}_{t+1}\\right) \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{s}_{t+1}-\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)\\right\\|_{2}^{2}\n$$\n\n解析：\n\n- $\\mathcal{D}_{k}$为第k个学习模型采样的“经验”\n\n- $\\phi$为用神经网络表示的环境模型的参数\n\n- $\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)$为第k个学习模型针对状态$s_{t}$执行动作$a_{t}$后转移状态的预测，其中，神经网络的输出不直接是预测的状态$\\color{red}{s_{t+1}}$，而是$\\color{red}{\\Delta s=s_{t+1}-s_{t}}$，所以$\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)=s_{t}+\\Delta s$\n\n  - > We follow the standard practice in model-based RL of training the neural network to predict the change in state $\\Delta s=s_{t+1}-s_{t}$ (rather than the next state $s_{t+1}$) \n\n为了防止过拟合，文中使用了3个trick：\n\n1. 早停\n2. 归一化神经网络输入与输出\n3. 权重归一化\n\n## 基于环境动态模型的元强化学习\n\n假设学到了K个近似模型$\\left\\{\\hat{f}_{\\phi_{1}}, \\hat{f}_{\\phi_{2}}, \\ldots, \\hat{f}_{\\phi_{K}}\\right\\}$，把每个模型转换成一个MDP过程，即$\\mathcal{M}_{k}=\\left(S, A, \\hat{f}_{\\phi_{k}}, r, \\gamma, p_{0}\\right)$，其中，**奖励函数相同**\n\n由此给每个学习到的动态模型分配的行为策略目标函数为：\n$$\nJ_{k}(\\boldsymbol{\\theta})=\\mathbb{E}_{\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{a}_{t} | s_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right) | \\boldsymbol{s}_{t+1}=\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)\\right]\n$$\n定义MB=MPO的最终目标函数为：\n$$\n\\max _{\\boldsymbol{\\theta}} \\frac{1}{K} \\sum_{k=0}^{K} J_{k}\\left(\\boldsymbol{\\theta}_{k}^{\\prime}\\right) \\quad \\text { s.t.: } \\quad \\boldsymbol{\\theta}_{k}^{\\prime}=\\boldsymbol{\\theta}+\\alpha \\nabla_{\\boldsymbol{\\theta}} J_{k}(\\boldsymbol{\\theta})\n$$\n小写k代表第k个学到的模型，大写K代表模型的总数。\n\n注意看，这里公式后边使用的是$\\color{red}{\\theta'_{k}}$，而不是$\\theta$。这里并没有写错，我起初以为写错了，具体请看后边的伪代码解释。\n\n## 伪代码\n\n![](./rl-rough-reading/mb-mpo-pseudo.png)\n\n解析：\n\n- MB-MPO分为两部分更新，第一部分更新每个模型分配的行为策略，第二部分更新元策略。**注意：行为策略的更新是不连贯的，即不是自身迭代，而是不断使用元策略进行稍加修改然后替换，所以叫做adapted policy**\n- 上一项提到的两次更新都是对元策略的参数$\\theta$进行更新，区别是，第一次更新将更新后的参数赋值给了行为策略，未更改元策略本身，第二次更新直接更新元策略本身\n- $\\alpha, \\beta$为两部分更新的学习率\n- 行为策略使用VPG，即传统策略梯度算法进行优化，元策略使用TRPO算法进行优化\n- 伪代码中的大致流程如下：\n  1. 初始化策略$\\pi_{\\theta}$并将其复制K份$\\pi_{\\theta_{1}^{\\prime}}, \\dots, \\pi_{\\boldsymbol{\\theta}_{K}^{\\prime}}$\n  2. 使用$\\pi_{\\theta_{1}^{\\prime}}, \\dots, \\pi_{\\boldsymbol{\\theta}_{K}^{\\prime}}$对**真实的环境模型进行采样（这一步是实际交互，即真实数据）**，将数据存入经验池\n  3. 根据经验池训练K个环境模型，即使用`学习环境动态模型`部分的公式\n  4. 对于每个更新后的环境模型，用元策略$\\color{red}{\\pi_{\\theta}}$进行**虚拟采样（这一步是预测采样，即不实际进行交互）**，采样到$\\mathcal{T}_{k}$以适应性修改行为策略$\\boldsymbol{\\theta}_{k}^{\\prime}$。这里也是前边提到的行为策略更新是不连贯的原因。\n  5. 再用适应性策略$\\boldsymbol{\\theta}_{k}^{\\prime}$进行**虚拟采样**，采样到$\\mathcal{T}_{k}^{\\prime}$以更新元策略$\\pi_{\\theta}$\n  6. 跳向第2步\n- 伪代码中虽然没有明确指出，但是其实使用了baseline的trick用来减少方差\n\n## 流程示意图\n\n![](./rl-rough-reading/mb-mpo-visio.png)\n\n## 效果\n\n1. 比之前的model-based方法效果好、收敛快\n2. 可以达到model-free算法的渐进性能\n3. 需要更少的经验，低采样复杂性。其实是使用了虚拟采样，提高了数据效率，减少了交互采样的代价。\n4. 对于模型偏差（model-bias，即环境模型没学到位）的情况，之前的算法不能有效处理，该算法对不完美、不完全、不完整的模型具有很好地鲁棒性\n\n<h1 align=\"center\" style=\"color:blue\" id=\"DIAYN\">[UCB/Google AI]Diversity is All Your Need: Learning Skills Without a Reward Function[DIAYN]</h1>\n\n论文地址：[https://arxiv.org/pdf/1802.06070.pdf](https://arxiv.org/pdf/1802.06070.pdf)\n\nGoogle网页：[https://sites.google.com/view/diayn/home](https://sites.google.com/view/diayn/home)\n\nGithub项目：[https://github.com/ben-eysenbach/sac/blob/master/DIAYN.md](https://github.com/ben-eysenbach/sac/blob/master/DIAYN.md)\n\n这篇文章使用信息论中最大熵的方法来构造强化学习的学习目标，**期望学习到具有多样性的技能（skills）**。\n\n个人认为，此文章中所提的方法虽然很新颖，但是不能作为优化一项任务的可用算法，因为虽然其可以学到以各种花样完成目标，但是没有奖励函数的控制使得无法规范、指引智能体“解题”过程的效果，如柔顺性、实用性、实际可行性等。从另一方面来讲，将这样虽然不规划、不严谨决策行为的策略用于元策略的预训练模式还是可用的。\n\n## 技能\n\n技能的定义在文中有如下表述：\n\n> A skill is a latent-conditioned policy that alters that state of the environment in a consistent\n> way. \n> we refer to a/the policy conditioned on a fixed Z as a “skill” .\n\n意思是，设定一个隐变量，以（状态$S$，隐变量$Z$）为条件进行动作选择，即为技能——skill。\n\nDIAYN就好像是要给每个状态赋予各个不同技能的概率，并且使其中一个技能的概率最大，这样就使得在整个状态空间中，不同的技能“占领”着状态空间的不同部分，每个技能在各自偏好的局部状态空间中作用，但是作者同样希望每个技能在各自的状态空间中尽可能随机决策。\n\n![](./rl-rough-reading/skill.png)\n\n假设以不同的颜色代表不同的技能skill，每个方格代表一个状态，那么每个状态对于每个技能到达此状态的“偏好”概率是不同的。总的来说，作者希望技能之间的重合度尽可能小，但每个技能在各自的领域内尽可能随机地完成目标。\n\n## 亮点与作用\n\n1. 去掉了奖励函数\n2. 修改了目标函数，$\\mathcal{F}(\\theta)\\triangleq \\mathcal{G}(\\color{red}{\\theta, \\phi})$\n   - $\\color{red}{\\theta}$代表Actor网络中的参数\n   - $\\color{red}{\\phi}$代表Critic网络中的参数\n3. 学习到的技能可以用于*分层强化学习、迁移学习、模仿学习*\n\n## 目标函数\n\n$$\n\\begin{aligned} \n\\mathcal{F}(\\theta) & \\triangleq \\color{red}{I(S ; Z)+\\mathcal{H}[A | S]-I(A ; Z | S)} \\\\ \n&=(\\mathcal{H}[Z]-\\mathcal{H}[Z | S])+\\mathcal{H}[A | S]-(\\mathcal{H}[A | S]-\\mathcal{H}[A | S, Z]) \\\\ \n&=\\color{blue}{\\mathcal{H}[Z]-\\mathcal{H}[Z | S]+\\mathcal{H}[A | S, Z]} \\\\\n&=\\mathcal{H}[A | S, Z]+\\mathbb{E}_{z \\sim p(z), s \\sim \\pi(z)}[\\log p(z | s)]-\\mathbb{E}_{z \\sim p(z)}[\\log p(z)] \\\\\n&{ \\color{orange}{\\geq} \\mathcal{H}[A | S, Z]+\\mathbb{E}_{z \\sim p(z), s \\sim \\pi(z)}\\left[\\log q_{\\phi}(z | s)-\\log p(z)\\right] \\\\ \n\\triangleq \\mathcal{G}(\\theta, \\phi)}\n\\end{aligned}\n$$\n\n解析：\n\n- 互信息，离散下为$I(X ; Y)=\\sum_{y \\in Y} \\sum_{x \\in X} p(x, y) \\log \\left(\\frac{p(x, y)}{p(x) p(y)}\\right)$，连续下为$I(X ; Y)=\\int_{Y} \\int_{X} p(x, y) \\log \\left(\\frac{p(x, y)}{p(x) p(y)}\\right) d x d y$\n\n- 信息熵表示为$H(X, Y)=-\\sum_{x, y} p(x, y) \\log p(x, y)=-\\sum_{i=1}^{n} \\sum_{j=1}^{m} p\\left(x_{i}, y_{i}\\right) \\log p\\left(x_{i}, y_{i}\\right)$\n\n- 推到中频繁使用了性质$I(X,Y)=H(X)-H(X | Y)$\n\n- 式中对数的底为自然指数$e$\n\n- 看红色部分，化简之前：\n\n  - **增大**：$I(S ; Z)$代表状态$S$与策略隐变量$Z$之间的互信息。因为作者希望可以通过策略所能到达的状态来判别其属于哪个技能，即将技能与状态挂钩。作者给出一个直观的解释：因为在有些状态下可以执行很多动作，但是却不改变环境（至少不明显改变），就像用机械手臂夹紧一个物体时，可使用力的大小、方向等都是很多的，不同技能选择不同动作导致的效果可能相同，所以作者不希望从动作的选择来区分学到的技能，而是通过可以明显观察到、数值化的状态$S$来作为区别不同技能的标准。**互信息$I(X,Y)$有一个直观的性质就是，它可以衡量两个随机变量的“相关性”，也就是说，互信息越大，代表知道$X$后对$Y$的不确定性减少，即知道其一可以加深对另一个的了解。**所以，目标函数希望最大化互信息$I(S ; Z)$，以将状态和技能相关联，使技能尽可能根据状态可以区分。\n  - **增大**：$\\mathcal{H}[A | S]$代表策略（不以隐变量$Z$区分技能，混合所有技能即为策略）的熵值。与SAC算法中想要使用熵增来使得动作的选择更加随机一样，作者希望随机性的动作同样可以完成目标，所以希望尽可能增大这一项。\n  - **减小**：$I(A ; Z | S)$代表动作$A$与策略隐变量$Z$在给定状态$S$时之间的互信息。为了避免歧义，应该写作为$I[(A ; Z) | S]$。作者希望技能根据状态可区分，而不是根据动作，所以需要最小化这一项。\n\n- 看蓝色部分，化简之后：\n\n  - **固定最大，为$\\ln n$**：$\\mathcal{H}[Z]$代表技能分布的不确定性，既然要最大化这个项，不如就固定它，使得技能从其中均匀采样，使熵为最大值。\n  - **减小**：$\\mathcal{H}[Z | S]$代表状态$S$条件下技能的不确定性，我们知道，熵越大，不确定性越大；熵越小，不确定性越小。作者希望技能根据状态可区分，可以需要使这一项最小，以减小给定状态下所属技能的不确定性，使其尽可能接近概率1。\n  - **增大**：$\\mathcal{H [ A | S}, Z ]$代表给定技能$(S,Z)$下动作的不确定性。因为作者希望动作的选择尽可能随机但又可以完成目标，所以需要最大化这一项。\n\n- 看橘色部分，使用Jensen不等式：\n\n  - 这一步推导使用了论文[《The IM Algorithm : A variational approach to Information Maximization》](https://pdfs.semanticscholar.org/f586/4b47b1d848e4426319a8bb28efeeaf55a52a.pdf)中的推导公式\n    $$\n    I(\\mathbf{x}, \\mathbf{y}) \\geq \\underbrace{H(\\mathbf{x})}_{\\text { ‘‘entrop’’ }}+\\underbrace{\\langle\\log q(\\mathbf{x} | \\mathbf{y})\\rangle_{p(\\mathbf{x}, \\mathbf{y})}}_{\\text { ‘‘energy’’ }} \\stackrel{\\mathrm{def}}{=} \\tilde{I}(\\mathbf{x}, \\mathbf{y})\n    $$\n\n  - ![](./rl-rough-reading/Agakov.png)\n\n  - 蓝色公式中，有$I(Z;S) = \\mathcal{H}[Z]-\\mathcal{H}[Z | S]$，可以应用上述性质进行推导，将真实分布$p(z | s)$替换为任意变分分布(variational distribution)$q(z | s)$\n\n  - 最后使用变分下界$\\mathcal{G}(\\theta, \\phi)$代替目标函数$\\mathcal{F}(\\theta)$\n\n- 至此，思路已经十分清晰。Actor网络以变量$\\theta$参数化，并使用SAC算法($\\alpha=0.1$)最大化$\\mathcal{G}(\\theta, \\phi)$中$\\mathcal{H}[A | S, Z]$部分；Critic网络以变量$\\phi$参数化，并最大化后半个期望部分。文中将期望内的元素定义为“伪奖励”：\n  $$\n  r_{z}(s, a) \\triangleq \\log q_{\\phi}(z | s)-\\log p(z)\n  $$\n  由于$p(z)$为均匀分布，是固定的；对数函数不改变原函数单调性，所以只需最大化$q_{\\phi}(z | s)$即可。\n\n## 伪代码\n\n![](./rl-rough-reading/diayn-pseudo.png)\n\n解析：\n\n- 每个episode都重新采样隐变量$z$\n- Actor网络的输入为$(S||Z)$，即状态与隐变量的连结(我猜的= =)\n- Critic网络的输入为状态$S$\n\n## 模型示意图\n\n![](./rl-rough-reading/diayn.png)\n\n解析：\n\n- 隐变量分布$p(z)$是固定的\n\n\n\n<h1 align=\"center\" style=\"color:blue\" id=\"CDP\">Curiosity-Driven Experience Prioritization via Density Estimation[CDP]</h1>\n论文地址：[https://arxiv.org/pdf/1902.08039.pdf](https://arxiv.org/pdf/1902.08039.pdf)\n\n这篇文章发于2018年的NIPS，作者为赵瑞，之前读过他的两篇论文，并写了博客，可以在论文精读里找，此处不贴链接了，分别是基于能量的HER和最大熵正则化多目标RL。\n\n这篇文章总的来说提出了**基于迹密度的优先经验回放**，人类的好奇心机制驱动他有了这样的想法，文中说受有监督学习使用过采样和降采样解决训练集样本不平衡问题的启发，想在强化学习中解决经验池中迹“不足（under-represented）”的问题。\n\n说起来也挺佩服这个作者的，目前（2019年6月21日14:59:05）总共发了三篇关于强化学习的论文，但都有很好地结果：\n\n1. 基于迹能量的优先级，发了CoRL\n2. 基于迹密度的优先级，也就是这篇，发了NIPS\n3. 基于迹最大熵的优先级，发了ICML\n\n我个人道行尚浅，对于几篇论文中的深奥精髓有些不能尽数参透，由于先验知识不足，对于文中内容也不敢完全苟同，但是从这几篇阅读总结下来，发得了这种高级别论文有以下几个“加分性”要求：\n\n1. 数学要好，这是必然的，数学公式写的越华丽，数学模型越复杂，当然越具有吸引力\n2. 工作要专一且连续，看这三篇论文虽然不是递进关系，但都是在解决经验池优先相关的工作，所以找准一个领域内的小角度也是很重要的\n3. 实验部分要做好，三篇都没用完整地、细节地比较各个算法，但是却新奇地比较了采样复杂性、数据利用率等等，总之，一定要用实验表明自己的方法在某方面有用\n4. 其他秘密因素\n\n## 流程\n\n这篇论文的方法流程如下：\n\n1. 计算**迹密度**$\\rho$\n2. 计算迹密度的补$\\overline{\\rho} \\propto 1-\\rho$\n3. 根据补排序，并设置优先级，补越大优先级越大\n4. 使用HER补充经验，设置相同的优先级和迹密度\n5. 优化算法\n\n## 迹密度的计算\n\n这一部分没有看太懂，主要是本人数学功底比较薄弱，感兴趣的可以亲自查看论文中2.4与3.2.1、3.2.2部分。\n\n根据文中的意思，思想大致如下：\n\n1. 用GMM（高斯混合模型）来估计迹密度\n   $$\n   \\rho(\\mathbf{x})=\\sum_{k=1}^{K} c_{k} \\mathcal{N}\\left(\\mathbf{x} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)\n   $$\n\n2. 在每个epoch，使用V-GMM（GMM的一个变体）+EM算法推断GMM参数($\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}$)的后验分布\n\n3. 在每个episode，使用如下公式计算迹密度\n   $$\n   \\rho=\\mathrm{V}-\\operatorname{GMM}(\\mathcal{T})=\\sum_{k=1}^{K} c_{k} \\mathcal{N}\\left(\\mathcal{T} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)\n   $$\n   其中，$\\mathcal{T}=\\left(s_{0}\\left\\|s_{1}\\right\\| \\ldots \\| s_{T}\\right)$，**每个迹的长度相同**，中间的符号代表连结操作的意思，然后进行归一化\n   $$\n   \\rho_{i}=\\frac{\\rho_{i}}{\\sum_{n=1}^{N} \\rho_{n}}\n   $$\n\n*注：我猜想上边符号表示的$s$其实包含了智能体的所在状态和要达到的真实目标，也就是$(s,g)$，文中有一段可能解释了这一部分，但是我没有太理解。*\n\n![](./rl-rough-reading/cdp-sg.png)\n\n## 优先级的设定\n\n作者说使用rank-based方法来设置优先级，因为其受异常点影响小而具有良好的鲁棒性。\n\n先计算迹密度的补\n$$\n\\overline{\\rho} \\propto 1-\\rho\n$$\n将补从小到大排序，并根据排名计算优先级，排名从0开始，即\n$$\n\\operatorname{rank}(\\cdot) \\in\\{0,1, \\ldots, N-1\\}\n$$\n\n$$\np\\left(\\mathcal{T}_{i}\\right)=\\frac{\\operatorname{rank}\\left(\\overline{\\rho}\\left(\\mathcal{T}_{i}\\right)\\right)}{\\sum_{n=1}^{N} \\operatorname{rank}\\left(\\left(\\overline{\\rho}\\left(\\mathcal{T}_{n}\\right)\\right)\\right.}\n$$\n\n## 伪代码\n\n![](./rl-rough-reading/cdp-pseudo.png)\n\n解析：\n\n- 每个epoch根据经验池中的样本数据重新拟合一次密度模型，也就是GMM中的参数\n- 每个episode都计算其迹密度\n- 红色框中的公式数字编号分别代表之间部分中关于计算迹密度和迹优先级的公式\n- 采样迹、采样经验转换之后，需要采样目标并存入经验池，重构后的经验其优先级及迹密度与真正目标下迹的相同\n\n## 优点\n\n实验部分的比较详见论文。\n\n1. 可以适用于任何Off-Policy算法\n2. 不使用TD-error计算优先级，而使用迹密度，减少了计算时间\n3. 提升了采样效率两倍左右\n4. 算法性能超过最新算法9%（这个结果看看即可，不必放在心上）\n\n<h1 align=\"center\" style=\"color:blue\" id=\"NAF\">[Google]Continuous Deep Q-Learning with Model-based Acceleration[NAF]</h1>\n论文地址：[https://arxiv.org/abs/1603.00748](https://arxiv.org/abs/1603.00748)\n\n本文介绍了标准化优势函数Normalized Advantage Function——NAF算法，该算法简化了A-C架构，将Q-Learning的思想应用于高维连续空间。\n\n本文的主要贡献是：\n\n1. 提出NAF，简化了Actor-Critic架构\n2. 将Q-Learning推广至高维连续空间\n3. 提出新的与已学模型结合的方法，提升了采样复杂性（也就是降低）和学习效率，不牺牲策略的最优性。原文中翻译意思是，评估了几种将已学习模型与Q-Learning结合的方案，提出将局部线性模型与局部On-Policy想定推演结合以加速Q-Learning算法在model-free、连续问题下的学习\n\n## 伪代码\n\n![](./rl-rough-reading/naf-pseudo.png)\n\n解析：\n\n- 。","slug":"rl-rough-reading","published":1,"updated":"2019-10-08T08:30:31.087Z","_id":"cjxd6ma84003mekvelx7qeeiy","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本文记录了一些粗读的强化学习相关的论文。</p>\n<a id=\"more\"></a>\n<h1 align=\"center\" style=\"color:blue\" id=\"Gorila\">[DeepMind]Massively Parallel Methods for Deep Reinforcement Learning[Gorila]</h1>\n\n<p>本文提出了一个分布式强化学习训练的架构：Gorila(General Reinforcement Learning Architecture)。2015年发于ICML，本文使用DQN算法进行分布式实现。</p>\n<p>论文地址：<a href=\"https://arxiv.org/pdf/1507.04296.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1507.04296.pdf</a></p>\n<h2 id=\"模型示意图\"><a href=\"#模型示意图\" class=\"headerlink\" title=\"模型示意图\"></a>模型示意图</h2><p><img src=\"./rl-rough-reading/gorila.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>shard代表参数分片的意思，即模型过大、参数过多，需要将参数分片放置多台机器上</li>\n<li>Bundled Mode模式指的是Actor中的Q网络与Learner中的Q网络一样，但是Learner比Actor多了一个目标Q网络，用于计算梯度</li>\n</ul>\n<h2 id=\"特点\"><a href=\"#特点\" class=\"headerlink\" title=\"特点\"></a>特点</h2><ul>\n<li>并行Actor采数据</li>\n<li>并行Learner计算梯度，<strong>不更新Learner中的模型</strong></li>\n<li>中心参数服务器，用于维持最新的网络模型。如果模型太大、参数过多，可以分片将网络模型放置多个参数服务器，每个参数服务器中的参数独立不关联，根据learner传的梯度更新相应的变量</li>\n<li>经验池机制，分为local与global两种<ul>\n<li>local，即每个actor节点一个经验池</li>\n<li>global，将所有actor节点的经验存至一个分布式数据库中，这个<strong>需要网络通信开销</strong></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-rough-reading/gorila-pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li><p>伪代码中为一个actor节点的流程</p>\n</li>\n<li><p>注意伪代码中出现两次<code>Update θ from parameters θ+ of the parameter server</code>，这句话的意思为从中心参数服务器拉取模型到actor和learner，拉取的时间点为：</p>\n<ul>\n<li>每个episode开始前</li>\n<li>每次执行动作$a_{t}$后，但是在计算梯度并将梯度传递至参数服务器之前</li>\n</ul>\n</li>\n<li><p>伪代码中<code>equation 2</code>，代表$g_{i}=\\left(r+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime} ; \\theta_{i}^{-}\\right)-Q\\left(s, a ; \\theta_{i}\\right)\\right) \\nabla_{\\theta_{i}} Q(s, a ; \\theta)$，这是DQN中的损失函数</p>\n</li>\n<li><p>注意，与传统DQN不同的是，<strong>该分布式DQN中给Learner中的目标Q网络赋值时，是直接将更新N次的中心参数服务器中的模型进行拉取覆盖，而不是使用Learner中的Q网络</strong></p>\n</li>\n<li><p>中心参数服务器中的参数梯度更新需要累计多个learner传来的梯度后进行更新，使用异步SGD即ASGD方法进行梯度下降。</p>\n<ul>\n<li><blockquote>\n<p>The parameter server then applies the updates that are accumulated from many learners. </p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p>因为每个actor都是阶段更新自己的模型，即从参数服务器中拉取。所以每个actor中的行为策略（采样策略）都不完全相同，事实上，每个actor节点可以采取不同的探索机制，这样可以更有效地探索环境</p>\n</li>\n</ul>\n<h2 id=\"稳定性\"><a href=\"#稳定性\" class=\"headerlink\" title=\"稳定性\"></a>稳定性</h2><p>为了应对节点退出、网速慢、节点机器运行慢等问题，该文章中指出使用了一个超参数用来控制actor和server之间最大延时。</p>\n<ul>\n<li><p>过时的梯度（低于时间阈值）将会被丢弃</p>\n<ul>\n<li><blockquote>\n<p>All gradients older than the threshold are discarded by the parameter server. </p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p>过高或过低的梯度也将被丢弃</p>\n<ul>\n<li><blockquote>\n<p>each actor/learner keeps a running average and standard deviation of the absolute DQN loss for the data it sees and discards gradients with absolute loss higher than the mean plus several standard deviations. </p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p>使用AdaGrad更新规则</p>\n</li>\n</ul>\n<h2 id=\"效果\"><a href=\"#效果\" class=\"headerlink\" title=\"效果\"></a>效果</h2><p>采用于提出DQN的论文中一样的网络结构，具体请见论文中第5部分。</p>\n<p>在Atari 2600 49个游戏中，41个明显优于单GPU DQN。</p>\n<p>Gorila进一步实现了DRL的希望：一个可伸缩的架构，随着计算和内存的增加，它的性能会越来越好</p>\n<h1 align=\"center\" style=\"color:blue\" id=\"MB-MPO\">[UCB/OpenAI]Model-Based Reinforcement Learning via Meta-Policy Optimization[MB-MPO]</h1>\n\n<p>论文地址：<a href=\"https://arxiv.org/pdf/1809.05214.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1809.05214.pdf</a></p>\n<p>本文2018年发布于CoRL，提出了一个基于模型的元强化学习算法MB-MPO。相比于一般的元强化学习是从多个MDPs任务中学习一个通用模型加速以后特定任务的模型训练，该文中的方法是将一个model-free的任务学习多个不确定、不完全、不完美的动态模型，即一个模型集合，然后使用这个模型集合学习出该任务的通用模型。因为它有一个从model-free学习动态模型的过程，所以为model-based方法。</p>\n<h2 id=\"元强化学习\"><a href=\"#元强化学习\" class=\"headerlink\" title=\"元强化学习\"></a>元强化学习</h2><script type=\"math/tex; mode=display\">\n\\max _{\\theta} \\mathbb{E}_{\\mathcal{M}_{k} \\sim \\rho(\\mathcal{M}),\\boldsymbol{s}_{t+1} \\sim p_{k},\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}^{\\prime}}\\left(\\boldsymbol{a}_{t} | \\boldsymbol{s}_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r_{k}\\left(s_{t}, a_{t}\\right)\\right] \\\\ s.t.:\\boldsymbol{\\theta}^{\\prime}=\\boldsymbol{\\theta}+\\alpha\\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}_{\\boldsymbol{s}_{t+1} \\sim p_{k},\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{a}_{t} | \\boldsymbol{s}_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r_{k}\\left(s_{t}, a_{t}\\right)\\right]</script><p>$\\mathcal{M}$为一系列MDP，共享相同的状态空间$\\mathcal{S}$与动作空间$\\mathcal{A}$，但是奖励函数可以不同</p>\n<h2 id=\"学习环境动态模型\"><a href=\"#学习环境动态模型\" class=\"headerlink\" title=\"学习环境动态模型\"></a>学习环境动态模型</h2><script type=\"math/tex; mode=display\">\n\\min _{\\boldsymbol{\\phi}_{k}} \\frac{1}{\\left|\\mathcal{D}_{k}\\right|} \\sum_{\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}, \\boldsymbol{s}_{t+1}\\right) \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{s}_{t+1}-\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)\\right\\|_{2}^{2}</script><p>解析：</p>\n<ul>\n<li><p>$\\mathcal{D}_{k}$为第k个学习模型采样的“经验”</p>\n</li>\n<li><p>$\\phi$为用神经网络表示的环境模型的参数</p>\n</li>\n<li><p>$\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)$为第k个学习模型针对状态$s_{t}$执行动作$a_{t}$后转移状态的预测，其中，神经网络的输出不直接是预测的状态$\\color{red}{s_{t+1}}$，而是$\\color{red}{\\Delta s=s_{t+1}-s_{t}}$，所以$\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)=s_{t}+\\Delta s$</p>\n<ul>\n<li><blockquote>\n<p>We follow the standard practice in model-based RL of training the neural network to predict the change in state $\\Delta s=s_{t+1}-s_{t}$ (rather than the next state $s_{t+1}$) </p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<p>为了防止过拟合，文中使用了3个trick：</p>\n<ol>\n<li>早停</li>\n<li>归一化神经网络输入与输出</li>\n<li>权重归一化</li>\n</ol>\n<h2 id=\"基于环境动态模型的元强化学习\"><a href=\"#基于环境动态模型的元强化学习\" class=\"headerlink\" title=\"基于环境动态模型的元强化学习\"></a>基于环境动态模型的元强化学习</h2><p>假设学到了K个近似模型$\\left\\{\\hat{f}_{\\phi_{1}}, \\hat{f}_{\\phi_{2}}, \\ldots, \\hat{f}_{\\phi_{K}}\\right\\}$，把每个模型转换成一个MDP过程，即$\\mathcal{M}_{k}=\\left(S, A, \\hat{f}_{\\phi_{k}}, r, \\gamma, p_{0}\\right)$，其中，<strong>奖励函数相同</strong></p>\n<p>由此给每个学习到的动态模型分配的行为策略目标函数为：</p>\n<script type=\"math/tex; mode=display\">\nJ_{k}(\\boldsymbol{\\theta})=\\mathbb{E}_{\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{a}_{t} | s_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right) | \\boldsymbol{s}_{t+1}=\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)\\right]</script><p>定义MB=MPO的最终目标函数为：</p>\n<script type=\"math/tex; mode=display\">\n\\max _{\\boldsymbol{\\theta}} \\frac{1}{K} \\sum_{k=0}^{K} J_{k}\\left(\\boldsymbol{\\theta}_{k}^{\\prime}\\right) \\quad \\text { s.t.: } \\quad \\boldsymbol{\\theta}_{k}^{\\prime}=\\boldsymbol{\\theta}+\\alpha \\nabla_{\\boldsymbol{\\theta}} J_{k}(\\boldsymbol{\\theta})</script><p>小写k代表第k个学到的模型，大写K代表模型的总数。</p>\n<p>注意看，这里公式后边使用的是$\\color{red}{\\theta’_{k}}$，而不是$\\theta$。这里并没有写错，我起初以为写错了，具体请看后边的伪代码解释。</p>\n<h2 id=\"伪代码-1\"><a href=\"#伪代码-1\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-rough-reading/mb-mpo-pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>MB-MPO分为两部分更新，第一部分更新每个模型分配的行为策略，第二部分更新元策略。<strong>注意：行为策略的更新是不连贯的，即不是自身迭代，而是不断使用元策略进行稍加修改然后替换，所以叫做adapted policy</strong></li>\n<li>上一项提到的两次更新都是对元策略的参数$\\theta$进行更新，区别是，第一次更新将更新后的参数赋值给了行为策略，未更改元策略本身，第二次更新直接更新元策略本身</li>\n<li>$\\alpha, \\beta$为两部分更新的学习率</li>\n<li>行为策略使用VPG，即传统策略梯度算法进行优化，元策略使用TRPO算法进行优化</li>\n<li>伪代码中的大致流程如下：<ol>\n<li>初始化策略$\\pi_{\\theta}$并将其复制K份$\\pi_{\\theta_{1}^{\\prime}}, \\dots, \\pi_{\\boldsymbol{\\theta}_{K}^{\\prime}}$</li>\n<li>使用$\\pi_{\\theta_{1}^{\\prime}}, \\dots, \\pi_{\\boldsymbol{\\theta}_{K}^{\\prime}}$对<strong>真实的环境模型进行采样（这一步是实际交互，即真实数据）</strong>，将数据存入经验池</li>\n<li>根据经验池训练K个环境模型，即使用<code>学习环境动态模型</code>部分的公式</li>\n<li>对于每个更新后的环境模型，用元策略$\\color{red}{\\pi_{\\theta}}$进行<strong>虚拟采样（这一步是预测采样，即不实际进行交互）</strong>，采样到$\\mathcal{T}_{k}$以适应性修改行为策略$\\boldsymbol{\\theta}_{k}^{\\prime}$。这里也是前边提到的行为策略更新是不连贯的原因。</li>\n<li>再用适应性策略$\\boldsymbol{\\theta}_{k}^{\\prime}$进行<strong>虚拟采样</strong>，采样到$\\mathcal{T}_{k}^{\\prime}$以更新元策略$\\pi_{\\theta}$</li>\n<li>跳向第2步</li>\n</ol>\n</li>\n<li>伪代码中虽然没有明确指出，但是其实使用了baseline的trick用来减少方差</li>\n</ul>\n<h2 id=\"流程示意图\"><a href=\"#流程示意图\" class=\"headerlink\" title=\"流程示意图\"></a>流程示意图</h2><p><img src=\"./rl-rough-reading/mb-mpo-visio.png\" alt=\"\"></p>\n<h2 id=\"效果-1\"><a href=\"#效果-1\" class=\"headerlink\" title=\"效果\"></a>效果</h2><ol>\n<li>比之前的model-based方法效果好、收敛快</li>\n<li>可以达到model-free算法的渐进性能</li>\n<li>需要更少的经验，低采样复杂性。其实是使用了虚拟采样，提高了数据效率，减少了交互采样的代价。</li>\n<li>对于模型偏差（model-bias，即环境模型没学到位）的情况，之前的算法不能有效处理，该算法对不完美、不完全、不完整的模型具有很好地鲁棒性</li>\n</ol>\n<h1 align=\"center\" style=\"color:blue\" id=\"DIAYN\">[UCB/Google AI]Diversity is All Your Need: Learning Skills Without a Reward Function[DIAYN]</h1>\n\n<p>论文地址：<a href=\"https://arxiv.org/pdf/1802.06070.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1802.06070.pdf</a></p>\n<p>Google网页：<a href=\"https://sites.google.com/view/diayn/home\" rel=\"external nofollow\" target=\"_blank\">https://sites.google.com/view/diayn/home</a></p>\n<p>Github项目：<a href=\"https://github.com/ben-eysenbach/sac/blob/master/DIAYN.md\" rel=\"external nofollow\" target=\"_blank\">https://github.com/ben-eysenbach/sac/blob/master/DIAYN.md</a></p>\n<p>这篇文章使用信息论中最大熵的方法来构造强化学习的学习目标，<strong>期望学习到具有多样性的技能（skills）</strong>。</p>\n<p>个人认为，此文章中所提的方法虽然很新颖，但是不能作为优化一项任务的可用算法，因为虽然其可以学到以各种花样完成目标，但是没有奖励函数的控制使得无法规范、指引智能体“解题”过程的效果，如柔顺性、实用性、实际可行性等。从另一方面来讲，将这样虽然不规划、不严谨决策行为的策略用于元策略的预训练模式还是可用的。</p>\n<h2 id=\"技能\"><a href=\"#技能\" class=\"headerlink\" title=\"技能\"></a>技能</h2><p>技能的定义在文中有如下表述：</p>\n<blockquote>\n<p>A skill is a latent-conditioned policy that alters that state of the environment in a consistent<br>way.<br>we refer to a/the policy conditioned on a fixed Z as a “skill” .</p>\n</blockquote>\n<p>意思是，设定一个隐变量，以（状态$S$，隐变量$Z$）为条件进行动作选择，即为技能——skill。</p>\n<p>DIAYN就好像是要给每个状态赋予各个不同技能的概率，并且使其中一个技能的概率最大，这样就使得在整个状态空间中，不同的技能“占领”着状态空间的不同部分，每个技能在各自偏好的局部状态空间中作用，但是作者同样希望每个技能在各自的状态空间中尽可能随机决策。</p>\n<p><img src=\"./rl-rough-reading/skill.png\" alt=\"\"></p>\n<p>假设以不同的颜色代表不同的技能skill，每个方格代表一个状态，那么每个状态对于每个技能到达此状态的“偏好”概率是不同的。总的来说，作者希望技能之间的重合度尽可能小，但每个技能在各自的领域内尽可能随机地完成目标。</p>\n<h2 id=\"亮点与作用\"><a href=\"#亮点与作用\" class=\"headerlink\" title=\"亮点与作用\"></a>亮点与作用</h2><ol>\n<li>去掉了奖励函数</li>\n<li>修改了目标函数，$\\mathcal{F}(\\theta)\\triangleq \\mathcal{G}(\\color{red}{\\theta, \\phi})$<ul>\n<li>$\\color{red}{\\theta}$代表Actor网络中的参数</li>\n<li>$\\color{red}{\\phi}$代表Critic网络中的参数</li>\n</ul>\n</li>\n<li>学习到的技能可以用于<em>分层强化学习、迁移学习、模仿学习</em></li>\n</ol>\n<h2 id=\"目标函数\"><a href=\"#目标函数\" class=\"headerlink\" title=\"目标函数\"></a>目标函数</h2><script type=\"math/tex; mode=display\">\n\\begin{aligned} \n\\mathcal{F}(\\theta) & \\triangleq \\color{red}{I(S ; Z)+\\mathcal{H}[A | S]-I(A ; Z | S)} \\\\ \n&=(\\mathcal{H}[Z]-\\mathcal{H}[Z | S])+\\mathcal{H}[A | S]-(\\mathcal{H}[A | S]-\\mathcal{H}[A | S, Z]) \\\\ \n&=\\color{blue}{\\mathcal{H}[Z]-\\mathcal{H}[Z | S]+\\mathcal{H}[A | S, Z]} \\\\\n&=\\mathcal{H}[A | S, Z]+\\mathbb{E}_{z \\sim p(z), s \\sim \\pi(z)}[\\log p(z | s)]-\\mathbb{E}_{z \\sim p(z)}[\\log p(z)] \\\\\n&{ \\color{orange}{\\geq} \\mathcal{H}[A | S, Z]+\\mathbb{E}_{z \\sim p(z), s \\sim \\pi(z)}\\left[\\log q_{\\phi}(z | s)-\\log p(z)\\right] \\\\ \n\\triangleq \\mathcal{G}(\\theta, \\phi)}\n\\end{aligned}</script><p>解析：</p>\n<ul>\n<li><p>互信息，离散下为$I(X ; Y)=\\sum_{y \\in Y} \\sum_{x \\in X} p(x, y) \\log \\left(\\frac{p(x, y)}{p(x) p(y)}\\right)$，连续下为$I(X ; Y)=\\int_{Y} \\int_{X} p(x, y) \\log \\left(\\frac{p(x, y)}{p(x) p(y)}\\right) d x d y$</p>\n</li>\n<li><p>信息熵表示为$H(X, Y)=-\\sum_{x, y} p(x, y) \\log p(x, y)=-\\sum_{i=1}^{n} \\sum_{j=1}^{m} p\\left(x_{i}, y_{i}\\right) \\log p\\left(x_{i}, y_{i}\\right)$</p>\n</li>\n<li><p>推到中频繁使用了性质$I(X,Y)=H(X)-H(X | Y)$</p>\n</li>\n<li><p>式中对数的底为自然指数$e$</p>\n</li>\n<li><p>看红色部分，化简之前：</p>\n<ul>\n<li><strong>增大</strong>：$I(S ; Z)$代表状态$S$与策略隐变量$Z$之间的互信息。因为作者希望可以通过策略所能到达的状态来判别其属于哪个技能，即将技能与状态挂钩。作者给出一个直观的解释：因为在有些状态下可以执行很多动作，但是却不改变环境（至少不明显改变），就像用机械手臂夹紧一个物体时，可使用力的大小、方向等都是很多的，不同技能选择不同动作导致的效果可能相同，所以作者不希望从动作的选择来区分学到的技能，而是通过可以明显观察到、数值化的状态$S$来作为区别不同技能的标准。<strong>互信息$I(X,Y)$有一个直观的性质就是，它可以衡量两个随机变量的“相关性”，也就是说，互信息越大，代表知道$X$后对$Y$的不确定性减少，即知道其一可以加深对另一个的了解。</strong>所以，目标函数希望最大化互信息$I(S ; Z)$，以将状态和技能相关联，使技能尽可能根据状态可以区分。</li>\n<li><strong>增大</strong>：$\\mathcal{H}[A | S]$代表策略（不以隐变量$Z$区分技能，混合所有技能即为策略）的熵值。与SAC算法中想要使用熵增来使得动作的选择更加随机一样，作者希望随机性的动作同样可以完成目标，所以希望尽可能增大这一项。</li>\n<li><strong>减小</strong>：$I(A ; Z | S)$代表动作$A$与策略隐变量$Z$在给定状态$S$时之间的互信息。为了避免歧义，应该写作为$I[(A ; Z) | S]$。作者希望技能根据状态可区分，而不是根据动作，所以需要最小化这一项。</li>\n</ul>\n</li>\n<li><p>看蓝色部分，化简之后：</p>\n<ul>\n<li><strong>固定最大，为$\\ln n$</strong>：$\\mathcal{H}[Z]$代表技能分布的不确定性，既然要最大化这个项，不如就固定它，使得技能从其中均匀采样，使熵为最大值。</li>\n<li><strong>减小</strong>：$\\mathcal{H}[Z | S]$代表状态$S$条件下技能的不确定性，我们知道，熵越大，不确定性越大；熵越小，不确定性越小。作者希望技能根据状态可区分，可以需要使这一项最小，以减小给定状态下所属技能的不确定性，使其尽可能接近概率1。</li>\n<li><strong>增大</strong>：$\\mathcal{H [ A | S}, Z ]$代表给定技能$(S,Z)$下动作的不确定性。因为作者希望动作的选择尽可能随机但又可以完成目标，所以需要最大化这一项。</li>\n</ul>\n</li>\n<li><p>看橘色部分，使用Jensen不等式：</p>\n<ul>\n<li><p>这一步推导使用了论文<a href=\"https://pdfs.semanticscholar.org/f586/4b47b1d848e4426319a8bb28efeeaf55a52a.pdf\" rel=\"external nofollow\" target=\"_blank\">《The IM Algorithm : A variational approach to Information Maximization》</a>中的推导公式</p>\n<script type=\"math/tex; mode=display\">\nI(\\mathbf{x}, \\mathbf{y}) \\geq \\underbrace{H(\\mathbf{x})}_{\\text { ‘‘entrop’’ }}+\\underbrace{\\langle\\log q(\\mathbf{x} | \\mathbf{y})\\rangle_{p(\\mathbf{x}, \\mathbf{y})}}_{\\text { ‘‘energy’’ }} \\stackrel{\\mathrm{def}}{=} \\tilde{I}(\\mathbf{x}, \\mathbf{y})</script></li>\n<li><p><img src=\"./rl-rough-reading/Agakov.png\" alt=\"\"></p>\n</li>\n<li><p>蓝色公式中，有$I(Z;S) = \\mathcal{H}[Z]-\\mathcal{H}[Z | S]$，可以应用上述性质进行推导，将真实分布$p(z | s)$替换为任意变分分布(variational distribution)$q(z | s)$</p>\n</li>\n<li><p>最后使用变分下界$\\mathcal{G}(\\theta, \\phi)$代替目标函数$\\mathcal{F}(\\theta)$</p>\n</li>\n</ul>\n</li>\n<li><p>至此，思路已经十分清晰。Actor网络以变量$\\theta$参数化，并使用SAC算法($\\alpha=0.1$)最大化$\\mathcal{G}(\\theta, \\phi)$中$\\mathcal{H}[A | S, Z]$部分；Critic网络以变量$\\phi$参数化，并最大化后半个期望部分。文中将期望内的元素定义为“伪奖励”：</p>\n<script type=\"math/tex; mode=display\">\nr_{z}(s, a) \\triangleq \\log q_{\\phi}(z | s)-\\log p(z)</script><p>由于$p(z)$为均匀分布，是固定的；对数函数不改变原函数单调性，所以只需最大化$q_{\\phi}(z | s)$即可。</p>\n</li>\n</ul>\n<h2 id=\"伪代码-2\"><a href=\"#伪代码-2\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-rough-reading/diayn-pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>每个episode都重新采样隐变量$z$</li>\n<li>Actor网络的输入为$(S||Z)$，即状态与隐变量的连结(我猜的= =)</li>\n<li>Critic网络的输入为状态$S$</li>\n</ul>\n<h2 id=\"模型示意图-1\"><a href=\"#模型示意图-1\" class=\"headerlink\" title=\"模型示意图\"></a>模型示意图</h2><p><img src=\"./rl-rough-reading/diayn.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>隐变量分布$p(z)$是固定的</li>\n</ul>\n<p></p><h1 align=\"center\" style=\"color:blue\" id=\"CDP\">Curiosity-Driven Experience Prioritization via Density Estimation[CDP]</h1><br>论文地址：<a href=\"https://arxiv.org/pdf/1902.08039.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1902.08039.pdf</a><p></p>\n<p>这篇文章发于2018年的NIPS，作者为赵瑞，之前读过他的两篇论文，并写了博客，可以在论文精读里找，此处不贴链接了，分别是基于能量的HER和最大熵正则化多目标RL。</p>\n<p>这篇文章总的来说提出了<strong>基于迹密度的优先经验回放</strong>，人类的好奇心机制驱动他有了这样的想法，文中说受有监督学习使用过采样和降采样解决训练集样本不平衡问题的启发，想在强化学习中解决经验池中迹“不足（under-represented）”的问题。</p>\n<p>说起来也挺佩服这个作者的，目前（2019年6月21日14:59:05）总共发了三篇关于强化学习的论文，但都有很好地结果：</p>\n<ol>\n<li>基于迹能量的优先级，发了CoRL</li>\n<li>基于迹密度的优先级，也就是这篇，发了NIPS</li>\n<li>基于迹最大熵的优先级，发了ICML</li>\n</ol>\n<p>我个人道行尚浅，对于几篇论文中的深奥精髓有些不能尽数参透，由于先验知识不足，对于文中内容也不敢完全苟同，但是从这几篇阅读总结下来，发得了这种高级别论文有以下几个“加分性”要求：</p>\n<ol>\n<li>数学要好，这是必然的，数学公式写的越华丽，数学模型越复杂，当然越具有吸引力</li>\n<li>工作要专一且连续，看这三篇论文虽然不是递进关系，但都是在解决经验池优先相关的工作，所以找准一个领域内的小角度也是很重要的</li>\n<li>实验部分要做好，三篇都没用完整地、细节地比较各个算法，但是却新奇地比较了采样复杂性、数据利用率等等，总之，一定要用实验表明自己的方法在某方面有用</li>\n<li>其他秘密因素</li>\n</ol>\n<h2 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a>流程</h2><p>这篇论文的方法流程如下：</p>\n<ol>\n<li>计算<strong>迹密度</strong>$\\rho$</li>\n<li>计算迹密度的补$\\overline{\\rho} \\propto 1-\\rho$</li>\n<li>根据补排序，并设置优先级，补越大优先级越大</li>\n<li>使用HER补充经验，设置相同的优先级和迹密度</li>\n<li>优化算法</li>\n</ol>\n<h2 id=\"迹密度的计算\"><a href=\"#迹密度的计算\" class=\"headerlink\" title=\"迹密度的计算\"></a>迹密度的计算</h2><p>这一部分没有看太懂，主要是本人数学功底比较薄弱，感兴趣的可以亲自查看论文中2.4与3.2.1、3.2.2部分。</p>\n<p>根据文中的意思，思想大致如下：</p>\n<ol>\n<li><p>用GMM（高斯混合模型）来估计迹密度</p>\n<script type=\"math/tex; mode=display\">\n\\rho(\\mathbf{x})=\\sum_{k=1}^{K} c_{k} \\mathcal{N}\\left(\\mathbf{x} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)</script></li>\n<li><p>在每个epoch，使用V-GMM（GMM的一个变体）+EM算法推断GMM参数($\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}$)的后验分布</p>\n</li>\n<li><p>在每个episode，使用如下公式计算迹密度</p>\n<script type=\"math/tex; mode=display\">\n\\rho=\\mathrm{V}-\\operatorname{GMM}(\\mathcal{T})=\\sum_{k=1}^{K} c_{k} \\mathcal{N}\\left(\\mathcal{T} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)</script><p>其中，$\\mathcal{T}=\\left(s_{0}\\left|s_{1}\\right| \\ldots | s_{T}\\right)$，<strong>每个迹的长度相同</strong>，中间的符号代表连结操作的意思，然后进行归一化</p>\n<script type=\"math/tex; mode=display\">\n\\rho_{i}=\\frac{\\rho_{i}}{\\sum_{n=1}^{N} \\rho_{n}}</script></li>\n</ol>\n<p><em>注：我猜想上边符号表示的$s$其实包含了智能体的所在状态和要达到的真实目标，也就是$(s,g)$，文中有一段可能解释了这一部分，但是我没有太理解。</em></p>\n<p><img src=\"./rl-rough-reading/cdp-sg.png\" alt=\"\"></p>\n<h2 id=\"优先级的设定\"><a href=\"#优先级的设定\" class=\"headerlink\" title=\"优先级的设定\"></a>优先级的设定</h2><p>作者说使用rank-based方法来设置优先级，因为其受异常点影响小而具有良好的鲁棒性。</p>\n<p>先计算迹密度的补</p>\n<script type=\"math/tex; mode=display\">\n\\overline{\\rho} \\propto 1-\\rho</script><p>将补从小到大排序，并根据排名计算优先级，排名从0开始，即</p>\n<script type=\"math/tex; mode=display\">\n\\operatorname{rank}(\\cdot) \\in\\{0,1, \\ldots, N-1\\}</script><script type=\"math/tex; mode=display\">\np\\left(\\mathcal{T}_{i}\\right)=\\frac{\\operatorname{rank}\\left(\\overline{\\rho}\\left(\\mathcal{T}_{i}\\right)\\right)}{\\sum_{n=1}^{N} \\operatorname{rank}\\left(\\left(\\overline{\\rho}\\left(\\mathcal{T}_{n}\\right)\\right)\\right.}</script><h2 id=\"伪代码-3\"><a href=\"#伪代码-3\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-rough-reading/cdp-pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>每个epoch根据经验池中的样本数据重新拟合一次密度模型，也就是GMM中的参数</li>\n<li>每个episode都计算其迹密度</li>\n<li>红色框中的公式数字编号分别代表之间部分中关于计算迹密度和迹优先级的公式</li>\n<li>采样迹、采样经验转换之后，需要采样目标并存入经验池，重构后的经验其优先级及迹密度与真正目标下迹的相同</li>\n</ul>\n<h2 id=\"优点\"><a href=\"#优点\" class=\"headerlink\" title=\"优点\"></a>优点</h2><p>实验部分的比较详见论文。</p>\n<ol>\n<li>可以适用于任何Off-Policy算法</li>\n<li>不使用TD-error计算优先级，而使用迹密度，减少了计算时间</li>\n<li>提升了采样效率两倍左右</li>\n<li>算法性能超过最新算法9%（这个结果看看即可，不必放在心上）</li>\n</ol>\n<p></p><h1 align=\"center\" style=\"color:blue\" id=\"NAF\">[Google]Continuous Deep Q-Learning with Model-based Acceleration[NAF]</h1><br>论文地址：<a href=\"https://arxiv.org/abs/1603.00748\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/abs/1603.00748</a><p></p>\n<p>本文介绍了标准化优势函数Normalized Advantage Function——NAF算法，该算法简化了A-C架构，将Q-Learning的思想应用于高维连续空间。</p>\n<p>本文的主要贡献是：</p>\n<ol>\n<li>提出NAF，简化了Actor-Critic架构</li>\n<li>将Q-Learning推广至高维连续空间</li>\n<li>提出新的与已学模型结合的方法，提升了采样复杂性（也就是降低）和学习效率，不牺牲策略的最优性。原文中翻译意思是，评估了几种将已学习模型与Q-Learning结合的方案，提出将局部线性模型与局部On-Policy想定推演结合以加速Q-Learning算法在model-free、连续问题下的学习</li>\n</ol>\n<h2 id=\"伪代码-4\"><a href=\"#伪代码-4\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-rough-reading/naf-pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>本文记录了一些粗读的强化学习相关的论文。</p>","more":"<h1 align=\"center\" style=\"color:blue\" id=\"Gorila\">[DeepMind]Massively Parallel Methods for Deep Reinforcement Learning[Gorila]</h1>\n\n<p>本文提出了一个分布式强化学习训练的架构：Gorila(General Reinforcement Learning Architecture)。2015年发于ICML，本文使用DQN算法进行分布式实现。</p>\n<p>论文地址：<a href=\"https://arxiv.org/pdf/1507.04296.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1507.04296.pdf</a></p>\n<h2 id=\"模型示意图\"><a href=\"#模型示意图\" class=\"headerlink\" title=\"模型示意图\"></a>模型示意图</h2><p><img src=\"./rl-rough-reading/gorila.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>shard代表参数分片的意思，即模型过大、参数过多，需要将参数分片放置多台机器上</li>\n<li>Bundled Mode模式指的是Actor中的Q网络与Learner中的Q网络一样，但是Learner比Actor多了一个目标Q网络，用于计算梯度</li>\n</ul>\n<h2 id=\"特点\"><a href=\"#特点\" class=\"headerlink\" title=\"特点\"></a>特点</h2><ul>\n<li>并行Actor采数据</li>\n<li>并行Learner计算梯度，<strong>不更新Learner中的模型</strong></li>\n<li>中心参数服务器，用于维持最新的网络模型。如果模型太大、参数过多，可以分片将网络模型放置多个参数服务器，每个参数服务器中的参数独立不关联，根据learner传的梯度更新相应的变量</li>\n<li>经验池机制，分为local与global两种<ul>\n<li>local，即每个actor节点一个经验池</li>\n<li>global，将所有actor节点的经验存至一个分布式数据库中，这个<strong>需要网络通信开销</strong></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-rough-reading/gorila-pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li><p>伪代码中为一个actor节点的流程</p>\n</li>\n<li><p>注意伪代码中出现两次<code>Update θ from parameters θ+ of the parameter server</code>，这句话的意思为从中心参数服务器拉取模型到actor和learner，拉取的时间点为：</p>\n<ul>\n<li>每个episode开始前</li>\n<li>每次执行动作$a_{t}$后，但是在计算梯度并将梯度传递至参数服务器之前</li>\n</ul>\n</li>\n<li><p>伪代码中<code>equation 2</code>，代表$g_{i}=\\left(r+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime} ; \\theta_{i}^{-}\\right)-Q\\left(s, a ; \\theta_{i}\\right)\\right) \\nabla_{\\theta_{i}} Q(s, a ; \\theta)$，这是DQN中的损失函数</p>\n</li>\n<li><p>注意，与传统DQN不同的是，<strong>该分布式DQN中给Learner中的目标Q网络赋值时，是直接将更新N次的中心参数服务器中的模型进行拉取覆盖，而不是使用Learner中的Q网络</strong></p>\n</li>\n<li><p>中心参数服务器中的参数梯度更新需要累计多个learner传来的梯度后进行更新，使用异步SGD即ASGD方法进行梯度下降。</p>\n<ul>\n<li><blockquote>\n<p>The parameter server then applies the updates that are accumulated from many learners. </p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p>因为每个actor都是阶段更新自己的模型，即从参数服务器中拉取。所以每个actor中的行为策略（采样策略）都不完全相同，事实上，每个actor节点可以采取不同的探索机制，这样可以更有效地探索环境</p>\n</li>\n</ul>\n<h2 id=\"稳定性\"><a href=\"#稳定性\" class=\"headerlink\" title=\"稳定性\"></a>稳定性</h2><p>为了应对节点退出、网速慢、节点机器运行慢等问题，该文章中指出使用了一个超参数用来控制actor和server之间最大延时。</p>\n<ul>\n<li><p>过时的梯度（低于时间阈值）将会被丢弃</p>\n<ul>\n<li><blockquote>\n<p>All gradients older than the threshold are discarded by the parameter server. </p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p>过高或过低的梯度也将被丢弃</p>\n<ul>\n<li><blockquote>\n<p>each actor/learner keeps a running average and standard deviation of the absolute DQN loss for the data it sees and discards gradients with absolute loss higher than the mean plus several standard deviations. </p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p>使用AdaGrad更新规则</p>\n</li>\n</ul>\n<h2 id=\"效果\"><a href=\"#效果\" class=\"headerlink\" title=\"效果\"></a>效果</h2><p>采用于提出DQN的论文中一样的网络结构，具体请见论文中第5部分。</p>\n<p>在Atari 2600 49个游戏中，41个明显优于单GPU DQN。</p>\n<p>Gorila进一步实现了DRL的希望：一个可伸缩的架构，随着计算和内存的增加，它的性能会越来越好</p>\n<h1 align=\"center\" style=\"color:blue\" id=\"MB-MPO\">[UCB/OpenAI]Model-Based Reinforcement Learning via Meta-Policy Optimization[MB-MPO]</h1>\n\n<p>论文地址：<a href=\"https://arxiv.org/pdf/1809.05214.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1809.05214.pdf</a></p>\n<p>本文2018年发布于CoRL，提出了一个基于模型的元强化学习算法MB-MPO。相比于一般的元强化学习是从多个MDPs任务中学习一个通用模型加速以后特定任务的模型训练，该文中的方法是将一个model-free的任务学习多个不确定、不完全、不完美的动态模型，即一个模型集合，然后使用这个模型集合学习出该任务的通用模型。因为它有一个从model-free学习动态模型的过程，所以为model-based方法。</p>\n<h2 id=\"元强化学习\"><a href=\"#元强化学习\" class=\"headerlink\" title=\"元强化学习\"></a>元强化学习</h2><script type=\"math/tex; mode=display\">\n\\max _{\\theta} \\mathbb{E}_{\\mathcal{M}_{k} \\sim \\rho(\\mathcal{M}),\\boldsymbol{s}_{t+1} \\sim p_{k},\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}^{\\prime}}\\left(\\boldsymbol{a}_{t} | \\boldsymbol{s}_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r_{k}\\left(s_{t}, a_{t}\\right)\\right] \\\\ s.t.:\\boldsymbol{\\theta}^{\\prime}=\\boldsymbol{\\theta}+\\alpha\\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}_{\\boldsymbol{s}_{t+1} \\sim p_{k},\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{a}_{t} | \\boldsymbol{s}_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r_{k}\\left(s_{t}, a_{t}\\right)\\right]</script><p>$\\mathcal{M}$为一系列MDP，共享相同的状态空间$\\mathcal{S}$与动作空间$\\mathcal{A}$，但是奖励函数可以不同</p>\n<h2 id=\"学习环境动态模型\"><a href=\"#学习环境动态模型\" class=\"headerlink\" title=\"学习环境动态模型\"></a>学习环境动态模型</h2><script type=\"math/tex; mode=display\">\n\\min _{\\boldsymbol{\\phi}_{k}} \\frac{1}{\\left|\\mathcal{D}_{k}\\right|} \\sum_{\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}, \\boldsymbol{s}_{t+1}\\right) \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{s}_{t+1}-\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)\\right\\|_{2}^{2}</script><p>解析：</p>\n<ul>\n<li><p>$\\mathcal{D}_{k}$为第k个学习模型采样的“经验”</p>\n</li>\n<li><p>$\\phi$为用神经网络表示的环境模型的参数</p>\n</li>\n<li><p>$\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)$为第k个学习模型针对状态$s_{t}$执行动作$a_{t}$后转移状态的预测，其中，神经网络的输出不直接是预测的状态$\\color{red}{s_{t+1}}$，而是$\\color{red}{\\Delta s=s_{t+1}-s_{t}}$，所以$\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)=s_{t}+\\Delta s$</p>\n<ul>\n<li><blockquote>\n<p>We follow the standard practice in model-based RL of training the neural network to predict the change in state $\\Delta s=s_{t+1}-s_{t}$ (rather than the next state $s_{t+1}$) </p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<p>为了防止过拟合，文中使用了3个trick：</p>\n<ol>\n<li>早停</li>\n<li>归一化神经网络输入与输出</li>\n<li>权重归一化</li>\n</ol>\n<h2 id=\"基于环境动态模型的元强化学习\"><a href=\"#基于环境动态模型的元强化学习\" class=\"headerlink\" title=\"基于环境动态模型的元强化学习\"></a>基于环境动态模型的元强化学习</h2><p>假设学到了K个近似模型$\\left\\{\\hat{f}_{\\phi_{1}}, \\hat{f}_{\\phi_{2}}, \\ldots, \\hat{f}_{\\phi_{K}}\\right\\}$，把每个模型转换成一个MDP过程，即$\\mathcal{M}_{k}=\\left(S, A, \\hat{f}_{\\phi_{k}}, r, \\gamma, p_{0}\\right)$，其中，<strong>奖励函数相同</strong></p>\n<p>由此给每个学习到的动态模型分配的行为策略目标函数为：</p>\n<script type=\"math/tex; mode=display\">\nJ_{k}(\\boldsymbol{\\theta})=\\mathbb{E}_{\\boldsymbol{a}_{t} \\sim \\pi_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{a}_{t} | s_{t}\\right)}\\left[\\sum_{t=0}^{H-1} r\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right) | \\boldsymbol{s}_{t+1}=\\hat{f}_{\\boldsymbol{\\phi}_{k}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)\\right]</script><p>定义MB=MPO的最终目标函数为：</p>\n<script type=\"math/tex; mode=display\">\n\\max _{\\boldsymbol{\\theta}} \\frac{1}{K} \\sum_{k=0}^{K} J_{k}\\left(\\boldsymbol{\\theta}_{k}^{\\prime}\\right) \\quad \\text { s.t.: } \\quad \\boldsymbol{\\theta}_{k}^{\\prime}=\\boldsymbol{\\theta}+\\alpha \\nabla_{\\boldsymbol{\\theta}} J_{k}(\\boldsymbol{\\theta})</script><p>小写k代表第k个学到的模型，大写K代表模型的总数。</p>\n<p>注意看，这里公式后边使用的是$\\color{red}{\\theta’_{k}}$，而不是$\\theta$。这里并没有写错，我起初以为写错了，具体请看后边的伪代码解释。</p>\n<h2 id=\"伪代码-1\"><a href=\"#伪代码-1\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-rough-reading/mb-mpo-pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>MB-MPO分为两部分更新，第一部分更新每个模型分配的行为策略，第二部分更新元策略。<strong>注意：行为策略的更新是不连贯的，即不是自身迭代，而是不断使用元策略进行稍加修改然后替换，所以叫做adapted policy</strong></li>\n<li>上一项提到的两次更新都是对元策略的参数$\\theta$进行更新，区别是，第一次更新将更新后的参数赋值给了行为策略，未更改元策略本身，第二次更新直接更新元策略本身</li>\n<li>$\\alpha, \\beta$为两部分更新的学习率</li>\n<li>行为策略使用VPG，即传统策略梯度算法进行优化，元策略使用TRPO算法进行优化</li>\n<li>伪代码中的大致流程如下：<ol>\n<li>初始化策略$\\pi_{\\theta}$并将其复制K份$\\pi_{\\theta_{1}^{\\prime}}, \\dots, \\pi_{\\boldsymbol{\\theta}_{K}^{\\prime}}$</li>\n<li>使用$\\pi_{\\theta_{1}^{\\prime}}, \\dots, \\pi_{\\boldsymbol{\\theta}_{K}^{\\prime}}$对<strong>真实的环境模型进行采样（这一步是实际交互，即真实数据）</strong>，将数据存入经验池</li>\n<li>根据经验池训练K个环境模型，即使用<code>学习环境动态模型</code>部分的公式</li>\n<li>对于每个更新后的环境模型，用元策略$\\color{red}{\\pi_{\\theta}}$进行<strong>虚拟采样（这一步是预测采样，即不实际进行交互）</strong>，采样到$\\mathcal{T}_{k}$以适应性修改行为策略$\\boldsymbol{\\theta}_{k}^{\\prime}$。这里也是前边提到的行为策略更新是不连贯的原因。</li>\n<li>再用适应性策略$\\boldsymbol{\\theta}_{k}^{\\prime}$进行<strong>虚拟采样</strong>，采样到$\\mathcal{T}_{k}^{\\prime}$以更新元策略$\\pi_{\\theta}$</li>\n<li>跳向第2步</li>\n</ol>\n</li>\n<li>伪代码中虽然没有明确指出，但是其实使用了baseline的trick用来减少方差</li>\n</ul>\n<h2 id=\"流程示意图\"><a href=\"#流程示意图\" class=\"headerlink\" title=\"流程示意图\"></a>流程示意图</h2><p><img src=\"./rl-rough-reading/mb-mpo-visio.png\" alt=\"\"></p>\n<h2 id=\"效果-1\"><a href=\"#效果-1\" class=\"headerlink\" title=\"效果\"></a>效果</h2><ol>\n<li>比之前的model-based方法效果好、收敛快</li>\n<li>可以达到model-free算法的渐进性能</li>\n<li>需要更少的经验，低采样复杂性。其实是使用了虚拟采样，提高了数据效率，减少了交互采样的代价。</li>\n<li>对于模型偏差（model-bias，即环境模型没学到位）的情况，之前的算法不能有效处理，该算法对不完美、不完全、不完整的模型具有很好地鲁棒性</li>\n</ol>\n<h1 align=\"center\" style=\"color:blue\" id=\"DIAYN\">[UCB/Google AI]Diversity is All Your Need: Learning Skills Without a Reward Function[DIAYN]</h1>\n\n<p>论文地址：<a href=\"https://arxiv.org/pdf/1802.06070.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1802.06070.pdf</a></p>\n<p>Google网页：<a href=\"https://sites.google.com/view/diayn/home\" rel=\"external nofollow\" target=\"_blank\">https://sites.google.com/view/diayn/home</a></p>\n<p>Github项目：<a href=\"https://github.com/ben-eysenbach/sac/blob/master/DIAYN.md\" rel=\"external nofollow\" target=\"_blank\">https://github.com/ben-eysenbach/sac/blob/master/DIAYN.md</a></p>\n<p>这篇文章使用信息论中最大熵的方法来构造强化学习的学习目标，<strong>期望学习到具有多样性的技能（skills）</strong>。</p>\n<p>个人认为，此文章中所提的方法虽然很新颖，但是不能作为优化一项任务的可用算法，因为虽然其可以学到以各种花样完成目标，但是没有奖励函数的控制使得无法规范、指引智能体“解题”过程的效果，如柔顺性、实用性、实际可行性等。从另一方面来讲，将这样虽然不规划、不严谨决策行为的策略用于元策略的预训练模式还是可用的。</p>\n<h2 id=\"技能\"><a href=\"#技能\" class=\"headerlink\" title=\"技能\"></a>技能</h2><p>技能的定义在文中有如下表述：</p>\n<blockquote>\n<p>A skill is a latent-conditioned policy that alters that state of the environment in a consistent<br>way.<br>we refer to a/the policy conditioned on a fixed Z as a “skill” .</p>\n</blockquote>\n<p>意思是，设定一个隐变量，以（状态$S$，隐变量$Z$）为条件进行动作选择，即为技能——skill。</p>\n<p>DIAYN就好像是要给每个状态赋予各个不同技能的概率，并且使其中一个技能的概率最大，这样就使得在整个状态空间中，不同的技能“占领”着状态空间的不同部分，每个技能在各自偏好的局部状态空间中作用，但是作者同样希望每个技能在各自的状态空间中尽可能随机决策。</p>\n<p><img src=\"./rl-rough-reading/skill.png\" alt=\"\"></p>\n<p>假设以不同的颜色代表不同的技能skill，每个方格代表一个状态，那么每个状态对于每个技能到达此状态的“偏好”概率是不同的。总的来说，作者希望技能之间的重合度尽可能小，但每个技能在各自的领域内尽可能随机地完成目标。</p>\n<h2 id=\"亮点与作用\"><a href=\"#亮点与作用\" class=\"headerlink\" title=\"亮点与作用\"></a>亮点与作用</h2><ol>\n<li>去掉了奖励函数</li>\n<li>修改了目标函数，$\\mathcal{F}(\\theta)\\triangleq \\mathcal{G}(\\color{red}{\\theta, \\phi})$<ul>\n<li>$\\color{red}{\\theta}$代表Actor网络中的参数</li>\n<li>$\\color{red}{\\phi}$代表Critic网络中的参数</li>\n</ul>\n</li>\n<li>学习到的技能可以用于<em>分层强化学习、迁移学习、模仿学习</em></li>\n</ol>\n<h2 id=\"目标函数\"><a href=\"#目标函数\" class=\"headerlink\" title=\"目标函数\"></a>目标函数</h2><script type=\"math/tex; mode=display\">\n\\begin{aligned} \n\\mathcal{F}(\\theta) & \\triangleq \\color{red}{I(S ; Z)+\\mathcal{H}[A | S]-I(A ; Z | S)} \\\\ \n&=(\\mathcal{H}[Z]-\\mathcal{H}[Z | S])+\\mathcal{H}[A | S]-(\\mathcal{H}[A | S]-\\mathcal{H}[A | S, Z]) \\\\ \n&=\\color{blue}{\\mathcal{H}[Z]-\\mathcal{H}[Z | S]+\\mathcal{H}[A | S, Z]} \\\\\n&=\\mathcal{H}[A | S, Z]+\\mathbb{E}_{z \\sim p(z), s \\sim \\pi(z)}[\\log p(z | s)]-\\mathbb{E}_{z \\sim p(z)}[\\log p(z)] \\\\\n&{ \\color{orange}{\\geq} \\mathcal{H}[A | S, Z]+\\mathbb{E}_{z \\sim p(z), s \\sim \\pi(z)}\\left[\\log q_{\\phi}(z | s)-\\log p(z)\\right] \\\\ \n\\triangleq \\mathcal{G}(\\theta, \\phi)}\n\\end{aligned}</script><p>解析：</p>\n<ul>\n<li><p>互信息，离散下为$I(X ; Y)=\\sum_{y \\in Y} \\sum_{x \\in X} p(x, y) \\log \\left(\\frac{p(x, y)}{p(x) p(y)}\\right)$，连续下为$I(X ; Y)=\\int_{Y} \\int_{X} p(x, y) \\log \\left(\\frac{p(x, y)}{p(x) p(y)}\\right) d x d y$</p>\n</li>\n<li><p>信息熵表示为$H(X, Y)=-\\sum_{x, y} p(x, y) \\log p(x, y)=-\\sum_{i=1}^{n} \\sum_{j=1}^{m} p\\left(x_{i}, y_{i}\\right) \\log p\\left(x_{i}, y_{i}\\right)$</p>\n</li>\n<li><p>推到中频繁使用了性质$I(X,Y)=H(X)-H(X | Y)$</p>\n</li>\n<li><p>式中对数的底为自然指数$e$</p>\n</li>\n<li><p>看红色部分，化简之前：</p>\n<ul>\n<li><strong>增大</strong>：$I(S ; Z)$代表状态$S$与策略隐变量$Z$之间的互信息。因为作者希望可以通过策略所能到达的状态来判别其属于哪个技能，即将技能与状态挂钩。作者给出一个直观的解释：因为在有些状态下可以执行很多动作，但是却不改变环境（至少不明显改变），就像用机械手臂夹紧一个物体时，可使用力的大小、方向等都是很多的，不同技能选择不同动作导致的效果可能相同，所以作者不希望从动作的选择来区分学到的技能，而是通过可以明显观察到、数值化的状态$S$来作为区别不同技能的标准。<strong>互信息$I(X,Y)$有一个直观的性质就是，它可以衡量两个随机变量的“相关性”，也就是说，互信息越大，代表知道$X$后对$Y$的不确定性减少，即知道其一可以加深对另一个的了解。</strong>所以，目标函数希望最大化互信息$I(S ; Z)$，以将状态和技能相关联，使技能尽可能根据状态可以区分。</li>\n<li><strong>增大</strong>：$\\mathcal{H}[A | S]$代表策略（不以隐变量$Z$区分技能，混合所有技能即为策略）的熵值。与SAC算法中想要使用熵增来使得动作的选择更加随机一样，作者希望随机性的动作同样可以完成目标，所以希望尽可能增大这一项。</li>\n<li><strong>减小</strong>：$I(A ; Z | S)$代表动作$A$与策略隐变量$Z$在给定状态$S$时之间的互信息。为了避免歧义，应该写作为$I[(A ; Z) | S]$。作者希望技能根据状态可区分，而不是根据动作，所以需要最小化这一项。</li>\n</ul>\n</li>\n<li><p>看蓝色部分，化简之后：</p>\n<ul>\n<li><strong>固定最大，为$\\ln n$</strong>：$\\mathcal{H}[Z]$代表技能分布的不确定性，既然要最大化这个项，不如就固定它，使得技能从其中均匀采样，使熵为最大值。</li>\n<li><strong>减小</strong>：$\\mathcal{H}[Z | S]$代表状态$S$条件下技能的不确定性，我们知道，熵越大，不确定性越大；熵越小，不确定性越小。作者希望技能根据状态可区分，可以需要使这一项最小，以减小给定状态下所属技能的不确定性，使其尽可能接近概率1。</li>\n<li><strong>增大</strong>：$\\mathcal{H [ A | S}, Z ]$代表给定技能$(S,Z)$下动作的不确定性。因为作者希望动作的选择尽可能随机但又可以完成目标，所以需要最大化这一项。</li>\n</ul>\n</li>\n<li><p>看橘色部分，使用Jensen不等式：</p>\n<ul>\n<li><p>这一步推导使用了论文<a href=\"https://pdfs.semanticscholar.org/f586/4b47b1d848e4426319a8bb28efeeaf55a52a.pdf\" rel=\"external nofollow\" target=\"_blank\">《The IM Algorithm : A variational approach to Information Maximization》</a>中的推导公式</p>\n<script type=\"math/tex; mode=display\">\nI(\\mathbf{x}, \\mathbf{y}) \\geq \\underbrace{H(\\mathbf{x})}_{\\text { ‘‘entrop’’ }}+\\underbrace{\\langle\\log q(\\mathbf{x} | \\mathbf{y})\\rangle_{p(\\mathbf{x}, \\mathbf{y})}}_{\\text { ‘‘energy’’ }} \\stackrel{\\mathrm{def}}{=} \\tilde{I}(\\mathbf{x}, \\mathbf{y})</script></li>\n<li><p><img src=\"./rl-rough-reading/Agakov.png\" alt=\"\"></p>\n</li>\n<li><p>蓝色公式中，有$I(Z;S) = \\mathcal{H}[Z]-\\mathcal{H}[Z | S]$，可以应用上述性质进行推导，将真实分布$p(z | s)$替换为任意变分分布(variational distribution)$q(z | s)$</p>\n</li>\n<li><p>最后使用变分下界$\\mathcal{G}(\\theta, \\phi)$代替目标函数$\\mathcal{F}(\\theta)$</p>\n</li>\n</ul>\n</li>\n<li><p>至此，思路已经十分清晰。Actor网络以变量$\\theta$参数化，并使用SAC算法($\\alpha=0.1$)最大化$\\mathcal{G}(\\theta, \\phi)$中$\\mathcal{H}[A | S, Z]$部分；Critic网络以变量$\\phi$参数化，并最大化后半个期望部分。文中将期望内的元素定义为“伪奖励”：</p>\n<script type=\"math/tex; mode=display\">\nr_{z}(s, a) \\triangleq \\log q_{\\phi}(z | s)-\\log p(z)</script><p>由于$p(z)$为均匀分布，是固定的；对数函数不改变原函数单调性，所以只需最大化$q_{\\phi}(z | s)$即可。</p>\n</li>\n</ul>\n<h2 id=\"伪代码-2\"><a href=\"#伪代码-2\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-rough-reading/diayn-pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>每个episode都重新采样隐变量$z$</li>\n<li>Actor网络的输入为$(S||Z)$，即状态与隐变量的连结(我猜的= =)</li>\n<li>Critic网络的输入为状态$S$</li>\n</ul>\n<h2 id=\"模型示意图-1\"><a href=\"#模型示意图-1\" class=\"headerlink\" title=\"模型示意图\"></a>模型示意图</h2><p><img src=\"./rl-rough-reading/diayn.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>隐变量分布$p(z)$是固定的</li>\n</ul>\n<p></p><h1 align=\"center\" style=\"color:blue\" id=\"CDP\">Curiosity-Driven Experience Prioritization via Density Estimation[CDP]</h1><br>论文地址：<a href=\"https://arxiv.org/pdf/1902.08039.pdf\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/pdf/1902.08039.pdf</a><p></p>\n<p>这篇文章发于2018年的NIPS，作者为赵瑞，之前读过他的两篇论文，并写了博客，可以在论文精读里找，此处不贴链接了，分别是基于能量的HER和最大熵正则化多目标RL。</p>\n<p>这篇文章总的来说提出了<strong>基于迹密度的优先经验回放</strong>，人类的好奇心机制驱动他有了这样的想法，文中说受有监督学习使用过采样和降采样解决训练集样本不平衡问题的启发，想在强化学习中解决经验池中迹“不足（under-represented）”的问题。</p>\n<p>说起来也挺佩服这个作者的，目前（2019年6月21日14:59:05）总共发了三篇关于强化学习的论文，但都有很好地结果：</p>\n<ol>\n<li>基于迹能量的优先级，发了CoRL</li>\n<li>基于迹密度的优先级，也就是这篇，发了NIPS</li>\n<li>基于迹最大熵的优先级，发了ICML</li>\n</ol>\n<p>我个人道行尚浅，对于几篇论文中的深奥精髓有些不能尽数参透，由于先验知识不足，对于文中内容也不敢完全苟同，但是从这几篇阅读总结下来，发得了这种高级别论文有以下几个“加分性”要求：</p>\n<ol>\n<li>数学要好，这是必然的，数学公式写的越华丽，数学模型越复杂，当然越具有吸引力</li>\n<li>工作要专一且连续，看这三篇论文虽然不是递进关系，但都是在解决经验池优先相关的工作，所以找准一个领域内的小角度也是很重要的</li>\n<li>实验部分要做好，三篇都没用完整地、细节地比较各个算法，但是却新奇地比较了采样复杂性、数据利用率等等，总之，一定要用实验表明自己的方法在某方面有用</li>\n<li>其他秘密因素</li>\n</ol>\n<h2 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a>流程</h2><p>这篇论文的方法流程如下：</p>\n<ol>\n<li>计算<strong>迹密度</strong>$\\rho$</li>\n<li>计算迹密度的补$\\overline{\\rho} \\propto 1-\\rho$</li>\n<li>根据补排序，并设置优先级，补越大优先级越大</li>\n<li>使用HER补充经验，设置相同的优先级和迹密度</li>\n<li>优化算法</li>\n</ol>\n<h2 id=\"迹密度的计算\"><a href=\"#迹密度的计算\" class=\"headerlink\" title=\"迹密度的计算\"></a>迹密度的计算</h2><p>这一部分没有看太懂，主要是本人数学功底比较薄弱，感兴趣的可以亲自查看论文中2.4与3.2.1、3.2.2部分。</p>\n<p>根据文中的意思，思想大致如下：</p>\n<ol>\n<li><p>用GMM（高斯混合模型）来估计迹密度</p>\n<script type=\"math/tex; mode=display\">\n\\rho(\\mathbf{x})=\\sum_{k=1}^{K} c_{k} \\mathcal{N}\\left(\\mathbf{x} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)</script></li>\n<li><p>在每个epoch，使用V-GMM（GMM的一个变体）+EM算法推断GMM参数($\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}$)的后验分布</p>\n</li>\n<li><p>在每个episode，使用如下公式计算迹密度</p>\n<script type=\"math/tex; mode=display\">\n\\rho=\\mathrm{V}-\\operatorname{GMM}(\\mathcal{T})=\\sum_{k=1}^{K} c_{k} \\mathcal{N}\\left(\\mathcal{T} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)</script><p>其中，$\\mathcal{T}=\\left(s_{0}\\left|s_{1}\\right| \\ldots | s_{T}\\right)$，<strong>每个迹的长度相同</strong>，中间的符号代表连结操作的意思，然后进行归一化</p>\n<script type=\"math/tex; mode=display\">\n\\rho_{i}=\\frac{\\rho_{i}}{\\sum_{n=1}^{N} \\rho_{n}}</script></li>\n</ol>\n<p><em>注：我猜想上边符号表示的$s$其实包含了智能体的所在状态和要达到的真实目标，也就是$(s,g)$，文中有一段可能解释了这一部分，但是我没有太理解。</em></p>\n<p><img src=\"./rl-rough-reading/cdp-sg.png\" alt=\"\"></p>\n<h2 id=\"优先级的设定\"><a href=\"#优先级的设定\" class=\"headerlink\" title=\"优先级的设定\"></a>优先级的设定</h2><p>作者说使用rank-based方法来设置优先级，因为其受异常点影响小而具有良好的鲁棒性。</p>\n<p>先计算迹密度的补</p>\n<script type=\"math/tex; mode=display\">\n\\overline{\\rho} \\propto 1-\\rho</script><p>将补从小到大排序，并根据排名计算优先级，排名从0开始，即</p>\n<script type=\"math/tex; mode=display\">\n\\operatorname{rank}(\\cdot) \\in\\{0,1, \\ldots, N-1\\}</script><script type=\"math/tex; mode=display\">\np\\left(\\mathcal{T}_{i}\\right)=\\frac{\\operatorname{rank}\\left(\\overline{\\rho}\\left(\\mathcal{T}_{i}\\right)\\right)}{\\sum_{n=1}^{N} \\operatorname{rank}\\left(\\left(\\overline{\\rho}\\left(\\mathcal{T}_{n}\\right)\\right)\\right.}</script><h2 id=\"伪代码-3\"><a href=\"#伪代码-3\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-rough-reading/cdp-pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>每个epoch根据经验池中的样本数据重新拟合一次密度模型，也就是GMM中的参数</li>\n<li>每个episode都计算其迹密度</li>\n<li>红色框中的公式数字编号分别代表之间部分中关于计算迹密度和迹优先级的公式</li>\n<li>采样迹、采样经验转换之后，需要采样目标并存入经验池，重构后的经验其优先级及迹密度与真正目标下迹的相同</li>\n</ul>\n<h2 id=\"优点\"><a href=\"#优点\" class=\"headerlink\" title=\"优点\"></a>优点</h2><p>实验部分的比较详见论文。</p>\n<ol>\n<li>可以适用于任何Off-Policy算法</li>\n<li>不使用TD-error计算优先级，而使用迹密度，减少了计算时间</li>\n<li>提升了采样效率两倍左右</li>\n<li>算法性能超过最新算法9%（这个结果看看即可，不必放在心上）</li>\n</ol>\n<p></p><h1 align=\"center\" style=\"color:blue\" id=\"NAF\">[Google]Continuous Deep Q-Learning with Model-based Acceleration[NAF]</h1><br>论文地址：<a href=\"https://arxiv.org/abs/1603.00748\" rel=\"external nofollow\" target=\"_blank\">https://arxiv.org/abs/1603.00748</a><p></p>\n<p>本文介绍了标准化优势函数Normalized Advantage Function——NAF算法，该算法简化了A-C架构，将Q-Learning的思想应用于高维连续空间。</p>\n<p>本文的主要贡献是：</p>\n<ol>\n<li>提出NAF，简化了Actor-Critic架构</li>\n<li>将Q-Learning推广至高维连续空间</li>\n<li>提出新的与已学模型结合的方法，提升了采样复杂性（也就是降低）和学习效率，不牺牲策略的最优性。原文中翻译意思是，评估了几种将已学习模型与Q-Learning结合的方案，提出将局部线性模型与局部On-Policy想定推演结合以加速Q-Learning算法在model-free、连续问题下的学习</li>\n</ol>\n<h2 id=\"伪代码-4\"><a href=\"#伪代码-4\" class=\"headerlink\" title=\"伪代码\"></a>伪代码</h2><p><img src=\"./rl-rough-reading/naf-pseudo.png\" alt=\"\"></p>\n<p>解析：</p>\n<ul>\n<li>。</li>\n</ul>"},{"title":"强化学习之MDP马尔科夫决策过程","copyright":true,"top":1,"date":"2019-05-08T03:04:20.000Z","mathjax":true,"_content":"\n# 强化学习之MDP马尔科夫决策过程\n\n每每提到强化学习，最先接触的理论肯定是马尔科夫决策过程（MDP，Markov Decision Process），为什么总提到MDP呢？并不是只有我一个人有这个疑问。\n\n<!--more-->\n\n百度上没有人提出这样的问题，可能是大家理解得都比较透彻吧，于是在Google查到相关提问和解释。\n\n> [What is the relationship between Markov Decision Processes and Reinforcement Learning?](https://datascience.stackexchange.com/a/38851)\n\n>>In Reinforcement Learning (RL), the problem to resolve is described as a Markov Decision Process (MDP). Theoretical results in RL rely on the MDP description being a correct match to the problem. If your problem is well described as a MDP, then RL may be a good framework to use to find solutions. That does not mean you need to fully describe the MDP (all the transition probabilities), just that you expect an MDP model could be made or discovered.\n\n>>Conversely, if you cannot map your problem onto a MDP, then the theory behind RL makes no guarantees of any useful result.\n\n>>One key factor that affects how well RL will work is that the states should have the Markov property - that the value of the current state is enough knowledge to fix immediate transition probabilities and immediate rewards following an action choice. Again you don't need to know in advance what those are, just that this relationship is expected to be reliable and stable. If it is not reliable, you may have a POMDP. If it is not stable, you may have a non-stationary problem. In either case, if the difference from a more strictly defined MDP is small enough, you may still get away with using RL techniques or need to adapt them slightly.\n>>\n>>**The general relationship between RL and MDP is that RL is a framework for solving problems that can be expressed as MDPs.**\n\nMDP是当前强化学习理论推导的基石，对强化学习来说，一般以马尔科夫决策过程作为形式化问题的手段。也就是说，对于目前的绝大部分强化学习算法，只有可以将问题抽象为MDP的才可以确保算法的性能（收敛性，效果等），对于违背MDP的问题并不一定确保算法有效，因为其数学公式都是基于MDP来进行推导的。\n\n## 马尔科夫性\n\n> 马尔科夫性质（英语：Markov property）是概率论中的一个概念，因为俄国数学家安德雷·马尔科夫得名。当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔科夫性质。[马尔科夫性-百度百科](https://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8/23149887?fr=aladdin)\n\n马尔科夫性，也就是无后效性：**某阶段的状态一旦确定，则此后过程的演变不再受此前各状态及决策的影响**。也就是说，**未来与过去无关**。\n\n具体地说，如果一个问题被划分各个阶段之后，阶段$k$中的状态只能通过阶段$k+1$中的状态通过状态转移方程得来，与其他状态没有关系，特别是与未发生的状态没有关系，这就是无后效性。\n\n公式描述：\n\n$$\nP[S_{t+1}|S_{t}]=P[S_{t+1}|S_{1},...,S_{t}]\n$$\n\n强化学习问题中的状态也符合马尔科夫性，即在当前状态$s_{t}$下执行动作$a_{t}$并转移至下一个状态$s_{t+1}$，而不需要考虑之前的状态$s_{t-1},...,s_{1}$。\n\n举一个不恰当的例子：\n\n![](./强化学习之MDP马尔科夫决策过程/M.jpg)\n\n假设天气预测符合马尔科夫性，如果以每天表示为一种状态，即周一、周二到周日。今天（5月8日，周三）天气为晴，明天（周四）会不会下雨只与今天的天气有关，而与之前周一、周二的天气状况无关。如果以时间节点表示为一种状态，即2点、5点、8点等，如图2点的温度为15.8°C,那么下个时间点5点的气温如何只与2点的温度有关系。\n\n强化学习中默认状态的转移是符合马尔科夫性质的，状态具体是什么，需要根据不同的问题进行不同的设定。\n\n## 马尔科夫过程\n\n马尔科夫过程是随机过程的一种，什么是随机过程呢？简单来说，一个商店从早上营业到晚上打烊这段时间，根据每个时间点店内顾客的人数所组成的序列就是随机过程。随机过程根据时间节点$T_{t}$取到的值是一个变量。\n\n马尔科夫过程是满足马尔科夫性的随机过程，它由二元组$M=(S,P)$组成，且满足：\n\n1. S是有限状态集合\n2. P是状态转移概率矩阵\n\n状态与状态之间的转换过程即为马尔科夫过程。***虽然我们可能不知道P的具体值到底是什么，但是通常我们假设P是存在的（转移概率存在，如果是确定的，无非就是概率为1），而且是稳定的（意思是从状态A到其他状态的转移虽然符合某个分布，但是其转移到某个状态的概率是确定的，不随时间变化的）。***\n\n这里说的**有限**二字我有自己的理解，在最开始的强化学习研究中，解决的都是表格式的问题，也就是状态的数量是有限可取的，但是后续强化学习研究的也有连续状态空间的问题，算法如DQN,PG,PPO等。状态的数量并不是有限的，但是其向量维度则是固定的、有限的，而且也同样符合马尔科夫性质，因此**我认为这里定义的有限并不是说状态数量有限，而是状态维度有限**。因为好像没有无限马尔科夫的叫法，所以姑且这么解释一下。\n\n马尔科夫过程有如下分类：\n\n![](./强化学习之MDP马尔科夫决策过程/MPs.jpg)\n\n### 状态转移矩阵\n\n状态转移矩阵由许多状态转移概率组成，状态转移概率是指从一个马尔科夫状态$s$转移到下一个状态$s'$的概率。\n\n公示表示：\n\n$$\n\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]\n$$\n\n等同于：\n\n$$\n\\mathcal{P}(s'|s)=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]\n$$\n\n假设有1到n个状态，将所有的状态从上到下、从左到右排列，组成一个$n \\times n$的矩阵，那么其状态转移矩阵如下所示：\n\n$$\n\\mathcal{P}=\n\\begin{bmatrix}\n\\mathcal{P}_{11} & \\cdots & \\mathcal{P}_{1n} \\\\ \n\\vdots & \\ddots & \\vdots \\\\ \n\\mathcal{P}_{n1} & \\cdots & \\mathcal{P}_{nn} \\\\\n\\end{bmatrix}\n$$\n\n其中，每行元素相加等于1，每列元素相加等于1，矩阵的总和为状态的数量n。\n\n对于可数状态，$\\sum_{s'=1}^{n}\\mathcal{P}(s'|s)=1$\n\n$$\nsum(\\mathcal{P}) = \\sum_{s'=1}^{n}\\sum_{s=1}^{n}\\mathcal{P}_{ss'} = n\n$$\n\n对于不可数状态（连续状态),$\\int_{s'}\\mathcal{P}(s'|s)=1$\n\n$$\nsum(\\mathcal{P}) = \\int_{s'}\\int_{s}\\mathcal{P}_{ss'} = n\n$$\n\n举一个马尔科夫过程的例子:\n\n假设一个学生，他目前在学习语文科目，那么他接下来进行的活动过程如下图所示，游戏的吸引力很大，所以他有50%的概率在学完语文去玩游戏，并且很容易沉迷其中，图示玩游戏这个循环有90%的可能性，他还可以选择学习其他科目或者去睡觉，最终学习结束之后是否能通过考试也是有一定的概率的，这些状态之间转移的概率即为状态转移概率。\n\n![](./强化学习之MDP马尔科夫决策过程/MP.jpg)\n\n如果把例子中的各项状态用字母表示，将其表示为：\n$$\n\\begin{bmatrix}\n玩游戏 & A\\\\ \n语文 & B\\\\ \n数学 & C\\\\ \n英语 & D\\\\ \n挂科 & E\\\\ \n\\mathcal{Pass} & F\\\\ \n睡觉 & G\n\\end{bmatrix}\n$$\n那么其状态转移矩阵$\\mathcal{P}$可以表示成：\n$$\n\\begin{array}{lc}\n\\mbox{}&\n\\begin{array}{cc}A & B & C & D & E & F & G \\end{array}\\\\\n\\begin{array}{c}A\\\\B\\\\C\\\\D\\\\E\\\\F\\\\G\\end{array}&\n\\left[\\begin{array}{cc}\n0.9&0.1\\\\\n0.5& &0.5\\\\\n& & &0.8& & &0.2\\\\\n&&&&0.4&0.6&\\\\\n&0.2&0.4&0.4&&&\\\\\n&&&&&&1\\\\\n&&&&&&&\n\\end{array}\\right]\n\\end{array}\n$$\n\n### 马尔科夫链与Episode\n\nEpisode可以翻译为片段、情节、回合等，在强化学习问题中，一个Episode就是一个马尔科夫链，根据状态转移矩阵可以得到许多不同的episode，也就是多个马尔科夫链。\n\n强化学习问题分两种：\n\n1. 如果一个任务总能达到终态，结束任务或者开启下一轮任务，那么这个任务就被称为回合任务，也就是episode任务。例如，让一个智能体学习如何下围棋，围棋棋盘只有那么大，游戏定会终局，所以是一个回合式任务。\n2. 如果一个任务可以无限持续下去，永远不会结束，即永远在训练当中，那么这个任务就被称为连续性任务。例如，教会一辆车能够进行自动驾驶就是一个连续性任务，*不要钻牛角尖说能源会耗尽，车子会磨损，我们只聚焦问题与环境本身，不涉及其他非稳定因素。*\n\n在上边举的例子中就是一个回合式任务，因为无论这个序列有多长，最终都会达到终态-“睡觉”。\n\n根据上述例子我们可能采样出如下episode：\n\n1. $B-C-D-E-C-G$，即“学语文→数学→英语→考试没通过,挂科→继续学数学→睡觉”\n2. $B-A-A-...-A-B-C-G$，即“学语文→玩王者荣耀→玩刺激战场→玩OverCooker→玩守望先锋→玩英雄联盟→玩CS:GO→...→看一会儿数学→睡觉”。（仿佛就是我自己嘛！）\n\n## 马尔科夫奖励过程\n\n马尔科夫过程（Markov Process）主要描述的是状态之间的转移关系，在各个状态的转移过程中赋予不同的奖励值就得到了马尔科夫奖励过程。\n\n定义：马尔科夫奖励过程（Markov Reward Process, MRP）由一个四元组组成$(S,P,R,\\gamma)$\n\n1. $S$代表了状态的集合(也是维度有限的)\n2. $P$描述了状态转移矩阵$\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]$\n3. $R$表示奖励函数，$R(s)$描述了在状态$s$下的期望(立即)奖励，$\\mathcal{R}(s)=\\mathbb{E}[R_{t+1}|S_{t}=s]$\n4. $\\gamma$表示衰减因子,即discounted factor,$\\gamma\\in[0,1]$\n\n$\\gamma$是用来计算累计奖励回报的,表示我们有多看中现在或者未来,为什么这么说呢?假设我们现在要计算一个episode始态$S_{0}$的奖励值$V(S_{0})$,不涉及具体公式推导的说,我们应该把$S_{0}$状态后续的奖励全部加和,这样就得到了对始态$S_{0}$的值估计,这些后续奖励的值的权重都是1,或者说此时$\\gamma=1$,但是当前状态对很多步之后的状态未必影响很大,我们这样计算过来并不能完全表示一个状态的值,那么我们应当顺势减少距离远的状态的权重,此时$\\gamma\\lt1$\n\n- 当$\\gamma=0$时,状态$S$的值完全由其转移的期望立即奖励表示,即**一点都不关心未来**\n- 当$\\gamma=1$时,状态$S$的值由以当前状态为始态,运行至终态所得到的所有立即奖励加和的值表示,即**未来与现在同等重要**\n- 当$0 \\lt\\gamma \\lt1$时,状态$S$的值是前两个模式的*trade-off*,即**对未来看重的程度由$\\gamma$决定**\n\n这只是我们的直观感受,其实是为了数学便利（虽然我也不知道具体哪里提高了数学便利，但是在有些情况下会使值函数更快迭代收敛这是真的）。\n\n*注：也有很多地方将MRP表示为三元组，即去掉$\\gamma$，但这不影响我们对这个过程的理解，下边的MDP也是一样，无论是三元组、四元组、还是五元组，只要能描述过程的性质就可以。*\n\n将上述马尔科夫过程的例子升级为马尔科夫奖励过程如下图所示:\n\n![](./强化学习之MDP马尔科夫决策过程/MRP.png)\n\n奖励值定义为:\n$$\n\\begin{bmatrix}\n玩游戏 & A & -1\\\\ \n语文 & B & -2\\\\ \n数学 & C & -2\\\\ \n英语 & D & -2\\\\ \n挂科 & E & -5\\\\ \n\\mathcal{Pass} & F & 10\\\\ \n睡觉 & G & 0\n\\end{bmatrix}\n$$\n这么定义奖励并没有什么复杂的含义,在这个例子中就拿身心愉悦程度来定义吧,学习固然是枯燥无味的,所以给予负奖励-2,玩游戏虽然会心情放松,但是始终面临着考试的压力,其实并不轻松,所以给予负奖励-1,挂科最痛苦为-5,考试全pass最开心为+10。\n\n在马尔科夫过程中的状态转移加入相应的奖励值即为马尔科夫奖励过程。\n\n\n\n## 马尔科夫决策过程\n\n马尔科夫决策过程(Markov Decision Process, MDP)相比马尔科夫奖励过程多了一个动作$A$,它可以用一个五元组$(S,A,P,R,\\gamma)$表示:\n\n1. $S$代表了状态的集合(也是维度有限的)\n2. $A$代表了决策过程中动作的集合(维度有限的)\n3. $P$描述了状态转移矩阵$\\mathcal{P}_{ss'}^{a}=\\mathbb{P}[S_{t+1}=s'|S_{t}=s,A_{t}=a]$\n4. $R$表示奖励函数，$R(s)$描述了在状态$s$下**执行某动作**的期望(立即)奖励，$\\mathcal{R}(s,a)=\\mathbb{E}[R_{t+1}|S_{t}=s,A_{t}=a]$\n5. $\\gamma$表示衰减因子,即discounted factor,$\\gamma\\in[0,1]$\n\nMDPs是一个从交互中达成目标的强化学习问题的一个直接的框架。学习者和决策者叫做Agent。Agent进行交互的其它一切Agent之外的东西都叫做环境。Agent不断的选择动作，而环境也给出相应的反应，并且向Agent表现出新的状态。环境同时也给出一个数值作为反馈。Agent的目标就是通过选择不同的Action来最大化这个反馈值。\n\n![](./强化学习之MDP马尔科夫决策过程/agent-env.png)\n\n强化学习所研究的内容就是得到一个状态$S$到动作$A$的映射关系,因此策略Policy可以表示成\n$$\n\\pi(a|s)=p(A_{t}=a|S_{t}=s)\n$$\n---\n\n注意:\n你可能会认为,在马尔科夫奖励过程(MRP)中没有定义动作,但是其实是包含动作的,因为每个状态有多个转移的下一状态,其实就是多个动作嘛！\n\n很多文章会将有限MDP分开来讲，有限MDP即状态、动作和奖励值都只有有限个元素，对于有限MDP最优策略有唯一解，但是现实世界中任务复杂，因此大多数深度强化学习算法并不局限于解决有限MDP问题，因此本文不将MDP分情况来讲，即默认基于MDP的最优策略**至少有一个解**。\n\n---\n\n没错,的确是这样的,MRP中也包含动作,但是我们并不关心,为什么这么说呢?**因为就算每个状态可以执行多个动作,但是其每个动作所能转移到的状态是确定的,不确定的只是动作的选择,而不是动作的转移,而MDP中不确定的却是动作的转移,即执行动作所转移的下一状态是有一定概率的.**什么意思呢?拿之前MRP的例子来说,语文状态有两个状态可以转移,数学和玩游戏,概率分别是0.5,但是当确定一个转移方向的时候(图中的箭头),其转移结果是确定的,获得的奖励也是确定的,但是在MDP中,执行动作导致转移的结果都未必是确定的.***需要注意的是,MRP是属于MDP的,MDP执行动作并不一定必须是随机的.***\n\n接下来,我们将MRP的例子转换至MDP, 为了方便理解而又不增加示例的复杂性,不妨将\"挂科\"这个状态看作是一个动作,因为这个节点正巧入度为1,姑且就认为从英语到挂科的这个箭头是英语状态所能执行的动作.如图所示:\n\n![](./强化学习之MDP马尔科夫决策过程/MDP.jpg)\n\n比较两个图可以发现区别,我把这个不确定的动作标为实心黑圆圈,这位刻苦的同学在学习完英语之后还想继续学习,但是他感觉三门科目都差不多了,于是他也很迷茫,他执行\"学习\"这个动作时的转移状态有三种:学语文、学数学、学英语.概率分别是:0.2、0.4,、0.4.这下就明白为什么我们要在MDP中加入动作$A$了吧,如果还不明白,请接着看下边的内容.\n\n顺便说一下,这个时候的转移矩阵已经不是简单的二维了,当然也可以用二维来表示,假设总共有$n$个状态,每个状态有$m$个动作,那么其行数为$n\\times m$,即遍历所有的状态和动作,得到$n \\times m$个状态-动作对$(s,a)$,其列数还是$n$.当然,也可以用一个三维tensor来表示,行和列都是$n$,第三维深度为$m$,很好理解.\n\n---\n\n**网上有写MDP在给定策略下会退化为MRP,我对此不置可否,认为此种说法不够严谨,因为即使说在某状态s下选择的动作a是确定的,并不意味着其转移结果是确定的.**\n\n---\n\n### 回报 Return\n\n在强化学习问题中，总是提到回报二字，论文中出现Return或者Discounted Return，我们已经知道奖励是什么，奖励就是转移到某个状态或者执行了某个动作之后转移至某个状态所获得的值$r$.\n\n回报就是由某时刻$t$之后决策序列所获得的奖励值经过一定规则计算出来的数值.\n\n公式描述:\n$$\nG_{t}\\doteq R_{t+1}+R_{t+2}+R_{t+3}+...+R_{t}\n$$\n.其中,$T$表示一个episode达到终态的时间点.\n\n像之前介绍的一样,我们可能对未来有不同的看重程度,于是引入折扣因子$\\gamma$的回报表示为:\n$$\nG_{t}\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+...=\\begin{cases}\n\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\\\\n\\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_{k}\n\\end{cases}\n$$\n其中,$ 0\\leq\\gamma \\leq1$\n\n可以推出回报有如下形式:\n$$\n\\begin{align*}\nG_{t} &\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\gamma^{3}R_{t+4}+...\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\gamma^{2}R_{t+4}+...)\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma (R_{t+3}+\\gamma R_{t+4}+...))\\\\\n&=R_{t+1}+\\gamma G_{t+1}\n\\end{align*}\n$$\n\n### 策略 Policy\n\n我们一般使用$\\pi$来表示一个策略,使用$\\pi(a|s)$来表示某状态$s$采取动作$a$的概率,公示表示为:\n$$\n\\pi(a|s)=P(A_{t}=a|S_{t}=s)\n$$\n策略完整定义了智能体在所有状态下的所有行为和其概率.\n\n给定一个MDP和一个策略$\\pi$,采样的状态序列\n$$\nS_{0},S_{1},S_{2},...,S_{n},...\n$$\n是一个马尔科夫过程$\\lt S,P \\gt ^{\\pi}$,\n\n采样的状态、奖励序列\n$$\n(S_{0},R_{0}),(S_{1},R_{1}),(S_{2},R_{2}),...,(S_{n},R_{n}),...\n$$\n是一个马尔科夫奖励过程$ \\lt S,P,R,\\gamma  \\gt^{\\pi}$,\n\n采样的状态、动作、奖励序列\n$$\n(S_{0},A_{0},R_{0}),(S_{1},A_{1},R_{1}),(S_{2},A_{2},R_{2}),...,(S_{n},A_{n},R_{n}),...\n$$\n是一个马尔科夫决策过程$ \\lt S,A^{\\pi},P,R,\\gamma  \\gt^{\\pi}$.\n\n*注意:在编程时一般以四元组$(s,a,r,s')$为单位存储\"经验\"*\n\n$\\pi$策略下$s\\rightarrow s'$转移概率由期望计算得$P_{ss'}^{\\pi}=\\sum_{a\\in A}\\pi(a|s)P_{ss'}^{a}$,$s$状态下的期望立即奖励为$R_{s}^{\\pi}=\\sum_{a\\in A}\\pi(a|s)R_{s}^{a}$.\n\n上述例子中\n$$\n\\begin{align*}\nR_{英语}&=\\sum_{a\\in A}\\pi(a|英语)R_{英语}^{a}\\\\\n&=0.2\\times-2+0.4\\times-2+0.4\\times-2\\\\\n&=-2\n\\end{align*}\n$$\n状态转移概率可以描述为：在执行策略$\\pi$时，状态从$s$转移至$s'$的概率等于执行该状态下所有行为的概率与对应行为能使状态从$s$转移至$s’$的概率的乘积的和。\n\n奖励函数可以描述为：在执行策略$\\pi$时获得的奖励等于执行该状态下所有行为的概率与对应行为产生的即时奖励的乘积的和。\n\n**强化学习的目标就是最大化期望回报,相应的结果就是找到从状态空间$S$映射到动作空间$A$的最优策略**,重点是,如何建立回报与策略之间的联系呢?","source":"_posts/强化学习之MDP马尔科夫决策过程.md","raw":"---\ntitle: 强化学习之MDP马尔科夫决策过程\ncopyright: true\ntop: 1\ndate: 2019-05-08 11:04:20\nmathjax: true\ncategories: ReinforcementLearning\ntags:\n- rl\n---\n\n# 强化学习之MDP马尔科夫决策过程\n\n每每提到强化学习，最先接触的理论肯定是马尔科夫决策过程（MDP，Markov Decision Process），为什么总提到MDP呢？并不是只有我一个人有这个疑问。\n\n<!--more-->\n\n百度上没有人提出这样的问题，可能是大家理解得都比较透彻吧，于是在Google查到相关提问和解释。\n\n> [What is the relationship between Markov Decision Processes and Reinforcement Learning?](https://datascience.stackexchange.com/a/38851)\n\n>>In Reinforcement Learning (RL), the problem to resolve is described as a Markov Decision Process (MDP). Theoretical results in RL rely on the MDP description being a correct match to the problem. If your problem is well described as a MDP, then RL may be a good framework to use to find solutions. That does not mean you need to fully describe the MDP (all the transition probabilities), just that you expect an MDP model could be made or discovered.\n\n>>Conversely, if you cannot map your problem onto a MDP, then the theory behind RL makes no guarantees of any useful result.\n\n>>One key factor that affects how well RL will work is that the states should have the Markov property - that the value of the current state is enough knowledge to fix immediate transition probabilities and immediate rewards following an action choice. Again you don't need to know in advance what those are, just that this relationship is expected to be reliable and stable. If it is not reliable, you may have a POMDP. If it is not stable, you may have a non-stationary problem. In either case, if the difference from a more strictly defined MDP is small enough, you may still get away with using RL techniques or need to adapt them slightly.\n>>\n>>**The general relationship between RL and MDP is that RL is a framework for solving problems that can be expressed as MDPs.**\n\nMDP是当前强化学习理论推导的基石，对强化学习来说，一般以马尔科夫决策过程作为形式化问题的手段。也就是说，对于目前的绝大部分强化学习算法，只有可以将问题抽象为MDP的才可以确保算法的性能（收敛性，效果等），对于违背MDP的问题并不一定确保算法有效，因为其数学公式都是基于MDP来进行推导的。\n\n## 马尔科夫性\n\n> 马尔科夫性质（英语：Markov property）是概率论中的一个概念，因为俄国数学家安德雷·马尔科夫得名。当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔科夫性质。[马尔科夫性-百度百科](https://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8/23149887?fr=aladdin)\n\n马尔科夫性，也就是无后效性：**某阶段的状态一旦确定，则此后过程的演变不再受此前各状态及决策的影响**。也就是说，**未来与过去无关**。\n\n具体地说，如果一个问题被划分各个阶段之后，阶段$k$中的状态只能通过阶段$k+1$中的状态通过状态转移方程得来，与其他状态没有关系，特别是与未发生的状态没有关系，这就是无后效性。\n\n公式描述：\n\n$$\nP[S_{t+1}|S_{t}]=P[S_{t+1}|S_{1},...,S_{t}]\n$$\n\n强化学习问题中的状态也符合马尔科夫性，即在当前状态$s_{t}$下执行动作$a_{t}$并转移至下一个状态$s_{t+1}$，而不需要考虑之前的状态$s_{t-1},...,s_{1}$。\n\n举一个不恰当的例子：\n\n![](./强化学习之MDP马尔科夫决策过程/M.jpg)\n\n假设天气预测符合马尔科夫性，如果以每天表示为一种状态，即周一、周二到周日。今天（5月8日，周三）天气为晴，明天（周四）会不会下雨只与今天的天气有关，而与之前周一、周二的天气状况无关。如果以时间节点表示为一种状态，即2点、5点、8点等，如图2点的温度为15.8°C,那么下个时间点5点的气温如何只与2点的温度有关系。\n\n强化学习中默认状态的转移是符合马尔科夫性质的，状态具体是什么，需要根据不同的问题进行不同的设定。\n\n## 马尔科夫过程\n\n马尔科夫过程是随机过程的一种，什么是随机过程呢？简单来说，一个商店从早上营业到晚上打烊这段时间，根据每个时间点店内顾客的人数所组成的序列就是随机过程。随机过程根据时间节点$T_{t}$取到的值是一个变量。\n\n马尔科夫过程是满足马尔科夫性的随机过程，它由二元组$M=(S,P)$组成，且满足：\n\n1. S是有限状态集合\n2. P是状态转移概率矩阵\n\n状态与状态之间的转换过程即为马尔科夫过程。***虽然我们可能不知道P的具体值到底是什么，但是通常我们假设P是存在的（转移概率存在，如果是确定的，无非就是概率为1），而且是稳定的（意思是从状态A到其他状态的转移虽然符合某个分布，但是其转移到某个状态的概率是确定的，不随时间变化的）。***\n\n这里说的**有限**二字我有自己的理解，在最开始的强化学习研究中，解决的都是表格式的问题，也就是状态的数量是有限可取的，但是后续强化学习研究的也有连续状态空间的问题，算法如DQN,PG,PPO等。状态的数量并不是有限的，但是其向量维度则是固定的、有限的，而且也同样符合马尔科夫性质，因此**我认为这里定义的有限并不是说状态数量有限，而是状态维度有限**。因为好像没有无限马尔科夫的叫法，所以姑且这么解释一下。\n\n马尔科夫过程有如下分类：\n\n![](./强化学习之MDP马尔科夫决策过程/MPs.jpg)\n\n### 状态转移矩阵\n\n状态转移矩阵由许多状态转移概率组成，状态转移概率是指从一个马尔科夫状态$s$转移到下一个状态$s'$的概率。\n\n公示表示：\n\n$$\n\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]\n$$\n\n等同于：\n\n$$\n\\mathcal{P}(s'|s)=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]\n$$\n\n假设有1到n个状态，将所有的状态从上到下、从左到右排列，组成一个$n \\times n$的矩阵，那么其状态转移矩阵如下所示：\n\n$$\n\\mathcal{P}=\n\\begin{bmatrix}\n\\mathcal{P}_{11} & \\cdots & \\mathcal{P}_{1n} \\\\ \n\\vdots & \\ddots & \\vdots \\\\ \n\\mathcal{P}_{n1} & \\cdots & \\mathcal{P}_{nn} \\\\\n\\end{bmatrix}\n$$\n\n其中，每行元素相加等于1，每列元素相加等于1，矩阵的总和为状态的数量n。\n\n对于可数状态，$\\sum_{s'=1}^{n}\\mathcal{P}(s'|s)=1$\n\n$$\nsum(\\mathcal{P}) = \\sum_{s'=1}^{n}\\sum_{s=1}^{n}\\mathcal{P}_{ss'} = n\n$$\n\n对于不可数状态（连续状态),$\\int_{s'}\\mathcal{P}(s'|s)=1$\n\n$$\nsum(\\mathcal{P}) = \\int_{s'}\\int_{s}\\mathcal{P}_{ss'} = n\n$$\n\n举一个马尔科夫过程的例子:\n\n假设一个学生，他目前在学习语文科目，那么他接下来进行的活动过程如下图所示，游戏的吸引力很大，所以他有50%的概率在学完语文去玩游戏，并且很容易沉迷其中，图示玩游戏这个循环有90%的可能性，他还可以选择学习其他科目或者去睡觉，最终学习结束之后是否能通过考试也是有一定的概率的，这些状态之间转移的概率即为状态转移概率。\n\n![](./强化学习之MDP马尔科夫决策过程/MP.jpg)\n\n如果把例子中的各项状态用字母表示，将其表示为：\n$$\n\\begin{bmatrix}\n玩游戏 & A\\\\ \n语文 & B\\\\ \n数学 & C\\\\ \n英语 & D\\\\ \n挂科 & E\\\\ \n\\mathcal{Pass} & F\\\\ \n睡觉 & G\n\\end{bmatrix}\n$$\n那么其状态转移矩阵$\\mathcal{P}$可以表示成：\n$$\n\\begin{array}{lc}\n\\mbox{}&\n\\begin{array}{cc}A & B & C & D & E & F & G \\end{array}\\\\\n\\begin{array}{c}A\\\\B\\\\C\\\\D\\\\E\\\\F\\\\G\\end{array}&\n\\left[\\begin{array}{cc}\n0.9&0.1\\\\\n0.5& &0.5\\\\\n& & &0.8& & &0.2\\\\\n&&&&0.4&0.6&\\\\\n&0.2&0.4&0.4&&&\\\\\n&&&&&&1\\\\\n&&&&&&&\n\\end{array}\\right]\n\\end{array}\n$$\n\n### 马尔科夫链与Episode\n\nEpisode可以翻译为片段、情节、回合等，在强化学习问题中，一个Episode就是一个马尔科夫链，根据状态转移矩阵可以得到许多不同的episode，也就是多个马尔科夫链。\n\n强化学习问题分两种：\n\n1. 如果一个任务总能达到终态，结束任务或者开启下一轮任务，那么这个任务就被称为回合任务，也就是episode任务。例如，让一个智能体学习如何下围棋，围棋棋盘只有那么大，游戏定会终局，所以是一个回合式任务。\n2. 如果一个任务可以无限持续下去，永远不会结束，即永远在训练当中，那么这个任务就被称为连续性任务。例如，教会一辆车能够进行自动驾驶就是一个连续性任务，*不要钻牛角尖说能源会耗尽，车子会磨损，我们只聚焦问题与环境本身，不涉及其他非稳定因素。*\n\n在上边举的例子中就是一个回合式任务，因为无论这个序列有多长，最终都会达到终态-“睡觉”。\n\n根据上述例子我们可能采样出如下episode：\n\n1. $B-C-D-E-C-G$，即“学语文→数学→英语→考试没通过,挂科→继续学数学→睡觉”\n2. $B-A-A-...-A-B-C-G$，即“学语文→玩王者荣耀→玩刺激战场→玩OverCooker→玩守望先锋→玩英雄联盟→玩CS:GO→...→看一会儿数学→睡觉”。（仿佛就是我自己嘛！）\n\n## 马尔科夫奖励过程\n\n马尔科夫过程（Markov Process）主要描述的是状态之间的转移关系，在各个状态的转移过程中赋予不同的奖励值就得到了马尔科夫奖励过程。\n\n定义：马尔科夫奖励过程（Markov Reward Process, MRP）由一个四元组组成$(S,P,R,\\gamma)$\n\n1. $S$代表了状态的集合(也是维度有限的)\n2. $P$描述了状态转移矩阵$\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]$\n3. $R$表示奖励函数，$R(s)$描述了在状态$s$下的期望(立即)奖励，$\\mathcal{R}(s)=\\mathbb{E}[R_{t+1}|S_{t}=s]$\n4. $\\gamma$表示衰减因子,即discounted factor,$\\gamma\\in[0,1]$\n\n$\\gamma$是用来计算累计奖励回报的,表示我们有多看中现在或者未来,为什么这么说呢?假设我们现在要计算一个episode始态$S_{0}$的奖励值$V(S_{0})$,不涉及具体公式推导的说,我们应该把$S_{0}$状态后续的奖励全部加和,这样就得到了对始态$S_{0}$的值估计,这些后续奖励的值的权重都是1,或者说此时$\\gamma=1$,但是当前状态对很多步之后的状态未必影响很大,我们这样计算过来并不能完全表示一个状态的值,那么我们应当顺势减少距离远的状态的权重,此时$\\gamma\\lt1$\n\n- 当$\\gamma=0$时,状态$S$的值完全由其转移的期望立即奖励表示,即**一点都不关心未来**\n- 当$\\gamma=1$时,状态$S$的值由以当前状态为始态,运行至终态所得到的所有立即奖励加和的值表示,即**未来与现在同等重要**\n- 当$0 \\lt\\gamma \\lt1$时,状态$S$的值是前两个模式的*trade-off*,即**对未来看重的程度由$\\gamma$决定**\n\n这只是我们的直观感受,其实是为了数学便利（虽然我也不知道具体哪里提高了数学便利，但是在有些情况下会使值函数更快迭代收敛这是真的）。\n\n*注：也有很多地方将MRP表示为三元组，即去掉$\\gamma$，但这不影响我们对这个过程的理解，下边的MDP也是一样，无论是三元组、四元组、还是五元组，只要能描述过程的性质就可以。*\n\n将上述马尔科夫过程的例子升级为马尔科夫奖励过程如下图所示:\n\n![](./强化学习之MDP马尔科夫决策过程/MRP.png)\n\n奖励值定义为:\n$$\n\\begin{bmatrix}\n玩游戏 & A & -1\\\\ \n语文 & B & -2\\\\ \n数学 & C & -2\\\\ \n英语 & D & -2\\\\ \n挂科 & E & -5\\\\ \n\\mathcal{Pass} & F & 10\\\\ \n睡觉 & G & 0\n\\end{bmatrix}\n$$\n这么定义奖励并没有什么复杂的含义,在这个例子中就拿身心愉悦程度来定义吧,学习固然是枯燥无味的,所以给予负奖励-2,玩游戏虽然会心情放松,但是始终面临着考试的压力,其实并不轻松,所以给予负奖励-1,挂科最痛苦为-5,考试全pass最开心为+10。\n\n在马尔科夫过程中的状态转移加入相应的奖励值即为马尔科夫奖励过程。\n\n\n\n## 马尔科夫决策过程\n\n马尔科夫决策过程(Markov Decision Process, MDP)相比马尔科夫奖励过程多了一个动作$A$,它可以用一个五元组$(S,A,P,R,\\gamma)$表示:\n\n1. $S$代表了状态的集合(也是维度有限的)\n2. $A$代表了决策过程中动作的集合(维度有限的)\n3. $P$描述了状态转移矩阵$\\mathcal{P}_{ss'}^{a}=\\mathbb{P}[S_{t+1}=s'|S_{t}=s,A_{t}=a]$\n4. $R$表示奖励函数，$R(s)$描述了在状态$s$下**执行某动作**的期望(立即)奖励，$\\mathcal{R}(s,a)=\\mathbb{E}[R_{t+1}|S_{t}=s,A_{t}=a]$\n5. $\\gamma$表示衰减因子,即discounted factor,$\\gamma\\in[0,1]$\n\nMDPs是一个从交互中达成目标的强化学习问题的一个直接的框架。学习者和决策者叫做Agent。Agent进行交互的其它一切Agent之外的东西都叫做环境。Agent不断的选择动作，而环境也给出相应的反应，并且向Agent表现出新的状态。环境同时也给出一个数值作为反馈。Agent的目标就是通过选择不同的Action来最大化这个反馈值。\n\n![](./强化学习之MDP马尔科夫决策过程/agent-env.png)\n\n强化学习所研究的内容就是得到一个状态$S$到动作$A$的映射关系,因此策略Policy可以表示成\n$$\n\\pi(a|s)=p(A_{t}=a|S_{t}=s)\n$$\n---\n\n注意:\n你可能会认为,在马尔科夫奖励过程(MRP)中没有定义动作,但是其实是包含动作的,因为每个状态有多个转移的下一状态,其实就是多个动作嘛！\n\n很多文章会将有限MDP分开来讲，有限MDP即状态、动作和奖励值都只有有限个元素，对于有限MDP最优策略有唯一解，但是现实世界中任务复杂，因此大多数深度强化学习算法并不局限于解决有限MDP问题，因此本文不将MDP分情况来讲，即默认基于MDP的最优策略**至少有一个解**。\n\n---\n\n没错,的确是这样的,MRP中也包含动作,但是我们并不关心,为什么这么说呢?**因为就算每个状态可以执行多个动作,但是其每个动作所能转移到的状态是确定的,不确定的只是动作的选择,而不是动作的转移,而MDP中不确定的却是动作的转移,即执行动作所转移的下一状态是有一定概率的.**什么意思呢?拿之前MRP的例子来说,语文状态有两个状态可以转移,数学和玩游戏,概率分别是0.5,但是当确定一个转移方向的时候(图中的箭头),其转移结果是确定的,获得的奖励也是确定的,但是在MDP中,执行动作导致转移的结果都未必是确定的.***需要注意的是,MRP是属于MDP的,MDP执行动作并不一定必须是随机的.***\n\n接下来,我们将MRP的例子转换至MDP, 为了方便理解而又不增加示例的复杂性,不妨将\"挂科\"这个状态看作是一个动作,因为这个节点正巧入度为1,姑且就认为从英语到挂科的这个箭头是英语状态所能执行的动作.如图所示:\n\n![](./强化学习之MDP马尔科夫决策过程/MDP.jpg)\n\n比较两个图可以发现区别,我把这个不确定的动作标为实心黑圆圈,这位刻苦的同学在学习完英语之后还想继续学习,但是他感觉三门科目都差不多了,于是他也很迷茫,他执行\"学习\"这个动作时的转移状态有三种:学语文、学数学、学英语.概率分别是:0.2、0.4,、0.4.这下就明白为什么我们要在MDP中加入动作$A$了吧,如果还不明白,请接着看下边的内容.\n\n顺便说一下,这个时候的转移矩阵已经不是简单的二维了,当然也可以用二维来表示,假设总共有$n$个状态,每个状态有$m$个动作,那么其行数为$n\\times m$,即遍历所有的状态和动作,得到$n \\times m$个状态-动作对$(s,a)$,其列数还是$n$.当然,也可以用一个三维tensor来表示,行和列都是$n$,第三维深度为$m$,很好理解.\n\n---\n\n**网上有写MDP在给定策略下会退化为MRP,我对此不置可否,认为此种说法不够严谨,因为即使说在某状态s下选择的动作a是确定的,并不意味着其转移结果是确定的.**\n\n---\n\n### 回报 Return\n\n在强化学习问题中，总是提到回报二字，论文中出现Return或者Discounted Return，我们已经知道奖励是什么，奖励就是转移到某个状态或者执行了某个动作之后转移至某个状态所获得的值$r$.\n\n回报就是由某时刻$t$之后决策序列所获得的奖励值经过一定规则计算出来的数值.\n\n公式描述:\n$$\nG_{t}\\doteq R_{t+1}+R_{t+2}+R_{t+3}+...+R_{t}\n$$\n.其中,$T$表示一个episode达到终态的时间点.\n\n像之前介绍的一样,我们可能对未来有不同的看重程度,于是引入折扣因子$\\gamma$的回报表示为:\n$$\nG_{t}\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+...=\\begin{cases}\n\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\\\\n\\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_{k}\n\\end{cases}\n$$\n其中,$ 0\\leq\\gamma \\leq1$\n\n可以推出回报有如下形式:\n$$\n\\begin{align*}\nG_{t} &\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\gamma^{3}R_{t+4}+...\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\gamma^{2}R_{t+4}+...)\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma (R_{t+3}+\\gamma R_{t+4}+...))\\\\\n&=R_{t+1}+\\gamma G_{t+1}\n\\end{align*}\n$$\n\n### 策略 Policy\n\n我们一般使用$\\pi$来表示一个策略,使用$\\pi(a|s)$来表示某状态$s$采取动作$a$的概率,公示表示为:\n$$\n\\pi(a|s)=P(A_{t}=a|S_{t}=s)\n$$\n策略完整定义了智能体在所有状态下的所有行为和其概率.\n\n给定一个MDP和一个策略$\\pi$,采样的状态序列\n$$\nS_{0},S_{1},S_{2},...,S_{n},...\n$$\n是一个马尔科夫过程$\\lt S,P \\gt ^{\\pi}$,\n\n采样的状态、奖励序列\n$$\n(S_{0},R_{0}),(S_{1},R_{1}),(S_{2},R_{2}),...,(S_{n},R_{n}),...\n$$\n是一个马尔科夫奖励过程$ \\lt S,P,R,\\gamma  \\gt^{\\pi}$,\n\n采样的状态、动作、奖励序列\n$$\n(S_{0},A_{0},R_{0}),(S_{1},A_{1},R_{1}),(S_{2},A_{2},R_{2}),...,(S_{n},A_{n},R_{n}),...\n$$\n是一个马尔科夫决策过程$ \\lt S,A^{\\pi},P,R,\\gamma  \\gt^{\\pi}$.\n\n*注意:在编程时一般以四元组$(s,a,r,s')$为单位存储\"经验\"*\n\n$\\pi$策略下$s\\rightarrow s'$转移概率由期望计算得$P_{ss'}^{\\pi}=\\sum_{a\\in A}\\pi(a|s)P_{ss'}^{a}$,$s$状态下的期望立即奖励为$R_{s}^{\\pi}=\\sum_{a\\in A}\\pi(a|s)R_{s}^{a}$.\n\n上述例子中\n$$\n\\begin{align*}\nR_{英语}&=\\sum_{a\\in A}\\pi(a|英语)R_{英语}^{a}\\\\\n&=0.2\\times-2+0.4\\times-2+0.4\\times-2\\\\\n&=-2\n\\end{align*}\n$$\n状态转移概率可以描述为：在执行策略$\\pi$时，状态从$s$转移至$s'$的概率等于执行该状态下所有行为的概率与对应行为能使状态从$s$转移至$s’$的概率的乘积的和。\n\n奖励函数可以描述为：在执行策略$\\pi$时获得的奖励等于执行该状态下所有行为的概率与对应行为产生的即时奖励的乘积的和。\n\n**强化学习的目标就是最大化期望回报,相应的结果就是找到从状态空间$S$映射到动作空间$A$的最优策略**,重点是,如何建立回报与策略之间的联系呢?","slug":"强化学习之MDP马尔科夫决策过程","published":1,"updated":"2019-05-13T01:09:08.291Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxd6ma86003oekvegd9d4oli","content":"<h1 id=\"强化学习之MDP马尔科夫决策过程\"><a href=\"#强化学习之MDP马尔科夫决策过程\" class=\"headerlink\" title=\"强化学习之MDP马尔科夫决策过程\"></a>强化学习之MDP马尔科夫决策过程</h1><p>每每提到强化学习，最先接触的理论肯定是马尔科夫决策过程（MDP，Markov Decision Process），为什么总提到MDP呢？并不是只有我一个人有这个疑问。</p>\n<a id=\"more\"></a>\n<p>百度上没有人提出这样的问题，可能是大家理解得都比较透彻吧，于是在Google查到相关提问和解释。</p>\n<blockquote>\n<p><a href=\"https://datascience.stackexchange.com/a/38851\" rel=\"external nofollow\" target=\"_blank\">What is the relationship between Markov Decision Processes and Reinforcement Learning?</a></p>\n<blockquote>\n<p>In Reinforcement Learning (RL), the problem to resolve is described as a Markov Decision Process (MDP). Theoretical results in RL rely on the MDP description being a correct match to the problem. If your problem is well described as a MDP, then RL may be a good framework to use to find solutions. That does not mean you need to fully describe the MDP (all the transition probabilities), just that you expect an MDP model could be made or discovered.</p>\n<p>Conversely, if you cannot map your problem onto a MDP, then the theory behind RL makes no guarantees of any useful result.</p>\n<p>One key factor that affects how well RL will work is that the states should have the Markov property - that the value of the current state is enough knowledge to fix immediate transition probabilities and immediate rewards following an action choice. Again you don’t need to know in advance what those are, just that this relationship is expected to be reliable and stable. If it is not reliable, you may have a POMDP. If it is not stable, you may have a non-stationary problem. In either case, if the difference from a more strictly defined MDP is small enough, you may still get away with using RL techniques or need to adapt them slightly.</p>\n<p><strong>The general relationship between RL and MDP is that RL is a framework for solving problems that can be expressed as MDPs.</strong></p>\n</blockquote>\n</blockquote>\n<p>MDP是当前强化学习理论推导的基石，对强化学习来说，一般以马尔科夫决策过程作为形式化问题的手段。也就是说，对于目前的绝大部分强化学习算法，只有可以将问题抽象为MDP的才可以确保算法的性能（收敛性，效果等），对于违背MDP的问题并不一定确保算法有效，因为其数学公式都是基于MDP来进行推导的。</p>\n<h2 id=\"马尔科夫性\"><a href=\"#马尔科夫性\" class=\"headerlink\" title=\"马尔科夫性\"></a>马尔科夫性</h2><blockquote>\n<p>马尔科夫性质（英语：Markov property）是概率论中的一个概念，因为俄国数学家安德雷·马尔科夫得名。当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔科夫性质。<a href=\"https://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8/23149887?fr=aladdin\" rel=\"external nofollow\" target=\"_blank\">马尔科夫性-百度百科</a></p>\n</blockquote>\n<p>马尔科夫性，也就是无后效性：<strong>某阶段的状态一旦确定，则此后过程的演变不再受此前各状态及决策的影响</strong>。也就是说，<strong>未来与过去无关</strong>。</p>\n<p>具体地说，如果一个问题被划分各个阶段之后，阶段$k$中的状态只能通过阶段$k+1$中的状态通过状态转移方程得来，与其他状态没有关系，特别是与未发生的状态没有关系，这就是无后效性。</p>\n<p>公式描述：</p>\n<script type=\"math/tex; mode=display\">\nP[S_{t+1}|S_{t}]=P[S_{t+1}|S_{1},...,S_{t}]</script><p>强化学习问题中的状态也符合马尔科夫性，即在当前状态$s_{t}$下执行动作$a_{t}$并转移至下一个状态$s_{t+1}$，而不需要考虑之前的状态$s_{t-1},…,s_{1}$。</p>\n<p>举一个不恰当的例子：</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/M.jpg\" alt=\"\"></p>\n<p>假设天气预测符合马尔科夫性，如果以每天表示为一种状态，即周一、周二到周日。今天（5月8日，周三）天气为晴，明天（周四）会不会下雨只与今天的天气有关，而与之前周一、周二的天气状况无关。如果以时间节点表示为一种状态，即2点、5点、8点等，如图2点的温度为15.8°C,那么下个时间点5点的气温如何只与2点的温度有关系。</p>\n<p>强化学习中默认状态的转移是符合马尔科夫性质的，状态具体是什么，需要根据不同的问题进行不同的设定。</p>\n<h2 id=\"马尔科夫过程\"><a href=\"#马尔科夫过程\" class=\"headerlink\" title=\"马尔科夫过程\"></a>马尔科夫过程</h2><p>马尔科夫过程是随机过程的一种，什么是随机过程呢？简单来说，一个商店从早上营业到晚上打烊这段时间，根据每个时间点店内顾客的人数所组成的序列就是随机过程。随机过程根据时间节点$T_{t}$取到的值是一个变量。</p>\n<p>马尔科夫过程是满足马尔科夫性的随机过程，它由二元组$M=(S,P)$组成，且满足：</p>\n<ol>\n<li>S是有限状态集合</li>\n<li>P是状态转移概率矩阵</li>\n</ol>\n<p>状态与状态之间的转换过程即为马尔科夫过程。<strong><em>虽然我们可能不知道P的具体值到底是什么，但是通常我们假设P是存在的（转移概率存在，如果是确定的，无非就是概率为1），而且是稳定的（意思是从状态A到其他状态的转移虽然符合某个分布，但是其转移到某个状态的概率是确定的，不随时间变化的）。</em></strong></p>\n<p>这里说的<strong>有限</strong>二字我有自己的理解，在最开始的强化学习研究中，解决的都是表格式的问题，也就是状态的数量是有限可取的，但是后续强化学习研究的也有连续状态空间的问题，算法如DQN,PG,PPO等。状态的数量并不是有限的，但是其向量维度则是固定的、有限的，而且也同样符合马尔科夫性质，因此<strong>我认为这里定义的有限并不是说状态数量有限，而是状态维度有限</strong>。因为好像没有无限马尔科夫的叫法，所以姑且这么解释一下。</p>\n<p>马尔科夫过程有如下分类：</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/MPs.jpg\" alt=\"\"></p>\n<h3 id=\"状态转移矩阵\"><a href=\"#状态转移矩阵\" class=\"headerlink\" title=\"状态转移矩阵\"></a>状态转移矩阵</h3><p>状态转移矩阵由许多状态转移概率组成，状态转移概率是指从一个马尔科夫状态$s$转移到下一个状态$s’$的概率。</p>\n<p>公示表示：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]</script><p>等同于：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{P}(s'|s)=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]</script><p>假设有1到n个状态，将所有的状态从上到下、从左到右排列，组成一个$n \\times n$的矩阵，那么其状态转移矩阵如下所示：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{P}=\n\\begin{bmatrix}\n\\mathcal{P}_{11} & \\cdots & \\mathcal{P}_{1n} \\\\ \n\\vdots & \\ddots & \\vdots \\\\ \n\\mathcal{P}_{n1} & \\cdots & \\mathcal{P}_{nn} \\\\\n\\end{bmatrix}</script><p>其中，每行元素相加等于1，每列元素相加等于1，矩阵的总和为状态的数量n。</p>\n<p>对于可数状态，$\\sum_{s’=1}^{n}\\mathcal{P}(s’|s)=1$</p>\n<script type=\"math/tex; mode=display\">\nsum(\\mathcal{P}) = \\sum_{s'=1}^{n}\\sum_{s=1}^{n}\\mathcal{P}_{ss'} = n</script><p>对于不可数状态（连续状态),$\\int_{s’}\\mathcal{P}(s’|s)=1$</p>\n<script type=\"math/tex; mode=display\">\nsum(\\mathcal{P}) = \\int_{s'}\\int_{s}\\mathcal{P}_{ss'} = n</script><p>举一个马尔科夫过程的例子:</p>\n<p>假设一个学生，他目前在学习语文科目，那么他接下来进行的活动过程如下图所示，游戏的吸引力很大，所以他有50%的概率在学完语文去玩游戏，并且很容易沉迷其中，图示玩游戏这个循环有90%的可能性，他还可以选择学习其他科目或者去睡觉，最终学习结束之后是否能通过考试也是有一定的概率的，这些状态之间转移的概率即为状态转移概率。</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/MP.jpg\" alt=\"\"></p>\n<p>如果把例子中的各项状态用字母表示，将其表示为：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n玩游戏 & A\\\\ \n语文 & B\\\\ \n数学 & C\\\\ \n英语 & D\\\\ \n挂科 & E\\\\ \n\\mathcal{Pass} & F\\\\ \n睡觉 & G\n\\end{bmatrix}</script><p>那么其状态转移矩阵$\\mathcal{P}$可以表示成：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{lc}\n\\mbox{}&\n\\begin{array}{cc}A & B & C & D & E & F & G \\end{array}\\\\\n\\begin{array}{c}A\\\\B\\\\C\\\\D\\\\E\\\\F\\\\G\\end{array}&\n\\left[\\begin{array}{cc}\n0.9&0.1\\\\\n0.5& &0.5\\\\\n& & &0.8& & &0.2\\\\\n&&&&0.4&0.6&\\\\\n&0.2&0.4&0.4&&&\\\\\n&&&&&&1\\\\\n&&&&&&&\n\\end{array}\\right]\n\\end{array}</script><h3 id=\"马尔科夫链与Episode\"><a href=\"#马尔科夫链与Episode\" class=\"headerlink\" title=\"马尔科夫链与Episode\"></a>马尔科夫链与Episode</h3><p>Episode可以翻译为片段、情节、回合等，在强化学习问题中，一个Episode就是一个马尔科夫链，根据状态转移矩阵可以得到许多不同的episode，也就是多个马尔科夫链。</p>\n<p>强化学习问题分两种：</p>\n<ol>\n<li>如果一个任务总能达到终态，结束任务或者开启下一轮任务，那么这个任务就被称为回合任务，也就是episode任务。例如，让一个智能体学习如何下围棋，围棋棋盘只有那么大，游戏定会终局，所以是一个回合式任务。</li>\n<li>如果一个任务可以无限持续下去，永远不会结束，即永远在训练当中，那么这个任务就被称为连续性任务。例如，教会一辆车能够进行自动驾驶就是一个连续性任务，<em>不要钻牛角尖说能源会耗尽，车子会磨损，我们只聚焦问题与环境本身，不涉及其他非稳定因素。</em></li>\n</ol>\n<p>在上边举的例子中就是一个回合式任务，因为无论这个序列有多长，最终都会达到终态-“睡觉”。</p>\n<p>根据上述例子我们可能采样出如下episode：</p>\n<ol>\n<li>$B-C-D-E-C-G$，即“学语文→数学→英语→考试没通过,挂科→继续学数学→睡觉”</li>\n<li>$B-A-A-…-A-B-C-G$，即“学语文→玩王者荣耀→玩刺激战场→玩OverCooker→玩守望先锋→玩英雄联盟→玩CS:GO→…→看一会儿数学→睡觉”。（仿佛就是我自己嘛！）</li>\n</ol>\n<h2 id=\"马尔科夫奖励过程\"><a href=\"#马尔科夫奖励过程\" class=\"headerlink\" title=\"马尔科夫奖励过程\"></a>马尔科夫奖励过程</h2><p>马尔科夫过程（Markov Process）主要描述的是状态之间的转移关系，在各个状态的转移过程中赋予不同的奖励值就得到了马尔科夫奖励过程。</p>\n<p>定义：马尔科夫奖励过程（Markov Reward Process, MRP）由一个四元组组成$(S,P,R,\\gamma)$</p>\n<ol>\n<li>$S$代表了状态的集合(也是维度有限的)</li>\n<li>$P$描述了状态转移矩阵$\\mathcal{P}_{ss’}=\\mathbb{P}[S_{t+1}=s’|S_{t}=s]$</li>\n<li>$R$表示奖励函数，$R(s)$描述了在状态$s$下的期望(立即)奖励，$\\mathcal{R}(s)=\\mathbb{E}[R_{t+1}|S_{t}=s]$</li>\n<li>$\\gamma$表示衰减因子,即discounted factor,$\\gamma\\in[0,1]$</li>\n</ol>\n<p>$\\gamma$是用来计算累计奖励回报的,表示我们有多看中现在或者未来,为什么这么说呢?假设我们现在要计算一个episode始态$S_{0}$的奖励值$V(S_{0})$,不涉及具体公式推导的说,我们应该把$S_{0}$状态后续的奖励全部加和,这样就得到了对始态$S_{0}$的值估计,这些后续奖励的值的权重都是1,或者说此时$\\gamma=1$,但是当前状态对很多步之后的状态未必影响很大,我们这样计算过来并不能完全表示一个状态的值,那么我们应当顺势减少距离远的状态的权重,此时$\\gamma\\lt1$</p>\n<ul>\n<li>当$\\gamma=0$时,状态$S$的值完全由其转移的期望立即奖励表示,即<strong>一点都不关心未来</strong></li>\n<li>当$\\gamma=1$时,状态$S$的值由以当前状态为始态,运行至终态所得到的所有立即奖励加和的值表示,即<strong>未来与现在同等重要</strong></li>\n<li>当$0 \\lt\\gamma \\lt1$时,状态$S$的值是前两个模式的<em>trade-off</em>,即<strong>对未来看重的程度由$\\gamma$决定</strong></li>\n</ul>\n<p>这只是我们的直观感受,其实是为了数学便利（虽然我也不知道具体哪里提高了数学便利，但是在有些情况下会使值函数更快迭代收敛这是真的）。</p>\n<p><em>注：也有很多地方将MRP表示为三元组，即去掉$\\gamma$，但这不影响我们对这个过程的理解，下边的MDP也是一样，无论是三元组、四元组、还是五元组，只要能描述过程的性质就可以。</em></p>\n<p>将上述马尔科夫过程的例子升级为马尔科夫奖励过程如下图所示:</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/MRP.png\" alt=\"\"></p>\n<p>奖励值定义为:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n玩游戏 & A & -1\\\\ \n语文 & B & -2\\\\ \n数学 & C & -2\\\\ \n英语 & D & -2\\\\ \n挂科 & E & -5\\\\ \n\\mathcal{Pass} & F & 10\\\\ \n睡觉 & G & 0\n\\end{bmatrix}</script><p>这么定义奖励并没有什么复杂的含义,在这个例子中就拿身心愉悦程度来定义吧,学习固然是枯燥无味的,所以给予负奖励-2,玩游戏虽然会心情放松,但是始终面临着考试的压力,其实并不轻松,所以给予负奖励-1,挂科最痛苦为-5,考试全pass最开心为+10。</p>\n<p>在马尔科夫过程中的状态转移加入相应的奖励值即为马尔科夫奖励过程。</p>\n<h2 id=\"马尔科夫决策过程\"><a href=\"#马尔科夫决策过程\" class=\"headerlink\" title=\"马尔科夫决策过程\"></a>马尔科夫决策过程</h2><p>马尔科夫决策过程(Markov Decision Process, MDP)相比马尔科夫奖励过程多了一个动作$A$,它可以用一个五元组$(S,A,P,R,\\gamma)$表示:</p>\n<ol>\n<li>$S$代表了状态的集合(也是维度有限的)</li>\n<li>$A$代表了决策过程中动作的集合(维度有限的)</li>\n<li>$P$描述了状态转移矩阵$\\mathcal{P}_{ss’}^{a}=\\mathbb{P}[S_{t+1}=s’|S_{t}=s,A_{t}=a]$</li>\n<li>$R$表示奖励函数，$R(s)$描述了在状态$s$下<strong>执行某动作</strong>的期望(立即)奖励，$\\mathcal{R}(s,a)=\\mathbb{E}[R_{t+1}|S_{t}=s,A_{t}=a]$</li>\n<li>$\\gamma$表示衰减因子,即discounted factor,$\\gamma\\in[0,1]$</li>\n</ol>\n<p>MDPs是一个从交互中达成目标的强化学习问题的一个直接的框架。学习者和决策者叫做Agent。Agent进行交互的其它一切Agent之外的东西都叫做环境。Agent不断的选择动作，而环境也给出相应的反应，并且向Agent表现出新的状态。环境同时也给出一个数值作为反馈。Agent的目标就是通过选择不同的Action来最大化这个反馈值。</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/agent-env.png\" alt=\"\"></p>\n<p>强化学习所研究的内容就是得到一个状态$S$到动作$A$的映射关系,因此策略Policy可以表示成</p>\n<script type=\"math/tex; mode=display\">\n\\pi(a|s)=p(A_{t}=a|S_{t}=s)</script><hr>\n<p>注意:<br>你可能会认为,在马尔科夫奖励过程(MRP)中没有定义动作,但是其实是包含动作的,因为每个状态有多个转移的下一状态,其实就是多个动作嘛！</p>\n<p>很多文章会将有限MDP分开来讲，有限MDP即状态、动作和奖励值都只有有限个元素，对于有限MDP最优策略有唯一解，但是现实世界中任务复杂，因此大多数深度强化学习算法并不局限于解决有限MDP问题，因此本文不将MDP分情况来讲，即默认基于MDP的最优策略<strong>至少有一个解</strong>。</p>\n<hr>\n<p>没错,的确是这样的,MRP中也包含动作,但是我们并不关心,为什么这么说呢?<strong>因为就算每个状态可以执行多个动作,但是其每个动作所能转移到的状态是确定的,不确定的只是动作的选择,而不是动作的转移,而MDP中不确定的却是动作的转移,即执行动作所转移的下一状态是有一定概率的.</strong>什么意思呢?拿之前MRP的例子来说,语文状态有两个状态可以转移,数学和玩游戏,概率分别是0.5,但是当确定一个转移方向的时候(图中的箭头),其转移结果是确定的,获得的奖励也是确定的,但是在MDP中,执行动作导致转移的结果都未必是确定的.<strong><em>需要注意的是,MRP是属于MDP的,MDP执行动作并不一定必须是随机的.</em></strong></p>\n<p>接下来,我们将MRP的例子转换至MDP, 为了方便理解而又不增加示例的复杂性,不妨将”挂科”这个状态看作是一个动作,因为这个节点正巧入度为1,姑且就认为从英语到挂科的这个箭头是英语状态所能执行的动作.如图所示:</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/MDP.jpg\" alt=\"\"></p>\n<p>比较两个图可以发现区别,我把这个不确定的动作标为实心黑圆圈,这位刻苦的同学在学习完英语之后还想继续学习,但是他感觉三门科目都差不多了,于是他也很迷茫,他执行”学习”这个动作时的转移状态有三种:学语文、学数学、学英语.概率分别是:0.2、0.4,、0.4.这下就明白为什么我们要在MDP中加入动作$A$了吧,如果还不明白,请接着看下边的内容.</p>\n<p>顺便说一下,这个时候的转移矩阵已经不是简单的二维了,当然也可以用二维来表示,假设总共有$n$个状态,每个状态有$m$个动作,那么其行数为$n\\times m$,即遍历所有的状态和动作,得到$n \\times m$个状态-动作对$(s,a)$,其列数还是$n$.当然,也可以用一个三维tensor来表示,行和列都是$n$,第三维深度为$m$,很好理解.</p>\n<hr>\n<p><strong>网上有写MDP在给定策略下会退化为MRP,我对此不置可否,认为此种说法不够严谨,因为即使说在某状态s下选择的动作a是确定的,并不意味着其转移结果是确定的.</strong></p>\n<hr>\n<h3 id=\"回报-Return\"><a href=\"#回报-Return\" class=\"headerlink\" title=\"回报 Return\"></a>回报 Return</h3><p>在强化学习问题中，总是提到回报二字，论文中出现Return或者Discounted Return，我们已经知道奖励是什么，奖励就是转移到某个状态或者执行了某个动作之后转移至某个状态所获得的值$r$.</p>\n<p>回报就是由某时刻$t$之后决策序列所获得的奖励值经过一定规则计算出来的数值.</p>\n<p>公式描述:</p>\n<script type=\"math/tex; mode=display\">\nG_{t}\\doteq R_{t+1}+R_{t+2}+R_{t+3}+...+R_{t}</script><p>.其中,$T$表示一个episode达到终态的时间点.</p>\n<p>像之前介绍的一样,我们可能对未来有不同的看重程度,于是引入折扣因子$\\gamma$的回报表示为:</p>\n<script type=\"math/tex; mode=display\">\nG_{t}\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+...=\\begin{cases}\n\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\\\\n\\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_{k}\n\\end{cases}</script><p>其中,$ 0\\leq\\gamma \\leq1$</p>\n<p>可以推出回报有如下形式:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nG_{t} &\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\gamma^{3}R_{t+4}+...\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\gamma^{2}R_{t+4}+...)\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma (R_{t+3}+\\gamma R_{t+4}+...))\\\\\n&=R_{t+1}+\\gamma G_{t+1}\n\\end{align*}</script><h3 id=\"策略-Policy\"><a href=\"#策略-Policy\" class=\"headerlink\" title=\"策略 Policy\"></a>策略 Policy</h3><p>我们一般使用$\\pi$来表示一个策略,使用$\\pi(a|s)$来表示某状态$s$采取动作$a$的概率,公示表示为:</p>\n<script type=\"math/tex; mode=display\">\n\\pi(a|s)=P(A_{t}=a|S_{t}=s)</script><p>策略完整定义了智能体在所有状态下的所有行为和其概率.</p>\n<p>给定一个MDP和一个策略$\\pi$,采样的状态序列</p>\n<script type=\"math/tex; mode=display\">\nS_{0},S_{1},S_{2},...,S_{n},...</script><p>是一个马尔科夫过程$\\lt S,P \\gt ^{\\pi}$,</p>\n<p>采样的状态、奖励序列</p>\n<script type=\"math/tex; mode=display\">\n(S_{0},R_{0}),(S_{1},R_{1}),(S_{2},R_{2}),...,(S_{n},R_{n}),...</script><p>是一个马尔科夫奖励过程$ \\lt S,P,R,\\gamma  \\gt^{\\pi}$,</p>\n<p>采样的状态、动作、奖励序列</p>\n<script type=\"math/tex; mode=display\">\n(S_{0},A_{0},R_{0}),(S_{1},A_{1},R_{1}),(S_{2},A_{2},R_{2}),...,(S_{n},A_{n},R_{n}),...</script><p>是一个马尔科夫决策过程$ \\lt S,A^{\\pi},P,R,\\gamma  \\gt^{\\pi}$.</p>\n<p><em>注意:在编程时一般以四元组$(s,a,r,s’)$为单位存储”经验”</em></p>\n<p>$\\pi$策略下$s\\rightarrow s’$转移概率由期望计算得$P_{ss’}^{\\pi}=\\sum_{a\\in A}\\pi(a|s)P_{ss’}^{a}$,$s$状态下的期望立即奖励为$R_{s}^{\\pi}=\\sum_{a\\in A}\\pi(a|s)R_{s}^{a}$.</p>\n<p>上述例子中</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nR_{英语}&=\\sum_{a\\in A}\\pi(a|英语)R_{英语}^{a}\\\\\n&=0.2\\times-2+0.4\\times-2+0.4\\times-2\\\\\n&=-2\n\\end{align*}</script><p>状态转移概率可以描述为：在执行策略$\\pi$时，状态从$s$转移至$s’$的概率等于执行该状态下所有行为的概率与对应行为能使状态从$s$转移至$s’$的概率的乘积的和。</p>\n<p>奖励函数可以描述为：在执行策略$\\pi$时获得的奖励等于执行该状态下所有行为的概率与对应行为产生的即时奖励的乘积的和。</p>\n<p><strong>强化学习的目标就是最大化期望回报,相应的结果就是找到从状态空间$S$映射到动作空间$A$的最优策略</strong>,重点是,如何建立回报与策略之间的联系呢?</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"强化学习之MDP马尔科夫决策过程\"><a href=\"#强化学习之MDP马尔科夫决策过程\" class=\"headerlink\" title=\"强化学习之MDP马尔科夫决策过程\"></a>强化学习之MDP马尔科夫决策过程</h1><p>每每提到强化学习，最先接触的理论肯定是马尔科夫决策过程（MDP，Markov Decision Process），为什么总提到MDP呢？并不是只有我一个人有这个疑问。</p>","more":"<p>百度上没有人提出这样的问题，可能是大家理解得都比较透彻吧，于是在Google查到相关提问和解释。</p>\n<blockquote>\n<p><a href=\"https://datascience.stackexchange.com/a/38851\" rel=\"external nofollow\" target=\"_blank\">What is the relationship between Markov Decision Processes and Reinforcement Learning?</a></p>\n<blockquote>\n<p>In Reinforcement Learning (RL), the problem to resolve is described as a Markov Decision Process (MDP). Theoretical results in RL rely on the MDP description being a correct match to the problem. If your problem is well described as a MDP, then RL may be a good framework to use to find solutions. That does not mean you need to fully describe the MDP (all the transition probabilities), just that you expect an MDP model could be made or discovered.</p>\n<p>Conversely, if you cannot map your problem onto a MDP, then the theory behind RL makes no guarantees of any useful result.</p>\n<p>One key factor that affects how well RL will work is that the states should have the Markov property - that the value of the current state is enough knowledge to fix immediate transition probabilities and immediate rewards following an action choice. Again you don’t need to know in advance what those are, just that this relationship is expected to be reliable and stable. If it is not reliable, you may have a POMDP. If it is not stable, you may have a non-stationary problem. In either case, if the difference from a more strictly defined MDP is small enough, you may still get away with using RL techniques or need to adapt them slightly.</p>\n<p><strong>The general relationship between RL and MDP is that RL is a framework for solving problems that can be expressed as MDPs.</strong></p>\n</blockquote>\n</blockquote>\n<p>MDP是当前强化学习理论推导的基石，对强化学习来说，一般以马尔科夫决策过程作为形式化问题的手段。也就是说，对于目前的绝大部分强化学习算法，只有可以将问题抽象为MDP的才可以确保算法的性能（收敛性，效果等），对于违背MDP的问题并不一定确保算法有效，因为其数学公式都是基于MDP来进行推导的。</p>\n<h2 id=\"马尔科夫性\"><a href=\"#马尔科夫性\" class=\"headerlink\" title=\"马尔科夫性\"></a>马尔科夫性</h2><blockquote>\n<p>马尔科夫性质（英语：Markov property）是概率论中的一个概念，因为俄国数学家安德雷·马尔科夫得名。当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔科夫性质。<a href=\"https://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8/23149887?fr=aladdin\" rel=\"external nofollow\" target=\"_blank\">马尔科夫性-百度百科</a></p>\n</blockquote>\n<p>马尔科夫性，也就是无后效性：<strong>某阶段的状态一旦确定，则此后过程的演变不再受此前各状态及决策的影响</strong>。也就是说，<strong>未来与过去无关</strong>。</p>\n<p>具体地说，如果一个问题被划分各个阶段之后，阶段$k$中的状态只能通过阶段$k+1$中的状态通过状态转移方程得来，与其他状态没有关系，特别是与未发生的状态没有关系，这就是无后效性。</p>\n<p>公式描述：</p>\n<script type=\"math/tex; mode=display\">\nP[S_{t+1}|S_{t}]=P[S_{t+1}|S_{1},...,S_{t}]</script><p>强化学习问题中的状态也符合马尔科夫性，即在当前状态$s_{t}$下执行动作$a_{t}$并转移至下一个状态$s_{t+1}$，而不需要考虑之前的状态$s_{t-1},…,s_{1}$。</p>\n<p>举一个不恰当的例子：</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/M.jpg\" alt=\"\"></p>\n<p>假设天气预测符合马尔科夫性，如果以每天表示为一种状态，即周一、周二到周日。今天（5月8日，周三）天气为晴，明天（周四）会不会下雨只与今天的天气有关，而与之前周一、周二的天气状况无关。如果以时间节点表示为一种状态，即2点、5点、8点等，如图2点的温度为15.8°C,那么下个时间点5点的气温如何只与2点的温度有关系。</p>\n<p>强化学习中默认状态的转移是符合马尔科夫性质的，状态具体是什么，需要根据不同的问题进行不同的设定。</p>\n<h2 id=\"马尔科夫过程\"><a href=\"#马尔科夫过程\" class=\"headerlink\" title=\"马尔科夫过程\"></a>马尔科夫过程</h2><p>马尔科夫过程是随机过程的一种，什么是随机过程呢？简单来说，一个商店从早上营业到晚上打烊这段时间，根据每个时间点店内顾客的人数所组成的序列就是随机过程。随机过程根据时间节点$T_{t}$取到的值是一个变量。</p>\n<p>马尔科夫过程是满足马尔科夫性的随机过程，它由二元组$M=(S,P)$组成，且满足：</p>\n<ol>\n<li>S是有限状态集合</li>\n<li>P是状态转移概率矩阵</li>\n</ol>\n<p>状态与状态之间的转换过程即为马尔科夫过程。<strong><em>虽然我们可能不知道P的具体值到底是什么，但是通常我们假设P是存在的（转移概率存在，如果是确定的，无非就是概率为1），而且是稳定的（意思是从状态A到其他状态的转移虽然符合某个分布，但是其转移到某个状态的概率是确定的，不随时间变化的）。</em></strong></p>\n<p>这里说的<strong>有限</strong>二字我有自己的理解，在最开始的强化学习研究中，解决的都是表格式的问题，也就是状态的数量是有限可取的，但是后续强化学习研究的也有连续状态空间的问题，算法如DQN,PG,PPO等。状态的数量并不是有限的，但是其向量维度则是固定的、有限的，而且也同样符合马尔科夫性质，因此<strong>我认为这里定义的有限并不是说状态数量有限，而是状态维度有限</strong>。因为好像没有无限马尔科夫的叫法，所以姑且这么解释一下。</p>\n<p>马尔科夫过程有如下分类：</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/MPs.jpg\" alt=\"\"></p>\n<h3 id=\"状态转移矩阵\"><a href=\"#状态转移矩阵\" class=\"headerlink\" title=\"状态转移矩阵\"></a>状态转移矩阵</h3><p>状态转移矩阵由许多状态转移概率组成，状态转移概率是指从一个马尔科夫状态$s$转移到下一个状态$s’$的概率。</p>\n<p>公示表示：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]</script><p>等同于：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{P}(s'|s)=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]</script><p>假设有1到n个状态，将所有的状态从上到下、从左到右排列，组成一个$n \\times n$的矩阵，那么其状态转移矩阵如下所示：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{P}=\n\\begin{bmatrix}\n\\mathcal{P}_{11} & \\cdots & \\mathcal{P}_{1n} \\\\ \n\\vdots & \\ddots & \\vdots \\\\ \n\\mathcal{P}_{n1} & \\cdots & \\mathcal{P}_{nn} \\\\\n\\end{bmatrix}</script><p>其中，每行元素相加等于1，每列元素相加等于1，矩阵的总和为状态的数量n。</p>\n<p>对于可数状态，$\\sum_{s’=1}^{n}\\mathcal{P}(s’|s)=1$</p>\n<script type=\"math/tex; mode=display\">\nsum(\\mathcal{P}) = \\sum_{s'=1}^{n}\\sum_{s=1}^{n}\\mathcal{P}_{ss'} = n</script><p>对于不可数状态（连续状态),$\\int_{s’}\\mathcal{P}(s’|s)=1$</p>\n<script type=\"math/tex; mode=display\">\nsum(\\mathcal{P}) = \\int_{s'}\\int_{s}\\mathcal{P}_{ss'} = n</script><p>举一个马尔科夫过程的例子:</p>\n<p>假设一个学生，他目前在学习语文科目，那么他接下来进行的活动过程如下图所示，游戏的吸引力很大，所以他有50%的概率在学完语文去玩游戏，并且很容易沉迷其中，图示玩游戏这个循环有90%的可能性，他还可以选择学习其他科目或者去睡觉，最终学习结束之后是否能通过考试也是有一定的概率的，这些状态之间转移的概率即为状态转移概率。</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/MP.jpg\" alt=\"\"></p>\n<p>如果把例子中的各项状态用字母表示，将其表示为：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n玩游戏 & A\\\\ \n语文 & B\\\\ \n数学 & C\\\\ \n英语 & D\\\\ \n挂科 & E\\\\ \n\\mathcal{Pass} & F\\\\ \n睡觉 & G\n\\end{bmatrix}</script><p>那么其状态转移矩阵$\\mathcal{P}$可以表示成：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{lc}\n\\mbox{}&\n\\begin{array}{cc}A & B & C & D & E & F & G \\end{array}\\\\\n\\begin{array}{c}A\\\\B\\\\C\\\\D\\\\E\\\\F\\\\G\\end{array}&\n\\left[\\begin{array}{cc}\n0.9&0.1\\\\\n0.5& &0.5\\\\\n& & &0.8& & &0.2\\\\\n&&&&0.4&0.6&\\\\\n&0.2&0.4&0.4&&&\\\\\n&&&&&&1\\\\\n&&&&&&&\n\\end{array}\\right]\n\\end{array}</script><h3 id=\"马尔科夫链与Episode\"><a href=\"#马尔科夫链与Episode\" class=\"headerlink\" title=\"马尔科夫链与Episode\"></a>马尔科夫链与Episode</h3><p>Episode可以翻译为片段、情节、回合等，在强化学习问题中，一个Episode就是一个马尔科夫链，根据状态转移矩阵可以得到许多不同的episode，也就是多个马尔科夫链。</p>\n<p>强化学习问题分两种：</p>\n<ol>\n<li>如果一个任务总能达到终态，结束任务或者开启下一轮任务，那么这个任务就被称为回合任务，也就是episode任务。例如，让一个智能体学习如何下围棋，围棋棋盘只有那么大，游戏定会终局，所以是一个回合式任务。</li>\n<li>如果一个任务可以无限持续下去，永远不会结束，即永远在训练当中，那么这个任务就被称为连续性任务。例如，教会一辆车能够进行自动驾驶就是一个连续性任务，<em>不要钻牛角尖说能源会耗尽，车子会磨损，我们只聚焦问题与环境本身，不涉及其他非稳定因素。</em></li>\n</ol>\n<p>在上边举的例子中就是一个回合式任务，因为无论这个序列有多长，最终都会达到终态-“睡觉”。</p>\n<p>根据上述例子我们可能采样出如下episode：</p>\n<ol>\n<li>$B-C-D-E-C-G$，即“学语文→数学→英语→考试没通过,挂科→继续学数学→睡觉”</li>\n<li>$B-A-A-…-A-B-C-G$，即“学语文→玩王者荣耀→玩刺激战场→玩OverCooker→玩守望先锋→玩英雄联盟→玩CS:GO→…→看一会儿数学→睡觉”。（仿佛就是我自己嘛！）</li>\n</ol>\n<h2 id=\"马尔科夫奖励过程\"><a href=\"#马尔科夫奖励过程\" class=\"headerlink\" title=\"马尔科夫奖励过程\"></a>马尔科夫奖励过程</h2><p>马尔科夫过程（Markov Process）主要描述的是状态之间的转移关系，在各个状态的转移过程中赋予不同的奖励值就得到了马尔科夫奖励过程。</p>\n<p>定义：马尔科夫奖励过程（Markov Reward Process, MRP）由一个四元组组成$(S,P,R,\\gamma)$</p>\n<ol>\n<li>$S$代表了状态的集合(也是维度有限的)</li>\n<li>$P$描述了状态转移矩阵$\\mathcal{P}_{ss’}=\\mathbb{P}[S_{t+1}=s’|S_{t}=s]$</li>\n<li>$R$表示奖励函数，$R(s)$描述了在状态$s$下的期望(立即)奖励，$\\mathcal{R}(s)=\\mathbb{E}[R_{t+1}|S_{t}=s]$</li>\n<li>$\\gamma$表示衰减因子,即discounted factor,$\\gamma\\in[0,1]$</li>\n</ol>\n<p>$\\gamma$是用来计算累计奖励回报的,表示我们有多看中现在或者未来,为什么这么说呢?假设我们现在要计算一个episode始态$S_{0}$的奖励值$V(S_{0})$,不涉及具体公式推导的说,我们应该把$S_{0}$状态后续的奖励全部加和,这样就得到了对始态$S_{0}$的值估计,这些后续奖励的值的权重都是1,或者说此时$\\gamma=1$,但是当前状态对很多步之后的状态未必影响很大,我们这样计算过来并不能完全表示一个状态的值,那么我们应当顺势减少距离远的状态的权重,此时$\\gamma\\lt1$</p>\n<ul>\n<li>当$\\gamma=0$时,状态$S$的值完全由其转移的期望立即奖励表示,即<strong>一点都不关心未来</strong></li>\n<li>当$\\gamma=1$时,状态$S$的值由以当前状态为始态,运行至终态所得到的所有立即奖励加和的值表示,即<strong>未来与现在同等重要</strong></li>\n<li>当$0 \\lt\\gamma \\lt1$时,状态$S$的值是前两个模式的<em>trade-off</em>,即<strong>对未来看重的程度由$\\gamma$决定</strong></li>\n</ul>\n<p>这只是我们的直观感受,其实是为了数学便利（虽然我也不知道具体哪里提高了数学便利，但是在有些情况下会使值函数更快迭代收敛这是真的）。</p>\n<p><em>注：也有很多地方将MRP表示为三元组，即去掉$\\gamma$，但这不影响我们对这个过程的理解，下边的MDP也是一样，无论是三元组、四元组、还是五元组，只要能描述过程的性质就可以。</em></p>\n<p>将上述马尔科夫过程的例子升级为马尔科夫奖励过程如下图所示:</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/MRP.png\" alt=\"\"></p>\n<p>奖励值定义为:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n玩游戏 & A & -1\\\\ \n语文 & B & -2\\\\ \n数学 & C & -2\\\\ \n英语 & D & -2\\\\ \n挂科 & E & -5\\\\ \n\\mathcal{Pass} & F & 10\\\\ \n睡觉 & G & 0\n\\end{bmatrix}</script><p>这么定义奖励并没有什么复杂的含义,在这个例子中就拿身心愉悦程度来定义吧,学习固然是枯燥无味的,所以给予负奖励-2,玩游戏虽然会心情放松,但是始终面临着考试的压力,其实并不轻松,所以给予负奖励-1,挂科最痛苦为-5,考试全pass最开心为+10。</p>\n<p>在马尔科夫过程中的状态转移加入相应的奖励值即为马尔科夫奖励过程。</p>\n<h2 id=\"马尔科夫决策过程\"><a href=\"#马尔科夫决策过程\" class=\"headerlink\" title=\"马尔科夫决策过程\"></a>马尔科夫决策过程</h2><p>马尔科夫决策过程(Markov Decision Process, MDP)相比马尔科夫奖励过程多了一个动作$A$,它可以用一个五元组$(S,A,P,R,\\gamma)$表示:</p>\n<ol>\n<li>$S$代表了状态的集合(也是维度有限的)</li>\n<li>$A$代表了决策过程中动作的集合(维度有限的)</li>\n<li>$P$描述了状态转移矩阵$\\mathcal{P}_{ss’}^{a}=\\mathbb{P}[S_{t+1}=s’|S_{t}=s,A_{t}=a]$</li>\n<li>$R$表示奖励函数，$R(s)$描述了在状态$s$下<strong>执行某动作</strong>的期望(立即)奖励，$\\mathcal{R}(s,a)=\\mathbb{E}[R_{t+1}|S_{t}=s,A_{t}=a]$</li>\n<li>$\\gamma$表示衰减因子,即discounted factor,$\\gamma\\in[0,1]$</li>\n</ol>\n<p>MDPs是一个从交互中达成目标的强化学习问题的一个直接的框架。学习者和决策者叫做Agent。Agent进行交互的其它一切Agent之外的东西都叫做环境。Agent不断的选择动作，而环境也给出相应的反应，并且向Agent表现出新的状态。环境同时也给出一个数值作为反馈。Agent的目标就是通过选择不同的Action来最大化这个反馈值。</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/agent-env.png\" alt=\"\"></p>\n<p>强化学习所研究的内容就是得到一个状态$S$到动作$A$的映射关系,因此策略Policy可以表示成</p>\n<script type=\"math/tex; mode=display\">\n\\pi(a|s)=p(A_{t}=a|S_{t}=s)</script><hr>\n<p>注意:<br>你可能会认为,在马尔科夫奖励过程(MRP)中没有定义动作,但是其实是包含动作的,因为每个状态有多个转移的下一状态,其实就是多个动作嘛！</p>\n<p>很多文章会将有限MDP分开来讲，有限MDP即状态、动作和奖励值都只有有限个元素，对于有限MDP最优策略有唯一解，但是现实世界中任务复杂，因此大多数深度强化学习算法并不局限于解决有限MDP问题，因此本文不将MDP分情况来讲，即默认基于MDP的最优策略<strong>至少有一个解</strong>。</p>\n<hr>\n<p>没错,的确是这样的,MRP中也包含动作,但是我们并不关心,为什么这么说呢?<strong>因为就算每个状态可以执行多个动作,但是其每个动作所能转移到的状态是确定的,不确定的只是动作的选择,而不是动作的转移,而MDP中不确定的却是动作的转移,即执行动作所转移的下一状态是有一定概率的.</strong>什么意思呢?拿之前MRP的例子来说,语文状态有两个状态可以转移,数学和玩游戏,概率分别是0.5,但是当确定一个转移方向的时候(图中的箭头),其转移结果是确定的,获得的奖励也是确定的,但是在MDP中,执行动作导致转移的结果都未必是确定的.<strong><em>需要注意的是,MRP是属于MDP的,MDP执行动作并不一定必须是随机的.</em></strong></p>\n<p>接下来,我们将MRP的例子转换至MDP, 为了方便理解而又不增加示例的复杂性,不妨将”挂科”这个状态看作是一个动作,因为这个节点正巧入度为1,姑且就认为从英语到挂科的这个箭头是英语状态所能执行的动作.如图所示:</p>\n<p><img src=\"./强化学习之MDP马尔科夫决策过程/MDP.jpg\" alt=\"\"></p>\n<p>比较两个图可以发现区别,我把这个不确定的动作标为实心黑圆圈,这位刻苦的同学在学习完英语之后还想继续学习,但是他感觉三门科目都差不多了,于是他也很迷茫,他执行”学习”这个动作时的转移状态有三种:学语文、学数学、学英语.概率分别是:0.2、0.4,、0.4.这下就明白为什么我们要在MDP中加入动作$A$了吧,如果还不明白,请接着看下边的内容.</p>\n<p>顺便说一下,这个时候的转移矩阵已经不是简单的二维了,当然也可以用二维来表示,假设总共有$n$个状态,每个状态有$m$个动作,那么其行数为$n\\times m$,即遍历所有的状态和动作,得到$n \\times m$个状态-动作对$(s,a)$,其列数还是$n$.当然,也可以用一个三维tensor来表示,行和列都是$n$,第三维深度为$m$,很好理解.</p>\n<hr>\n<p><strong>网上有写MDP在给定策略下会退化为MRP,我对此不置可否,认为此种说法不够严谨,因为即使说在某状态s下选择的动作a是确定的,并不意味着其转移结果是确定的.</strong></p>\n<hr>\n<h3 id=\"回报-Return\"><a href=\"#回报-Return\" class=\"headerlink\" title=\"回报 Return\"></a>回报 Return</h3><p>在强化学习问题中，总是提到回报二字，论文中出现Return或者Discounted Return，我们已经知道奖励是什么，奖励就是转移到某个状态或者执行了某个动作之后转移至某个状态所获得的值$r$.</p>\n<p>回报就是由某时刻$t$之后决策序列所获得的奖励值经过一定规则计算出来的数值.</p>\n<p>公式描述:</p>\n<script type=\"math/tex; mode=display\">\nG_{t}\\doteq R_{t+1}+R_{t+2}+R_{t+3}+...+R_{t}</script><p>.其中,$T$表示一个episode达到终态的时间点.</p>\n<p>像之前介绍的一样,我们可能对未来有不同的看重程度,于是引入折扣因子$\\gamma$的回报表示为:</p>\n<script type=\"math/tex; mode=display\">\nG_{t}\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+...=\\begin{cases}\n\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\\\\n\\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_{k}\n\\end{cases}</script><p>其中,$ 0\\leq\\gamma \\leq1$</p>\n<p>可以推出回报有如下形式:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nG_{t} &\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\gamma^{3}R_{t+4}+...\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\gamma^{2}R_{t+4}+...)\\\\\n&=R_{t+1}+\\gamma (R_{t+2}+\\gamma (R_{t+3}+\\gamma R_{t+4}+...))\\\\\n&=R_{t+1}+\\gamma G_{t+1}\n\\end{align*}</script><h3 id=\"策略-Policy\"><a href=\"#策略-Policy\" class=\"headerlink\" title=\"策略 Policy\"></a>策略 Policy</h3><p>我们一般使用$\\pi$来表示一个策略,使用$\\pi(a|s)$来表示某状态$s$采取动作$a$的概率,公示表示为:</p>\n<script type=\"math/tex; mode=display\">\n\\pi(a|s)=P(A_{t}=a|S_{t}=s)</script><p>策略完整定义了智能体在所有状态下的所有行为和其概率.</p>\n<p>给定一个MDP和一个策略$\\pi$,采样的状态序列</p>\n<script type=\"math/tex; mode=display\">\nS_{0},S_{1},S_{2},...,S_{n},...</script><p>是一个马尔科夫过程$\\lt S,P \\gt ^{\\pi}$,</p>\n<p>采样的状态、奖励序列</p>\n<script type=\"math/tex; mode=display\">\n(S_{0},R_{0}),(S_{1},R_{1}),(S_{2},R_{2}),...,(S_{n},R_{n}),...</script><p>是一个马尔科夫奖励过程$ \\lt S,P,R,\\gamma  \\gt^{\\pi}$,</p>\n<p>采样的状态、动作、奖励序列</p>\n<script type=\"math/tex; mode=display\">\n(S_{0},A_{0},R_{0}),(S_{1},A_{1},R_{1}),(S_{2},A_{2},R_{2}),...,(S_{n},A_{n},R_{n}),...</script><p>是一个马尔科夫决策过程$ \\lt S,A^{\\pi},P,R,\\gamma  \\gt^{\\pi}$.</p>\n<p><em>注意:在编程时一般以四元组$(s,a,r,s’)$为单位存储”经验”</em></p>\n<p>$\\pi$策略下$s\\rightarrow s’$转移概率由期望计算得$P_{ss’}^{\\pi}=\\sum_{a\\in A}\\pi(a|s)P_{ss’}^{a}$,$s$状态下的期望立即奖励为$R_{s}^{\\pi}=\\sum_{a\\in A}\\pi(a|s)R_{s}^{a}$.</p>\n<p>上述例子中</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nR_{英语}&=\\sum_{a\\in A}\\pi(a|英语)R_{英语}^{a}\\\\\n&=0.2\\times-2+0.4\\times-2+0.4\\times-2\\\\\n&=-2\n\\end{align*}</script><p>状态转移概率可以描述为：在执行策略$\\pi$时，状态从$s$转移至$s’$的概率等于执行该状态下所有行为的概率与对应行为能使状态从$s$转移至$s’$的概率的乘积的和。</p>\n<p>奖励函数可以描述为：在执行策略$\\pi$时获得的奖励等于执行该状态下所有行为的概率与对应行为产生的即时奖励的乘积的和。</p>\n<p><strong>强化学习的目标就是最大化期望回报,相应的结果就是找到从状态空间$S$映射到动作空间$A$的最优策略</strong>,重点是,如何建立回报与策略之间的联系呢?</p>"}],"PostAsset":[{"_id":"source/_posts/Hindsight-Experience-Replay/finalvsfuture.png","slug":"finalvsfuture.png","post":"cjxd6ma27002dekvez4bso9fs","modified":0,"renderable":0},{"_id":"source/_posts/Hindsight-Experience-Replay/pseudo.png","slug":"pseudo.png","post":"cjxd6ma27002dekvez4bso9fs","modified":0,"renderable":0},{"_id":"source/_posts/Prioritized-Experience-Replay/normalized-score.png","slug":"normalized-score.png","post":"cjxd6ma38002tekve7o57p1v8","modified":0,"renderable":0},{"_id":"source/_posts/Prioritized-Experience-Replay/pseudo.png","slug":"pseudo.png","post":"cjxd6ma38002tekve7o57p1v8","modified":0,"renderable":0},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/pearsoncorrelation.png","slug":"pearsoncorrelation.png","post":"cjxd6ma30002qekveia40bz6c","modified":0,"renderable":0},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/3.png","slug":"3.png","post":"cjxd6m9tr000sekve87bc9g04","modified":0,"renderable":0},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/7.png","slug":"7.png","post":"cjxd6m9tr000sekve87bc9g04","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/16.png","slug":"16.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/2.png","slug":"2.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/17.png","slug":"17.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/4.png","slug":"4.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/Hindsight-Experience-Replay/fourmodel.png","slug":"fourmodel.png","post":"cjxd6ma27002dekvez4bso9fs","modified":0,"renderable":0},{"_id":"source/_posts/dynamic-programming/iteration.png","slug":"iteration.png","post":"cjxd6ma2r002hekve0ekqsbs4","modified":0,"renderable":0},{"_id":"source/_posts/dynamic-programming/pi.png","slug":"pi.png","post":"cjxd6ma2r002hekve0ekqsbs4","modified":0,"renderable":0},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/mean-success.png","slug":"mean-success.png","post":"cjxd6ma2y002nekveqvkcldly","modified":0,"renderable":0},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/pseudo.png","slug":"pseudo.png","post":"cjxd6ma30002qekveia40bz6c","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/cdp-sg.png","slug":"cdp-sg.png","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0},{"_id":"source/_posts/universal-value-function-approximators/sg.png","slug":"sg.png","post":"cjxd6m9th000qekvezc5xzoew","modified":0,"renderable":0},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/2.png","slug":"2.png","post":"cjxd6m9tr000sekve87bc9g04","modified":0,"renderable":0},{"_id":"source/_posts/rl2/meta-rl.png","slug":"meta-rl.png","post":"cjxd6m9u90014ekvepcsrtohs","modified":0,"renderable":0},{"_id":"source/_posts/rl2/meta.png","slug":"meta.png","post":"cjxd6m9u90014ekvepcsrtohs","modified":0,"renderable":0},{"_id":"source/_posts/mc-td/dp.png","slug":"dp.png","post":"cjxd6m9sg000aekveiwixg3ik","modified":0,"renderable":0},{"_id":"source/_posts/mc-td/mc.png","slug":"mc.png","post":"cjxd6m9sg000aekveiwixg3ik","modified":0,"renderable":0},{"_id":"source/_posts/mc-td/td.png","slug":"td.png","post":"cjxd6m9sg000aekveiwixg3ik","modified":0,"renderable":0},{"_id":"source/_posts/强化学习的里程碑/GKBattleWithDeepBlue.jpeg","slug":"GKBattleWithDeepBlue.jpeg","post":"cjxd6m9ub0017ekveyl93vuxv","modified":0,"renderable":0},{"_id":"source/_posts/强化学习的里程碑/KeJieBattleWithAlphaGo.jpeg","slug":"KeJieBattleWithAlphaGo.jpeg","post":"cjxd6m9ub0017ekveyl93vuxv","modified":0,"renderable":0},{"_id":"source/_posts/强化学习的里程碑/LeeSedolBattleWithAlphaGo.jpeg","slug":"LeeSedolBattleWithAlphaGo.jpeg","post":"cjxd6m9ub0017ekveyl93vuxv","modified":0,"renderable":0},{"_id":"source/_posts/use-conda-env-in-jupyter/1.png","slug":"1.png","post":"cjxd6m9t6000lekve2s6k1efv","modified":0,"renderable":0},{"_id":"source/_posts/use-conda-env-in-jupyter/2.png","slug":"2.png","post":"cjxd6m9t6000lekve2s6k1efv","modified":0,"renderable":0},{"_id":"source/_posts/use-conda-env-in-jupyter/3.png","slug":"3.png","post":"cjxd6m9t6000lekve2s6k1efv","modified":0,"renderable":0},{"_id":"source/_posts/use-conda-env-in-jupyter/4.png","slug":"4.png","post":"cjxd6m9t6000lekve2s6k1efv","modified":0,"renderable":0},{"_id":"source/_posts/win-rightclick-create-md/1546050455.jpg","slug":"1546050455.jpg","post":"cjxd6m9u60011ekve047kui9x","modified":0,"renderable":0},{"_id":"source/_posts/win-rightclick-create-md/20181229103503.png","slug":"20181229103503.png","post":"cjxd6m9u60011ekve047kui9x","modified":0,"renderable":0},{"_id":"source/_posts/win-rightclick-create-md/20181229103752.png","slug":"20181229103752.png","post":"cjxd6m9u60011ekve047kui9x","modified":0,"renderable":0},{"_id":"source/_posts/win-rightclick-create-md/20181229105300.png","slug":"20181229105300.png","post":"cjxd6m9u60011ekve047kui9x","modified":0,"renderable":0},{"_id":"source/_posts/win-rightclick-create-md/20181229105408.png","slug":"20181229105408.png","post":"cjxd6m9u60011ekve047kui9x","modified":0,"renderable":0},{"_id":"source/_posts/rl-classification/model-classification.png","slug":"model-classification.png","post":"cjxd6m9sv000fekvezsrt1s37","modified":0,"renderable":0},{"_id":"source/_posts/rl-classification/non-stationary.png","slug":"non-stationary.png","post":"cjxd6m9sv000fekvezsrt1s37","modified":0,"renderable":0},{"_id":"source/_posts/rl-classification/policy-based.png","slug":"policy-based.png","post":"cjxd6m9sv000fekvezsrt1s37","modified":0,"renderable":0},{"_id":"source/_posts/rl-classification/stationary.png","slug":"stationary.png","post":"cjxd6m9sv000fekvezsrt1s37","modified":0,"renderable":0},{"_id":"source/_posts/rl-classification/value-based.png","slug":"value-based.png","post":"cjxd6m9sv000fekvezsrt1s37","modified":0,"renderable":0},{"_id":"source/_posts/rl-classification/图.vsdx","slug":"图.vsdx","post":"cjxd6m9sv000fekvezsrt1s37","modified":0,"renderable":0},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/1.png","slug":"1.png","post":"cjxd6m9tr000sekve87bc9g04","modified":0,"renderable":0},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/10.png","slug":"10.png","post":"cjxd6m9tr000sekve87bc9g04","modified":0,"renderable":0},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/4.png","slug":"4.png","post":"cjxd6m9tr000sekve87bc9g04","modified":0,"renderable":0},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/5.png","slug":"5.png","post":"cjxd6m9tr000sekve87bc9g04","modified":0,"renderable":0},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/6.png","slug":"6.png","post":"cjxd6m9tr000sekve87bc9g04","modified":0,"renderable":0},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/8.png","slug":"8.png","post":"cjxd6m9tr000sekve87bc9g04","modified":0,"renderable":0},{"_id":"source/_posts/为远程Ubuntu服务器安装图像界面/9.png","slug":"9.png","post":"cjxd6m9tr000sekve87bc9g04","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/5.png","slug":"5.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/6.png","slug":"6.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/7.png","slug":"7.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/26.png","slug":"26.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/Evolution-Strategies-2017/atari.png","slug":"atari.png","post":"cjxd6ma3d002wekvercmn0xoe","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/1.png","slug":"1.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/10.png","slug":"10.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/11.png","slug":"11.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/12.png","slug":"12.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/13.png","slug":"13.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/14.png","slug":"14.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/15.png","slug":"15.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/18.png","slug":"18.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/19.png","slug":"19.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/20.png","slug":"20.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/21.png","slug":"21.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/22.png","slug":"22.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/23.png","slug":"23.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/24.png","slug":"24.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/25.png","slug":"25.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/27.png","slug":"27.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/28.png","slug":"28.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/3.png","slug":"3.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/8.png","slug":"8.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/AnderewNg-deeplearning-note-summary/9.png","slug":"9.png","post":"cjxd6m9rb0000ekvez87f60ik","modified":0,"renderable":0},{"_id":"source/_posts/asynchronous-methods-for-drl/threeoptimizer.png","slug":"threeoptimizer.png","post":"cjxd6ma2e002fekveuph10svx","modified":0,"renderable":0},{"_id":"source/_posts/something-hard-install-docker/1.png","slug":"1.png","post":"cjxd6ma24002cekvevi8klydb","modified":0,"renderable":0},{"_id":"source/_posts/dynamic-programming/dp.png","slug":"dp.png","post":"cjxd6ma2r002hekve0ekqsbs4","modified":0,"renderable":0},{"_id":"source/_posts/dynamic-programming/gridworld.png","slug":"gridworld.png","post":"cjxd6ma2r002hekve0ekqsbs4","modified":0,"renderable":0},{"_id":"source/_posts/dynamic-programming/pivsvi.png","slug":"pivsvi.png","post":"cjxd6ma2r002hekve0ekqsbs4","modified":0,"renderable":0},{"_id":"source/_posts/dynamic-programming/vi.png","slug":"vi.png","post":"cjxd6ma2r002hekve0ekqsbs4","modified":0,"renderable":0},{"_id":"source/_posts/Prioritized-Experience-Replay/learning-speed.png","slug":"learning-speed.png","post":"cjxd6ma38002tekve7o57p1v8","modified":0,"renderable":0},{"_id":"source/_posts/Prioritized-Experience-Replay/normalized-score1.png","slug":"normalized-score1.png","post":"cjxd6ma38002tekve7o57p1v8","modified":0,"renderable":0},{"_id":"source/_posts/Prioritized-Experience-Replay/sum-tree.png","slug":"sum-tree.png","post":"cjxd6ma38002tekve7o57p1v8","modified":0,"renderable":0},{"_id":"source/_posts/Prioritized-Experience-Replay/visio.vsdx","slug":"visio.vsdx","post":"cjxd6ma38002tekve7o57p1v8","modified":0,"renderable":0},{"_id":"source/_posts/Evolution-Strategies-2017/algorithm1.png","slug":"algorithm1.png","post":"cjxd6ma3d002wekvercmn0xoe","modified":0,"renderable":0},{"_id":"source/_posts/Evolution-Strategies-2017/algorithm2.png","slug":"algorithm2.png","post":"cjxd6ma3d002wekvercmn0xoe","modified":0,"renderable":0},{"_id":"source/_posts/Evolution-Strategies-2017/frame-skip.png","slug":"frame-skip.png","post":"cjxd6ma3d002wekvercmn0xoe","modified":0,"renderable":0},{"_id":"source/_posts/Evolution-Strategies-2017/mujoco.png","slug":"mujoco.png","post":"cjxd6ma3d002wekvercmn0xoe","modified":0,"renderable":0},{"_id":"source/_posts/Evolution-Strategies-2017/parallelization.png","slug":"parallelization.png","post":"cjxd6ma3d002wekvercmn0xoe","modified":0,"renderable":0},{"_id":"source/_posts/Hindsight-Experience-Replay/Her.png","slug":"Her.png","post":"cjxd6ma27002dekvez4bso9fs","modified":0,"renderable":0},{"_id":"source/_posts/Hindsight-Experience-Replay/hindsight.png","slug":"hindsight.png","post":"cjxd6ma27002dekvez4bso9fs","modified":0,"renderable":0},{"_id":"source/_posts/Hindsight-Experience-Replay/rewardshape.png","slug":"rewardshape.png","post":"cjxd6ma27002dekvez4bso9fs","modified":0,"renderable":0},{"_id":"source/_posts/Hindsight-Experience-Replay/singlegoal.png","slug":"singlegoal.png","post":"cjxd6ma27002dekvez4bso9fs","modified":0,"renderable":0},{"_id":"source/_posts/Hindsight-Experience-Replay/tasks.png","slug":"tasks.png","post":"cjxd6ma27002dekvez4bso9fs","modified":0,"renderable":0},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/MEP.png","slug":"MEP.png","post":"cjxd6ma2y002nekveqvkcldly","modified":0,"renderable":0},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/env.png","slug":"env.png","post":"cjxd6ma2y002nekveqvkcldly","modified":0,"renderable":0},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/inequality.png","slug":"inequality.png","post":"cjxd6ma2y002nekveqvkcldly","modified":0,"renderable":0},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/pseudo.png","slug":"pseudo.png","post":"cjxd6ma2y002nekveqvkcldly","modified":0,"renderable":0},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/sample-efficiency.png","slug":"sample-efficiency.png","post":"cjxd6ma2y002nekveqvkcldly","modified":0,"renderable":0},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/training-time.png","slug":"training-time.png","post":"cjxd6ma2y002nekveqvkcldly","modified":0,"renderable":0},{"_id":"source/_posts/maximum-entropy-regularized-multi-goal-reinforcement-learning/x-x^2.png","slug":"x-x^2.png","post":"cjxd6ma2y002nekveqvkcldly","modified":0,"renderable":0},{"_id":"source/_posts/asynchronous-methods-for-drl/a1stepq.png","slug":"a1stepq.png","post":"cjxd6ma2e002fekveuph10svx","modified":0,"renderable":0},{"_id":"source/_posts/asynchronous-methods-for-drl/a3c.png","slug":"a3c.png","post":"cjxd6ma2e002fekveuph10svx","modified":0,"renderable":0},{"_id":"source/_posts/asynchronous-methods-for-drl/anstepq.png","slug":"anstepq.png","post":"cjxd6ma2e002fekveuph10svx","modified":0,"renderable":0},{"_id":"source/_posts/asynchronous-methods-for-drl/gd.png","slug":"gd.png","post":"cjxd6ma2e002fekveuph10svx","modified":0,"renderable":0},{"_id":"source/_posts/asynchronous-methods-for-drl/lossfunction.png","slug":"lossfunction.png","post":"cjxd6ma2e002fekveuph10svx","modified":0,"renderable":0},{"_id":"source/_posts/asynchronous-methods-for-drl/regression.png","slug":"regression.png","post":"cjxd6ma2e002fekveuph10svx","modified":0,"renderable":0},{"_id":"source/_posts/asynchronous-methods-for-drl/sgdvsgd.png","slug":"sgdvsgd.png","post":"cjxd6ma2e002fekveuph10svx","modified":0,"renderable":0},{"_id":"source/_posts/asynchronous-methods-for-drl/table1.png","slug":"table1.png","post":"cjxd6ma2e002fekveuph10svx","modified":0,"renderable":0},{"_id":"source/_posts/asynchronous-methods-for-drl/table2.png","slug":"table2.png","post":"cjxd6ma2e002fekveuph10svx","modified":0,"renderable":0},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/env.png","slug":"env.png","post":"cjxd6ma30002qekveia40bz6c","modified":0,"renderable":0},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/finalmeanrate.png","slug":"finalmeanrate.png","post":"cjxd6ma30002qekveia40bz6c","modified":0,"renderable":0},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/issue.png","slug":"issue.png","post":"cjxd6ma30002qekveia40bz6c","modified":0,"renderable":0},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/issue2.png","slug":"issue2.png","post":"cjxd6ma30002qekveia40bz6c","modified":0,"renderable":0},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/issue3.png","slug":"issue3.png","post":"cjxd6ma30002qekveia40bz6c","modified":0,"renderable":0},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/meansuccessrate.png","slug":"meansuccessrate.png","post":"cjxd6ma30002qekveia40bz6c","modified":0,"renderable":0},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/sampleefficiency.png","slug":"sampleefficiency.png","post":"cjxd6ma30002qekveia40bz6c","modified":0,"renderable":0},{"_id":"source/_posts/energy-based-hindsight-experience-prioritization/trainingtime.png","slug":"trainingtime.png","post":"cjxd6ma30002qekveia40bz6c","modified":0,"renderable":0},{"_id":"source/_posts/价值与贝尔曼方程/example1.png","slug":"example1.png","post":"cjxd6ma3r0033ekve0yhbtcsx","modified":0,"renderable":0},{"_id":"source/_posts/价值与贝尔曼方程/example2.png","slug":"example2.png","post":"cjxd6ma3r0033ekve0yhbtcsx","modified":0,"renderable":0},{"_id":"source/_posts/价值与贝尔曼方程/example3.png","slug":"example3.png","post":"cjxd6ma3r0033ekve0yhbtcsx","modified":0,"renderable":0},{"_id":"source/_posts/价值与贝尔曼方程/example4.png","slug":"example4.png","post":"cjxd6ma3r0033ekve0yhbtcsx","modified":0,"renderable":0},{"_id":"source/_posts/价值与贝尔曼方程/example5.png","slug":"example5.png","post":"cjxd6ma3r0033ekve0yhbtcsx","modified":0,"renderable":0},{"_id":"source/_posts/价值与贝尔曼方程/example6.png","slug":"example6.png","post":"cjxd6ma3r0033ekve0yhbtcsx","modified":0,"renderable":0},{"_id":"source/_posts/价值与贝尔曼方程/q.jpg","slug":"q.jpg","post":"cjxd6ma3r0033ekve0yhbtcsx","modified":0,"renderable":0},{"_id":"source/_posts/价值与贝尔曼方程/qsa.jpg","slug":"qsa.jpg","post":"cjxd6ma3r0033ekve0yhbtcsx","modified":0,"renderable":0},{"_id":"source/_posts/价值与贝尔曼方程/v.jpg","slug":"v.jpg","post":"cjxd6ma3r0033ekve0yhbtcsx","modified":0,"renderable":0},{"_id":"source/_posts/价值与贝尔曼方程/vq.jpg","slug":"vq.jpg","post":"cjxd6ma3r0033ekve0yhbtcsx","modified":0,"renderable":0},{"_id":"source/_posts/价值与贝尔曼方程/vs.jpg","slug":"vs.jpg","post":"cjxd6ma3r0033ekve0yhbtcsx","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-44-58.png","slug":"Snipaste_2019-01-04_10-44-58.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-52-49.png","slug":"Snipaste_2019-01-04_10-52-49.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-29-30.png","slug":"Snipaste_2019-01-04_11-29-30.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-36-31.png","slug":"Snipaste_2019-01-04_11-36-31.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-01-32.png","slug":"Snipaste_2019-01-04_12-01-32.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-02-47.png","slug":"Snipaste_2019-01-04_12-02-47.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-28-19.png","slug":"Snipaste_2019-01-04_13-28-19.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-15.png","slug":"Snipaste_2019-01-04_13-31-15.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-52.png","slug":"Snipaste_2019-01-04_13-31-52.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-16-44.png","slug":"Snipaste_2019-01-04_15-16-44.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-18-40.png","slug":"Snipaste_2019-01-04_15-18-40.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-25-17.png","slug":"Snipaste_2019-01-11_13-25-17.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-35-56.png","slug":"Snipaste_2019-01-11_13-35-56.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-39-26.png","slug":"Snipaste_2019-01-11_13-39-26.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-11_15-02-16.png","slug":"Snipaste_2019-01-11_15-02-16.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/创建ML-Agents的Docker镜像/Snipaste_2019-01-11_22-38-42.png","slug":"Snipaste_2019-01-11_22-38-42.png","post":"cjxd6ma3n0030ekveq9584kkb","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_08-30-41.png","slug":"Snipaste_2019-01-03_08-30-41.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_09-42-07.png","slug":"Snipaste_2019-01-03_09-42-07.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png","slug":"Snipaste_2019-01-03_09-56-41.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_10-03-30.png","slug":"Snipaste_2019-01-03_10-03-30.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_10-11-01.png","slug":"Snipaste_2019-01-03_10-11-01.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png","slug":"Snipaste_2019-01-03_10-56-08.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_10-57-56.png","slug":"Snipaste_2019-01-03_10-57-56.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_13-49-28.png","slug":"Snipaste_2019-01-03_13-49-28.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-24-49.png","slug":"Snipaste_2019-01-03_16-24-49.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-29-58.png","slug":"Snipaste_2019-01-03_16-29-58.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-32-08.png","slug":"Snipaste_2019-01-03_16-32-08.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-33-27.png","slug":"Snipaste_2019-01-03_16-33-27.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-40-53.png","slug":"Snipaste_2019-01-03_16-40-53.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_16-44-22.png","slug":"Snipaste_2019-01-03_16-44-22.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_17-05-30.png","slug":"Snipaste_2019-01-03_17-05-30.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_17-12-36.png","slug":"Snipaste_2019-01-03_17-12-36.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_17-39-07.png","slug":"Snipaste_2019-01-03_17-39-07.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_18-08-49.png","slug":"Snipaste_2019-01-03_18-08-49.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_18-11-49.png","slug":"Snipaste_2019-01-03_18-11-49.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_18-14-21.png","slug":"Snipaste_2019-01-03_18-14-21.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_18-20-07.png","slug":"Snipaste_2019-01-03_18-20-07.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/create-sniper-docker-image/Snipaste_2019-01-03_18-31-16.png","slug":"Snipaste_2019-01-03_18-31-16.png","post":"cjxd6ma2v002kekvef8cmnf3z","modified":0,"renderable":0},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/MRP.png","slug":"MRP.png","post":"cjxd6ma86003oekvegd9d4oli","modified":0,"renderable":0},{"_id":"source/_posts/rl-with-deep-energy-based-policies/1dgmm.gif","slug":"1dgmm.gif","post":"cjxd6ma81003kekvekgs6f8q9","modified":0,"renderable":0},{"_id":"source/_posts/rl-with-deep-energy-based-policies/multimodal-policy.png","slug":"multimodal-policy.png","post":"cjxd6ma81003kekvekgs6f8q9","modified":0,"renderable":0},{"_id":"source/_posts/rl-with-deep-energy-based-policies/pseudo.png","slug":"pseudo.png","post":"cjxd6ma81003kekvekgs6f8q9","modified":0,"renderable":0},{"_id":"source/_posts/rl-with-deep-energy-based-policies/unimodal-policy.png","slug":"unimodal-policy.png","post":"cjxd6ma81003kekvekgs6f8q9","modified":0,"renderable":0},{"_id":"source/_posts/rl-with-deep-energy-based-policies/vp.gif","slug":"vp.gif","post":"cjxd6ma81003kekvekgs6f8q9","modified":0,"renderable":0},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/M.jpg","slug":"M.jpg","post":"cjxd6ma86003oekvegd9d4oli","modified":0,"renderable":0},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/MDP.jpg","slug":"MDP.jpg","post":"cjxd6ma86003oekvegd9d4oli","modified":0,"renderable":0},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/MP.jpg","slug":"MP.jpg","post":"cjxd6ma86003oekvegd9d4oli","modified":0,"renderable":0},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/MPs.jpg","slug":"MPs.jpg","post":"cjxd6ma86003oekvegd9d4oli","modified":0,"renderable":0},{"_id":"source/_posts/强化学习之MDP马尔科夫决策过程/agent-env.png","slug":"agent-env.png","post":"cjxd6ma86003oekvegd9d4oli","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/Agakov.png","slug":"Agakov.png","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/cdp-pseudo.png","slug":"cdp-pseudo.png","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/diayn-pseudo.png","slug":"diayn-pseudo.png","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/diayn.png","slug":"diayn.png","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/gorila-pseudo.png","slug":"gorila-pseudo.png","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/gorila.png","slug":"gorila.png","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/mb-mpo-pseudo.png","slug":"mb-mpo-pseudo.png","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/mb-mpo-visio.png","slug":"mb-mpo-visio.png","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/skill.png","slug":"skill.png","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/visio.vsdx","slug":"visio.vsdx","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0},{"_id":"source/_posts/rl-rough-reading/naf-pseudo.png","slug":"naf-pseudo.png","post":"cjxd6ma84003mekvelx7qeeiy","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cjxd6m9rb0000ekvez87f60ik","category_id":"cjxd6m9s00004ekveb0b3tima","_id":"cjxd6m9sw000gekve4xzy59pk"},{"post_id":"cjxd6m9rp0002ekvepc7g52i8","category_id":"cjxd6m9si000bekvewgsxgaf2","_id":"cjxd6m9t8000nekvedw2a5n80"},{"post_id":"cjxd6m9s50006ekve4q655etg","category_id":"cjxd6m9sx000hekvef7j07efs","_id":"cjxd6m9tt000uekvesu2wvv9z"},{"post_id":"cjxd6m9sb0008ekvevjgqf2ss","category_id":"cjxd6m9si000bekvewgsxgaf2","_id":"cjxd6m9u5000zekvenpsnnbul"},{"post_id":"cjxd6m9tx000wekve8q49en92","category_id":"cjxd6m9si000bekvewgsxgaf2","_id":"cjxd6m9ua0015ekvebqeqm4yi"},{"post_id":"cjxd6m9sg000aekveiwixg3ik","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6m9uc0018ekvejgiyul7i"},{"post_id":"cjxd6m9u4000yekvej9ip68jh","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6m9uh001cekve3ox7cpl7"},{"post_id":"cjxd6m9u60011ekve047kui9x","category_id":"cjxd6m9si000bekvewgsxgaf2","_id":"cjxd6m9uj001eekve9zlhs3g9"},{"post_id":"cjxd6m9u90014ekvepcsrtohs","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6m9ul001iekve3g4v3776"},{"post_id":"cjxd6m9sr000eekvez6rytift","category_id":"cjxd6m9u50010ekve7p1jy6d4","_id":"cjxd6m9um001kekve73i4cgzh"},{"post_id":"cjxd6m9ub0017ekveyl93vuxv","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6m9uo001nekve4szyx4le"},{"post_id":"cjxd6m9sv000fekvezsrt1s37","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6m9ur001pekvew2ms8p8h"},{"post_id":"cjxd6m9t2000jekve7czuq25g","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6m9uu001sekve3i0sgvc3"},{"post_id":"cjxd6m9t6000lekve2s6k1efv","category_id":"cjxd6m9u50010ekve7p1jy6d4","_id":"cjxd6m9uz001vekvevnj0x2nz"},{"post_id":"cjxd6m9th000qekvezc5xzoew","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6m9v1001xekve6l3zypup"},{"post_id":"cjxd6m9tr000sekve87bc9g04","category_id":"cjxd6m9uy001uekveo7yiqflj","_id":"cjxd6m9v40020ekveud8x058s"},{"post_id":"cjxd6ma24002cekvevi8klydb","category_id":"cjxd6m9sx000hekvef7j07efs","_id":"cjxd6ma2t002iekve72tqptzt"},{"post_id":"cjxd6ma27002dekvez4bso9fs","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma2w002lekverrm133gu"},{"post_id":"cjxd6ma2e002fekveuph10svx","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma2z002oekveuwvp2549"},{"post_id":"cjxd6ma2r002hekve0ekqsbs4","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma33002rekve4wj7ljr4"},{"post_id":"cjxd6ma2v002kekvef8cmnf3z","category_id":"cjxd6m9sx000hekvef7j07efs","_id":"cjxd6ma3b002uekved1dk7zmn"},{"post_id":"cjxd6ma2y002nekveqvkcldly","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma3g002yekvenr6ghfdv"},{"post_id":"cjxd6ma30002qekveia40bz6c","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma3p0031ekve9end84n1"},{"post_id":"cjxd6ma38002tekve7o57p1v8","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma3t0034ekve9rhttwa3"},{"post_id":"cjxd6ma3d002wekvercmn0xoe","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma420038ekveh33ant1w"},{"post_id":"cjxd6ma3r0033ekve0yhbtcsx","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma44003aekve954vx7d7"},{"post_id":"cjxd6ma3n0030ekveq9584kkb","category_id":"cjxd6m9sx000hekvef7j07efs","_id":"cjxd6ma4f003eekvek1qk4o1j"},{"post_id":"cjxd6ma3n0030ekveq9584kkb","category_id":"cjxd6ma400036ekve03rjwdu3","_id":"cjxd6ma4m003gekveqtgc7hod"},{"post_id":"cjxd6ma80003jekvehf1wgcry","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma87003pekve7f2v45sm"},{"post_id":"cjxd6ma81003kekvekgs6f8q9","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma8f003rekve5g7sjafg"},{"post_id":"cjxd6ma84003mekvelx7qeeiy","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma8k003tekvef49u6yvl"},{"post_id":"cjxd6ma86003oekvegd9d4oli","category_id":"cjxd6m9tt000tekvex8mn3w05","_id":"cjxd6ma8u003uekvenfk3yith"}],"PostTag":[{"post_id":"cjxd6m9rb0000ekvez87f60ik","tag_id":"cjxd6m9s30005ekvey54m5cyk","_id":"cjxd6m9t4000kekveasrax3dw"},{"post_id":"cjxd6m9rb0000ekvez87f60ik","tag_id":"cjxd6m9sl000cekvetdof95b7","_id":"cjxd6m9t8000mekvefzazy1cx"},{"post_id":"cjxd6m9rp0002ekvepc7g52i8","tag_id":"cjxd6m9sx000iekvesn7pvh76","_id":"cjxd6m9tl000rekveugkngwy1"},{"post_id":"cjxd6m9s50006ekve4q655etg","tag_id":"cjxd6m9t9000pekve9lk8ouqk","_id":"cjxd6m9u0000xekveivc2ct94"},{"post_id":"cjxd6m9sb0008ekvevjgqf2ss","tag_id":"cjxd6m9tw000vekveh86xlui5","_id":"cjxd6m9u80013ekveb40ts4t3"},{"post_id":"cjxd6m9u60011ekve047kui9x","tag_id":"cjxd6m9tw000vekveh86xlui5","_id":"cjxd6m9ua0016ekvewd2v8m83"},{"post_id":"cjxd6m9u90014ekvepcsrtohs","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6m9ug001bekveq9p0jftr"},{"post_id":"cjxd6m9sg000aekveiwixg3ik","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6m9ui001dekvemqx42dpt"},{"post_id":"cjxd6m9ub0017ekveyl93vuxv","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6m9uk001hekvewayzgf7g"},{"post_id":"cjxd6m9sr000eekvez6rytift","tag_id":"cjxd6m9ue001aekvembhz14cp","_id":"cjxd6m9ul001jekvejyg7qsy0"},{"post_id":"cjxd6m9sv000fekvezsrt1s37","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6m9up001oekverypqcipt"},{"post_id":"cjxd6m9t2000jekve7czuq25g","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6m9uv001tekvernv053vt"},{"post_id":"cjxd6m9t6000lekve2s6k1efv","tag_id":"cjxd6m9ue001aekvembhz14cp","_id":"cjxd6m9v3001zekvefxd7rbbn"},{"post_id":"cjxd6m9t6000lekve2s6k1efv","tag_id":"cjxd6m9v0001wekvexezbosl6","_id":"cjxd6m9v40021ekvesre7xmsd"},{"post_id":"cjxd6m9th000qekvezc5xzoew","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6m9v50023ekvejeuncyja"},{"post_id":"cjxd6m9tr000sekve87bc9g04","tag_id":"cjxd6m9v40022ekvehwp7l17h","_id":"cjxd6m9v70026ekvep6vudigt"},{"post_id":"cjxd6m9tr000sekve87bc9g04","tag_id":"cjxd6m9v60024ekve9doqovgc","_id":"cjxd6m9v80027ekvez21vwwzi"},{"post_id":"cjxd6m9tx000wekve8q49en92","tag_id":"cjxd6m9v40022ekvehwp7l17h","_id":"cjxd6m9va0029ekvew88zzbel"},{"post_id":"cjxd6m9tx000wekve8q49en92","tag_id":"cjxd6m9s30005ekvey54m5cyk","_id":"cjxd6m9va002aekvelo94jrsp"},{"post_id":"cjxd6m9u4000yekvej9ip68jh","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6m9vb002bekve2ph6z9dq"},{"post_id":"cjxd6ma24002cekvevi8klydb","tag_id":"cjxd6m9t9000pekve9lk8ouqk","_id":"cjxd6ma2d002eekve98u95ytk"},{"post_id":"cjxd6ma27002dekvez4bso9fs","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma2o002gekvemeary2rv"},{"post_id":"cjxd6ma2e002fekveuph10svx","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma2u002jekve7o1c5w7w"},{"post_id":"cjxd6ma2r002hekve0ekqsbs4","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma2x002mekvegyzqsuw7"},{"post_id":"cjxd6ma2y002nekveqvkcldly","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma35002sekve8nfmwel0"},{"post_id":"cjxd6ma30002qekveia40bz6c","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma3d002vekvenrae6tgw"},{"post_id":"cjxd6ma38002tekve7o57p1v8","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma3l002zekve4micwvoe"},{"post_id":"cjxd6ma3d002wekvercmn0xoe","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma3q0032ekvexkkawwpy"},{"post_id":"cjxd6ma2v002kekvef8cmnf3z","tag_id":"cjxd6m9t9000pekve9lk8ouqk","_id":"cjxd6ma410037ekvery75is7r"},{"post_id":"cjxd6ma2v002kekvef8cmnf3z","tag_id":"cjxd6ma2z002pekveju0np643","_id":"cjxd6ma430039ekve48wz14m4"},{"post_id":"cjxd6ma2v002kekvef8cmnf3z","tag_id":"cjxd6ma3f002xekvevi903ery","_id":"cjxd6ma45003cekvezrpnf6lu"},{"post_id":"cjxd6ma3r0033ekve0yhbtcsx","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma46003dekveeb55e7k5"},{"post_id":"cjxd6ma3n0030ekveq9584kkb","tag_id":"cjxd6m9t9000pekve9lk8ouqk","_id":"cjxd6ma4m003fekve6kwpqg6t"},{"post_id":"cjxd6ma3n0030ekveq9584kkb","tag_id":"cjxd6ma3w0035ekvea4ymgp5g","_id":"cjxd6ma4n003hekveubf2a3im"},{"post_id":"cjxd6ma3n0030ekveq9584kkb","tag_id":"cjxd6ma45003bekvez72o3zgs","_id":"cjxd6ma4u003iekvepy2xz25p"},{"post_id":"cjxd6ma80003jekvehf1wgcry","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma83003lekvemlu3h3tz"},{"post_id":"cjxd6ma81003kekvekgs6f8q9","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma86003nekvee3zprbke"},{"post_id":"cjxd6ma84003mekvelx7qeeiy","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma8d003qekveodngcs16"},{"post_id":"cjxd6ma86003oekvegd9d4oli","tag_id":"cjxd6m9u70012ekvee8ix6x53","_id":"cjxd6ma8j003sekvelt10fqio"}],"Tag":[{"name":"note","_id":"cjxd6m9s30005ekvey54m5cyk"},{"name":"deeplearning","_id":"cjxd6m9sl000cekvetdof95b7"},{"name":"Git","_id":"cjxd6m9sx000iekvesn7pvh76"},{"name":"docker","_id":"cjxd6m9t9000pekve9lk8ouqk"},{"name":"markdown","_id":"cjxd6m9tw000vekveh86xlui5"},{"name":"rl","_id":"cjxd6m9u70012ekvee8ix6x53"},{"name":"conda","_id":"cjxd6m9ue001aekvembhz14cp"},{"name":"jupyter notebook","_id":"cjxd6m9v0001wekvexezbosl6"},{"name":"ubuntu","_id":"cjxd6m9v40022ekvehwp7l17h"},{"name":"x2go","_id":"cjxd6m9v60024ekve9doqovgc"},{"name":"mxnet","_id":"cjxd6ma2z002pekveju0np643"},{"name":"sniper","_id":"cjxd6ma3f002xekvevi903ery"},{"name":"unity","_id":"cjxd6ma3w0035ekvea4ymgp5g"},{"name":"ml-agents","_id":"cjxd6ma45003bekvez72o3zgs"}]}}