<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Keavnn&#39;Blog</title>
  
  <subtitle>If it is to be, it is up to me.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://StepNeverStop.github.io/"/>
  <updated>2019-05-10T09:03:17.545Z</updated>
  <id>http://StepNeverStop.github.io/</id>
  
  <author>
    <name>Keavnn</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>强化学习的类别</title>
    <link href="http://StepNeverStop.github.io/rl-classification.html"/>
    <id>http://StepNeverStop.github.io/rl-classification.html</id>
    <published>2019-05-10T07:18:23.000Z</published>
    <updated>2019-05-10T09:03:17.545Z</updated>
    
    <content type="html"><![CDATA[<p>本文讲述了强化学习中各种算法、问题的分类规则。</p><a id="more"></a><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h1 id="Policy-or-Value"><a href="#Policy-or-Value" class="headerlink" title="Policy or Value"></a>Policy or Value</h1><h1 id="MC-or-TD"><a href="#MC-or-TD" class="headerlink" title="MC or TD"></a>MC or TD</h1><h1 id="On-policy-or-Off-policy"><a href="#On-policy-or-Off-policy" class="headerlink" title="On-policy or Off-policy"></a>On-policy or Off-policy</h1><p>首先需要理解什么是行为策略与目标策略。</p><p>行为策略$\mathcal{Behavior Policy}$：</p><ul><li>采样时间序列$S_{0},A_{0},R_{0},S_{1},A_{1},R_{2},…,S_{n},A_{n},R_{n}$的策略</li><li>官话：指导个体产生与环境进行实际交互行为的策略</li><li>未必由一个模型表示</li></ul><p>目标策略$\mathcal{TargetPolicy}$:</p><ul><li>待优化的策略</li><li>官话：用来评价状态或行为价值的策略或者待优化的策略称为目标策略</li></ul><p>同步策略学习$\mathcal{On-Policy}$:</p><ul><li>简言之，边采样边学习</li><li>官话：如果个体在学习过程中优化的策略与自己的行为策略是同一个策略时，这种学习方式称为<strong>同步策略学习（on-policy learning）</strong></li><li>行为策略与目标策略是同一个</li></ul><p>异步策略学习$\mathcal{Off-Policy}$:</p><ul><li>简言之，你采样我学习</li><li>官话：如果个体在学习过程中优化的策略与自己的行为策略是不同的策略时，这种学习方式称为<strong>异步策略学习（off-policy learning）</strong></li><li>行为策略与目标策略不同，行为策略可能是目标策略的“分身”（双网络结构），或者完全是另一个采样的策略</li></ul><p>例如：</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">SARSA</th><th style="text-align:center">Q-learning</th></tr></thead><tbody><tr><td style="text-align:center">Choosing A’</td><td style="text-align:center">π</td><td style="text-align:center">π</td></tr><tr><td style="text-align:center">Updating Q</td><td style="text-align:center">π</td><td style="text-align:center">μ</td></tr></tbody></table></div><blockquote><p><a href="https://stackoverflow.com/a/41420616" rel="external nofollow" target="_blank">一个以Q-Learning和Sarsa算法做比较的解释</a></p></blockquote><h1 id="Stochastic-or-Deterministic"><a href="#Stochastic-or-Deterministic" class="headerlink" title="Stochastic or Deterministic"></a>Stochastic or Deterministic</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文讲述了强化学习中各种算法、问题的分类规则。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>价值与贝尔曼方程</title>
    <link href="http://StepNeverStop.github.io/%E4%BB%B7%E5%80%BC%E4%B8%8E%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B.html"/>
    <id>http://StepNeverStop.github.io/价值与贝尔曼方程.html</id>
    <published>2019-05-09T10:09:02.000Z</published>
    <updated>2019-05-10T07:19:10.116Z</updated>
    
    <content type="html"><![CDATA[<h1 id="价值与贝尔曼方程"><a href="#价值与贝尔曼方程" class="headerlink" title="价值与贝尔曼方程"></a>价值与贝尔曼方程</h1><p>我们人在做决策的时候往往会判断做这件事的价值和后果，就像失恋了去喝不喝闷酒一样，不同的人有不同的选择，但是选择前肯定会判断这么做能给自己带来什么。</p><p>选择去喝酒的人觉得这可以缓解自己的痛苦，这就是判断喝酒这个动作的价值。因为身体原因不选择去喝酒的人觉得喝醉之后身体很不舒服，还会说胡话、闹事，这就是衡量后果、判断喝酒后状态的价值。</p><p>在乎过程的会根据动作的价值进行抉择，在乎结果的会根据状态的价值进行抉择。总之，衡量价值，毫无疑问是我们做决策的重要评判标准。</p><p>机器也一样，我们想教会机器学会自主决策，必然得让它们有一个价值导向，毕竟它可不会、也决不能像人一样”没有原因呀，就随便选择了一个而已”。</p><p>本文介绍了<strong>绝大部分强化学习问题及算法</strong>中值函数与贝尔曼方程的定义。因为有一些研究探索的，如好奇心、信息熵等方向的算法对值函数的定义有稍许不同。</p><a id="more"></a><hr><p>注：以下公式及推导过程可能与其他博客、论文、书本上有稍许不同，不过都是经过细细分析，一步步推导的，或许有些公式难以理解，但都是尽可能细化每一处细节。使读者可以更清楚地了解每一个值的来龙去脉。</p><hr><h2 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h2><p>值函数分为状态值函数与动作值函数，分别用来表示状态和状态下执行某动作的好坏程度、优劣程度。</p><p>回顾一下回报：</p><script type="math/tex; mode=display">\begin{align*}G_{t} &\doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\gamma^{3}R_{t+4}+...\\&=R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+\gamma^{2}R_{t+4}+...)\\&=R_{t+1}+\gamma (R_{t+2}+\gamma (R_{t+3}+\gamma R_{t+4}+...))\\&=R_{t+1}+\gamma G_{t+1}\end{align*}</script><script type="math/tex; mode=display">G_{t}\doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+...=\begin{cases}\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}\\\sum_{k=t+1}^{T}\gamma^{k-t-1}R_{k}\end{cases}</script><p>回顾一下之前的MDP例子：</p><p><img src="./强化学习之MDP马尔科夫决策过程/MDP.jpg" alt=""></p><p>将状态用符号表示为</p><script type="math/tex; mode=display">\begin{bmatrix}玩游戏 & A\\ 语文 & B\\ 数学 & C\\ 英语 & D\\ \mathcal{Pass} & E\\ 睡觉 & F\end{bmatrix}</script><p>将转移概率矩阵$\mathcal{P}$写成如下形式</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">A</th><th style="text-align:center">B</th><th style="text-align:center">C</th><th style="text-align:center">D</th><th style="text-align:center">E</th><th style="text-align:center">F</th></tr></thead><tbody><tr><td style="text-align:center">Reward</td><td style="text-align:center">-1</td><td style="text-align:center">-2</td><td style="text-align:center">-2</td><td style="text-align:center">-2</td><td style="text-align:center">10</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">A</td><td style="text-align:center">0.9</td><td style="text-align:center">0.1</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">0.5</td><td style="text-align:center"></td><td style="text-align:center">0.5</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">C</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">0.8</td><td style="text-align:center"></td><td style="text-align:center">0.2</td></tr><tr><td style="text-align:center">D,0.4</td><td style="text-align:center"></td><td style="text-align:center">0.2</td><td style="text-align:center">0.4</td><td style="text-align:center">0.4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">D,0.6</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">0.6</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">E</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">1.0</td></tr></tbody></table></div><p>其中，D状态有两个动作，但是其0.4概率选到的动作并不一定确定地转移到另一个状态，所以将两个动作分开写，其实除了Reward的每一行都是一个$(s,a)$的状态-动作对，但是除了D状态有特殊外，其他状态的转移都是确定的，于是省略了动作。后续将会看到如果根据$(D,0.4)$这个状态-动作对去进行相应的计算。</p><h3 id="状态值函数-V-s"><a href="#状态值函数-V-s" class="headerlink" title="状态值函数$V(s)$"></a>状态值函数$V(s)$</h3><p>$\pi$策略下$s$状态的价值函数可以表示为$v_{\pi}(s)$，由<strong>期望回报</strong>表示</p><script type="math/tex; mode=display">v_{\pi}(s) \doteq \mathbb{E}_{\pi}[G_{t}|S_{t}=s] = \mathbb{E}_{\pi}\left [ \sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}\mid S_{t}=s \right ], \ for \ all \ s\in S</script><p>有了这个公式，我们能根据上述表格计算出每个状态的价值吗？当然可以，只是很麻烦，如果对于连续状态空间的问题就不只是麻烦的问题，而是不能计算。</p><p>为什么呢？因为要求期望需要遍历所有可能性的episode，连续状态空间根本无法遍历所有的情况。</p><h3 id="动作值函数-Q-s-a"><a href="#动作值函数-Q-s-a" class="headerlink" title="动作值函数$Q(s,a)$"></a>动作值函数$Q(s,a)$</h3><p>动作值函数与状态值函数在公式表示上差别不大，$\pi$策略$s$状态下执行a动作的价值函数可以表示为$Q_{\pi}(s，a)$，由<strong>期望回报</strong>表示</p><script type="math/tex; mode=display">Q_{\pi}(s,a) \doteq \mathbb{E}_{\pi}[G_{t}|S_{t}=s,A_{t}=a] = \mathbb{E}_{\pi}\left [ \sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}\mid S_{t}=s,A_{t}=a \right ]</script><h2 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h2><blockquote><p><a href="https://baike.baidu.com/item/贝尔曼方程/5500990?fr=aladdin" rel="external nofollow" target="_blank">贝尔曼方程（Bellman Equation）(百度百科)</a>也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。</p><p>贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。此方程把“决策问题在特定时间怎么的值”以“来自初始选择的报酬比从初始选择衍生的决策问题的值”的形式表示。借此这个方式把动态最佳化问题变成简单的子问题，而这些子问题遵守从贝尔曼所提出来的“最佳化还原理”。</p></blockquote><p><strong>贝尔曼方程将状态值函数$V(s)$与动作值函数$Q(s,a)$、将当前的值函数与之后状态$V(s‘)$或动作的值函数$Q(s’,a‘)$联系起来。</strong></p><h3 id="状态值函数-V-s-与动作值函数-Q-s-a-的关系"><a href="#状态值函数-V-s-与动作值函数-Q-s-a-的关系" class="headerlink" title="状态值函数$V(s)$与动作值函数$Q(s,a)$的关系"></a>状态值函数$V(s)$与动作值函数$Q(s,a)$的关系</h3><p><img src="./价值与贝尔曼方程/vs.jpg" alt=""></p><script type="math/tex; mode=display">v_{\pi}(s)=\sum_{a}\pi(a\mid s) q_{\pi}(s,a)</script><p><img src="./价值与贝尔曼方程/qsa.jpg" alt=""></p><script type="math/tex; mode=display">q_{\pi}(s,a) = \sum_{s',r}p(s',r \mid s,a)\left[r+\gamma v_{\pi}(s')\right]</script><h3 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h3><p>状态值函数$V(s)$可以写成如下形式：</p><script type="math/tex; mode=display">\begin{align*}v_{\pi}(s) & \doteq \mathbb{E}_{\pi}\left [ G_{t}\mid S_{t}=s \right ]\\&=\mathbb{E}_{t} \left [R_{t+1}+\gamma G_{t+1} \mid S_{t}=s \right]\\&=\sum_{a}\pi(a\mid s)\sum_{s'}\sum_{r}p(s',r\mid s,a)\left[r+\gamma \mathbb{E}\left[G_{t+1}\mid S_{t+1}=s' \right]\right]\\&=\sum_{a}\pi(a\mid s)\sum_{s',r}p(s',r \mid s,a)\left[r+\gamma v_{\pi}(s')\right]\\&=\sum_{a}\pi(a\mid s) q_{\pi}(s,a)\end{align*},for \ all \ s\in S</script><p><img src="./价值与贝尔曼方程/v.jpg" alt=""></p><p>看到没有，此时可以将当前状态的状态值$v_{\pi}(s)$与下一个可到达状态的状态值$v_{\pi}(s’)$联系起来！</p><p>动作值函数$Q_{\pi}(s,a)$也可以进行类似推导：</p><script type="math/tex; mode=display">\begin{align*}q_{\pi}(s,a) & \doteq \mathbb{E}_{\pi}\left [ G_{t}\mid S_{t}=s,A_{t}=a \right ]\\&=\mathbb{E}_{t} \left [R_{t+1}+\gamma G_{t+1} \mid S_{t}=s,A_{t}=a \right]\\&=\sum_{s',r}p(s',r\mid s,a)\left[r+\gamma \sum_{a'}\pi(a'\mid s') \mathbb{E}\left[G_{t+1}\mid S_{t+1}=s',A_{t+1}=a' \right]\right]\\&=\sum_{s',r}p(s',r \mid s,a)\left[r+\gamma \sum_{a'}\pi(a'\mid s') q_{\pi}(s',a')\right]\\&=\sum_{s',r}p(s',r \mid s,a)\left[r+\gamma v_{\pi}(s')\right]\end{align*}</script><p><img src="./价值与贝尔曼方程/q.jpg" alt=""></p><h3 id="最优值函数"><a href="#最优值函数" class="headerlink" title="最优值函数"></a>最优值函数</h3><p>解决一个强化学习问题也就是意味着找到一种选择动作的策略能够获得足够多的回报。如果执行每个动作所产生的转移都是确定的（有限MDP），那么能够定义出一个最优策略，如果一个策略$\pi’$的所有状态值函数都大于$\pi$，那么就说策略$\pi’$更好，但不一定是最好的，我们把最优策略用$\pi_{*}$表示。</p><p>最优状态值函数：</p><script type="math/tex; mode=display">v_{*}(s)=\max_{\pi} v_{\pi}(s)</script><p>最优动作值函数：</p><script type="math/tex; mode=display">q_{*}(s,a)=\max_{\pi} q_{\pi}(s,a)</script><h3 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h3><script type="math/tex; mode=display">\begin{align*}v_{*}(s) &= \max_{a} q_{*}(s,a)\\&=\max_{a}\mathbb{E}\left[r+\gamma v_{*}(s')\mid s,a\right]\\&=\max_{a}\sum_{s',r}p(s',r\mid s,a)\left[r+\gamma v_{*}(s')\right]\end{align*}</script><script type="math/tex; mode=display">\begin{align*}q_{*}(s,a) &= \mathbb{E}\left[r+\gamma v_{*}(s')\mid s,a\right]\\&=\mathbb{E}\left[r+\gamma \max_{a'} q_{*}(s',a')\mid s,a\right]\\&=\sum_{s',r}p(s',r\mid s,a)\left[r+\gamma \max_{a'}q_{*}(s',a')\right]\end{align*}</script><p><img src="./价值与贝尔曼方程/vq.jpg" alt=""></p><p>虽然我们已经定义出了最优值函数和最优策略，而且理论上也可以直接计算出来。但是通常情况下我们没法得到这么多的计算资源。与此同时内存溢出也是一个很大的问题，因为很多问题的状态数量太多超过存储范围。对于这些情况我们就不能够使用直接存储每个状态的值函数而是必须使用一种更精简的参数型函数表示的方法。</p><p>强化学习的框架迫使我们进行近似求解，而且这个框架同时也很容易进行近似，比如对于很多小概率出现的状态，选择最优解和次优解区别不大。</p><h3 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h3><p>定义策略之间的偏序关系</p><script type="math/tex; mode=display">\pi \geq \pi' \ if \ v_{\pi}(s) \geq v_{\pi'}(s) \ , \ \forall s</script><p>那么有如下定理成立：</p><p>对任意MDP：</p><ul><li>存在最优策略$\pi_{\ast}$，满足$\pi_{\ast} \geq \pi,\forall \pi$</li><li>所有最优策略的状态值函数都等于最优状态值函数$v_{\pi_{\ast}}(s)=v_{\ast}(s)$</li><li>所有最优策略的动作值函数都等于最优动作值函数$q_{\pi_{\ast}}(s,a)=q_{\ast}(s,a)$</li></ul><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>对于上述例子和表格，我们来试着计算一下$V(S)、Q(S,A)$。</p><p>第一个问题，怎么计算这些值？初始化终态的状态值为0，然后从后向前递归？我们来试一下！</p><h3 id="只初始化终态"><a href="#只初始化终态" class="headerlink" title="只初始化终态"></a>只初始化终态</h3><p>根据上述公式，设$\gamma =1$：</p><script type="math/tex; mode=display">\begin{align*}&v(F)=r=0\\&q(E，)=1\times (0+v(F))=0\\&1表示选择这个动作转移至另一个状态的概率\\&v(E)=1\times q(E,)=0\\&q(D,0.6)=1\times (10+v(E))=10\\&q(D,0.4)=0.2\times(-2+v(B))+0.4\times(-2+v(C))+0.4\times(-2+v(D))=\\&v(D)=0.4\times q(D,0.4)+0.6\times q(D,0.6)=\\&q(C,0.8)=1\times (-2+v(D))=\\&q(C,0.2)=1\times (0+v(F))=0\\&v(C)=0.2\times q(C,0.2)+0.8\times q(C,0.8)=\\&q(B,0.5_{C})=1\times (-2+v(C))=\\&q(B,0.5_{A})=1\times (-1+v(A))=\\&v(B)=0.5\times q(B,0.5_{C})+0.5 \times q(B,0.5_{A})=\\&q(A,0.9)=1\times (-1+v(A))=\\&q(A,0.1)=1\times(-2+v(B))=\\&v(A)=0.1\times q(A,0.1)+0.9\times q(A,0.9)=\end{align*}</script><p>哎呀，卡住了，解不出来，$v(D)、v(A)、v(B)$互相依赖，解不出来，看来这样计算是行不通了。其实，很多问题中终态都很难定义，更别说使用这种方法了。</p><h3 id="初始化全部状态值"><a href="#初始化全部状态值" class="headerlink" title="初始化全部状态值"></a>初始化全部状态值</h3><p>初始化所有状态的值函数为0，即</p><script type="math/tex; mode=display">v(s)=0,\ for\ all\ s\in S</script><p>先试验一下$\gamma =0.5$，</p><div class="table-container"><table><thead><tr><th style="text-align:center">V和Q，$\gamma =0.5$</th><th style="text-align:center">初始化V计算Q</th><th style="text-align:center">迭代→V→Q</th><th>第46轮完全收敛</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>-2.171</td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>-1.880</td></tr><tr><td style="text-align:center">C</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>0.651</td></tr><tr><td style="text-align:center">D</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>5.627</td></tr><tr><td style="text-align:center">E</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>0</td></tr><tr><td style="text-align:center">F</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>0</td></tr><tr><td style="text-align:center">$(A,0.1)\rightarrow B$</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>-2.940</td></tr><tr><td style="text-align:center">$(A,0.9)\rightarrow A$</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>-2.085</td></tr><tr><td style="text-align:center">$(B,0.5_{A})\rightarrow A$</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>-2.085</td></tr><tr><td style="text-align:center">$(B,0.5_{C})\rightarrow C$</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>-1.675</td></tr><tr><td style="text-align:center">$(C,0.2)\rightarrow F$</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>0</td></tr><tr><td style="text-align:center">$(C,0.8)\rightarrow D$</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>0.814</td></tr><tr><td style="text-align:center">$(D,0.4)\rightarrow \begin{cases}B,0.2\\C,0.4\\D,0.4\end{cases}$</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>-0.932</td></tr><tr><td style="text-align:center">$(D,0.6)\rightarrow E$</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>10</td></tr><tr><td style="text-align:center">$(E,1.0)\rightarrow F$</td><td style="text-align:center">0</td><td style="text-align:center">……</td><td>0</td></tr></tbody></table></div><p>这是代码计算的结果，接下来我使$\gamma =1$，计算结果如下，每迭代100次输出一下：</p><p><img src="./价值与贝尔曼方程/example1.png" alt=""></p><p>可以发现，在700至800次迭代后值函数最终收敛。</p><p>如果我将$\gamma $设置为0.1呢？来看一下结果：</p><p><img src="./价值与贝尔曼方程/example2.png" alt=""></p><p>仅仅需要十几次就可以迭代至收敛。</p><p>如果设置为0呢？会怎么样？看结果：</p><p><img src="./价值与贝尔曼方程/example3.png" alt=""></p><p>仅需一次迭代就可以收敛，而且就是转移状态的立即奖励值，这下可以理解$\gamma$为什么表示对未来的看重程度了吧。</p><p>一般我们是不会将$\gamma$设置为0的，从这个例子的直观感受也可以得到，就拿$\gamma =0$与$\gamma =0.5来比较$：</p><ul><li><p>$\gamma =0$状态值最高的是$v(D)=5.2$，这很容易理解，D状态距离最大奖励值10最近，理应最好，这点与$\gamma =0.5$时相同。</p></li><li><p>但是对于状态C，$\gamma =0$时认为这个状态最差，$v(C)=-1.6$，$\gamma =0$时认为这个状态次优，$v(C)=0.651$，其实这就是目光短浅与目光长远的不同，$\gamma =0$并没有考虑到其附近状态的临近状态的价值，导致其主观的认为最接近我的都是负的，于是状态肯定差。</p></li><li><p>对于动作值也是一样，一个认为次优，一个认为最差。</p></li></ul><p><strong>注意：并不是说$\gamma$越接近于1越好，因为在有些问题上，$\gamma=1$时其值函数永远不收敛，必须设置$0 \leq \gamma \lt 1$，值函数才能收敛。为什么呢？试着计算一下$\gamma^{n}$，看看对不同的$\gamma$值，$n$取什么值时结果接近0。</strong></p><p>试着计算一下这个例子，红色代表立即奖励，蓝色代表选择动作的概率以及状态转移的概率，小写字母代表动作，大写字母代表状态。</p><p><img src="./价值与贝尔曼方程/example4.png" alt=""></p><p>$\gamma =1$时，迭代100W次也不收敛：</p><p><img src="./价值与贝尔曼方程/example5.png" alt=""></p><p>$\gamma =0.5$时，迭代50多次即可收敛：</p><p><img src="./价值与贝尔曼方程/example6.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;价值与贝尔曼方程&quot;&gt;&lt;a href=&quot;#价值与贝尔曼方程&quot; class=&quot;headerlink&quot; title=&quot;价值与贝尔曼方程&quot;&gt;&lt;/a&gt;价值与贝尔曼方程&lt;/h1&gt;&lt;p&gt;我们人在做决策的时候往往会判断做这件事的价值和后果，就像失恋了去喝不喝闷酒一样，不同的人有不同的选择，但是选择前肯定会判断这么做能给自己带来什么。&lt;/p&gt;
&lt;p&gt;选择去喝酒的人觉得这可以缓解自己的痛苦，这就是判断喝酒这个动作的价值。因为身体原因不选择去喝酒的人觉得喝醉之后身体很不舒服，还会说胡话、闹事，这就是衡量后果、判断喝酒后状态的价值。&lt;/p&gt;
&lt;p&gt;在乎过程的会根据动作的价值进行抉择，在乎结果的会根据状态的价值进行抉择。总之，衡量价值，毫无疑问是我们做决策的重要评判标准。&lt;/p&gt;
&lt;p&gt;机器也一样，我们想教会机器学会自主决策，必然得让它们有一个价值导向，毕竟它可不会、也决不能像人一样”没有原因呀，就随便选择了一个而已”。&lt;/p&gt;
&lt;p&gt;本文介绍了&lt;strong&gt;绝大部分强化学习问题及算法&lt;/strong&gt;中值函数与贝尔曼方程的定义。因为有一些研究探索的，如好奇心、信息熵等方向的算法对值函数的定义有稍许不同。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>强化学习</title>
    <link href="http://StepNeverStop.github.io/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.html"/>
    <id>http://StepNeverStop.github.io/强化学习.html</id>
    <published>2019-05-09T08:24:54.000Z</published>
    <updated>2019-05-10T09:04:14.972Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><ul><li><a href="./强化学习基本概念.html">强化学习基本概念</a></li><li><a href="./强化学习之MDP马尔科夫决策过程.html">强化学习之MDP马尔科夫决策过程</a></li><li><a href="./价值与贝尔曼方程.html">价值与贝尔曼方程</a></li><li><a href="./rl-classification.html">强化学习的类别</a></li></ul><h1 id="论文精读"><a href="#论文精读" class="headerlink" title="论文精读"></a>论文精读</h1><ul><li>-</li></ul><h1 id="相关信息"><a href="#相关信息" class="headerlink" title="相关信息"></a>相关信息</h1><ul><li><a href="./强化学习的里程碑.html">强化学习的里程碑</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基础知识&quot;&gt;&lt;a href=&quot;#基础知识&quot; class=&quot;headerlink&quot; title=&quot;基础知识&quot;&gt;&lt;/a&gt;基础知识&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;./强化学习基本概念.html&quot;&gt;强化学习基本概念&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=
      
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>强化学习之MDP马尔科夫决策过程</title>
    <link href="http://StepNeverStop.github.io/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BMDP%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B.html"/>
    <id>http://StepNeverStop.github.io/强化学习之MDP马尔科夫决策过程.html</id>
    <published>2019-05-08T03:04:20.000Z</published>
    <updated>2019-05-10T05:37:53.262Z</updated>
    
    <content type="html"><![CDATA[<h1 id="强化学习之MDP马尔科夫决策过程"><a href="#强化学习之MDP马尔科夫决策过程" class="headerlink" title="强化学习之MDP马尔科夫决策过程"></a>强化学习之MDP马尔科夫决策过程</h1><p>每每提到强化学习，最先接触的理论肯定是马尔科夫决策过程（MDP，Markov Decision Process），为什么总提到MDP呢？并不是只有我一个人有这个疑问。</p><a id="more"></a><p>百度上没有人提出这样的问题，可能是大家理解得都比较透彻吧，于是在Google查到相关提问和解释。</p><blockquote><p><a href="https://datascience.stackexchange.com/a/38851" rel="external nofollow" target="_blank">What is the relationship between Markov Decision Processes and Reinforcement Learning?</a></p><blockquote><p>In Reinforcement Learning (RL), the problem to resolve is described as a Markov Decision Process (MDP). Theoretical results in RL rely on the MDP description being a correct match to the problem. If your problem is well described as a MDP, then RL may be a good framework to use to find solutions. That does not mean you need to fully describe the MDP (all the transition probabilities), just that you expect an MDP model could be made or discovered.</p><p>Conversely, if you cannot map your problem onto a MDP, then the theory behind RL makes no guarantees of any useful result.</p><p>One key factor that affects how well RL will work is that the states should have the Markov property - that the value of the current state is enough knowledge to fix immediate transition probabilities and immediate rewards following an action choice. Again you don’t need to know in advance what those are, just that this relationship is expected to be reliable and stable. If it is not reliable, you may have a POMDP. If it is not stable, you may have a non-stationary problem. In either case, if the difference from a more strictly defined MDP is small enough, you may still get away with using RL techniques or need to adapt them slightly.</p></blockquote></blockquote><p>MDP是当前强化学习理论推导的基石，对强化学习来说，一般以马尔科夫决策过程作为形式化问题的手段。也就是说，对于目前的绝大部分强化学习算法，只有可以将问题抽象为MDP的才可以确保算法的性能（收敛性，效果等），对于违背MDP的问题并不一定确保算法有效，因为其数学公式都是基于MDP来进行推导的。</p><h2 id="马尔科夫性"><a href="#马尔科夫性" class="headerlink" title="马尔科夫性"></a>马尔科夫性</h2><blockquote><p>马尔科夫性质（英语：Markov property）是概率论中的一个概念，因为俄国数学家安德雷·马尔科夫得名。当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔科夫性质。<a href="https://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8/23149887?fr=aladdin" rel="external nofollow" target="_blank">马尔科夫性-百度百科</a></p></blockquote><p>马尔科夫性，也就是无后效性：<strong>某阶段的状态一旦确定，则此后过程的演变不再受此前各状态及决策的影响</strong>。也就是说，<strong>未来与过去无关</strong>。</p><p>具体地说，如果一个问题被划分各个阶段之后，阶段$k$中的状态只能通过阶段$k+1$中的状态通过状态转移方程得来，与其他状态没有关系，特别是与未发生的状态没有关系，这就是无后效性。</p><p>公式描述：</p><script type="math/tex; mode=display">P[S_{t+1}|S_{t}]=P[S_{t+1}|S_{1},...,S_{t}]</script><p>强化学习问题中的状态也符合马尔科夫性，即在当前状态$s_{t}$下执行动作$a_{t}$并转移至下一个状态$s_{t+1}$，而不需要考虑之前的状态$s_{t-1},…,s_{1}$。</p><p>举一个不恰当的例子：</p><p><img src="./强化学习之MDP马尔科夫决策过程/M.jpg" alt=""></p><p>假设天气预测符合马尔科夫性，如果以每天表示为一种状态，即周一、周二到周日。今天（5月8日，周三）天气为晴，明天（周四）会不会下雨只与今天的天气有关，而与之前周一、周二的天气状况无关。如果以时间节点表示为一种状态，即2点、5点、8点等，如图2点的温度为15.8°C,那么下个时间点5点的气温如何只与2点的温度有关系。</p><p>强化学习中默认状态的转移是符合马尔科夫性质的，状态具体是什么，需要根据不同的问题进行不同的设定。</p><h2 id="马尔科夫过程"><a href="#马尔科夫过程" class="headerlink" title="马尔科夫过程"></a>马尔科夫过程</h2><p>马尔科夫过程是随机过程的一种，什么是随机过程呢？简单来说，一个商店从早上营业到晚上打烊这段时间，根据每个时间点店内顾客的人数所组成的序列就是随机过程。随机过程根据时间节点$T_{t}$取到的值是一个变量。</p><p>马尔科夫过程是满足马尔科夫性的随机过程，它由二元组$M=(S,P)$组成，且满足：</p><ol><li>S是有限状态集合</li><li>P是状态转移概率矩阵</li></ol><p>状态与状态之间的转换过程即为马尔科夫过程。<strong><em>虽然我们可能不知道P的具体值到底是什么，但是通常我们假设P是存在的（转移概率存在，如果是确定的，无非就是概率为1），而且是稳定的（意思是从状态A到其他状态的转移虽然符合某个分布，但是其转移到某个状态的概率是确定的，不随时间变化的）。</em></strong></p><p>这里说的<strong>有限</strong>二字我有自己的理解，在最开始的强化学习研究中，解决的都是表格式的问题，也就是状态的数量是有限可取的，但是后续强化学习研究的也有连续状态空间的问题，算法如DQN,PG,PPO等。状态的数量并不是有限的，但是其向量维度则是固定的、有限的，而且也同样符合马尔科夫性质，因此<strong>我认为这里定义的有限并不是说状态数量有限，而是状态维度有限</strong>。因为好像没有无限马尔科夫的叫法，所以姑且这么解释一下。</p><p>马尔科夫过程有如下分类：</p><p><img src="./强化学习之MDP马尔科夫决策过程/MPs.jpg" alt=""></p><h3 id="状态转移矩阵"><a href="#状态转移矩阵" class="headerlink" title="状态转移矩阵"></a>状态转移矩阵</h3><p>状态转移矩阵由许多状态转移概率组成，状态转移概率是指从一个马尔科夫状态$s$转移到下一个状态$s’$的概率。</p><p>公示表示：</p><script type="math/tex; mode=display">\mathcal{P}_{ss'}=\mathbb{P}[S_{t+1}=s'|S_{t}=s]</script><p>等同于：</p><script type="math/tex; mode=display">\mathcal{P}(s'|s)=\mathbb{P}[S_{t+1}=s'|S_{t}=s]</script><p>假设有1到n个状态，将所有的状态从上到下、从左到右排列，组成一个$n \times n$的矩阵，那么其状态转移矩阵如下所示：</p><script type="math/tex; mode=display">\mathcal{P}=\begin{bmatrix}\mathcal{P}_{11} & \cdots & \mathcal{P}_{1n} \\ \vdots & \ddots & \vdots \\ \mathcal{P}_{n1} & \cdots & \mathcal{P}_{nn} \\\end{bmatrix}</script><p>其中，每行元素相加等于1，每列元素相加等于1，矩阵的总和为状态的数量n。</p><p>对于可数状态，$\sum_{s’=1}^{n}\mathcal{P}(s’|s)=1$</p><script type="math/tex; mode=display">sum(\mathcal{P}) = \sum_{s'=1}^{n}\sum_{s=1}^{n}\mathcal{P}_{ss'} = n</script><p>对于不可数状态（连续状态),$\int_{s’}\mathcal{P}(s’|s)=1$</p><script type="math/tex; mode=display">sum(\mathcal{P}) = \int_{s'}\int_{s}\mathcal{P}_{ss'} = n</script><p>举一个马尔科夫过程的例子:</p><p>假设一个学生，他目前在学习语文科目，那么他接下来进行的活动过程如下图所示，游戏的吸引力很大，所以他有50%的概率在学完语文去玩游戏，并且很容易沉迷其中，图示玩游戏这个循环有90%的可能性，他还可以选择学习其他科目或者去睡觉，最终学习结束之后是否能通过考试也是有一定的概率的，这些状态之间转移的概率即为状态转移概率。</p><p><img src="./强化学习之MDP马尔科夫决策过程/MP.jpg" alt=""></p><p>如果把例子中的各项状态用字母表示，将其表示为：</p><script type="math/tex; mode=display">\begin{bmatrix}玩游戏 & A\\ 语文 & B\\ 数学 & C\\ 英语 & D\\ 挂科 & E\\ \mathcal{Pass} & F\\ 睡觉 & G\end{bmatrix}</script><p>那么其状态转移矩阵$\mathcal{P}$可以表示成：</p><script type="math/tex; mode=display">\begin{array}{lc}\mbox{}&\begin{array}{cc}A & B & C & D & E & F & G \end{array}\\\begin{array}{c}A\\B\\C\\D\\E\\F\\G\end{array}&\left[\begin{array}{cc}0.9&0.1\\0.5& &0.5\\& & &0.8& & &0.2\\&&&&0.4&0.6&\\&0.2&0.4&0.4&&&\\&&&&&&1\\&&&&&&&\end{array}\right]\end{array}</script><h3 id="马尔科夫链与Episode"><a href="#马尔科夫链与Episode" class="headerlink" title="马尔科夫链与Episode"></a>马尔科夫链与Episode</h3><p>Episode可以翻译为片段、情节、回合等，在强化学习问题中，一个Episode就是一个马尔科夫链，根据状态转移矩阵可以得到许多不同的episode，也就是多个马尔科夫链。</p><p>强化学习问题分两种：</p><ol><li>如果一个任务总能达到终态，结束任务或者开启下一轮任务，那么这个任务就被称为回合任务，也就是episode任务。例如，让一个智能体学习如何下围棋，围棋棋盘只有那么大，游戏定会终局，所以是一个回合式任务。</li><li>如果一个任务可以无限持续下去，永远不会结束，即永远在训练当中，那么这个任务就被称为连续性任务。例如，教会一辆车能够进行自动驾驶就是一个连续性任务，<em>不要钻牛角尖说能源会耗尽，车子会磨损，我们只聚焦问题与环境本身，不涉及其他非稳定因素。</em></li></ol><p>在上边举的例子中就是一个回合式任务，因为无论这个序列有多长，最终都会达到终态-“睡觉”。</p><p>根据上述例子我们可能采样出如下episode：</p><ol><li>$B-C-D-E-C-G$，即“学语文→数学→英语→考试没通过,挂科→继续学数学→睡觉”</li><li>$B-A-A-…-A-B-C-G$，即“学语文→玩王者荣耀→玩刺激战场→玩OverCooker→玩守望先锋→玩英雄联盟→玩CS:GO→…→看一会儿数学→睡觉”。（仿佛就是我自己嘛！）</li></ol><h2 id="马尔科夫奖励过程"><a href="#马尔科夫奖励过程" class="headerlink" title="马尔科夫奖励过程"></a>马尔科夫奖励过程</h2><p>马尔科夫过程（Markov Process）主要描述的是状态之间的转移关系，在各个状态的转移过程中赋予不同的奖励值就得到了马尔科夫奖励过程。</p><p>定义：马尔科夫奖励过程（Markov Reward Process, MRP）由一个四元组组成$(S,P,R,\gamma)$</p><ol><li>$S$代表了状态的集合(也是维度有限的)</li><li>$P$描述了状态转移矩阵$\mathcal{P}_{ss’}=\mathbb{P}[S_{t+1}=s’|S_{t}=s]$</li><li>$R$表示奖励函数，$R(s)$描述了在状态$s$下的期望(立即)奖励，$\mathcal{R}(s)=\mathbb{E}[R_{t+1}|S_{t}=s]$</li><li>$\gamma$表示衰减因子,即discounted factor,$\gamma\in[0,1]$</li></ol><p>$\gamma$是用来计算累计奖励回报的,表示我们有多看中现在或者未来,为什么这么说呢?假设我们现在要计算一个episode始态$S_{0}$的奖励值$V(S_{0})$,不涉及具体公式推导的说,我们应该把$S_{0}$状态后续的奖励全部加和,这样就得到了对始态$S_{0}$的值估计,这些后续奖励的值的权重都是1,或者说此时$\gamma=1$,但是当前状态对很多步之后的状态未必影响很大,我们这样计算过来并不能完全表示一个状态的值,那么我们应当顺势减少距离远的状态的权重,此时$\gamma\lt1$</p><ul><li>当$\gamma=0$时,状态$S$的值完全由其转移的期望立即奖励表示,即<strong>一点都不关心未来</strong></li><li>当$\gamma=1$时,状态$S$的值由以当前状态为始态,运行至终态所得到的所有立即奖励加和的值表示,即<strong>未来与现在同等重要</strong></li><li>当$0 \lt\gamma \lt1$时,状态$S$的值是前两个模式的<em>trade-off</em>,即<strong>对未来看重的程度由$\gamma$决定</strong></li></ul><p>这只是我们的直观感受,其实是为了数学便利（虽然我也不知道具体哪里提高了数学便利，但是在有些情况下会使值函数更快迭代收敛这是真的）。</p><p>将上述马尔科夫过程的例子升级为马尔科夫奖励过程如下图所示:</p><p><img src="./强化学习之MDP马尔科夫决策过程/MRP.png" alt=""></p><p>奖励值定义为:</p><script type="math/tex; mode=display">\begin{bmatrix}玩游戏 & A & -1\\ 语文 & B & -2\\ 数学 & C & -2\\ 英语 & D & -2\\ 挂科 & E & -5\\ \mathcal{Pass} & F & 10\\ 睡觉 & G & 0\end{bmatrix}</script><p>这么定义奖励并没有什么复杂的含义,在这个例子中就拿身心愉悦程度来定义吧,学习固然是枯燥无味的,所以给予负奖励-2,玩游戏虽然会心情放松,但是始终面临着考试的压力,其实并不轻松,所以给予负奖励-1,挂科最痛苦为-5,考试全pass最开心为+10。</p><p>在马尔科夫过程中的状态转移加入相应的奖励值即为马尔科夫奖励过程。</p><h2 id="马尔科夫决策过程"><a href="#马尔科夫决策过程" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h2><p>马尔科夫决策过程(Markov Decision Process, MDP)相比马尔科夫奖励过程多了一个动作$A$,它可以用一个五元组$(S,A,P,R,\gamma)$表示:</p><ol><li>$S$代表了状态的集合(也是维度有限的)</li><li>$A$代表了决策过程中动作的集合(维度有限的)</li><li>$P$描述了状态转移矩阵$\mathcal{P}_{ss’}^{a}=\mathbb{P}[S_{t+1}=s’|S_{t}=s,A_{t}=a]$</li><li>$R$表示奖励函数，$R(s)$描述了在状态$s$下<strong>执行某动作</strong>的期望(立即)奖励，$\mathcal{R}(s,a)=\mathbb{E}[R_{t+1}|S_{t}=s,A_{t}=a]$</li><li>$\gamma$表示衰减因子,即discounted factor,$\gamma\in[0,1]$</li></ol><p>MDPs是一个从交互中达成目标的强化学习问题的一个直接的框架。学习者和决策者叫做Agent。Agent进行交互的其它一切Agent之外的东西都叫做环境。Agent不断的选择动作，而环境也给出相应的反应，并且向Agent表现出新的状态。环境同时也给出一个数值作为反馈。Agent的目标就是通过选择不同的Action来最大化这个反馈值。</p><p><img src="./强化学习之MDP马尔科夫决策过程/agent-env.png" alt=""></p><p>强化学习所研究的内容就是得到一个状态$S$到动作$A$的映射关系,因此策略Policy可以表示成</p><script type="math/tex; mode=display">\pi(a|s)=p(A_{t}=a|S_{t}=s)</script><hr><p>注意:<br>你可能会认为,在马尔科夫奖励过程(MRP)中没有定义动作,但是其实是包含动作的,因为每个状态有多个转移的下一状态,其实就是多个动作嘛！</p><p>很多文章会将有限MDP分开来讲，有限MDP即状态、动作和奖励值都只有有限个元素，对于有限MDP最优策略有唯一解，但是现实世界中任务复杂，因此大多数深度强化学习算法并不局限于解决有限MDP问题，因此本文不将MDP分情况来讲，即默认基于MDP的最优策略<strong>至少有一个解</strong>。</p><hr><p>没错,的确是这样的,MRP中也包含动作,但是我们并不关心,为什么这么说呢?<strong>因为就算每个状态可以执行多个动作,但是其每个动作所能转移到的状态是确定的,不确定的只是动作的选择,而不是动作的转移,而MDP中不确定的却是动作的转移,即执行动作所转移的下一状态是有一定概率的.</strong>什么意思呢?拿之前MRP的例子来说,语文状态有两个状态可以转移,数学和玩游戏,概率分别是0.5,但是当确定一个转移方向的时候(图中的箭头),其转移结果是确定的,获得的奖励也是确定的,但是在MDP中,执行动作导致转移的结果都未必是确定的.<strong><em>需要注意的是,MRP是属于MDP的,MDP执行动作并不一定必须是随机的.</em></strong></p><p>接下来,我们将MRP的例子转换至MDP, 为了方便理解而又不增加示例的复杂性,不妨将”挂科”这个状态看作是一个动作,因为这个节点正巧入度为1,姑且就认为从英语到挂科的这个箭头是英语状态所能执行的动作.如图所示:</p><p><img src="./强化学习之MDP马尔科夫决策过程/MDP.jpg" alt=""></p><p>比较两个图可以发现区别,我把这个不确定的动作标为实心黑圆圈,这位刻苦的同学在学习完英语之后还想继续学习,但是他感觉三门科目都差不多了,于是他也很迷茫,他执行”学习”这个动作时的转移状态有三种:学语文、学数学、学英语.概率分别是:0.2、0.4,、0.4.这下就明白为什么我们要在MDP中加入动作$A$了吧,如果还不明白,请接着看下边的内容.</p><p>顺便说一下,这个时候的转移矩阵已经不是简单的二维了,当然也可以用二维来表示,假设总共有$n$个状态,每个状态有$m$个动作,那么其行数为$n\times m$,即遍历所有的状态和动作,得到$n \times m$个状态-动作对$(s,a)$,其列数还是$n$.当然,也可以用一个三维tensor来表示,行和列都是$n$,第三维深度为$m$,很好理解.</p><hr><p><strong>网上有写MDP在给定策略下会退化为MRP,我对此不置可否,认为此种说法不够严谨,因为即使说在某状态s下选择的动作a是确定的,并不意味着其转移结果是确定的.</strong></p><hr><h3 id="回报-Return"><a href="#回报-Return" class="headerlink" title="回报 Return"></a>回报 Return</h3><p>在强化学习问题中，总是提到回报二字，论文中出现Return或者Discounted Return，我们已经知道奖励是什么，奖励就是转移到某个状态或者执行了某个动作之后转移至某个状态所获得的值$r$.</p><p>回报就是由某时刻$t$之后决策序列所获得的奖励值经过一定规则计算出来的数值.</p><p>公式描述:</p><script type="math/tex; mode=display">G_{t}\doteq R_{t+1}+R_{t+2}+R_{t+3}+...+R_{t}</script><p>.其中,$T$表示一个episode达到终态的时间点.</p><p>像之前介绍的一样,我们可能对未来有不同的看重程度,于是引入折扣因子$\gamma$的回报表示为:</p><script type="math/tex; mode=display">G_{t}\doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+...=\begin{cases}\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}\\\sum_{k=t+1}^{T}\gamma^{k-t-1}R_{k}\end{cases}</script><p>其中,$ 0\leq\gamma \leq1$</p><p>可以推出回报有如下形式:</p><script type="math/tex; mode=display">\begin{align*}G_{t} &\doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\gamma^{3}R_{t+4}+...\\&=R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+\gamma^{2}R_{t+4}+...)\\&=R_{t+1}+\gamma (R_{t+2}+\gamma (R_{t+3}+\gamma R_{t+4}+...))\\&=R_{t+1}+\gamma G_{t+1}\end{align*}</script><h3 id="策略-Policy"><a href="#策略-Policy" class="headerlink" title="策略 Policy"></a>策略 Policy</h3><p>我们一般使用$\pi$来表示一个策略,使用$\pi(a|s)$来表示某状态$s$采取动作$a$的概率,公示表示为:</p><script type="math/tex; mode=display">\pi(a|s)=P(A_{t}=a|S_{t}=s)</script><p>策略完整定义了智能体在所有状态下的所有行为和其概率.</p><p>给定一个MDP和一个策略$\pi$,采样的状态序列</p><script type="math/tex; mode=display">S_{0},S_{1},S_{2},...,S_{n},...</script><p>是一个马尔科夫过程$\lt S,P \gt ^{\pi}$,</p><p>采样的状态、奖励序列</p><script type="math/tex; mode=display">(S_{0},R_{0}),(S_{1},R_{1}),(S_{2},R_{2}),...,(S_{n},R_{n}),...</script><p>是一个马尔科夫奖励过程$ \lt S,P,R,\gamma  \gt^{\pi}$,</p><p>采样的状态、动作、奖励序列</p><script type="math/tex; mode=display">(S_{0},A_{0},R_{0}),(S_{1},A_{1},R_{1}),(S_{2},A_{2},R_{2}),...,(S_{n},A_{n},R_{n}),...</script><p>是一个马尔科夫决策过程$ \lt S,A^{\pi},P,R,\gamma  \gt^{\pi}$.</p><p><em>注意:在编程时一般以四元组$(s,a,r,s’)$为单位存储”经验”</em></p><p>$\pi$策略下$s\rightarrow s’$转移概率由期望计算得$P_{ss’}^{\pi}=\sum_{a\in A}\pi(a|s)P_{ss’}^{a}$,$s$状态下的期望立即奖励为$R_{s}^{\pi}=\sum_{a\in A}\pi(a|s)R_{s}^{a}$.</p><p>上述例子中</p><script type="math/tex; mode=display">\begin{align*}R_{英语}&=\sum_{a\in A}\pi(a|英语)R_{英语}^{a}\\&=0.2\times-2+0.4\times-2+0.4\times-2\\&=-2\end{align*}</script><p>状态转移概率可以描述为：在执行策略$\pi$时，状态从$s$转移至$s’$的概率等于执行该状态下所有行为的概率与对应行为能使状态从$s$转移至$s’$的概率的乘积的和。</p><p>奖励函数可以描述为：在执行策略$\pi$时获得的奖励等于执行该状态下所有行为的概率与对应行为产生的即时奖励的乘积的和。</p><p><strong>强化学习的目标就是最大化期望回报,相应的结果就是找到从状态空间$S$映射到动作空间$A$的最优策略</strong>,重点是,如何建立回报与策略之间的联系呢?</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;强化学习之MDP马尔科夫决策过程&quot;&gt;&lt;a href=&quot;#强化学习之MDP马尔科夫决策过程&quot; class=&quot;headerlink&quot; title=&quot;强化学习之MDP马尔科夫决策过程&quot;&gt;&lt;/a&gt;强化学习之MDP马尔科夫决策过程&lt;/h1&gt;&lt;p&gt;每每提到强化学习，最先接触的理论肯定是马尔科夫决策过程（MDP，Markov Decision Process），为什么总提到MDP呢？并不是只有我一个人有这个疑问。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>强化学习的里程碑</title>
    <link href="http://StepNeverStop.github.io/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%87%8C%E7%A8%8B%E7%A2%91.html"/>
    <id>http://StepNeverStop.github.io/强化学习的里程碑.html</id>
    <published>2019-05-07T00:26:39.000Z</published>
    <updated>2019-05-10T01:25:19.932Z</updated>
    
    <content type="html"><![CDATA[<h1 id="强化学习的里程碑"><a href="#强化学习的里程碑" class="headerlink" title="强化学习的里程碑"></a>强化学习的里程碑</h1><a id="more"></a><h2 id="Alpha-Go"><a href="#Alpha-Go" class="headerlink" title="Alpha Go"></a>Alpha Go</h2><blockquote><p>阿尔法围棋（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能机器人，由谷歌（Google）旗下<strong>DeepMind</strong>公司戴密斯·哈萨比斯领衔的团队开发。其主要工作原理是“深度学习”。</p><blockquote><p>2016年3月，阿尔法围棋与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜；</p><p><img src="./强化学习的里程碑/LeeSedolBattleWithAlphaGo.jpeg" alt=""></p><p>2016年末2017年初，该程序在中国棋类网站上以“大师”（Master）为注册账号与中日韩数十位围棋高手进行快棋对决，连续60局无一败绩；</p><p>2017年5月，在中国乌镇围棋峰会上，它与排名世界第一的世界围棋冠军柯洁对战，以3比0的总比分获胜。围棋界公认阿尔法围棋的棋力已经超过人类职业围棋顶尖水平，在GoRatings网站公布的世界职业围棋排名中，其等级分曾超过排名人类第一的棋手柯洁。</p><p><img src="./强化学习的里程碑/KeJieBattleWithAlphaGo.jpeg" alt=""></p><p>2017年5月27日，在柯洁与阿尔法围棋的人机大战之后，阿尔法围棋团队宣布阿尔法围棋将不再参加围棋比赛。</p><p>2017年10月18日，DeepMind团队公布了最强版阿尔法围棋，代号AlphaGo Zero。</p></blockquote></blockquote><p>2016年3月机器学习一个重要的时间就是：名为AlphaGo的计算机程序打败了围棋世界冠军李世石，比分4：1。按理来说我们对机器在某项比赛、某些运动中击败人类顶尖选手不会感到大惊小怪，最著名的就是97年IBM的“深蓝（Deep Blue）”计算机程序打败了世界象棋冠军Garry Kasparov。</p><p><img src="./强化学习的里程碑/GKBattleWithDeepBlue.jpeg" alt=""></p><p>机器同样是使用强大的算力以数倍、数十倍、数百倍的训练时间去击败人类（通常人类训练十年的时间，机器可以模拟训练几百年），为什么Alpha Go的取胜这么重要、这么引人关注（世界各地媒体疯狂报道，一股狂潮如炒作一般）呢？</p><p>原因有两个：</p><ol><li>AlphaGo解决的围棋问题比之前的都要复杂，西洋双陆棋只有$10^{20}$种不同的“棋位”空间配置，深蓝打败人类的国际象棋有$10^{43}$种不同的“棋位”空间配置，而围棋却有$10^{170}$种不同的“棋位”空间配置，这种量级的数字人类已经无法处理（意思是对于这么多种不同的状态，就是目前算力最强的计算机也无能为力）。举个例子，$10^{170}$这个数字比宇宙中存在的原子数还多。为什么AlphaGo可以在围棋上击败人类就如此重要呢？因为机器如果可以解决这个大的状态空间的问题，那么在机器学习也应该能解决很复杂的现实世界中的问题。这意味着机器真正融入我们的劳动力市场，为我们的日常生活提供便利的日子已经不远啦（真的吗？）！</li><li>AlphaGo解决的围棋问题不可能通过纯粹的、暴力计算的方式来学习出很好的模型，这就需要为AlphaGo设计一个更加“智能、聪明”的算法。AlphaGo引起热潮的另一个原因就是，其训练算法是一个通用算法，而不是一个专门为解决某项任务特别设计的算法，这与97年IBM的深蓝计算机程序完全不同，因为深蓝只能用于学习下国际象棋，在中国象棋中就不适于训练。此前，AlphaGo的前身已经能够在Atari 49个不同规则、不同游戏模式中使用相同的通用训练算法训练出比人类还厉害的模型，AlphaGo的成功意味着不仅在虚拟环境可以使用这一套学习方法训练模型，而且可以在不同的现实世界问题中使用这一套学习方法、代码结构。</li></ol><p><strong>有能力解决状态空间非常大的问题</strong>和<strong>通用学习算法</strong>是使AlphaGo警报一时的两个主要原因，这也解释了为什么这场比赛在媒体上引起了轰动。有些人认为李世石的失败是机器占据人类劳动力市场的先兆，也有些人认为这预示着人工智能迎来了黄金时代，实际上我们距离真正的人工智能还有很长的路要走，就算机器可以在某项非常复杂的任务中超过人类的表现能力，其也没有真正的思维方式，不会进行思考，说到底也只是曲线的拟合罢了，但是，只有基础做好了，才能向上研究人工智能。</p><p>构建AlphaGo和其前身（应用于Atari游戏）的学习算法的设计思路、计算架构在一系列论文和视频中都可以获得，而没有被Google（收购了英国公司DeepMind）私藏。为什么他不私藏呢？这么厉害的代码、设计思路没必要公开出来嘛，因为Google想把自己打造为基于云的机器学习和大数据的领导者，而它在2016年是全球第三大云服务提供商，排在微软和亚马逊之后，它需要把客户从其他平台引流到自己的平台上。由此可见，大公司们之间的竞争反而可以使我们平民获益。</p><blockquote><p><a href="https://randomant.net/the-algorithm-behind-the-curtain/" rel="external nofollow" target="_blank">The Algorithm Behind the Curtain: How DeepMind Built a Machine that Beat a Go Master (1 of 5)</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;强化学习的里程碑&quot;&gt;&lt;a href=&quot;#强化学习的里程碑&quot; class=&quot;headerlink&quot; title=&quot;强化学习的里程碑&quot;&gt;&lt;/a&gt;强化学习的里程碑&lt;/h1&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>conda环境和pip包的转移</title>
    <link href="http://StepNeverStop.github.io/conda%E7%8E%AF%E5%A2%83%E5%92%8Cpip%E5%8C%85%E7%9A%84%E8%BD%AC%E7%A7%BB.html"/>
    <id>http://StepNeverStop.github.io/conda环境和pip包的转移.html</id>
    <published>2019-04-14T08:52:41.000Z</published>
    <updated>2019-05-10T01:23:53.564Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Conda-环境导出导入和其中pip安装包的迁移"><a href="#Conda-环境导出导入和其中pip安装包的迁移" class="headerlink" title="Conda 环境导出导入和其中pip安装包的迁移"></a>Conda 环境导出导入和其中pip安装包的迁移</h1><a id="more"></a><h2 id="Conda-环境"><a href="#Conda-环境" class="headerlink" title="Conda 环境"></a>Conda 环境</h2><ol><li>激活环境<br><code>conda activate [env_name]</code></li><li>导出环境<br><code>conda env export &gt; [env_filename].yaml</code><br>当前环境将被保存在定义的<code>.yaml</code>文件中</li><li>导入环境<br><code>conda env create -f [env_filename].yaml</code><br>移植过来的conda环境只安装了原环境中使用<code>conda install</code>等命令安装的包, 使用<code>pip</code>命令安装的包需要重新安装</li></ol><h2 id="pip包"><a href="#pip包" class="headerlink" title="pip包"></a>pip包</h2><ol><li>导出<br><code>pip freeze &gt; requirements.txt</code></li><li>导入<br><code>pip install -r requirements.txt</code></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Conda-环境导出导入和其中pip安装包的迁移&quot;&gt;&lt;a href=&quot;#Conda-环境导出导入和其中pip安装包的迁移&quot; class=&quot;headerlink&quot; title=&quot;Conda 环境导出导入和其中pip安装包的迁移&quot;&gt;&lt;/a&gt;Conda 环境导出导入和其中pip安装包的迁移&lt;/h1&gt;
    
    </summary>
    
      <category term="Conda" scheme="http://StepNeverStop.github.io/categories/Conda/"/>
    
    
      <category term="conda" scheme="http://StepNeverStop.github.io/tags/conda/"/>
    
  </entry>
  
  <entry>
    <title>强化学习基本概念</title>
    <link href="http://StepNeverStop.github.io/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.html"/>
    <id>http://StepNeverStop.github.io/强化学习基本概念.html</id>
    <published>2019-04-08T11:23:16.000Z</published>
    <updated>2019-05-10T01:25:17.271Z</updated>
    
    <content type="html"><![CDATA[<h1 id="强化学习基本概念"><a href="#强化学习基本概念" class="headerlink" title="强化学习基本概念"></a>强化学习基本概念</h1><p>学习了这么久的强化学习, 不做笔记总是会忘记, 于是写在博客里方便自己复习, 也与同路人分享.</p><h2 id="强化学习是什么"><a href="#强化学习是什么" class="headerlink" title="强化学习是什么?"></a>强化学习是什么?</h2><p>强化学习是什么? 它的英文名字是<em>Reinforcement Learning</em>, 和<em>Machine Learning</em>一样, 都是以<em>‘ing’</em>结尾的. 它是一个问题、一组解决这个问题的方案以及探求这些解决方案的方法. 对于问题和方法一定要有清晰的认识, 很多人在学习强化学习时遇到的各种困惑与不解都是因为不能清晰的认识问题和方法的区别和联系.</p><a id="more"></a><p>强化学习与有监督学习(<em>supervised learning</em>)不同. 有监督学习是目前机器学习领域研究最多的方向, 它从由经验丰富的、学识渊博的专家(监督者)提供一系列带有标签(如每个样本被正确分类的类别)的样本数据中进行学习, 这种方法通常被用于分类问题. 有监督学习的目标是当给定一个没有在训练样本集出现的数据时, 可以准确推断出它的标签/类别. 这种有监督学习非常重要而且有用, 但是它没有能力从<strong>交互</strong>中进行学习, 而强化学习在智能体与环境进行交互的过程中进行学习. 为什么有监督学习不能从交互中学习呢? 因为有监督学习需要的近乎完全的样本以及其准确的信息都是在交互问题中很难获得的(不现实的). 在未知的交互场景中, 我们往往只能根据智能体的经验进行学习.</p><p>强化学习与无监督学习(<em>unsupervised learning</em>)也是不同的. 无监督学习通常被用于发现无标签样本集的隐藏结构. 我们一般任务机器学习只分为有、无监督学习两种, 而且将强化学习分为无监督学习一类. 但其实强化学习与无监督学习有本质的区别. <strong>强化学习的目的是最大化可获得的奖励值</strong>, 而无监督学习是发现隐藏结构. 当然, 如果在强化学习问题中可以发现其样本内的隐藏结构, 这对于强化学习肯定是很有帮助的, 但是仅仅这些隐藏结构并不能处理强化学习最大化奖励值方法的问题. 因此, 我们通常将强化学习归为机器学习的第三个类别, 与有、无监督学习并列.</p><p><em>注: 在强化学习问题中, 任何可以反映当前动作所带来的影响的元素都可以被理解为奖励值.(个人见解)</em></p><blockquote><p>Reinforcement learning is learning what to do——how to map situations to actions——so as to maxmize a numerical reward signal.    ——《Reinforcement Learning: An Introduction》</p></blockquote><p>强化学习学习的是从状态<em>s</em>到要执行的最优动作<em>a</em>之间的映射关系, 也就是找到一个策略(函数/逻辑规则)使得在给定状态下通过该策略所产生的决策可以最终带来最大的回报. 学习者不被告知应该采取什么动作, 而是通过训练使它们发现采取什么样的动作可以产生最高的奖励值. 这与婴儿学习的方式很像, 你可能会说:”瞎讲, 婴儿可以模仿你的动作进行学习.”. 但你要知道, 当你对婴儿的动作进行批评(吵)和奖励(笑)时, 这就已经是一个强化学习的过程了.</p><h2 id="强化学习的两个要素"><a href="#强化学习的两个要素" class="headerlink" title="强化学习的两个要素"></a>强化学习的两个要素</h2><p>强化学习必不可少的两个要素是智能体<code>Agent</code>和环境<code>Environment</code>.<br>既然强化学习是在交互过程中进行学习, 那么交互必定是双方或者多方的, 在强化学习问题中, 交互的双方是智能体和环境.</p><ol><li>智能体</li></ol><ul><li>智能体是环境的观察者</li><li>智能体是策略的载体</li><li>智能体是动作的执行者</li></ul><ol><li>环境</li></ol><ul><li>环境是对智能体动作的评判者, 即给出立即奖励</li><li>环境是智能体进行运动等行为的基本空间</li><li>环境给出当前时刻的观察信息, 供智能体进行采集</li></ul><h2 id="强化学习的两个特点"><a href="#强化学习的两个特点" class="headerlink" title="强化学习的两个特点"></a>强化学习的两个特点</h2><ol><li><strong>trial-and-error/试错学习</strong><br>智能体在与环境交互的过程中进行学习时, 不会得到任何人为的或者示例的指导(如果进行指导, 则为有监督学习/模仿学习/逆强化学习等), 智能体只能通过在环境中不断地<strong>试错</strong>, 积累经验, 最终学到可以完成目标并获得最大奖励值的策略.</li><li><strong>delayed reward/延迟奖励</strong><br>在大多数强化学习问题中, 某一状态<em>s</em>下执行的动作<em>a</em>不仅会影响当前的立即奖励<em>r</em>, 而且还会影响后续的状态序列, 以及后续的奖励值. 当前的立即奖励值并不能反映出在这个动作对(<em>s,a</em>)对整个决策过程的影响, 只有等到这一决策过程结束时, 才能判断其在这个状态序列的奖励(价值), 所以, 延迟奖励也是强化学习过程中的一个特点.</li></ol><h2 id="强化学习的难点-Challenge"><a href="#强化学习的难点-Challenge" class="headerlink" title="强化学习的难点(Challenge)"></a>强化学习的难点(Challenge)</h2><p>相比其他学习, 强化学习中的一大难点是<strong>探索与利用</strong>, 也就是<strong>exploration and exploitation</strong>, 这个难题已经被数学家研究了几十年了, 但仍然没有解决. 为了获得尽量多的奖励, 智能体需要根据过去学习的经验选择产生立即奖励值最高的动作, 但是给定状态下可供选择的动作有很多, 有些被执行过, 有些没有被执行过, 为了去发现产生立即奖励值最高的动作, 必须尝试选择之前被选择过动作. 这个问题就出现了, <strong>智能体必须利用它已经探索过的产生大奖励值的动作, 也必须探索未知奖励值的动作(有可能很小)为了以后可以选择更好的动作</strong>. 只探索不利用、只利用不探索在强化学习问题中都是独木难支. 在随机任务中, 一个同样的动作往往需要被探索很多次才可能对它的期望奖励值有较准确的估计. </p><h2 id="强化学习的四个元素"><a href="#强化学习的四个元素" class="headerlink" title="强化学习的四个元素"></a>强化学习的四个元素</h2><p>除了智能体与环境两个要素之外, 强化学习系统/框架中还有四个子元素: 策略、奖励机制、值函数、模型(未必有).</p><ol><li><p>策略 Policy<br>策略定义了智能体在当前时刻应该做出的行为. 与人类的刺激-反应机制很像, 策略是从感知到的环境信息到执行的行为之间的映射, 策略是强化学习智能体的<strong>核心</strong>, 它决定了智能体的行为. 在一般的强化学习问题中, 策略可能是随机的、非确定的, 它通常给出可选择执行的动作的概率或概率分布.</p></li><li><p>奖励机制 Reward Signal<br>奖励机制定义了强化学习问题的<strong>目标</strong>, 在交互的每一步, 环境都会向智能体传递一个数字信息, 我们称之为”奖励”. 智能体的唯一目标就是在整个交互过程中最大化总的奖励之和. 因此, 奖励定义了某个动作的好坏(但并不意味着坏的动作在交互过程中是坏的, 其作用由值函数来定义). 类比于我们人类, 奖励就像我们高兴或者痛苦一样, 它们是我们对当前环境-动作的立即反应和评价.<br>奖励机制是智能体更新策略Policy的基础, 如果智能体成功进行了学习, 当在当前策略选择了一个较低回报的动作时, 之后它可能会选择其他动作.<br>通常, 奖励机制由状态<em>s</em>和动作<em>a</em>的随机函数表示<code>R(s,a)</code></p></li><li><p>值函数 Value Function<br>立即奖励表示着当前动作或状态带来的立即效果是好是坏, 但是值函数表示这个动作在整个交互过程中扮演的角色是好是坏. 一个状态的值是从该状态开始到交互结束所积累的立即奖励的总和.<br>一个状态可能总是产生很低的立即奖励, 但是它有很高的值, 因为该状态之后的后续状态中会产生很大的立即奖励. 相反也是一样. 类比于我们人类, 立即奖励的高低相当于我们高兴或痛苦, 但是值函数给出的值则表示了在整个事件过程中我们有多高兴或不高兴的深刻判断.<br>引入值函数的唯一目的就是为了训练智能体以获得更大的奖励, 当智能体做决策以及评估决策时, 我们一直关心的都是值函数而不是立即奖励, 对于动作的选择也是基于对值函数的判断/评估. 值比立即奖励更难以确定, 因为立即奖励可以由环境准确的给出, 但是值却需要评估甚至多次评估才可能相对准确(因为有可能交互过程永远不结束, 那么对值的估计会有偏差). 我们希望选择的动作带来最高的值, 而不是最高的立即奖励, 实际上, 几乎所有强化学习算法中最重要的部分就是对于值函数的有效估计方法. 关于值函数估计所扮演的核心角色在近60年被广泛研究. </p></li><li><p>模型 Model<br>模型是对环境行为的仿真, 我们可以通过模型推断出动作对环境的改变, 给出准确的立即奖励和状态信息. 例如, 给定一个状态和动作, 模型可以预测出下一个要转移的状态以及下一个立即奖励值. 如果模型是确定的, 我们一般使用规划(<em>planning</em>)的方法来选择最优动作, 对于这种方式我们称之为基于模型<strong>model-based</strong>的方法, 相反, 如果模型是不确定的, 也就是<strong>model-free</strong>, 我们只能通过试错的方式进行学习并选择动作. </p></li></ol><p><em>注: 对于什么是model-based和model-free将在以后进行深入讨论.</em></p><h2 id="强化学习的通用符号表示-Notation"><a href="#强化学习的通用符号表示-Notation" class="headerlink" title="强化学习的通用符号表示 Notation"></a>强化学习的通用符号表示 Notation</h2><p>$←$    赋值</p><p>$\varepsilon$ 在$\varepsilon-greedy$策略中随机选择动作的概率</p><p>$\gamma$ 计算总奖励的折扣因子</p><p>$\lambda$ 资格迹的衰减率或者GAE的权重因子</p><p>$s,s’$ 状态，下一个状态</p><p>$a$ 一个动作</p><p>$r$ 一个奖励值（标量）</p><p>$S$ 状态集（不包含终态）</p><p>$S^{+}$ 状态集（包含终态）</p><p>$A(s)$ $s$状态下可选择的动作</p><p>$R$ 奖励集合</p><p>$|S|$ 状态集中的元素数</p><p>$t$ 单个时间步</p><p>$T$ 一个episode的终态时间点</p><p>$A_{t}$ $t$时刻选择的动作</p><p>$S_{t}$ $t$时刻所在的状态</p><p>$R_{t}$ $t$时刻获得的奖励</p><p>$\pi$ 策略（从状态到动作的映射）</p><p>$\pi(s)$ 在$s$状态下使用$\pi$策略所选择的动作</p><p>$\pi(a|s)$ 在$s$状态下使用$\pi$策略选择到动作$a$的概率</p><p>$G_{t}$ 以$t$时刻为起始时间点，到终态所能获得的总奖励（回报）</p><p>$p(s’,r|s,a)$ 在$s$状态执行a动作转移到$s‘$状态并获得奖励值为$r$的概率</p><p>$p(s’|s,a)$ 在$s$状态执行$a$动作转移到$s’$状态的概率</p><p>$r(s,a)$ 在$s$状态执行$a$动作所获得的<strong>期望</strong>立即奖励（即时奖励）</p><p>$r(s,a,s’)$ 在$s$状态执行$a$动作转移到$s’$状态所获得的<strong>期望</strong>立即奖励</p><p>$v_{\pi}(s)$ $\pi$策略下状态$s$的值（以该状态为始态的期望奖励回报）</p><p>$v_{<em>}(s)$ <em>*最优</em></em>策略下状态s的值</p><p>$q_{\pi}(s,a)$ $\pi$策略下状态-行动对$(s,a)$的值</p><p>$q_{<em>}(s,a)$ <em>*最优</em></em>策略下状态-行动对$(s,a)$的值</p><p>$V,V_{t}$ 状态值的矩阵估计，行和列分别是时间点$t$和每个状态的估计值$v_{\pi}$或$v_{*}$</p><p>$Q,Q_{t}$ 状态-行动对$(s,a)$的矩阵估计，一般为一个3维矩阵,行、列和深度分别为状态、动作、时间点</p><p>$V_{t}(s)$ 状态$s$的期望估计值</p><p>$\delta_{t}$ $t$时刻的TD-error时间差分量</p><p>$\theta,\theta_{t}$ 目标策略的参数（向量）</p><p>$\pi(a|s,\theta)$ 目标策略的参数为$\theta$时，在$s$状态选择到$a$动作的概率</p><p>$\pi_{\theta}$ 表示参数为$\theta$的策略</p><p>$\nabla{\pi(a|s,\theta)}$ $\pi(a|s,\theta)$对于参数$\theta$的偏微分</p><p>$J(\theta)$ 参数为$\theta$的策略的性能度量、期望奖励(performance measure)</p><p>$\nabla{J(\theta)}$ 性能度量对于策略参数$\theta$的偏导数</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;强化学习基本概念&quot;&gt;&lt;a href=&quot;#强化学习基本概念&quot; class=&quot;headerlink&quot; title=&quot;强化学习基本概念&quot;&gt;&lt;/a&gt;强化学习基本概念&lt;/h1&gt;&lt;p&gt;学习了这么久的强化学习, 不做笔记总是会忘记, 于是写在博客里方便自己复习, 也与同路人分享.&lt;/p&gt;
&lt;h2 id=&quot;强化学习是什么&quot;&gt;&lt;a href=&quot;#强化学习是什么&quot; class=&quot;headerlink&quot; title=&quot;强化学习是什么?&quot;&gt;&lt;/a&gt;强化学习是什么?&lt;/h2&gt;&lt;p&gt;强化学习是什么? 它的英文名字是&lt;em&gt;Reinforcement Learning&lt;/em&gt;, 和&lt;em&gt;Machine Learning&lt;/em&gt;一样, 都是以&lt;em&gt;‘ing’&lt;/em&gt;结尾的. 它是一个问题、一组解决这个问题的方案以及探求这些解决方案的方法. 对于问题和方法一定要有清晰的认识, 很多人在学习强化学习时遇到的各种困惑与不解都是因为不能清晰的认识问题和方法的区别和联系.&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达deeplearning.ai课程笔记总结</title>
    <link href="http://StepNeverStop.github.io/AnderewNg-deeplearning-note-summary.html"/>
    <id>http://StepNeverStop.github.io/AnderewNg-deeplearning-note-summary.html</id>
    <published>2019-03-25T10:30:30.000Z</published>
    <updated>2019-05-10T06:09:12.663Z</updated>
    
    <content type="html"><![CDATA[<h1 id="吴恩达deeplearning-ai课程笔记总结"><a href="#吴恩达deeplearning-ai课程笔记总结" class="headerlink" title="吴恩达deeplearning.ai课程笔记总结"></a>吴恩达deeplearning.ai课程笔记总结</h1><p>在吴恩达机器学习系列课程完结后不久，一位名叫<a href="https://www.slideshare.net/TessFerrandez?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideview" title="Tess Ferrandez小姐姐的主页" rel="external nofollow" target="_blank">Tess Ferrandez</a>的小姐姐在推特上分享了一套自己的课程笔记，瞬间收获了3k+赞和1k+转发。</p><p>不同于满屏公式代码的黑白笔记，这套信息图不仅知识点满满，且行文构图都像插画一样颜值颇高。吴恩达自己也在推特上转发称赞了这一位有诚意的学习者，毕竟他一直倡导学习是一件简单快乐的事情。</p><p>Link: <a href="https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng" title="笔记源地址" rel="external nofollow" target="_blank">笔记源地址</a></p><a id="more"></a><h2 id="深度学习介绍"><a href="#深度学习介绍" class="headerlink" title="深度学习介绍"></a>深度学习介绍</h2><p><img src="./AnderewNg-deeplearning-note-summary/1.png" alt=""></p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p><img src="./AnderewNg-deeplearning-note-summary/2.png" alt=""></p><h2 id="浅层神经网络"><a href="#浅层神经网络" class="headerlink" title="浅层神经网络"></a>浅层神经网络</h2><p><img src="./AnderewNg-deeplearning-note-summary/3.png" alt=""></p><h2 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h2><p><img src="./AnderewNg-deeplearning-note-summary/4.png" alt=""></p><h2 id="机器学习应用程序设置"><a href="#机器学习应用程序设置" class="headerlink" title="机器学习应用程序设置"></a>机器学习应用程序设置</h2><p><img src="./AnderewNg-deeplearning-note-summary/5.png" alt=""></p><h2 id="正则化——防止过拟合"><a href="#正则化——防止过拟合" class="headerlink" title="正则化——防止过拟合"></a>正则化——防止过拟合</h2><p><img src="./AnderewNg-deeplearning-note-summary/6.png" alt=""></p><h2 id="优化训练"><a href="#优化训练" class="headerlink" title="优化训练"></a>优化训练</h2><p><img src="./AnderewNg-deeplearning-note-summary/7.png" alt=""></p><h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h2><p><img src="./AnderewNg-deeplearning-note-summary/8.png" alt=""></p><h2 id="超参数调试"><a href="#超参数调试" class="headerlink" title="超参数调试"></a>超参数调试</h2><p><img src="./AnderewNg-deeplearning-note-summary/9.png" alt=""></p><h2 id="机器学习项目构建"><a href="#机器学习项目构建" class="headerlink" title="机器学习项目构建"></a>机器学习项目构建</h2><p><img src="./AnderewNg-deeplearning-note-summary/10.png" alt=""></p><h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><p><img src="./AnderewNg-deeplearning-note-summary/11.png" alt=""></p><h2 id="训练-vs-验证-测试-失配"><a href="#训练-vs-验证-测试-失配" class="headerlink" title="训练 vs 验证/测试 失配"></a>训练 vs 验证/测试 失配</h2><p><img src="./AnderewNg-deeplearning-note-summary/12.png" alt=""></p><h2 id="扩展学习"><a href="#扩展学习" class="headerlink" title="扩展学习"></a>扩展学习</h2><p><img src="./AnderewNg-deeplearning-note-summary/13.png" alt=""></p><h2 id="卷积基础"><a href="#卷积基础" class="headerlink" title="卷积基础"></a>卷积基础</h2><p><img src="./AnderewNg-deeplearning-note-summary/14.png" alt=""></p><h2 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h2><p><img src="./AnderewNg-deeplearning-note-summary/15.png" alt=""></p><h2 id="深层-CNN"><a href="#深层-CNN" class="headerlink" title="深层 CNN"></a>深层 CNN</h2><p><img src="./AnderewNg-deeplearning-note-summary/16.png" alt=""></p><h2 id="典型的-CNN-模型"><a href="#典型的-CNN-模型" class="headerlink" title="典型的 CNN 模型"></a>典型的 CNN 模型</h2><p><img src="./AnderewNg-deeplearning-note-summary/17.png" alt=""></p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p><img src="./AnderewNg-deeplearning-note-summary/18.png" alt=""></p><h2 id="实用建议"><a href="#实用建议" class="headerlink" title="实用建议"></a>实用建议</h2><p><img src="./AnderewNg-deeplearning-note-summary/19.png" alt=""></p><h2 id="检测算法"><a href="#检测算法" class="headerlink" title="检测算法"></a>检测算法</h2><p><img src="./AnderewNg-deeplearning-note-summary/20.png" alt=""></p><h2 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h2><p><img src="./AnderewNg-deeplearning-note-summary/21.png" alt=""></p><h2 id="神经风格迁移"><a href="#神经风格迁移" class="headerlink" title="神经风格迁移"></a>神经风格迁移</h2><p><img src="./AnderewNg-deeplearning-note-summary/22.png" alt=""></p><h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p><img src="./AnderewNg-deeplearning-note-summary/23.png" alt=""></p><h2 id="更多-RNN-模型"><a href="#更多-RNN-模型" class="headerlink" title="更多 RNN 模型"></a>更多 RNN 模型</h2><p><img src="./AnderewNg-deeplearning-note-summary/24.png" alt=""></p><h2 id="NLP-词嵌入"><a href="#NLP-词嵌入" class="headerlink" title="NLP-词嵌入"></a>NLP-词嵌入</h2><p><img src="./AnderewNg-deeplearning-note-summary/25.png" alt=""></p><h2 id="词嵌入详解"><a href="#词嵌入详解" class="headerlink" title="词嵌入详解"></a>词嵌入详解</h2><p><img src="./AnderewNg-deeplearning-note-summary/26.png" alt=""></p><h2 id="序列到序列基本模型"><a href="#序列到序列基本模型" class="headerlink" title="序列到序列基本模型"></a>序列到序列基本模型</h2><p><img src="./AnderewNg-deeplearning-note-summary/27.png" alt=""></p><h2 id="序列到序列"><a href="#序列到序列" class="headerlink" title="序列到序列"></a>序列到序列</h2><p><img src="./AnderewNg-deeplearning-note-summary/28.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;吴恩达deeplearning-ai课程笔记总结&quot;&gt;&lt;a href=&quot;#吴恩达deeplearning-ai课程笔记总结&quot; class=&quot;headerlink&quot; title=&quot;吴恩达deeplearning.ai课程笔记总结&quot;&gt;&lt;/a&gt;吴恩达deeplearning.ai课程笔记总结&lt;/h1&gt;&lt;p&gt;在吴恩达机器学习系列课程完结后不久，一位名叫&lt;a href=&quot;https://www.slideshare.net/TessFerrandez?utm_campaign=profiletracking&amp;amp;utm_medium=sssite&amp;amp;utm_source=ssslideview&quot; title=&quot;Tess Ferrandez小姐姐的主页&quot; rel=&quot;external nofollow&quot; target=&quot;_blank&quot;&gt;Tess Ferrandez&lt;/a&gt;的小姐姐在推特上分享了一套自己的课程笔记，瞬间收获了3k+赞和1k+转发。&lt;/p&gt;
&lt;p&gt;不同于满屏公式代码的黑白笔记，这套信息图不仅知识点满满，且行文构图都像插画一样颜值颇高。吴恩达自己也在推特上转发称赞了这一位有诚意的学习者，毕竟他一直倡导学习是一件简单快乐的事情。&lt;/p&gt;
&lt;p&gt;Link: &lt;a href=&quot;https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng&quot; title=&quot;笔记源地址&quot; rel=&quot;external nofollow&quot; target=&quot;_blank&quot;&gt;笔记源地址&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://StepNeverStop.github.io/categories/DeepLearning/"/>
    
    
      <category term="note" scheme="http://StepNeverStop.github.io/tags/note/"/>
    
      <category term="deeplearning" scheme="http://StepNeverStop.github.io/tags/deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>一些在Docker中比较难以安装的库(整理)</title>
    <link href="http://StepNeverStop.github.io/something-hard-install-docker.html"/>
    <id>http://StepNeverStop.github.io/something-hard-install-docker.html</id>
    <published>2019-03-24T10:57:50.000Z</published>
    <updated>2019-05-10T01:24:42.029Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一些在Docker中比较难以安装的库-整理"><a href="#一些在Docker中比较难以安装的库-整理" class="headerlink" title="一些在Docker中比较难以安装的库(整理)"></a>一些在Docker中比较难以安装的库(整理)</h1><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在配置镜像时, <strong>强烈建议将源更改为国内镜像站</strong>, 因为国外有些镜像站链接速度很慢, 更新也很慢, 很多库无法正确安装</p><p>我所使用的镜像站为<code>sources.list</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class="line">deb http://archive.canonical.com/ubuntu xenial partner</span><br><span class="line">deb-src http://archive.canonical.com/ubuntu xenial partner</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</span><br></pre></td></tr></table></figure><p>在<code>Dockerfile</code>或者在容器内使用命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">COPY sources.list /etc/apt/sources.list</span><br></pre></td></tr></table></figure></p><p>将源替换.</p><h2 id="CUDA-9-0-开发者版"><a href="#CUDA-9-0-开发者版" class="headerlink" title="CUDA 9.0 开发者版"></a>CUDA 9.0 开发者版</h2><p><code>dockerfile</code>如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">FROM nvidia/cuda:9.0-runtime-ubuntu16.04</span><br><span class="line">LABEL maintainer &quot;Keavnn &lt;https://stepneverstop.github.io&gt;&quot;</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y --allow-unauthenticated --no-install-recommends \</span><br><span class="line">        cuda-libraries-dev-$CUDA_PKG_VERSION \</span><br><span class="line">        cuda-nvml-dev-$CUDA_PKG_VERSION \</span><br><span class="line">        cuda-minimal-build-$CUDA_PKG_VERSION \</span><br><span class="line">        cuda-command-line-tools-$CUDA_PKG_VERSION \</span><br><span class="line">        cuda-core-9-0=9.0.176.3-1 \</span><br><span class="line">        cuda-cublas-dev-9-0=9.0.176.4-1 \</span><br><span class="line">        libnccl-dev=$NCCL_VERSION-1+cuda9.0 &amp;&amp; \</span><br><span class="line">    rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line">ENV LIBRARY_PATH /usr/local/cuda/lib64/stubs</span><br></pre></td></tr></table></figure></p><p><strong>—allow-unauthenticated</strong> 这句命令很重要, 不使用的话很有可能安装失败</p><h2 id="cudnn-7-0-5"><a href="#cudnn-7-0-5" class="headerlink" title="cudnn 7.0.5"></a>cudnn 7.0.5</h2><ul><li><a href="https://developer.nvidia.com/rdp/cudnn-archive" rel="external nofollow" target="_blank">https://developer.nvidia.com/rdp/cudnn-archive</a> 下载cuDNN Libraries for Linux,不要下载 Power 8</li><li>把下载好的包上传到FTP服务器, 或者传输到容器内, 或者直接在容器中下载好</li><li><code>cd</code>到包位置</li><li><code>cp cudnn-9.0-linux-x64-v7.solitairetheme8 cudnn-9.0-linux-x64-v7.tgz</code></li><li><code>tar -xvf cudnn-9.0-linux-x64-v7.tgz</code></li><li><code>cp include/* /usr/local/cuda-9.0/include</code></li><li><code>cp lib64/* /usr/local/cuda-9.0/lib64</code></li><li><code>chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn*</code> <strong>这一步如果cuda是base版本,则没有include文件夹,需要手动创建后再执行</strong></li><li><code>export PATH=/usr/local/cuda-9.0/bin:$PATH</code></li><li><code>cd</code>到<code>/usr/local/cuda-9.0/lib64</code></li><li><code>nano ~/.bashrc</code>,关联环境变量</li><li>在最后一行加入<code>export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH</code></li><li><code>source ~/.bashrc</code></li><li><code>ldconfig -v</code></li><li>使用<code>cat /usr/local/cuda-9.0/include/cudnn.h | grep CUDNN_MAJOR -A 2</code> 查看cudnn版本<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png" alt=""></li></ul><h2 id="jemalloc"><a href="#jemalloc" class="headerlink" title="jemalloc"></a>jemalloc</h2><p>选择安装<code>jemalloc</code>,这个工具可以加速编译,碎片整理,具体请自行谷歌</p><ul><li><code>apt-get install autoconf</code></li><li><code>apt-get install automake</code></li><li><code>apt-get install libtool</code></li><li><code>git clone https://github.com/jemalloc/jemalloc.git</code></li><li><code>cd jemalloc</code></li><li><code>git checkout 4.5.0</code>安装4.5.0版本的jemalloc,5.x版本的有坑,深坑</li><li><code>./autogen.sh</code></li><li><code>make</code></li><li><code>make install_bin install_include install_lib</code>,之所以不使用<code>make install</code>是因为会报错,如下: <img src="./create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png" alt=""></li></ul><h2 id="Python3-6"><a href="#Python3-6" class="headerlink" title="Python3.6"></a>Python3.6</h2><p>记得<code>sudo</code></p><ul><li><code>apt-get install software-properties-common</code></li><li><code>add-apt-repository ppa:jonathonf/python-3.6</code>, 按<code>ENTER</code></li><li><code>apt-get update &amp;&amp; apt-get install python3.6 -y</code></li><li>修改系统默认的<code>python</code>版本为3.6</li><li><code>cd /usr/bin</code>, 保险起见,建议分两步</li><li><code>rm python</code></li><li><code>ln -s python3.6m python</code></li><li>如需更新,<code>pip3 install --upgrade pip</code>, 8.1.1-&gt;19.0.3</li><li><code>python -V</code><br><img src="./something-hard-install-docker/1.png" alt=""></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一些在Docker中比较难以安装的库-整理&quot;&gt;&lt;a href=&quot;#一些在Docker中比较难以安装的库-整理&quot; class=&quot;headerlink&quot; title=&quot;一些在Docker中比较难以安装的库(整理)&quot;&gt;&lt;/a&gt;一些在Docker中比较难以安装的库(整理)&lt;/h1&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://StepNeverStop.github.io/categories/Docker/"/>
    
    
      <category term="docker" scheme="http://StepNeverStop.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>在Jupyter Notebook中使用本机的conda环境</title>
    <link href="http://StepNeverStop.github.io/use-conda-env-in-jupyter.html"/>
    <id>http://StepNeverStop.github.io/use-conda-env-in-jupyter.html</id>
    <published>2019-03-22T05:16:02.000Z</published>
    <updated>2019-05-10T01:24:54.492Z</updated>
    
    <content type="html"><![CDATA[<h1 id="在Jupyter-Notebook中使用本机的conda环境"><a href="#在Jupyter-Notebook中使用本机的conda环境" class="headerlink" title="在Jupyter Notebook中使用本机的conda环境"></a>在Jupyter Notebook中使用本机的conda环境</h1><a id="more"></a><h2 id="Jupyter下conda多环境管理"><a href="#Jupyter下conda多环境管理" class="headerlink" title="Jupyter下conda多环境管理"></a>Jupyter下conda多环境管理</h2><h3 id="1-手撸命令"><a href="#1-手撸命令" class="headerlink" title="1. 手撸命令"></a>1. 手撸命令</h3><ol><li>在<code>base</code>环境下安装内核管理工具<br><code>pip install ipykernel</code></li><li>将环境内核添加到<code>jupyter kernel</code>中<br><code>python -m ipykernel install --user --name [env_name] --display-name &quot;[show name in jupyter]&quot;</code></li><li>查看已在<code>jupyter</code>中创建的虚拟环境内核<br><code>jupyter kernelspec list</code></li><li>删除内核<br><code>jupyter kernelspec uninstall [env_name]</code></li></ol><h3 id="2-使用插件"><a href="#2-使用插件" class="headerlink" title="2. 使用插件"></a>2. 使用插件</h3><p>简单粗暴, 在<code>base</code>环境下使用命令<br><code>conda install nb_conda</code><br><img src="./use-conda-env-in-jupyter/1.png" alt=""><br><img src="./use-conda-env-in-jupyter/2.png" alt=""><br>接下来, 看看jupyter中能不能显示<code>conda</code>环境<br><code>jupyter notebook</code><br><img src="./use-conda-env-in-jupyter/3.png" alt=""><br>在文件内部也可以很方便的切换环境<br><img src="./use-conda-env-in-jupyter/4.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;在Jupyter-Notebook中使用本机的conda环境&quot;&gt;&lt;a href=&quot;#在Jupyter-Notebook中使用本机的conda环境&quot; class=&quot;headerlink&quot; title=&quot;在Jupyter Notebook中使用本机的conda环境&quot;&gt;&lt;/a&gt;在Jupyter Notebook中使用本机的conda环境&lt;/h1&gt;
    
    </summary>
    
      <category term="Conda" scheme="http://StepNeverStop.github.io/categories/Conda/"/>
    
    
      <category term="conda" scheme="http://StepNeverStop.github.io/tags/conda/"/>
    
      <category term="jupyter notebook" scheme="http://StepNeverStop.github.io/tags/jupyter-notebook/"/>
    
  </entry>
  
  <entry>
    <title>为远程Ubuntu服务器安装图像界面</title>
    <link href="http://StepNeverStop.github.io/%E4%B8%BA%E8%BF%9C%E7%A8%8BUbuntu%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85%E5%9B%BE%E5%83%8F%E7%95%8C%E9%9D%A2.html"/>
    <id>http://StepNeverStop.github.io/为远程Ubuntu服务器安装图像界面.html</id>
    <published>2019-01-09T06:25:43.000Z</published>
    <updated>2019-05-10T01:24:58.142Z</updated>
    
    <content type="html"><![CDATA[<h1 id="为远程服务器Ubuntu系统安装图形界面"><a href="#为远程服务器Ubuntu系统安装图形界面" class="headerlink" title="为远程服务器Ubuntu系统安装图形界面"></a>为远程服务器Ubuntu系统安装图形界面</h1><a id="more"></a><h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><ul><li><a href="https://wiki.x2go.org/doku.php/doc:installation:x2goclient" rel="external nofollow" target="_blank">X2GO</a></li></ul><h2 id="在服务器上安装X2go服务器"><a href="#在服务器上安装X2go服务器" class="headerlink" title="在服务器上安装X2go服务器"></a>在服务器上安装X2go服务器</h2><ol><li><p>安装这个<code>add-apt-repository</code>命令</p><ul><li><code>apt-get install -y python-software-properties software-properties-common</code></li></ul></li><li><p>添加PPA</p><ul><li><code>apt-add-repository -y ppa:x2go/stable</code></li></ul></li><li><p>更新包列表并安装<code>x2go</code>服务器端</p><ul><li><code>apt-get update</code></li><li><code>apt-get install x2goserver x2goserver-xsession</code></li></ul></li></ol><h3 id="安装XFCE图像界面"><a href="#安装XFCE图像界面" class="headerlink" title="安装XFCE图像界面"></a>安装XFCE图像界面</h3><p>在安装<code>XFCE</code>桌面环境时，有可能会出错，原因是perl为系统使用<code>zh_CN.UTF-8</code>，但系统不知道<code>zh_CN.UTF-8</code>是什么东西，所以需要安装一个中文语言，系统就知道<code>zh_CN.UTF-8</code>了，这个时候perl就不会报错了<br>​    - <code>apt-get install language-pack-zh-hans</code><br>​    - <code>apt-get install xfce4</code></p><h3 id="安装GNOME图像界面"><a href="#安装GNOME图像界面" class="headerlink" title="安装GNOME图像界面"></a>安装GNOME图像界面</h3><p><code>apt-get install -y gnome</code><br><em>没有测试成功,似乎是不兼容的问题</em></p><h3 id="安装MATE图像界面"><a href="#安装MATE图像界面" class="headerlink" title="安装MATE图像界面"></a>安装MATE图像界面</h3><p><code>apt-get install -y mate</code><br><img src="./为远程Ubuntu服务器安装图像界面/9.png" alt=""></p><h3 id="安装LXDE图像界面"><a href="#安装LXDE图像界面" class="headerlink" title="安装LXDE图像界面"></a>安装LXDE图像界面</h3><p><code>apt-get install -y xorg lxde</code><br><img src="./为远程Ubuntu服务器安装图像界面/10.png" alt=""></p><h3 id="重要配置"><a href="#重要配置" class="headerlink" title="重要配置"></a><strong>重要配置</strong></h3><p>开启远程连接时有可能会出现<code>mesg: ttyname failed: Inappropriate ioctl for device</code>错误，所以需要修改一下文件<br>​    - <code>nano /root/.profile</code><br>​    - 把<code>mesg n</code> 替换成 <code>tty -s &amp;&amp; mesg n</code></p><h2 id="在客户端上安装X2go客户端"><a href="#在客户端上安装X2go客户端" class="headerlink" title="在客户端上安装X2go客户端"></a>在客户端上安装X2go客户端</h2><h3 id="MAC"><a href="#MAC" class="headerlink" title="MAC"></a>MAC</h3><ol><li>安装<code>Xquartz</code> <a href="https://www.xquartz.org/" rel="external nofollow" target="_blank">XQuartz</a></li></ol><p><img src="./为远程Ubuntu服务器安装图像界面/1.png" alt=""></p><ol><li>输入命令</li></ol><p><code>echo &quot;*VT100.translations: #override Meta &lt;KeyPress&gt; V: insert-selection(PRIMARY, CUT_BUFFER0) \n&quot; &gt; ~/.Xdefaults</code></p><ol><li><p>安装<code>X2Go Client</code> <a href="https://code.x2go.org/releases/binary-macosx/x2goclient/" rel="external nofollow" target="_blank">X2Go Client</a></p></li><li><p>打开客户端，设置连接</p></li></ol><p><img src="./为远程Ubuntu服务器安装图像界面/2.png" alt=""></p><ol><li>检查客户端设置，确保X11被正确引导</li></ol><p><img src="./为远程Ubuntu服务器安装图像界面/3.png" alt=""></p><ol><li>开始连接</li></ol><p><img src="./为远程Ubuntu服务器安装图像界面/4.png" alt=""></p><ol><li>连接成功</li></ol><p><img src="./为远程Ubuntu服务器安装图像界面/5.png" alt=""></p><h3 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h3><ol><li>安装<code>x2goclient</code> <a href="https://code.x2go.org/releases/binary-win32/x2goclient/releases/4.1.2.0-2018.06.22/" rel="external nofollow" target="_blank">x2goclient</a></li></ol><p><img src="./为远程Ubuntu服务器安装图像界面/6.png" alt=""></p><ol><li>配置好之后连接成功</li></ol><p><img src="./为远程Ubuntu服务器安装图像界面/7.png" alt=""></p><h2 id="Linux下各种图像界面测评"><a href="#Linux下各种图像界面测评" class="headerlink" title="Linux下各种图像界面测评"></a>Linux下各种图像界面测评</h2><p><img src="./为远程Ubuntu服务器安装图像界面/8.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;为远程服务器Ubuntu系统安装图形界面&quot;&gt;&lt;a href=&quot;#为远程服务器Ubuntu系统安装图形界面&quot; class=&quot;headerlink&quot; title=&quot;为远程服务器Ubuntu系统安装图形界面&quot;&gt;&lt;/a&gt;为远程服务器Ubuntu系统安装图形界面&lt;/h1&gt;
    
    </summary>
    
      <category term="Ubuntu" scheme="http://StepNeverStop.github.io/categories/Ubuntu/"/>
    
    
      <category term="ubuntu" scheme="http://StepNeverStop.github.io/tags/ubuntu/"/>
    
      <category term="x2go" scheme="http://StepNeverStop.github.io/tags/x2go/"/>
    
  </entry>
  
  <entry>
    <title>创建ML-Agents的Docker镜像</title>
    <link href="http://StepNeverStop.github.io/%E5%88%9B%E5%BB%BAML-Agents%E7%9A%84Docker%E9%95%9C%E5%83%8F.html"/>
    <id>http://StepNeverStop.github.io/创建ML-Agents的Docker镜像.html</id>
    <published>2019-01-04T02:38:59.000Z</published>
    <updated>2019-05-10T01:25:04.774Z</updated>
    
    <content type="html"><![CDATA[<h1 id="创建ML-Agents的Docker镜像"><a href="#创建ML-Agents的Docker镜像" class="headerlink" title="创建ML-Agents的Docker镜像"></a>创建ML-Agents的Docker镜像</h1><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>  如果需要在镜像中使用GPU训练,可以将Nvidia的官方镜像作为基础镜像,<code>Dockerfile</code>如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">FROM nvidia/cuda:9.0-base-ubuntu16.04</span><br><span class="line">LABEL maintainer &quot;NVIDIA CORPORATION &lt;cudatools@nvidia.com&gt;&quot;</span><br><span class="line"></span><br><span class="line">ENV NCCL_VERSION 2.3.7</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">apt-utils \</span><br><span class="line">        cuda-libraries-$CUDA_PKG_VERSION \</span><br><span class="line">        cuda-cublas-9-0=9.0.176.4-1 \</span><br><span class="line">        libnccl2=$NCCL_VERSION-1+cuda9.0 &amp;&amp; \</span><br><span class="line">    apt-mark hold libnccl2 &amp;&amp; \</span><br><span class="line">    rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y openssh-server</span><br><span class="line"></span><br><span class="line">RUN apt-get install -y nano</span><br><span class="line"></span><br><span class="line">RUN mkdir /var/run/sshd</span><br><span class="line"></span><br><span class="line">RUN echo &quot;root:1234&quot; | chpasswd</span><br><span class="line"></span><br><span class="line">RUN sed -i &apos;s/prohibit-password/yes/g&apos; /etc/ssh/sshd_config</span><br><span class="line"></span><br><span class="line">EXPOSE 22</span><br><span class="line"></span><br><span class="line">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure></p><h2 id="ML-Agents-v0-6-0"><a href="#ML-Agents-v0-6-0" class="headerlink" title="ML-Agents v0.6.0"></a>ML-Agents v0.6.0</h2><p><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-44-58.png" alt=""></p><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>本机环境</p><ul><li>ML-Agents 0.6.0</li><li>Windows 10 专业版</li><li>docker client version 18.09.0</li><li>docker server version 18.09.0</li></ul><p>平台</p><ul><li><a href="http://10.0.4.228" rel="external nofollow" target="_blank">机器学习平台</a></li></ul><h3 id="创建镜像"><a href="#创建镜像" class="headerlink" title="创建镜像"></a>创建镜像</h3><ol><li>打开<code>~/ml-agents-0.6.0/</code>目录,看到有一个官方给定的<code>Dockerfile</code><br><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_10-52-49.png" alt=""></li><li>直接<code>Build</code>,在该目录下运行<code>docker build -t [name]:[tag] .</code>,一定要注意最后的<code>.</code>,很<strong>重要</strong><br><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-29-30.png" alt=""></li><li>新建一个<code>sources.list</code>文件,为镜像内换源,因为将来有可能需要在容器内安装某些包,有一些国外的资源往往会下载失败,所以需要<strong>换源</strong></li></ol><ul><li>新建一个<code>sources.list</code></li><li><p>用文本编辑器打开,写入以下内容<br><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_11-36-31.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class="line">deb http://archive.canonical.com/ubuntu xenial partner</span><br><span class="line">deb-src http://archive.canonical.com/ubuntu xenial partner</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</span><br></pre></td></tr></table></figure></li><li><p>保存退出</p></li></ul><ol><li>新建一个<code>DockerfilePlus</code>,在官方生成的基础镜像上安装一些可以在平台上运行的包,<code>openssh-server</code>,联网工具<code>net-tools</code>,心爱的<code>apt-file</code>等等</li></ol><ul><li>新建一个<code>DockerfilePlus</code></li><li>用文本编辑器打开,输入以下内容<br><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-01-32.png" alt=""><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">FROM hub.hoc.ccshu.net/wjs/mlunityv060:v0.1</span><br><span class="line"></span><br><span class="line">RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">COPY sources.list /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">ENV PYTHONPATH /ml-agents:$PYTHONPATH</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y \</span><br><span class="line">        apt-file \</span><br><span class="line">        nano \</span><br><span class="line">        net-tools \</span><br><span class="line">        iputils-ping \</span><br><span class="line">        openssh-server \</span><br><span class="line">        apt-utils \</span><br><span class="line">    &amp;&amp; rm -rf /var/lib/apt/lists/* \</span><br><span class="line">    &amp;&amp; mkdir /var/run/sshd \</span><br><span class="line">    &amp;&amp; echo &quot;root:1234&quot; | chpasswd \</span><br><span class="line">    &amp;&amp; sed -i &apos;s/prohibit-password/yes/g&apos; /etc/ssh/sshd_config</span><br><span class="line"></span><br><span class="line">EXPOSE 22</span><br><span class="line"></span><br><span class="line">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure></li></ul><ol><li>在<code>DockerfilePlus</code>所在文件夹下,执行<code>build -t [name]:[tag] -f DockerfilePlus .</code></li></ol><p><strong>为了使用GPU.改写完的Dockerfile如下(不需要看):</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line">FROM nvidia/cuda:9.0-base-ubuntu16.04</span><br><span class="line">LABEL maintainer &quot;Keavnn &lt;https://stepneverstop.github.io&gt;&quot;</span><br><span class="line"></span><br><span class="line">ENV NCCL_VERSION 2.3.7</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">apt-utils \</span><br><span class="line">        cuda-libraries-$CUDA_PKG_VERSION \</span><br><span class="line">        cuda-cublas-9-0=9.0.176.4-1 \</span><br><span class="line">        libnccl2=$NCCL_VERSION-1+cuda9.0 &amp;&amp; \</span><br><span class="line">    apt-mark hold libnccl2 &amp;&amp; \</span><br><span class="line">    rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ensure local python is preferred over distribution python</span><br><span class="line">ENV PATH /usr/local/bin:$PATH</span><br><span class="line"></span><br><span class="line"># http://bugs.python.org/issue19846</span><br><span class="line"># &gt; At the moment, setting &quot;LANG=C&quot; on a Linux system *fundamentally breaks Python 3*, and that&apos;s not OK.</span><br><span class="line">ENV LANG C.UTF-8</span><br><span class="line"></span><br><span class="line"># runtime dependencies</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">ca-certificates \</span><br><span class="line">libexpat1 \</span><br><span class="line">libffi6 \</span><br><span class="line">libgdbm3 \</span><br><span class="line">libreadline6 \</span><br><span class="line">libsqlite3-0 \</span><br><span class="line">libssl1.0.0 \</span><br><span class="line">&amp;&amp; rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line">ENV GPG_KEY 0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D</span><br><span class="line">ENV PYTHON_VERSION 3.6.4</span><br><span class="line"></span><br><span class="line">RUN set -ex \</span><br><span class="line">&amp;&amp; buildDeps=&quot; \</span><br><span class="line">dpkg-dev \</span><br><span class="line">gcc \</span><br><span class="line">libbz2-dev \</span><br><span class="line">libc6-dev \</span><br><span class="line">libexpat1-dev \</span><br><span class="line">libffi-dev \</span><br><span class="line">libgdbm-dev \</span><br><span class="line">liblzma-dev \</span><br><span class="line">libncursesw5-dev \</span><br><span class="line">libreadline-dev \</span><br><span class="line">libsqlite3-dev \</span><br><span class="line">libssl-dev \</span><br><span class="line">make \</span><br><span class="line">tcl-dev \</span><br><span class="line">tk-dev \</span><br><span class="line">wget \</span><br><span class="line">xz-utils \</span><br><span class="line">zlib1g-dev \</span><br><span class="line"># as of Stretch, &quot;gpg&quot; is no longer included by default</span><br><span class="line">$(command -v gpg &gt; /dev/null || echo &apos;gnupg dirmngr&apos;) \</span><br><span class="line">&quot; \</span><br><span class="line">&amp;&amp; apt-get update &amp;&amp; apt-get install -y $buildDeps --no-install-recommends &amp;&amp; rm -rf /var/lib/apt/lists/* \</span><br><span class="line">\</span><br><span class="line">&amp;&amp; wget -O python.tar.xz &quot;https://www.python.org/ftp/python/$&#123;PYTHON_VERSION%%[a-z]*&#125;/Python-$PYTHON_VERSION.tar.xz&quot; \</span><br><span class="line">&amp;&amp; wget -O python.tar.xz.asc &quot;https://www.python.org/ftp/python/$&#123;PYTHON_VERSION%%[a-z]*&#125;/Python-$PYTHON_VERSION.tar.xz.asc&quot; \</span><br><span class="line">&amp;&amp; export GNUPGHOME=&quot;$(mktemp -d)&quot; \</span><br><span class="line">&amp;&amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-keys &quot;$GPG_KEY&quot; \</span><br><span class="line">&amp;&amp; gpg --batch --verify python.tar.xz.asc python.tar.xz \</span><br><span class="line">&amp;&amp; rm -rf &quot;$GNUPGHOME&quot; python.tar.xz.asc \</span><br><span class="line">&amp;&amp; mkdir -p /usr/src/python \</span><br><span class="line">&amp;&amp; tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz \</span><br><span class="line">&amp;&amp; rm python.tar.xz \</span><br><span class="line">\</span><br><span class="line">&amp;&amp; cd /usr/src/python \</span><br><span class="line">&amp;&amp; gnuArch=&quot;$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)&quot; \</span><br><span class="line">&amp;&amp; ./configure \</span><br><span class="line">--build=&quot;$gnuArch&quot; \</span><br><span class="line">--enable-loadable-sqlite-extensions \</span><br><span class="line">--enable-shared \</span><br><span class="line">--with-system-expat \</span><br><span class="line">--with-system-ffi \</span><br><span class="line">--without-ensurepip \</span><br><span class="line">&amp;&amp; make -j &quot;$(nproc)&quot; \</span><br><span class="line">&amp;&amp; make install \</span><br><span class="line">&amp;&amp; ldconfig \</span><br><span class="line">\</span><br><span class="line">&amp;&amp; apt-get purge -y --auto-remove $buildDeps \</span><br><span class="line">\</span><br><span class="line">&amp;&amp; find /usr/local -depth \</span><br><span class="line">\( \</span><br><span class="line">\( -type d -a \( -name test -o -name tests \) \) \</span><br><span class="line">-o \</span><br><span class="line">\( -type f -a \( -name &apos;*.pyc&apos; -o -name &apos;*.pyo&apos; \) \) \</span><br><span class="line">\) -exec rm -rf &apos;&#123;&#125;&apos; + \</span><br><span class="line">&amp;&amp; rm -rf /usr/src/python</span><br><span class="line"></span><br><span class="line"># make some useful symlinks that are expected to exist</span><br><span class="line">RUN cd /usr/local/bin \</span><br><span class="line">&amp;&amp; ln -s idle3 idle \</span><br><span class="line">&amp;&amp; ln -s pydoc3 pydoc \</span><br><span class="line">&amp;&amp; ln -s python3 python \</span><br><span class="line">&amp;&amp; ln -s python3-config python-config</span><br><span class="line"></span><br><span class="line"># if this is called &quot;PIP_VERSION&quot;, pip explodes with &quot;ValueError: invalid truth value &apos;&lt;VERSION&gt;&apos;&quot;</span><br><span class="line">ENV PYTHON_PIP_VERSION 9.0.3</span><br><span class="line"></span><br><span class="line">RUN set -ex; \</span><br><span class="line">\</span><br><span class="line">apt-get update; \</span><br><span class="line">apt-get install -y --no-install-recommends wget; \</span><br><span class="line">rm -rf /var/lib/apt/lists/*; \</span><br><span class="line">\</span><br><span class="line">wget -O get-pip.py &apos;https://bootstrap.pypa.io/get-pip.py&apos;; \</span><br><span class="line">\</span><br><span class="line">apt-get purge -y --auto-remove wget; \</span><br><span class="line">\</span><br><span class="line">python get-pip.py \</span><br><span class="line">--disable-pip-version-check \</span><br><span class="line">--no-cache-dir \</span><br><span class="line">&quot;pip==$PYTHON_PIP_VERSION&quot; \</span><br><span class="line">; \</span><br><span class="line">pip --version; \</span><br><span class="line">\</span><br><span class="line">find /usr/local -depth \</span><br><span class="line">\( \</span><br><span class="line">\( -type d -a \( -name test -o -name tests \) \) \</span><br><span class="line">-o \</span><br><span class="line">\( -type f -a \( -name &apos;*.pyc&apos; -o -name &apos;*.pyo&apos; \) \) \</span><br><span class="line">\) -exec rm -rf &apos;&#123;&#125;&apos; +; \</span><br><span class="line">rm -f get-pip.py</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get -y upgrade</span><br><span class="line"></span><br><span class="line"># xvfb is used to do CPU based rendering of Unity</span><br><span class="line">RUN apt-get install -y xvfb</span><br><span class="line"></span><br><span class="line">COPY ml-agents /ml-agents</span><br><span class="line">WORKDIR /ml-agents</span><br><span class="line">RUN pip install .</span><br><span class="line"></span><br><span class="line"># port 5005 is the port used in in Editor training.</span><br><span class="line">EXPOSE 5005</span><br><span class="line"></span><br><span class="line">RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">COPY sources.list /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">ENV PYTHONPATH /ml-agents:$PYTHONPATH</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y \</span><br><span class="line">        apt-file \</span><br><span class="line">        nano \</span><br><span class="line">        net-tools \</span><br><span class="line">        iputils-ping \</span><br><span class="line">        openssh-server \</span><br><span class="line">        apt-utils \</span><br><span class="line">    &amp;&amp; rm -rf /var/lib/apt/lists/* \</span><br><span class="line">    &amp;&amp; mkdir /var/run/sshd \</span><br><span class="line">    &amp;&amp; echo &quot;root:1234&quot; | chpasswd \</span><br><span class="line">    &amp;&amp; sed -i &apos;s/prohibit-password/yes/g&apos; /etc/ssh/sshd_config</span><br><span class="line"></span><br><span class="line">EXPOSE 22</span><br><span class="line"></span><br><span class="line">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure></p><h3 id="PUSH镜像"><a href="#PUSH镜像" class="headerlink" title="PUSH镜像"></a>PUSH镜像</h3><p><code>docker push [name]:[tag]</code><br><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_12-02-47.png" alt=""></p><h3 id="测试镜像"><a href="#测试镜像" class="headerlink" title="测试镜像"></a>测试镜像</h3><ul><li>登录<a href="http://10.0.4.228" rel="external nofollow" target="_blank">机器学习平台</a>,没有使用平台的可以在本地使用<code>docker run</code>直接开启容器</li><li>先测试使用容器的方式<ul><li>创建容器<br><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-28-19.png" alt=""><br>将要运行和储存的文件夹放在数据卷<code>data</code>下,这个目录要在运行时由<code>--docker-target-name</code>指定</li><li>等待容器创建成功<br><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-15.png" alt=""></li><li>容器创建成功后进入容器<br><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_13-31-52.png" alt=""></li><li>执行<code>mlagents-learn trainer_config.yaml --docker-target-name=data/unity-volume --env=3dball --train --run-id=test --save-freq=5000 | tee /data/unity-volume/log.txt</code>,如果不想在屏幕输出,可以在后边加上<code>&gt;/dev/null</code><br><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-16-44.png" alt=""><br><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-04_15-18-40.png" alt=""></li></ul></li></ul><hr><h2 id="安装Miniconda"><a href="#安装Miniconda" class="headerlink" title="安装Miniconda"></a>安装Miniconda</h2><p>确保你的安装包放在了data文件夹下<br><code>apt-get update &amp;&amp; apt-get install bzip2 -y &amp;&amp; cd /data &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh</code></p><p>一路按回车、yes等等就成功了.如果需要安装到指定目录,在安装过程中会有提示告诉你让你指定安装路径</p><p><em>注意</em></p><p>在<a href="http://10.0.4.228/" rel="external nofollow" target="_blank">学校机器学习平台</a>上使用时,如果是使用容器的方式,那么新开的容器就可以使用<code>conda</code>命令,不存在<code>conda:command not found</code>的错误信息.</p><p>但是,如果在平台上以<strong>提交任务</strong>的形式来使用带conda的镜像所产生的容器时,就算是在镜像中配置了<code>echo &#39;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&#39; &gt;&gt; ~/.bashrc</code>,当提交任务时环境变量中仍然没有<code>~/anaconda3/bin</code>,这个问题目前没有找到比较方便的解决办法,目前所采用的方式是:</p><p>在提交任务时, 首先加上命令<code>export PATH=&quot;~/anaconda3/bin:$PATH&quot; &amp;&amp;</code></p><p><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-25-17.png" alt=""></p><p>接着又实验了一下<code>echo &#39;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&#39; &gt;&gt; /etc/profile</code>, 正常来说, 如果在容器中这样设置环境变量, 等待下次从镜像创建容器时, 这个环境变量一般并不会生效, 但是不知道这样设置对于提交任务方式来说有没有效, 索性试了一下</p><p><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-35-56.png" alt=""></p><p>根据输出日志来看, 这种方式也并没有奏效</p><p><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_13-39-26.png" alt=""></p><p>当然, 如果觉得上述配置比较麻烦的话,可以使用<code>Dockerfile</code>的<code>ENV</code>命令来设置环境变量, 这样设置99%是不会有问题的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FROM hub.hoc.ccshu.net/wjs/mlunityv060:v1.4.5</span><br><span class="line">ENV PATH /usr/miniconda3/bin:$PATH</span><br></pre></td></tr></table></figure><p>实验了一下,<br>结果如下:</p><p><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_15-02-16.png" alt=""></p><p>表示可以使用<code>conda</code>命令, 但是不能使用<code>conda activate</code>命令激活环境</p><p>根据错误信息, 在<code>Dockerfile</code>中写入以下代码也不可行:</p><p><code>RUN ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh</code></p><p>查了一下相关资料,发现在4.5版本的conda是无解的<br><a href="https://github.com/ContinuumIO/docker-images/issues/89" rel="external nofollow" target="_blank">https://github.com/ContinuumIO/docker-images/issues/89</a></p><p><img src="./创建ML-Agents的Docker镜像/Snipaste_2019-01-11_22-38-42.png" alt=""></p><p>希望4.6版本可以解决吧</p><p><strong>更新2019年1月14日14:26:01</strong><br><strong>已解决</strong></p><p>4.6版本的确可以解决以提交任务模式运行时的问题, 需要使用命令<code>conda run -n [环境名字] [要执行的命令]</code>, 而不是使用<code>conda activate [环境名字]</code>先激活一个环境.</p><p>不过, 更新至4.6版本需要相关配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels conda-canary</span><br><span class="line">conda update conda</span><br></pre></td></tr></table></figure></p><p>我使用Dockerfile来生成镜像, 代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM hub.hoc.ccshu.net/wjs/mlunityv060:v1.4.6</span><br><span class="line"></span><br><span class="line">ENV PATH /usr/miniconda3/bin:$PATH</span><br><span class="line">RUN conda config --add channels conda-canary &amp;&amp; conda update conda -y</span><br></pre></td></tr></table></figure></p><p>原因是, 使用Dockerfile比较容易设置环境变量, 减少出错</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;创建ML-Agents的Docker镜像&quot;&gt;&lt;a href=&quot;#创建ML-Agents的Docker镜像&quot; class=&quot;headerlink&quot; title=&quot;创建ML-Agents的Docker镜像&quot;&gt;&lt;/a&gt;创建ML-Agents的Docker镜像&lt;/h1&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://StepNeverStop.github.io/categories/Docker/"/>
    
      <category term="Unity" scheme="http://StepNeverStop.github.io/categories/Docker/Unity/"/>
    
    
      <category term="docker" scheme="http://StepNeverStop.github.io/tags/docker/"/>
    
      <category term="unity" scheme="http://StepNeverStop.github.io/tags/unity/"/>
    
      <category term="ml-agents" scheme="http://StepNeverStop.github.io/tags/ml-agents/"/>
    
  </entry>
  
  <entry>
    <title>Docker命令学习</title>
    <link href="http://StepNeverStop.github.io/Docker%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0.html"/>
    <id>http://StepNeverStop.github.io/Docker命令学习.html</id>
    <published>2019-01-03T05:52:04.000Z</published>
    <updated>2019-05-10T01:24:08.302Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker常用以及组合命令学习"><a href="#Docker常用以及组合命令学习" class="headerlink" title="Docker常用以及组合命令学习"></a>Docker常用以及组合命令学习</h1><a id="more"></a><ul><li><p>停止并删除正在运行的容器<br><code>docker rm $(docker stop $(docker ps -aq))</code></p></li><li><p>查看容器的长ID<br><code>docker inspect -f &#39;{?{.ID}}&#39; [name]</code><br><strong>去掉命令中的<code>?</code>,因为双括号会转义失败</strong></p></li><li><p>宿主机向容器内传输文件/文件夹<br><code>docker cp 本地文件路径 ID全称:容器路径</code></p></li><li><p>容器传输文件/文件夹到宿主机<br><code>docker cp ID全称:容器文件路径 本地路径</code></p></li><li><p>修改本地已有镜像的名字<br><code>docker tag [ImageID] [NewImageNmae]:[tag]</code></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Docker常用以及组合命令学习&quot;&gt;&lt;a href=&quot;#Docker常用以及组合命令学习&quot; class=&quot;headerlink&quot; title=&quot;Docker常用以及组合命令学习&quot;&gt;&lt;/a&gt;Docker常用以及组合命令学习&lt;/h1&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://StepNeverStop.github.io/categories/Docker/"/>
    
    
      <category term="docker" scheme="http://StepNeverStop.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Build一个基于Mxnet的Sniper镜像</title>
    <link href="http://StepNeverStop.github.io/create-sniper-docker-image.html"/>
    <id>http://StepNeverStop.github.io/create-sniper-docker-image.html</id>
    <published>2019-01-02T13:58:44.000Z</published>
    <updated>2019-05-10T01:24:01.578Z</updated>
    
    <content type="html"><![CDATA[<h1 id="创建一个基于Mxnet的Sniper-Docker镜像"><a href="#创建一个基于Mxnet的Sniper-Docker镜像" class="headerlink" title="创建一个基于Mxnet的Sniper Docker镜像"></a>创建一个基于Mxnet的Sniper Docker镜像</h1><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>由于此镜像是用于学校机器学习平台,所以文中会出现FTP服务器等字眼,其实是在平台上使用镜像创建一个容器时,平台会<strong>自动</strong>将服务器上我所申请的文件存储区<code>mount</code>到创建的容器,我通过<code>FileZilla</code>FTP工具与在平台申请的文件存储区进行连接<br>​<br>本文教程虽然有了一个FTP过程,但是如果是生成本地镜像,不考虑FTP,无视文中相关部分即可</p><p><strong>虽然本文中写了关于压缩的相关内容,但是最终并没有使用压缩,原因是由于压缩后出现未知问题,导致在平台上创建的容器不能使用宿主机的NVIDIA驱动,并不能成功运行Demo</strong></p><a id="more"></a><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>本机环境</p><ul><li>windows 10 专业版</li><li>docker client version 18.09.0</li><li>docker server version 18.09.0</li><li>FTP工具 FileZilla</li></ul><p>平台环境</p><ul><li>docker version 17.06.2-ce</li></ul><p>镜像环境</p><ul><li>python 2.7.12</li><li>CUDA version 9.0.176</li><li>pip 9.0.3</li></ul><p><a href="https://github.com/mahyarnajibi/SNIPER" rel="external nofollow" target="_blank">SNIPER</a><br><a href="http://10.0.4.228" rel="external nofollow" target="_blank">机器学习平台</a>,这是学校资源</p><h2 id="一-配置基础镜像"><a href="#一-配置基础镜像" class="headerlink" title="一 配置基础镜像"></a>一 配置基础镜像</h2><p>从学校机器学习平台上拉取原始镜像,因为这个镜像配好了一些基本的环境,如python2.x,CUDA9.0等等,所以直接使用它们的镜像作为基础镜像比较省心省力<br><code>docker pull hub.hoc.ccshu.net/ces/deepo:all-py27-jupyter-ssh</code></p><p>拉取到镜像之后,可以选择使用<code>Dockerfile</code>来生成我们需要的镜像,但是往往我们需要在镜像中添加许多库/包/插件,而且使用<code>Dockerfile</code>来生成镜像很容易出BUG.当然,最好的方式是使用<code>Dockerfile</code>,前提是你能确保<code>Dockerfile</code>文件中的每一行命令都不会出错.<br>在当前情况下,我选择使用从容器生成镜像的方法,这种方式会使得最终生成的镜像占内存巨大,但是可以在容器内部调试每一步配置过程.<br>使用<code>docker run -itd --name [name] hub.hoc.ccshu.net/ces/deepo:all-py27-jupyter-ssh</code>开启一个容器</p><p>使用<code>docker ps -a</code>查看正在运行的容器<code>ID</code></p><p>使用<code>docker exec -it [name] /bin/bash</code>进入容器</p><p>在容器中使用<code>cat /etc/issue</code>命令查看容器的操作系统版本</p><p>结果输出: <code>Ubuntu 16.04.4 LTS \n \l</code></p><h3 id="安装-apt-file"><a href="#安装-apt-file" class="headerlink" title="安装 apt-file"></a>安装 apt-file</h3><p>安装<code>apt-file</code></p><p><code>apt-get install apt-file -y</code></p><p>出现错误:</p><p><img src="./create-sniper-docker-image/Snipaste_2019-01-03_08-30-41.png" alt=""></p><p>使用<code>apt-get install apt-file -y --fix-missing</code>同样不能解决问题</p><p>考虑<strong>换源</strong></p><p><code>cp /etc/apt/sources.list /etc/apt/sources.list.bak</code>备份系统原有的源</p><p>安装Linux下的文本编辑器<code>nano</code>,执行命令<code>apt-get install nano -y</code><br>安装<code>nano</code>成功后,执行<code>nano /etc/apt/sources.list</code>修改源文件<br>在打开的文件中,将内容替换为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class="line">deb http://archive.canonical.com/ubuntu xenial partner</span><br><span class="line">deb-src http://archive.canonical.com/ubuntu xenial partner</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</span><br></pre></td></tr></table></figure></p><p>这里使用的源是阿里的镜像站,也可以使用网易163的,源如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">deb http://mirrors.163.com/ubuntu/ xenial main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ xenial main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure></p><p>更改好源文件后,执行<code>sudo apt-get update</code>更新源</p><p>再次执行<code>apt-get install apt-file -y</code>,可以成功安装<code>apt-file</code>包</p><p>之后执行<code>apt-file update</code>更新apt-file cache<br>使用<code>apt-file find [name]</code>可以查找计算机上文件的位置,很方便<br>使用<code>apt-file search [name]</code>可以搜索缺少的库,解决文件缺失依赖<br>选择好自己需要的包,然后使用<code>apt-get install [name]</code>即可</p><ul><li>如果需要把镜像上传到云上使用,有可能需要网络服务,</li><li>执行<code>apt-get install net-tools</code>安装ifconfig</li><li>执行<code>apt-get install iputils-ping</code>安装ping</li></ul><p>此时为了避免诸如使用<code>ping [IP]</code>有效,但是<code>ping [HOST]</code>无效的情况,需要使用<code>nano /etc/resolv.conf</code>修改配置文件<br>将<code>namespace</code>后的IP地址更改为<code>8.8.8.8</code>或者<code>4.4.4.4</code><br><em>或者使用<code>echo &quot;nameserver 114.114.114.114 &gt; /etc/resolv.conf&quot;</code>也可以</em><br>退出保存即可</p><p><em>有可能上述修改DNS的方式并不成功,原因是在云上运行容器时,配置文件自动修改,如果发生这种情况,请每次在新开一个容器时,手动修改配置文件的DNS服务器,使其可以使用网络服务</em></p><h2 id="二-安装编译依赖各种包"><a href="#二-安装编译依赖各种包" class="headerlink" title="二 安装编译依赖各种包"></a>二 安装编译依赖各种包</h2><p>在电脑上空闲的地方,从Github拉取Sniper项目</p><p><code>git clone --recursive https://github.com/mahyarnajibi/SNIPER.git</code></p><ul><li><p>因为我是在学校机器学习平台上运行docker容器,所以选择直接将clone下的文件上传至容器<code>mount</code>的ftp服务器,使用的软件是<code>FileZilla</code></p></li><li><p>上传成功后可以在容器内通过<code>cd /data/[file or folder name]</code>进行访问</p></li></ul><p>如果要在本地镜像内操作的话,也可以直接把本机文件或文件夹拷贝过去<br><code>docker cp 本地文件路径 ID全称:容器路径</code></p><hr><p><code>cd /data/SNIPER/SNIPER-mxnet</code><br><code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br>输出信息:<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_09-42-07.png" alt=""></p><h3 id="安装-jemalloc"><a href="#安装-jemalloc" class="headerlink" title="安装 jemalloc"></a>安装 jemalloc</h3><p>选择安装<code>jemalloc</code>,这个工具可以加速编译,碎片整理,具体请自行谷歌</p><ul><li><code>apt-get install autoconf</code></li><li><code>apt-get install automake</code></li><li><code>apt-get install libtool</code></li><li><code>git clone https://github.com/jemalloc/jemalloc.git</code></li><li><code>cd jemalloc</code></li><li><code>git checkout 4.5.0</code>安装4.5.0版本的jemalloc,5.x版本的有坑,深坑</li><li><code>./autogen.sh</code></li><li><code>make</code></li><li><code>make install_bin install_include install_lib</code>,之所以不使用<code>make install</code>是因为会报错,如下: <img src="./create-sniper-docker-image/Snipaste_2019-01-03_09-56-41.png" alt=""></li></ul><p>切换至<code>SNIPER-mxnet</code>文件夹,再次<code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br>虽然可以编译,但是有以下信息:<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_10-03-30.png" alt=""><br>强迫症必须搞定它,果断<code>ctrl+c</code>终止编译</p><h3 id="安装-pkg-config"><a href="#安装-pkg-config" class="headerlink" title="安装 pkg-config"></a>安装 pkg-config</h3><ul><li>打开<a href="https://pkg-config.freedesktop.org/releases/" rel="external nofollow" target="_blank">https://pkg-config.freedesktop.org/releases/</a></li><li>下载最新的,现在看到的是<code>pkg-config-0.29.2.tar.gz</code></li><li>下载好之后,通过<code>FileZilla</code>等工具传输到FTP服务器</li><li>在容器内<code>cd</code>到压缩包位置</li><li><code>tar -xf pkg-config-0.29.2.tar.gz</code></li><li><code>cd pkg-config-0.29.2</code></li><li><code>./configure --with-internal-glib</code>,注意,中间是一个空格,非常关键</li><li><code>make &amp;&amp; make install</code><br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_10-11-01.png" alt=""></li></ul><p>再次<code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br>算了，还是安装一下cudnn吧</p><h3 id="安装-cudnn7-0"><a href="#安装-cudnn7-0" class="headerlink" title="安装 cudnn7.0"></a>安装 cudnn7.0</h3><ul><li><a href="https://developer.nvidia.com/rdp/cudnn-archive" rel="external nofollow" target="_blank">https://developer.nvidia.com/rdp/cudnn-archive</a> 下载cuDNN Libraries for Linux,不要下载 Power 8</li><li>把下载好的包上传到FTP服务器</li><li><code>cd</code>到包位置</li><li><code>cp cudnn-9.0-linux-x64-v7.solitairetheme8 cudnn-9.0-linux-x64-v7.tgz</code></li><li><code>tar -xvf cudnn-9.0-linux-x64-v7.tgz</code></li><li><code>cp include/* /usr/local/cuda-9.0/include</code></li><li><code>cp lib64/* /usr/local/cuda-9.0/lib64</code></li><li><code>chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn*</code></li><li><code>export PATH=/usr/local/cuda-9.0/bin:$PATH</code></li><li><code>cd</code>到<code>/usr/local/cuda-9.0/lib64</code></li><li><code>nano ~/.bashrc</code>,关联环境变量</li><li>在最后一行加入<code>export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH</code></li><li><code>source ~/.bashrc</code></li><li><code>ldconfig -v</code></li><li>使用<code>cat /usr/local/cuda-9.0/include/cudnn.h | grep CUDNN_MAJOR -A 2</code> 查看cudnn版本<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_10-56-08.png" alt=""></li></ul><h3 id="安装-OpenCV"><a href="#安装-OpenCV" class="headerlink" title="安装 OpenCV"></a>安装 OpenCV</h3><ul><li>使用<code>pkg-config opencv --modversion</code>查看</li><li>发现已经有OpenCV<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_10-57-56.png" alt=""></li></ul><h3 id="安装-OpenBLAS"><a href="#安装-OpenBLAS" class="headerlink" title="安装 OpenBLAS"></a>安装 OpenBLAS</h3><ul><li><code>apt-get install libopenblas-dev</code></li></ul><h3 id="编译-Mxnet"><a href="#编译-Mxnet" class="headerlink" title="编译 Mxnet"></a>编译 Mxnet</h3><p><code>make USE_CUDA_PATH=/usr/local/cuda-9.0</code><br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_13-49-28.png" alt=""></p><p><strong>心好累,总共make了将近两个半小时</strong></p><p>编译<code>c++</code>文件<code>bash scripts/compile.sh</code><br>这一步一定要在<code>/SNIPER/</code>文件夹下,不然贼坑,绝对不要<code>cd</code>到<code>/SNIPER/scripts</code>文件夹下再<code>bash compile.sh</code>,因为代码内有<code>cd lib/nms</code>等,如果不在<code>/SNIPER</code>文件夹下,会找不到文件</p><p>如果出现<code>syntax error near unexpected token</code>$’\r’’<code>错误,可以使用</code>sed<code>命令将</code>\r<code>去掉,或者是在[Github](https://github.com/mahyarnajibi/SNIPER/blob/master/scripts/compile.sh)上将代码复制,使用</code>nano<code>编辑然后粘贴![](./create-sniper-docker-image/Snipaste_2019-01-03_16-32-08.png)可以使用</code>cat -v [filename]<code>查看![]./create-sniper-docker-image/Snipaste_2019-01-03_16-33-27.png)以</code>^M<code>结尾的代表你所处理的文件换行符是dos格式的</code>“\r\n”`</p><p>我选择第二种笨方法,因为涉及的代码并不多</p><p>执行结果:<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_16-24-49.png" alt=""></p><h3 id="安装-dos2unix"><a href="#安装-dos2unix" class="headerlink" title="安装 dos2unix"></a>安装 dos2unix</h3><p>由于发现这种简单的复制粘贴方式并不能很好的解决,所以查了一些<a href="https://blog.csdn.net/lovelovelovelovelo/article/details/79239068" rel="external nofollow" target="_blank">相关资料</a><br>选择使用<code>dos2unix</code>来转换</p><ul><li><code>apt-get install dos2unix</code></li><li><code>dos2unix [filename]</code></li></ul><p><img src="./create-sniper-docker-image/Snipaste_2019-01-03_16-40-53.png" alt=""><br>问题解决啦</p><h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><p>在<code>/SNIPER/</code>文件夹下<code>pip install -r requirements.txt</code><br>一定要确保镜像内可以联网</p><p><img src="./create-sniper-docker-image/Snipaste_2019-01-03_16-29-58.png" alt=""></p><h3 id="测试Demo"><a href="#测试Demo" class="headerlink" title="测试Demo"></a>测试Demo</h3><ul><li><code>bash download_sniper_detector.sh</code>,download_sniper_detector.sh<br>文件在<code>/SNIPER/scripts</code>文件夹下<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_16-44-22.png" alt=""></li><li><code>cd .. &amp;&amp; python demo.py</code><br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_17-05-30.png" alt=""></li></ul><p><strong>运行成功!!!</strong></p><h2 id="三-生成镜像"><a href="#三-生成镜像" class="headerlink" title="三 生成镜像"></a>三 生成镜像</h2><ul><li>使用<code>exit</code>退出容器</li><li>使用<code>docker ps -a</code>查看容器ID</li><li>使用<code>docker stop [ID]</code>停止容器</li><li>使用<code>docker commit -a &quot;作者信息&quot; -m &quot;附带信息&quot; [ID] [name]:[tag]</code>生成镜像,会返回一个<code>sha256</code>开头的长ID,这个就是生成的镜像ID</li><li>使用<code>docker images</code>查看生成的镜像</li><li>如果需要的话,使用<code>docker push [name]:[tag]</code>将刚刚生成的镜像推送到云上</li></ul><h2 id="四-压缩镜像"><a href="#四-压缩镜像" class="headerlink" title="四 压缩镜像"></a>四 压缩镜像</h2><p><strong>压缩镜像非常麻烦,但是也是有方法的,目前大概三种方法</strong></p><ol><li>使用<code>Dockerfile</code>生成镜像</li><li>这种方法需要让容器在运行状态,使用<code>docker export [ID] | docker import - [name]:[tag]</code>导出容器快照,并从快照生成镜像,这种方式可以大大压缩镜像,但是缺点是有可能会使得镜像中的环境变量、开放端口、默认进入命令改变或消失.使用这种方式时,最好在生成镜像之后,创建一个<code>Dockerfile</code>文件,<code>From</code>这个镜像,并添加端口和命令入口</li><li>使用<code>docker-squash</code>压缩镜像,这个方法适用于Linux和Mac系统</li></ol><p>目前可以运行的镜像是13.6G<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_17-12-36.png" alt=""><br><code>hub.hoc.ccshu.net/wjs/sniper:v1.1</code><br>现在要对它进行压缩</p><h3 id="第一步-移除镜像内的SNIPER文件夹-把其放到FTP服务器上去"><a href="#第一步-移除镜像内的SNIPER文件夹-把其放到FTP服务器上去" class="headerlink" title="第一步,移除镜像内的SNIPER文件夹,把其放到FTP服务器上去"></a>第一步,移除镜像内的SNIPER文件夹,把其放到FTP服务器上去</h3><ul><li>开启一个容器<code>docker run -itd --name [name] [id]</code></li><li>复制容器内文件到本地<code>docker cp [长ID]:[容器内路径] [本地路径]</code>,将放置在本地的文件夹上传至FTP服务器</li><li>进入容器<code>docker exec -it [name] /bin/bash</code></li><li>删除容器内文件夹<code>/SNIPER/</code>,使用<code>rm -rf SNIPER</code>,<strong>一定要小心使用</strong></li><li>退出容器<code>exit</code></li></ul><h3 id="第二步-压缩镜像"><a href="#第二步-压缩镜像" class="headerlink" title="第二步,压缩镜像"></a>第二步,压缩镜像</h3><p>压缩容器<br><code>docker export [ID] | docker import - [name]:[tag]</code></p><p>可以看到,镜像体积少了大约2个G<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_17-39-07.png" alt=""></p><p>由于使用这种方法会使得镜像丢失部分信息,所以,创建一个新的<code>Dockerfile</code>,在其中添加缺失的信息</p><h3 id="第三步-完善镜像"><a href="#第三步-完善镜像" class="headerlink" title="第三步,完善镜像"></a>第三步,完善镜像</h3><p>在任意位置新建<code>Dockerfile</code><br>输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM [name]:[tag]</span><br><span class="line">EXPOSE 22</span><br><span class="line">ENTRYPOINT [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure></p><p><img src="./create-sniper-docker-image/Snipaste_2019-01-03_18-08-49.png" alt=""></p><p>然后<code>docker build -t [name]:[tag] .</code>,不要忘了最后的<code>.</code></p><h3 id="第四步-Push"><a href="#第四步-Push" class="headerlink" title="第四步 Push"></a>第四步 Push</h3><p><code>docker push [name]:[tag]</code></p><p>至此,所有配置以及完成<br>镜像在<code>hoc.hoc.ccshu.net</code>的私有仓库里<br>SNIPER文件夹放置在机器学习平台服务器<code>mount</code>的目录里</p><h2 id="五-测试"><a href="#五-测试" class="headerlink" title="五 测试"></a>五 测试</h2><ul><li><p>在平台上创建容器<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_18-11-49.png" alt=""></p></li><li><p>耐心等待创建完成<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_18-14-21.png" alt=""></p></li><li><p>创建成功<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_18-20-07.png" alt=""></p></li><li><p>测试结果<br><img src="./create-sniper-docker-image/Snipaste_2019-01-03_18-31-16.png" alt=""><br><strong>测试失败</strong></p></li></ul><p><strong>但是,使用未压缩的镜像测试成功</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;创建一个基于Mxnet的Sniper-Docker镜像&quot;&gt;&lt;a href=&quot;#创建一个基于Mxnet的Sniper-Docker镜像&quot; class=&quot;headerlink&quot; title=&quot;创建一个基于Mxnet的Sniper Docker镜像&quot;&gt;&lt;/a&gt;创建一个基于Mxnet的Sniper Docker镜像&lt;/h1&gt;&lt;h2 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h2&gt;&lt;p&gt;由于此镜像是用于学校机器学习平台,所以文中会出现FTP服务器等字眼,其实是在平台上使用镜像创建一个容器时,平台会&lt;strong&gt;自动&lt;/strong&gt;将服务器上我所申请的文件存储区&lt;code&gt;mount&lt;/code&gt;到创建的容器,我通过&lt;code&gt;FileZilla&lt;/code&gt;FTP工具与在平台申请的文件存储区进行连接&lt;br&gt;​&lt;br&gt;本文教程虽然有了一个FTP过程,但是如果是生成本地镜像,不考虑FTP,无视文中相关部分即可&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;虽然本文中写了关于压缩的相关内容,但是最终并没有使用压缩,原因是由于压缩后出现未知问题,导致在平台上创建的容器不能使用宿主机的NVIDIA驱动,并不能成功运行Demo&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://StepNeverStop.github.io/categories/Docker/"/>
    
    
      <category term="docker" scheme="http://StepNeverStop.github.io/tags/docker/"/>
    
      <category term="mxnet" scheme="http://StepNeverStop.github.io/tags/mxnet/"/>
    
      <category term="sniper" scheme="http://StepNeverStop.github.io/tags/sniper/"/>
    
  </entry>
  
  <entry>
    <title>Git命令学习记录</title>
    <link href="http://StepNeverStop.github.io/Git-learn.html"/>
    <id>http://StepNeverStop.github.io/Git-learn.html</id>
    <published>2018-12-29T07:48:51.000Z</published>
    <updated>2019-01-02T06:38:24.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Git命令学习记录"><a href="#Git命令学习记录" class="headerlink" title="Git命令学习记录"></a>Git命令学习记录</h1><a id="more"></a><ul><li><p>删除远程仓库的文件,保留本地的文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git rm -r /path/to/filename</span><br><span class="line">git commit -m &quot;msg&quot;</span><br><span class="line">git push</span><br></pre></td></tr></table></figure></li><li><p>删除远程仓库的文件,同时删除本地文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git rm /path/to/filename</span><br><span class="line">git commit -m &quot;msg&quot;</span><br><span class="line">git push</span><br></pre></td></tr></table></figure></li><li><p>查看本地所有分支</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -a</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>查看本地分支</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch</span><br></pre></td></tr></table></figure></li><li><p>切换分支</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout [branchname]</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>查看各个分支当前所指的对象</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git log --oneline --decorate</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>如果我同一项目有两个不同的版本，怎么切换某一版本到master分支呢？比如说我有两个分支名字为A和B，目前默认master分支指向A，现在我想把master切换至B，该怎么做呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git branch -m master A</span><br><span class="line">把当前的master分支内容放置分支A</span><br><span class="line">git branch -m B master</span><br><span class="line">将分支B重命名为master</span><br><span class="line">git push -f origin master</span><br><span class="line">更新master分支</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>如果在本地git仓库下有另外一个clone过来的git仓库，那么当使用<code>git add .</code>，然后再<code>git commit ...</code>时会报错。并且上传到仓库的文件夹是空的，解决方案如下：</p><ol><li><code>cd</code>到<code>clone</code>的仓库目录下，执行<code>rd /s/q .git</code>命令，删除<code>clone</code>的仓库目录下的<code>.git</code>文件夹</li><li><p>回到仓库根目录删除仓库中的空文件夹</p><p>2.1 <code>git rm -r --cached &quot;themes/[branchname]&quot;</code></p><p>2.2 <code>git commit -m &quot;remove empty folder&quot;</code></p><p>2.3 <code>git push origin master</code></p></li><li><p>在仓库根目录重新提交代码</p><p>3.1 <code>git add .</code></p><p>3.2 <code>git commit -m &quot;repush&quot;</code></p><p>3.3 <code>git push origin master</code></p></li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Git命令学习记录&quot;&gt;&lt;a href=&quot;#Git命令学习记录&quot; class=&quot;headerlink&quot; title=&quot;Git命令学习记录&quot;&gt;&lt;/a&gt;Git命令学习记录&lt;/h1&gt;
    
    </summary>
    
      <category term="小知识" scheme="http://StepNeverStop.github.io/categories/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="Git" scheme="http://StepNeverStop.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>MarkDown基本语法</title>
    <link href="http://StepNeverStop.github.io/MarkDown-Grammar.html"/>
    <id>http://StepNeverStop.github.io/MarkDown-Grammar.html</id>
    <published>2018-12-29T07:23:57.000Z</published>
    <updated>2019-05-10T01:24:30.025Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MarkDown基本语法"><a href="#MarkDown基本语法" class="headerlink" title="MarkDown基本语法"></a>MarkDown基本语法</h1><p>所有使用Markdown语法标记的符号后要加一个空格<code>Space</code></p><a id="more"></a><h2 id="一、-标题-…"><a href="#一、-标题-…" class="headerlink" title="一、 标题    …}"></a>一、 标题    {#…}</h2><p>使用<code>#</code>来设置标题级数,一个<code>#</code>则代表一级标题,字体大小最大</p><p><code>#</code> </p><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><p><code>##</code></p><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><p><code>###</code></p><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><p><code>####</code></p><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><p><code>#####</code></p><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><p><code>######</code></p><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><h2 id="二、-列表"><a href="#二、-列表" class="headerlink" title="二、 列表"></a>二、 列表</h2><h3 id="1-无序列表"><a href="#1-无序列表" class="headerlink" title="1. 无序列表"></a>1. 无序列表</h3><p>使用<code>-</code>、<code>+</code>、<code>*</code>三个符号都可以</p><ul><li>使用<code>-</code></li></ul><ul><li>使用<code>+</code></li></ul><ul><li>使用<code>*</code></li></ul><p>如果在列表中想取消下一行的列表性质,需要按下退格<code>Backspace</code>删除列表前的圆点后,然后按<code>Shift</code>+<code>Tab</code>组合键来退回首位.</p><ul><li>一级列表<ul><li>二级列表<ul><li>三级列表<ul><li>四级列表</li></ul></li></ul></li></ul></li></ul><p>共有三级标题</p><h3 id="2-有序列表"><a href="#2-有序列表" class="headerlink" title="2. 有序列表"></a>2. 有序列表</h3><p>数字加点加空格,如<code>1.[Space]</code>、<code>2.[Space]</code></p><p>需要往前挪动请按<code>Tab</code>键,往后挪动请按<code>Shift</code>+<code>Tab</code>组合键</p><ol><li>第一级</li><li>第二级<ol><li>第二级第一小节</li><li>第二级第二小节<ol><li>第二级第二小节第一小小节</li><li>第二级第二小节第二小小节</li></ol></li><li>第二级第三小节</li></ol></li><li>第三级</li></ol><h1 id="三、-字体"><a href="#三、-字体" class="headerlink" title="三、 字体"></a>三、 字体</h1><ul><li><p><em>斜体</em></p><p>用法:<code>*[内容]*</code>或<code>_[内容]_</code>,包含在两个<code>*</code>星号或两个<code>_</code>下划线中间的内容会倾斜</p><p><code>*Hello World*</code>:<em>Hello World</em></p><p><code>_Hello World_</code>:_Hello World_</p></li><li><p><strong>加粗</strong></p><p>用法:<code>**[内容]**</code>,包含在四个<code>*</code>星号中间的内容会加粗</p><p><code>**Hello World**</code>:<strong>Hello World</strong></p></li><li><p><strong><em>斜体加粗</em></strong></p><p>用法:<code>***[内容]***</code>,包含在六个<code>*</code>星号中间的内容会加粗并斜体</p><p><code>***Hello World***</code>:<strong><em>Hello World</em></strong></p></li><li><p><del>删除线</del></p><p>用法:<code>~~[内容]~~</code>,包含在四个<code>~</code>波浪号中间的内容会添加删除线</p><p><code>~~Hello World~~</code>:<del>Hello World</del></p></li></ul><h1 id="四、-引用"><a href="#四、-引用" class="headerlink" title="四、 引用"></a>四、 引用</h1><p><code>&gt;</code>表示引用,与<code>#</code>用法相同</p><p><code>&gt;</code></p><blockquote><p>一级引用</p></blockquote><p><code>&gt;&gt;</code></p><blockquote><blockquote><p>二级引用</p></blockquote></blockquote><p><code>&gt;&gt;&gt;</code></p><blockquote><blockquote><blockquote><p>三级引用</p></blockquote></blockquote></blockquote><p>退格使用<code>Shift</code>+<code>Tab</code></p><h1 id="五、-分割线"><a href="#五、-分割线" class="headerlink" title="五、 分割线"></a>五、 分割线</h1><p>大于等于三个的<code>-</code>或<code>+</code>或<code>*</code></p><p><code>---</code></p><hr><p><code>+++</code></p><p>+++</p><p><code>***</code></p><hr><h1 id="六、-图片"><a href="#六、-图片" class="headerlink" title="六、 图片"></a>六、 图片</h1><p>语法:<code>![图片文字](图片地址 &quot;鼠标放置时显示的信息&quot;)</code></p><p>例子:</p><p><code>![大桥](https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1546072097132&amp;di=8669a22f7be9af8266cb1580d15c155d&amp;imgtype=0&amp;src=http%3A%2F%2Fd.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2F63d9f2d3572c11dfea8f528e6e2762d0f603c2c5.jpg &quot;美丽的桥梁&quot;)</code></p><p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1546072097132&amp;di=8669a22f7be9af8266cb1580d15c155d&amp;imgtype=0&amp;src=http%3A%2F%2Fd.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2F63d9f2d3572c11dfea8f528e6e2762d0f603c2c5.jpg" alt="大桥" title="美丽的桥梁"></p><h1 id="七、-超链接"><a href="#七、-超链接" class="headerlink" title="七、 超链接"></a>七、 超链接</h1><p>语法:<code>[超链接名](超链接地址 &quot;鼠标放置时显示的信息&quot;)</code></p><p>例子:<code>[百度一下,你就知道](www.baidu.com &quot;我就是百度&quot;)</code></p><p><a href="https://www.baidu.com" title="我就是百度" rel="external nofollow" target="_blank">百度一下,你就知道</a></p><h1 id="八、-代码"><a href="#八、-代码" class="headerlink" title="八、 代码"></a>八、 代码</h1><h2 id="1-单行"><a href="#1-单行" class="headerlink" title="1. 单行"></a>1. 单行</h2><p>使用<code>&#96;</code>反引号包裹</p><h2 id="2-多行-代码块"><a href="#2-多行-代码块" class="headerlink" title="2.多行,代码块"></a>2.多行,代码块</h2><p>使用三个反引号包裹</p><p><code>&#96;&#96;&#96;</code></p><p>使用<code>&#96;&#96;&#96;</code>+编程语言可以打开代码编辑器</p><p>如 <code>&#96;&#96;&#96;</code>+python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">这是一个python语法的编译器</span><br></pre></td></tr></table></figure><h1 id="九、-表格"><a href="#九、-表格" class="headerlink" title="九、 表格"></a>九、 表格</h1><p>每一行都使用<code>|</code>隔开</p><p>第二行使用<code>:</code>设置对齐,两边都加表示文字居中,加在左边表示居左</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">|标题1|标题2|标题3|</span><br><span class="line">|-|:-:|:-|</span><br><span class="line">|1|2|3|</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>标题1</th><th style="text-align:center">标题2</th><th style="text-align:left">标题3</th></tr></thead><tbody><tr><td>1</td><td style="text-align:center">2</td><td style="text-align:left">3</td></tr></tbody></table></div><h1 id="十、-流程图"><a href="#十、-流程图" class="headerlink" title="十、 流程图"></a>十、 流程图</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">​```flow</span><br><span class="line">st=&gt;start: 开始</span><br><span class="line">op=&gt;operation: My Operation</span><br><span class="line">cond=&gt;condition: Yes or No?</span><br><span class="line">e=&gt;end</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br><span class="line">​</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```flow</span><br><span class="line">st=&gt;start: 开始</span><br><span class="line">op=&gt;operation: My Operation</span><br><span class="line">cond=&gt;condition: Yes or No?</span><br><span class="line">e=&gt;end</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;MarkDown基本语法&quot;&gt;&lt;a href=&quot;#MarkDown基本语法&quot; class=&quot;headerlink&quot; title=&quot;MarkDown基本语法&quot;&gt;&lt;/a&gt;MarkDown基本语法&lt;/h1&gt;&lt;p&gt;所有使用Markdown语法标记的符号后要加一个空格&lt;code&gt;Space&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="小知识" scheme="http://StepNeverStop.github.io/categories/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="markdown" scheme="http://StepNeverStop.github.io/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>Windows下右键新建.md文件教程</title>
    <link href="http://StepNeverStop.github.io/win-rightclick-create-md.html"/>
    <id>http://StepNeverStop.github.io/win-rightclick-create-md.html</id>
    <published>2018-12-28T16:00:00.000Z</published>
    <updated>2019-05-10T01:23:38.343Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Windows下右键新建-md文件教程"><a href="#Windows下右键新建-md文件教程" class="headerlink" title="Windows下右键新建.md文件教程"></a>Windows下右键新建.md文件教程</h1><a id="more"></a><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>windows10操作系统<br>Typora编辑器</p><h2 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h2><p><img src="./win-rightclick-create-md/1546050455.jpg" alt=""></p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="1-打开注册表"><a href="#1-打开注册表" class="headerlink" title="1. 打开注册表"></a>1. 打开注册表</h3><ol><li><code>CMD+R</code>，打开运行对话框</li><li>输入<code>regedit</code>，打开注册表编辑器</li></ol><h3 id="2-修改注册表"><a href="#2-修改注册表" class="headerlink" title="2. 修改注册表"></a>2. 修改注册表</h3><ol><li>在<code>计算机&gt;HKEY_CLASSES_ROOT</code>右键查找，输入<code>Typora</code>，勾选项，取消勾选值和数据</li></ol><p><img src="./win-rightclick-create-md/20181229103503.png" alt=""></p><ol><li>确认运行的程序名字，我的电脑如图所示，运行文件是<code>Typora.exe</code></li></ol><p><img src="./win-rightclick-create-md/20181229103752.png" alt=""><br>如果使用的是markdownpad或者其他编辑器，同理</p><ol><li>在磁盘任意位置新建一个文件，后缀为<code>.reg</code></li><li>打开编辑刚刚创建好的注册表文件，写入一下内容：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Windows Registry Editor Version 5.00</span><br><span class="line">[HKEY_CLASSES_ROOT\.md]</span><br><span class="line">@=&quot;Typora.exe&quot;</span><br><span class="line">[HKEY_CLASSES_ROOT\.md\ShellNew]</span><br><span class="line">&quot;NullFile&quot;=&quot;&quot;</span><br><span class="line">[HKEY_CLASSES_ROOT\Typora.exe]</span><br><span class="line">@=&quot;Markdown&quot;</span><br></pre></td></tr></table></figure></li></ol><p><code>@=&quot;Typora.exe&quot;</code> 代表的是指定.md文件的运行程序<br><code>@=&quot;Markdown&quot;</code> 代表的是右键时默认的文件名字，这样写新建为<code>新建Markdown.md</code>文件，而且右键菜单中显示<code>MarkDown</code></p><ol><li>编辑好之后,另存为,设置如图所示:</li></ol><p><img src="./win-rightclick-create-md/20181229105408.png" alt=""></p><p>文件名可以随便设置，但是后缀必须是<code>.reg</code>文件,保存类型一定要是<code>文本文档(*.txt)</code>,编码选择<code>Unicode</code>,非常重要!!!!!</p><ol><li>保存文件后,双击运行,修改注册表即可,现在右键即可达到预期效果,如果不行,请重启一下.</li></ol><h3 id="3-编辑新建图标-非必须"><a href="#3-编辑新建图标-非必须" class="headerlink" title="3. 编辑新建图标(非必须)"></a>3. 编辑新建图标(非必须)</h3><ol><li>以<code>Typora</code>为例,在注册表<code>Typora.exe</code>下点击项<code>DefaultIcon</code>,右键修改</li><li>将属性修改为想要设置的Markdown文件图标</li></ol><p><img src="./win-rightclick-create-md/20181229105300.png" alt=""></p><p>文档有错或转载请联系邮箱<code>stepneverstop@qq.com</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Windows下右键新建-md文件教程&quot;&gt;&lt;a href=&quot;#Windows下右键新建-md文件教程&quot; class=&quot;headerlink&quot; title=&quot;Windows下右键新建.md文件教程&quot;&gt;&lt;/a&gt;Windows下右键新建.md文件教程&lt;/h1&gt;
    
    </summary>
    
      <category term="小知识" scheme="http://StepNeverStop.github.io/categories/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="markdown" scheme="http://StepNeverStop.github.io/tags/markdown/"/>
    
  </entry>
  
</feed>
