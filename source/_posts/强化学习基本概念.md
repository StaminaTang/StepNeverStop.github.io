---
title: 强化学习基本概念
copyright: true
top: 1
date: 2019-04-08 19:23:16
mathjax: true
categories: ReinforcementLearning
tags:
- rl
---

# 强化学习基本概念

学习了这么久的强化学习, 不做笔记总是会忘记, 于是写在博客里方便自己复习, 也与同路人分享.

## 强化学习是什么?

强化学习是什么? 它的英文名字是*Reinforcement Learning*, 和*Machine Learning*一样, 都是以*'ing'*结尾的. 它是一个问题、一组解决这个问题的方案以及探求这些解决方案的方法. 对于问题和方法一定要有清晰的认识, 很多人在学习强化学习时遇到的各种困惑与不解都是因为不能清晰的认识问题和方法的区别和联系.

<!--more-->

强化学习与有监督学习(*supervised learning*)不同. 有监督学习是目前机器学习领域研究最多的方向, 它从由经验丰富的、学识渊博的专家(监督者)提供一系列带有标签(如每个样本被正确分类的类别)的样本数据中进行学习, 这种方法通常被用于分类问题. 有监督学习的目标是当给定一个没有在训练样本集出现的数据时, 可以准确推断出它的标签/类别. 这种有监督学习非常重要而且有用, 但是它没有能力从**交互**中进行学习, 而强化学习在智能体与环境进行交互的过程中进行学习. 为什么有监督学习不能从交互中学习呢? 因为有监督学习需要的近乎完全的样本以及其准确的信息都是在交互问题中很难获得的(不现实的). 在未知的交互场景中, 我们往往只能根据智能体的经验进行学习.

强化学习与无监督学习(*unsupervised learning*)也是不同的. 无监督学习通常被用于发现无标签样本集的隐藏结构. 我们一般任务机器学习只分为有、无监督学习两种, 而且将强化学习分为无监督学习一类. 但其实强化学习与无监督学习有本质的区别. **强化学习的目的是最大化可获得的奖励值**, 而无监督学习是发现隐藏结构. 当然, 如果在强化学习问题中可以发现其样本内的隐藏结构, 这对于强化学习肯定是很有帮助的, 但是仅仅这些隐藏结构并不能处理强化学习最大化奖励值方法的问题. 因此, 我们通常将强化学习归为机器学习的第三个类别, 与有、无监督学习并列.

*注: 在强化学习问题中, 任何可以反映当前动作所带来的影响的元素都可以被理解为奖励值.(个人见解)*

> Reinforcement learning is learning what to do——how to map situations to actions——so as to maxmize a numerical reward signal.	——《Reinforcement Learning: An Introduction》

强化学习学习的是从状态*s*到要执行的最优动作*a*之间的映射关系, 也就是找到一个策略(函数/逻辑规则)使得在给定状态下通过该策略所产生的决策可以最终带来最大的回报. 学习者不被告知应该采取什么动作, 而是通过训练使它们发现采取什么样的动作可以产生最高的奖励值. 这与婴儿学习的方式很像, 你可能会说:"瞎讲, 婴儿可以模仿你的动作进行学习.". 但你要知道, 当你对婴儿的动作进行批评(吵)和奖励(笑)时, 这就已经是一个强化学习的过程了.

## 强化学习的两个要素

强化学习必不可少的两个要素是智能体`Agent`和环境`Environment`.
既然强化学习是在交互过程中进行学习, 那么交互必定是双方或者多方的, 在强化学习问题中, 交互的双方是智能体和环境.

1. 智能体
- 智能体是环境的观察者
- 智能体是策略的载体
- 智能体是动作的执行者
3. 环境
- 环境是对智能体动作的评判者, 即给出立即奖励
- 环境是智能体进行运动等行为的基本空间
- 环境给出当前时刻的观察信息, 供智能体进行采集

## 强化学习的两个特点

1. **trial-and-error/试错学习**
智能体在与环境交互的过程中进行学习时, 不会得到任何人为的或者示例的指导(如果进行指导, 则为有监督学习/模仿学习/逆强化学习等), 智能体只能通过在环境中不断地**试错**, 积累经验, 最终学到可以完成目标并获得最大奖励值的策略.
2. **delayed reward/延迟奖励**
在大多数强化学习问题中, 某一状态*s*下执行的动作*a*不仅会影响当前的立即奖励*r*, 而且还会影响后续的状态序列, 以及后续的奖励值. 当前的立即奖励值并不能反映出在这个动作对(*s,a*)对整个决策过程的影响, 只有等到这一决策过程结束时, 才能判断其在这个状态序列的奖励(价值), 所以, 延迟奖励也是强化学习过程中的一个特点.

## 强化学习的难点(Challenge)

相比其他学习, 强化学习中的一大难点是**探索与利用**, 也就是**exploration and exploitation**, 这个难题已经被数学家研究了几十年了, 但仍然没有解决. 为了获得尽量多的奖励, 智能体需要根据过去学习的经验选择产生立即奖励值最高的动作, 但是给定状态下可供选择的动作有很多, 有些被执行过, 有些没有被执行过, 为了去发现产生立即奖励值最高的动作, 必须尝试选择之前被选择过动作. 这个问题就出现了, **智能体必须利用它已经探索过的产生大奖励值的动作, 也必须探索未知奖励值的动作(有可能很小)为了以后可以选择更好的动作**. 只探索不利用、只利用不探索在强化学习问题中都是独木难支. 在随机任务中, 一个同样的动作往往需要被探索很多次才可能对它的期望奖励值有较准确的估计. 

## 强化学习的四个元素

除了智能体与环境两个要素之外, 强化学习系统/框架中还有四个子元素: 策略、奖励机制、值函数、模型(未必有).

1. 策略 Policy
策略定义了智能体在当前时刻应该做出的行为. 与人类的刺激-反应机制很像, 策略是从感知到的环境信息到执行的行为之间的映射, 策略是强化学习智能体的**核心**, 它决定了智能体的行为. 在一般的强化学习问题中, 策略可能是随机的、非确定的, 它通常给出可选择执行的动作的概率或概率分布.

2. 奖励机制 Reward Signal
奖励机制定义了强化学习问题的**目标**, 在交互的每一步, 环境都会向智能体传递一个数字信息, 我们称之为"奖励". 智能体的唯一目标就是在整个交互过程中最大化总的奖励之和. 因此, 奖励定义了某个动作的好坏(但并不意味着坏的动作在交互过程中是坏的, 其作用由值函数来定义). 类比于我们人类, 奖励就像我们高兴或者痛苦一样, 它们是我们对当前环境-动作的立即反应和评价. 
奖励机制是智能体更新策略Policy的基础, 如果智能体成功进行了学习, 当在当前策略选择了一个较低回报的动作时, 之后它可能会选择其他动作. 
通常, 奖励机制由状态*s*和动作*a*的随机函数表示`R(s,a)`

3. 值函数 Value Function
立即奖励表示着当前动作或状态带来的立即效果是好是坏, 但是值函数表示这个动作在整个交互过程中扮演的角色是好是坏. 一个状态的值是从该状态开始到交互结束所积累的立即奖励的总和.
一个状态可能总是产生很低的立即奖励, 但是它有很高的值, 因为该状态之后的后续状态中会产生很大的立即奖励. 相反也是一样. 类比于我们人类, 立即奖励的高低相当于我们高兴或痛苦, 但是值函数给出的值则表示了在整个事件过程中我们有多高兴或不高兴的深刻判断. 
引入值函数的唯一目的就是为了训练智能体以获得更大的奖励, 当智能体做决策以及评估决策时, 我们一直关心的都是值函数而不是立即奖励, 对于动作的选择也是基于对值函数的判断/评估. 值比立即奖励更难以确定, 因为立即奖励可以由环境准确的给出, 但是值却需要评估甚至多次评估才可能相对准确(因为有可能交互过程永远不结束, 那么对值的估计会有偏差). 我们希望选择的动作带来最高的值, 而不是最高的立即奖励, 实际上, 几乎所有强化学习算法中最重要的部分就是对于值函数的有效估计方法. 关于值函数估计所扮演的核心角色在近60年被广泛研究. 

4. 模型 Model
模型是对环境行为的仿真, 我们可以通过模型推断出动作对环境的改变, 给出准确的立即奖励和状态信息. 例如, 给定一个状态和动作, 模型可以预测出下一个要转移的状态以及下一个立即奖励值. 如果模型是确定的, 我们一般使用规划(*planning*)的方法来选择最优动作, 对于这种方式我们称之为基于模型**model-based**的方法, 相反, 如果模型是不确定的, 也就是**model-free**, 我们只能通过试错的方式进行学习并选择动作. 

*注: 对于什么是model-based和model-free将在以后进行深入讨论.*

## 强化学习的通用符号表示 Notation

$←$	赋值

$\varepsilon$ 在$\varepsilon-greedy$策略中随机选择动作的概率

$\gamma$ 计算总奖励的折扣因子

$\lambda$ 资格迹的衰减率或者GAE的权重因子

$s,s'$ 状态，下一个状态

$a$ 一个动作

$r$ 一个奖励值（标量）

$S$ 状态集（不包含终态）

$S^{+}$ 状态集（包含终态）

$A(s)$ $s$状态下可选择的动作

$R$ 奖励集合

$|S|$ 状态集中的元素数

$t$ 单个时间步

$T$ 一个episode的终态时间点

$A_{t}$ $t$时刻选择的动作

$S_{t}$ $t$时刻所在的状态

$R_{t}$ $t$时刻获得的奖励

$\pi$ 策略（从状态到动作的映射）

$\pi(s)$ 在$s$状态下使用$\pi$策略所选择的动作

$\pi(a|s)$ 在$s$状态下使用$\pi$策略选择到动作$a$的概率

$G_{t}$ 以$t$时刻为起始时间点，到终态所能获得的总奖励（回报）

$p(s',r|s,a)$ 在$s$状态执行a动作转移到$s‘$状态并获得奖励值为$r$的概率

$p(s'|s,a)$ 在$s$状态执行$a$动作转移到$s’$状态的概率

$r(s,a)$ 在$s$状态执行$a$动作所获得的**期望**立即奖励（即时奖励）

$r(s,a,s')$ 在$s$状态执行$a$动作转移到$s'$状态所获得的**期望**立即奖励

$v_{\pi}(s)$ $\pi$策略下状态$s$的值（以该状态为始态的期望奖励回报）

$v_{*}(s)$ **最优**策略下状态s的值

$q_{\pi}(s,a)$ $\pi$策略下状态-行动对$(s,a)$的值

$q_{*}(s,a)$ **最优**策略下状态-行动对$(s,a)$的值

$V,V_{t}$ 状态值的矩阵估计，行和列分别是时间点$t$和每个状态的估计值$v_{\pi}$或$v_{*}$

$Q,Q_{t}$ 状态-行动对$(s,a)$的矩阵估计，一般为一个3维矩阵,行、列和深度分别为状态、动作、时间点

$V_{t}(s)$ 状态$s$的期望估计值

$\delta_{t}$ $t$时刻的TD-error时间差分量

$\theta,\theta_{t}$ 目标策略的参数（向量）

$\pi(a|s,\theta)$ 目标策略的参数为$\theta$时，在$s$状态选择到$a$动作的概率

$\pi_{\theta}$ 表示参数为$\theta$的策略

$\nabla{\pi(a|s,\theta)}$ $\pi(a|s,\theta)$对于参数$\theta$的偏微分

$J(\theta)$ 参数为$\theta$的策略的性能度量、期望奖励(performance measure)

$\nabla{J(\theta)}$ 性能度量对于策略参数$\theta$的偏导数